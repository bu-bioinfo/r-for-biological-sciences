---
output: html_document
---

```{r, include=FALSE}
library(tidyverse)
```

## Statistical Distributions

Biological data, like all data, is uncertain. Measurements always contain noise,
but a collection of measurements do not always contain signal. The field of
statistics grew from the recognition that mathematics can be used to quantify
uncertainty and help us reason about whether a signal exists within a dataset,
and how certain we are of that signal. At its core, statistics is about
separating the signal from the noise of a dataset in a rigorous and precise way.

One of the fundamental statistical tools used when estimating uncertainty is the
*statistical distribution*, or *probability distribution*, or simply
*distribution*. There are two broad classes of distributions: statistical, or
theoretical, distributions and empirical distributions. In this section we will
discuss some general properties of distributions, briefly describe some common
probability distributions, and explain how to specify and use these
distributions in R.

### Random Variables

A random variable is an object or quantity which depends upon random events and
that can have samples drawn from it. For example, a six-sided die is a random
variable with six possible outcomes, and a single roll of the die will yield one
of those outcomes with some probability (equal probability, if the die is fair).
A coin is also a random variable, with heads or tails as the outcome. A gene's
expression in a RNASeq experiment is a random variable, where the possible
outcomes are any non-negative integer corresponding to the number of reads that
map to it. In these examples, the outcome is a simple category or real number,
but random variables can be associated with more complex outcomes as well,
including trees, sets, shapes, sequences, etc. The probability of each outcome
is specified by the distribution associated with the random variable.

Random variables are usually notated as capital letters, like $X,Y,Z,$ etc. A
sample drawn from a random variable is usually notated as a lowercase of the
same letter, e.g. $x$ is a sample drawn from the random variable $X$. The
distribution of a random variable is usually described like "$X$ follows a
binomial distribution" or "$Y$ is a normally distributed random variable".

The probability of a random variable taking one of its possible values is
usually written $P(X = x)$. The value of $P(X = x)$ is defined by the
distribution of $X$. As we will see later in the [p-values] section, sometimes
we are also interested in the probability of observing a value of $x$ or larger
(or smaller). These probabilities are written $P(X > x)$ (or $P(X < x)$). How
these probabilities are computed is described in the next section.

### Statistical Distribution Basics

By definition, a statistical distribution is a function that maps **the possible
values for a variable to how often they occur**. Said differently, a statistical
distribution is used to compute the probability of seeing a single value, or a
range of values, relative to all other possible values, assuming the random
variable in question follows the statistical distribution. The following is a
visualization of the theoretical distribution of a normally distributed random
variable:

```{r stat normal}
tibble(
  x = seq(-4,4,by=0.1),
  `Probability Density`=dnorm(x,0,1)
) %>%
  ggplot(aes(x=x,y=`Probability Density`)) +
  geom_line() +
  labs(title="Probability Density Function for a Normal Distribution")

```



The plot above depicts to the *probability density function* (PDF) of a normal
distribution with mean of zero and a standard deviation of one. The PDF defines
the probability associated with every possible value of $x$, which for the
normal distribution is all real numbers. All PDFs have a closed mathematical
form. The PDF for the normal distribution is:

$$
P(X = x|\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(x-\mu)^2}{2\sigma}}
$$

The notation $P(X = x|\mu,\sigma)$ is read like "the probability that the random
variable $X$ takes the value $x$, given mean $\mu$ and standard deviation
$\sigma$". The normal distribution is an example of a *parametric distribution*
because its PDF requires two parameters - mean and standard deviation - to
compute probabilities. The choice of parameter values $\mu$ and $\sigma$ in the
normal distribution determine the probability of the value of $x$.

::: {.box .note}
Probability density functions are defined for continuous distributions only,
as described later in this section. Discrete distributions have a probability
mass function (PMF) instead of a PDF, since the probability distribution is
subdivided into a set of different categories. PMFs have different mathematical
characteristics than PDFs (e.g. they are not continuous and therefore are not
differentiable), but serve the same purpose.
:::

In probability theory, if a plausible event has a probability of zero, this
*does not mean that event can never occur*. In fact, every specific value in a
distribution that supports all real numbers has a probability of zero (i.e. one
specific value out of an infinite number of values). Instead, the probability
distribution function allows us to reason about the *relative likelihood* of
observing values in one range of the distribution compared with the others. 
While most values have extremely small relative probabilities, they never are
equal to zero. This is due to the asymptotic properties of probability
distributions, where every [supported
value](https://en.wikipedia.org/wiki/Support_(mathematics)) of the distribution
has a non-zero value by definition, though many values may be very close to
zero.

The PDF provides the probability density of specific values within the
distribution, but sometimes we are interested in the probability of a value
being less than or equal to a particular value. We compute this using the
*cumulative distribution function* (CDF) or sometimes just *distribution
function*. In the plot below, both the PDF and CDF are plotted for a normal
distribution with mean zero and standard deviation of 1:

```{r stat pdf cdf}
tibble(
  x = seq(-4,4,by=0.1),
  PDF=dnorm(x,0,1),
  CDF=pnorm(x,0,1)
) %>%
  ggplot() +
  geom_line(aes(x=x,y=PDF,color="PDF")) +
  geom_line(aes(x=x,y=CDF,color="CDF"),linetype="dashed")
```

The value of the CDF corresponds to the area under the density curve up to the
corresponding value of $x$; 1 minus that value is the area under the curve
greater than that value. The following figures illustrate this:

```{r stat cdfs, echo=FALSE}
d <- tibble(
  x = c(-4,4),
) 
g <- ggplot(d,aes(x=x)) +
  stat_function(fun=dnorm,mapping=aes(color="PDF")) +
  geom_point(aes(x=x,y=y),data=tibble(x=c(1.37),y=c(pnorm(1.37)))) +
  stat_function(fun=pnorm,linetype="dashed",mapping=aes(color="CDF"))+
  geom_segment(
    aes(x=x,y=y,xend=xend,yend=yend),
    data=tibble(x=1.37,xend=1.37,y=pnorm(1.37),yend=0),
    linetype="dashed"
    )

(g + stat_function(fun=dnorm,
                xlim=c(-4,1.37),
                geom="area",
                alpha=0.2,
                ) + labs(title=str_c("P(X < x| x=1.37) = ",round(pnorm(1.37),2)))) /
(g + stat_function(fun=dnorm,
                xlim=c(1.37,4),
                geom="area",
                alpha=0.2
                ) + labs(title=str_c("P(X > x| x=1.37) = ",round(1-pnorm(1.37),2))))
```

91% of the probability density is less than the (arbitrary) value of $x=1.37$,
and likewise 9% is greater. This is how [p-values] are calculated, as described
later.

The CDF is also useful for generating samples from the distribution. In the
following plot, 1000 random samples were drawn from a uniform distribution in
the interval $(0,1)$ and plotted using the inverse CDF function:

```{r stat inv cdf, echo=FALSE}
d <- tibble(
  p=runif(1000,0,1),
  x=qnorm(p,0,1)
)
unif <- ggplot(d,aes(y=x)) +
  geom_histogram(bins=30)
norm <- ggplot(d,aes(x=p)) +
  geom_histogram(bins=30)
scatter <- ggplot(d,aes(x=p,y=x)) +
  geom_point() +
  stat_function(
    fun=qnorm,
    xlim=c(0,1),
    linetype="dashed",
    color="red"
    )
 
norm + plot_spacer() + scatter + unif + 
  plot_layout(
    ncol = 2, 
    nrow = 2, 
    widths = c(4, 1),
    heights = c(1, 4)
  ) 
```

The histograms on the margins show the distributions of the $p$ and $x$ values
for the scatter points. The $p$ coordinates are uniform, while the $x$
distribution is normal with mean of zero and standard deviation of one. In this
way, we can draw samples from a normal distribution, or any other distribution
that has an invertible CDF.

### Distributions in R

There are four key operations we performed with the normal distribution in the
previous section:

1. Calculate probabilities using the PDF
2. Calculate cumulative probabilities using the CDF
3. Calculate the value associated with a cumulative probability
4. Sample values from a parameterized distribution

In R, each of these operations has a dedicated function for each different
distribution, prefixed by `d`, `p`, `q`, and `r`. For the normal distribution,
these functions are `dnorm`, `pnorm`, `qnorm`, and `rnorm`:

1. `dnorm(x, mean=0, sd=1)` - PDF of the normal distribution
2. `pnorm(q, mean=0, sd=1)` - CDF of the normal distribution
3. `qnorm(p, mean=0, sd=1)` - inverse CDF; accepts quantiles between 0 and 1 and returns the
value of the distribution for those quantiles
4. `rnorm(n, mean=0, sd=1)` - generate `n` samples from a normal distribution

R uses this scheme for all of its base distributions, which include:

+-------------------+------------------------------+
| Distribution      | Probability Density Function |
+===================+==============================+
| Normal            | `dnorm(x,mean,sd)`           |
+-------------------+------------------------------+
| t Distribution    | `dt(x,df)`                   |
+-------------------+------------------------------+
| Poisson           | `dpois(n,lambda)`            |
+-------------------+------------------------------+
| Binomial          | `dbinom(x, size, prob)`      |
+-------------------+------------------------------+
| Negative Binomial | `dnbinom(x, size, prob, mu)` |
+-------------------+------------------------------+
| Exponential       | `dexp(x, rate)`              |
+-------------------+------------------------------+
| $\chi^2$          | `dchisq(x, df)`              |
+-------------------+------------------------------+

There are [many more distributions](https://www.stat.umn.edu/geyer/old/5101/rlook.html)
implemented in R beyond these, and they all follow the same scheme. 

The next two sections will cover examples of some of these distributions.
Generally, statistical distributions are divided into two categories: discrete
distributions and continuous distributions.

### Discrete Distributions

Discrete distributions are defined over countable, possibly infinite sets.
Common discrete distributions include binomial (e.g. coin flips), multinomial
(e.g. dice rolls), Poisson (e.g. number of WGS sequencing reads that map to a
specific locus), etc. Below we describe a few of these in detail.

#### Bernoulli random trail and more  

One of the examples for discrete random variable distribution is the **Bernoulli**
function. A Bernoulli trail has only 2 outcomes with probability $p$ and $(1-p)$.
Consider flipping a fair coin, and the random variable X can take value 0 or 1
indicating you get a head or a tail. If it's a fair coin, we would expect the
$Pr(X = 0) = Pr(X = 1) = 0.5$. Or, if we throw a die and we record X = 1 when
we get a six, and X = 0 otherwise, then $Pr(X = 0) = 5/6$ and $Pr(X = 1) = 1/6$.  

Now consider a slightly complicated situation: what if we are throwing the die
$n$ times and we would like to analyze the total number of six, say $x$, we get
during those $n$ throws? Now, this leads us to the **binomial distribution**.
If it's a fair die, we would say our proportion parameter $p = 1/6$, which means
the probability we are getting a six is 1/6 for each throw.  

$$
\begin{equation}
f(x) = \frac {n!} {x!(n-x)!}p^x (1-p) ^{(n-x)}
\end{equation}
$$

The **geometric** random variable, similar to the binomial, is also from a
sequence of random Bernoulli trials with a constant probability parameter $p$.
But this time, we define the random variable $X$ as the number of consecutive
failures before the first success. In this case, the probability of $x$
consecutive failures followed by success on trial $x+1$ is:  

$$
\begin{equation}
f(x) = p * (1-p)^x
\end{equation}
$$

The **negative binomial** distribution goes one step forward. This time, we are
still performing a sequence of independent Bernoulli random trials with a
constant probability of success equal to $p$. But now we would like to record
the random variable $Z$ to be the total number of failures before we finally get
to the $r^{th}$ success. In other words, when we get to the $r^{th}$ success, we
had $x+r$ Bernoulli random trails, in which $x$ times failed and $r$ times
succeeded.

$$
\begin{equation}
f(x) = \frac {x+r-1} {r-1} p^r {(1-p)}^x
\end{equation}
$$

#### Poisson  

The Poisson distribution is used to express the probability of a given number
of events occurring in a fixed interval of time or space, and these events
occur with a known constant mean rate and independently of the time since
the last event. ~~But no one understands this definition.~~  

The formula for Poisson distribution is:  

$$
\begin{equation}
f(k; \lambda) = Pr(X=k) = \frac {\lambda^k e^{-\lambda}} {k!}
\end{equation}
$$

- lambda is the expected value of the random variable X  
- k is the number of occurrences  
- e is Euler's number (e=2.71828)  
~~okay, the formula makes it even more confusing.~~  

Imagine you are working at a mail reception center, and your responsibility
is to receive incoming letters. Assume the number of incoming letters is not
affected by the day of the week or season of the year. You are expected to get
20 letters on average in a day. But, the actual number of letters you receive
each day will not be perfectly 20.  

You recorded the number of letters you receive each day in a month (30 days).
In the following plot, each dot represents a day. The x-axis is calender day,
and y-axis is the number of letters you receive on that day. Although on average
you are receiving 20 letters each day, the actual number of letters each day
vary a lot.

```{r}
set.seed(2)
my_letter <- rpois(n = 30, lambda = 20)
plot(my_letter,
  main = "Letters received each day",
  xlab = "day of the month", ylab = "number of letters",
  pch = 19, col = "royalblue"
)
abline(a = 20, b = 0, lwd = 2, lty = 3, col = "salmon")
```

Now, let's plot the density plot of our data. The $x$-axis is the number of
letters on a single day, and the $y$-axis is the probability.  

```{r}
plot(density(my_letter),
  lwd = 2, col = "royalblue",
  main = "Probability of number of letters each day",
  xlab = "number of letters"
)
```

Since we only have 30 data points, it doesn't look like a good curve. But,
after we worked at the mail reception for 5000 days, it becomes much closer
to the theoretical Poisson distribution with lambda = 20.  

```{r}
set.seed(3)
plot(density(rpois(n = 5000, lambda = 20)),
  lwd = 2, col = "royalblue",
  main = "Probability of number of letters each day",
  xlab = "number of letters"
)
```

Here is the theoretical Poisson distribution with lambda = 20:

```{r}
plot(dpois(c(1:40), lambda = 20),
  lwd = 2, type = "l", col = "royalblue",
  ylab = "probability", main = "Poisson lambda=20"
)
```

If we want to know what's the probability to receive, for example, 18 letters,  
we can use `dpois()` function.

```{r}
dpois(x = 18, lambda = 20)
```

If we want to know the probability of receiving 18 or less letters,
use `ppois()` function.

```{r}
ppois(q = 18, lambda = 20, lower.tail = T)
```

It is the cumulative area colored in the following plot:  

```{r}
plot(dpois(c(1:40), lambda = 20),
  lwd = 2, type = "l", col = "royalblue",
  ylab = "probability", main = "Poisson lambda=20"
)
polygon(
  x = c(1:18, 18),
  y = c(dpois(c(1:18), lambda = 20), 0),
  border = "royalblue", col = "lightblue1"
)
segments(x0 = 18, y0 = 0, y1 = 0.08439355, lwd = 2, lty = 2, col = "salmon")
```

`qpois()` is like the "reverse" of `ppois()`.  

```{r}
qpois(p = 0.3814219, lambda = 20)
```

Let's review the definition of Poisson distribution.  

- "these events occur in a fixed interval of time or space", which is a day;  
- "these events occur with a known constant mean rate", which is 20 letters;  
- "independently of the time since the last event", which means the number of
letters you receive today is independent of the letter you receive tomorrow.
~~more work today don't guarantee less work tomorrow, just like in real life~~  

Now let's re-visit the formula.  

$$
\begin{equation}
f(k; \lambda) = Pr(X=k) = \frac {\lambda^k e^{-\lambda}} {k!}
\end{equation}
$$

- lambda is the expected value of $X$, which is 20 letters in this example.  
- $k$ is the number of occurrences, which is the number of letters you get on a
specific day.
- e is Euler's number (e=2.71828)  

### Continuous Distributions

In contrast with discrete distributions, continuous distributions are defined
over infinite, possibly bounded domains, e.g. all real numbers. There are a
number of different continuous distributions. Here we will focus on the **Normal
distribution (Gaussian distribution)**. A lot of variables in real life are
normally distributed, common examples include people's height, blood pressure,
and students' exam score.

This is what a normal distribution with mean = 0 and standard deviation = 1
looks like:

```{r}
set.seed(2)
norm <- rnorm(n = 50000, mean = 0, sd = 1)
plot(density(norm),
  main = "A Normal distribution", xlab = "x",
  lwd = 2, col = "royalblue"
)
```

Similar as the Poisson distribution above, there are several functions to work
with normal distribution, including `rnorm()`, `dnorm()`, `pnorm()`, and
`qnorm()`.

`rnorm()` is used to draw random data points from a normal distribution with
a given mean and standard deviation.

```{r}
set.seed(2)
rnorm(
  n = 6, # number of data points to draw
  mean = 0, # mean
  sd = 1 # standard deviation
) 
```

`dnorm()` is the density at a given quantile. For instance, in the normal
distribution (mean=0, standard deviation=1) above, the probability density at
0.5 is roughly 0.35.

```{r}
dnorm(x = 0.5, mean = 0, sd = 1)
```

```{r}
plot(density(norm),
  main = "A Normal distribution", xlab = "x",
  lwd = 2, col = "royalblue"
)
segments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 3, col = "salmon")
segments(x0 = -5, y0 = 0.3520653, x1 = 0.5, lwd = 2, lty = 3, col = "salmon")
points(x = 0.5, y = 0.3520653, cex = 1.5, lwd = 2, col = "red")
text(x = 1.1, y = 0.36, labels = "0.3521", col = "red2")
```

`pnorm()` gives the distribution function. Or, you can think of it as the
cumulative of the left side of the density function until a given value,
which is the area colored in light blue in the following plot.  

```{r}
pnorm(q = 0.5, mean = 0, sd = 1)
```

```{r}
plot(density(norm),
  main = "A Normal distribution", xlab = "x",
  lwd = 2, col = "royalblue"
)
polygon(
  x = c(density(norm)$x[density(norm)$x <= 0.5], 0.5),
  y = c(density(norm)$y[density(norm)$x <= 0.5], 0),
  border = "royalblue", col = "lightblue1"
)
segments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 2, col = "salmon")
```

`qnorm()` gives the quantile function. You can think of it as the "reverse" of
`pnorm()`. For instance:

```{r}
pnorm(q = 0.5, mean = 0, sd = 1)
qnorm(p = 0.6914625, mean = 0, sd = 1)
```

Similarly, other distributions such as chi-square distribution, they all have
the same set of functions: `rchisq()`, `dchisq()`, `pchisq()` and `qchisq()` etc.  

### Empirical Distributions

Empirical distributions describe the relative frequency of observed values in a
dataset. Empirical distributions may have any shape, and may be visualized using
any of the methods described in the [Visualizing Distributions] section, e.g. a
[density plot](#density-plots). The following beeswarm and density plot both
visualize the same made-up dataset:

```{r stat hist, echo=FALSE}
library(ggbeeswarm)
library(patchwork)
# synthetic dataset generated with three normal distributions
d <- tibble(
  Value=c(rnorm(1000,0,2),rnorm(1000,5,1),rnorm(1000,10,5))  
)
g_dens <- ggplot(d,aes(x=Value,fill="orange")) +
  geom_density() +
  guides(fill="none") # turns off the legend
g_bee <- ggplot(d, aes(x=1,y=Value)) +
  geom_beeswarm()
g_bee | g_dens
```

Note the $y$ axis in the plot on the right is scaled as a density, rather than a
count. A distribution plotted as a density ensures the values sum to 1, thus
making a probability distribution. This empirical distribution looks quite
complicated, and cannot be easily captured with a single mean and confidence
interval.

## Statistical Tests

Note: many parts of this section were shamelessly copied from wikipedia. I hope
mentioning this suffices as attribution.

Statistical or hypothesis testing forms the basis of many types of inference we
conduct in biological data analysis. The purpose is to quantify how likely it is
that we see an observed value of some quantity, assuming it was drawn from a
random variable with a specific distribution.

### What is a statistical test

A statistical hypothesis test is a method of statistical inference used to
determine a possible conclusion from two different, and likely conflicting,
hypotheses. If we state one hypothesis only and the aim of the statistical test
is to see whether this hypothesis is tenable, but not to investigate other
specific hypotheses, then such a test is called a null hypothesis test.

In a statistical hypothesis test, a null hypothesis and an alternative
hypothesis are proposed for the probability distribution of the data. If the
sample obtained has a probability of occurrence less than the pre-specified
threshold probability, the significance level, given the null hypothesis is
true, the difference between the sample and the null hypothesis is deemed
statistically significant. The hypothesis test may then lead to the rejection of
null hypothesis and acceptance of alternate hypothesis.

The earliest use of statistical hypothesis testing is generally credited to the
question of whether male and female births are equally likely (null hypothesis),
which was addressed in the 1700s by John Arbuthnot (1710), and later by
Pierre-Simon Laplace (1770s).

Arbuthnot examined birth records in London for each of the 82 years from 1629 to
1710, and applied the *sign test*, a simple non-parametric test. In every year,
the number of males born in London exceeded the number of females. Assuming more
male or more female births as equally likely, the probability of the observed
outcome is $0.5^{82}$, or about 1 in 483,600,000,000,000,000,000,000. Arbuthnot
concluded that this is too small to be due to chance and must instead be due to
divine providence.

In this example, the null hypothesis is that the number of males and females
born every year is the same. If we observed exactly 1:1 ratio of males to
females in each of the 82 years, the probability of there being more males than
females in any given year would be 0.5. Observing 82 consecutive years with more
males than females therefore results in a probability of $0.5^{82}$, as stated
above. Since every year in our observed sample had more males than females, this
is the most extreme outcome possible (other than every year having more females
than males). If we had observed one year out of 82 having more females than
males, the probability of observing that event is $82*0.5^{82}$, or one for
ever possible way of observing a single year with more females than males out of
82. 

In modern terms, he rejected the null hypothesis of equally likely male and
female births at the $p = 0.5^{82}$ significance level. This significance level
is called a *p-value*.

### P-values

In statistics, the p-value is the probability of obtaining a result as or more
extreme as the observed value, assuming that the null hypothesis is true.

For instance, imagine there is a game that randomly picks one out of four
colors. The game developer decides to add a 5th color, yellow, so that each
color now has 1/5 chance of winning and a 4/5 chance of losing. How many of its
first games would yellow have to lose in a row before you suspect the dev forgot
to code in the ability for yellow to win? If you want to wait to until there is
a < 5% chance of yellow losing every single game in a row under normal
circumstances before pointing this out to the dev, then yellow would have to
lose 14/14 games (a 4.4% likelihood).

To visualize how 14 losses in a row in a game of chance with 1/5 success rate
would occur in less than 5% of all instances, we can run a simulation of 1000
instances of 14 games with 1/5 chance success and displaying the results with
their frequencies of success:

```{r}
roll <- function(nrolls, noptions){
 outcomes <- sample(1:noptions, nrolls, TRUE)
 return(length(outcomes[outcomes == 1]))
}

dataset <- function(sample, nrolls, noptions){
  simulations <- c()
  for(i in 1:sample){simulations[i] <- roll(nrolls, noptions)}
  return(simulations)
}

set.seed(18)
Yellow_Wins <- dataset(1000, 14, 5)
hist(Yellow_Wins, freq=FALSE, right=FALSE)
```

Here you see that slightly less than 5% of all run instances had 0 wins. 


In this situation, we are comparing if yellow’s win rate is significantly lower
than if yellow had an equal chance at winning.  The research hypothesis or
alternative hypothesis in this case is that yellow’s chance of winning is
significantly lower than the rest of the colors while the null hypothesis would
be that yellow’s chance of winning is the same as the rest of the colors.

The p-value of this test would be 0.043 because 43 out of 1000 instances of 14
rolls with a 1/5 chance of success totaled 0 successes, the number of successes
we saw. If we instead were looking for 1 success or less out of 14 rolls, the
p-value would be 0.19, because our simulation ran 147 instances of 1 success and
42 instances of 0 success.

Since p-values are calculated by the percentage of outcomes that are higher than the one observed, on a normal distribution they may look something like this:
```{r}
set.seed(2)
norm <- rnorm(n = 50000, mean = 4, sd = 1)
plot(density(norm),
     main = "P-Values on a Normal Distribution", xlab = "X-Values",
     lwd = 2, col = "royalblue"
)
polygon(
  x = c(density(norm)$x[density(norm)$x >= 6], 6),
  y = c(density(norm)$y[density(norm)$x >= 6], 0),
  border = "royalblue", col = "lightblue1"
)
segments(x0 = 6, y0 = 0, y1 = 0.4, lwd = 2, lty = 2, col = "salmon")
text(7, 0.2, "Observed Value (@ 2sd)")
text(3.9, 0.01, "Instances higher than crit. val ->")
```

Here the observed value is 6, which happens to be exactly 2 standard deviations above the mean. To compute the p-value of this instance, the area under the curve would need to be calculated which can be easily done by looking up the appropriate values on a z-table. [Here is a link about finding the area under the normal curve]( https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/sas/sas4-onesamplettest/sas4-onesamplettest3.html)

[SPH’s information about p-values here]( https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module7-T-tests/PH717-Module7-T-tests3.html)

### Controlling Errors

The purpose of the significance level is to control the expected *error rate* of
the test. Since by definition a p-value corresponds to the probability of seeing
a value as extreme or greater than the observed value, this corresponds to the
probability of observing that value by chance, assuming the distribution is
true. In such cases, rejecting the null hypothesis due to a large but random
value where the null hypothesis actually holds is called a *false positive* or
a *type I error*. Setting a significance level to a smaller probability
therefore reduces the chances of making false positive inferences.

The trade-off of choosing a smaller significance level is the increased chance
of not rejecting the null hypothesis for instances when it is false. These types
of errors are called *false negatives* or *type II errors*.

Intuitively, type I errors can be thought of as errors of *commission*, i.e. the
researcher unluckily concludes that something is the fact. For instance,
consider a study where researchers compare a drug with a placebo. If the
patients who are given the drug get better than the patients given the placebo
by chance, it may appear that the drug is effective, but in fact the conclusion
is incorrect. In reverse, type II errors are errors of *omission*. In the
example above, if the patients who got the drug did not get better at a higher
rate than the ones who got the placebo, but this was a random fluke, that would
be a type II error. The consequence of a type II error depends on the size and
direction of the missed determination and the circumstances. An expensive cure
for one in a million patients may be inconsequential even if it truly is a cure.

The significance level chosen for any test is always subjective, and is ideally
made by taking the context of the data into account and assessing whether false
positives or false negatives are more tolerable. A common but arbitrary
convention is to choose 0.05 as the significance level, which corresponds to a
95% confidence of correctly rejecting the null hypothesis, but has no special
meaning. If false negatives are less tolerable than false positives, e.g. when
an inexpensive and risk-free treatment is available for patients who might
possibly have a disease that has serious potential health implications, then a
lower significance level may be warranted.

### Common Statistical Tests

Here are some common statistical tests and a quick overview as to how to run
them in R. If you would like more information about a specific test, links are
included in the descriptions

These are only some statistical tests. [Here's a link to where you can find a
few more](https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/)

[And here's a link to Wikipedia's list of statistical tests if you want to
overload on statistical tests because it's your new
hobby](https://en.wikipedia.org/wiki/Category:Statistical_tests)

#### __One-Sample T-Test__ {-}


_Dependent Variables_: 1, continuous [ratio and interval]

_Independent Variables_: 0 variables

_Design_: 1 group, 1 level

_Parametric_: Yes

::: {.box .readmore}
[More t-test information shamelessly linked from SPH]( https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module7-T-tests/PH717-Module7-T-tests4.html)
[Null and Alternate (Research) Hypotheses](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistest-means-proportions/BS704_HypothesisTest-Means-Proportions2.html#headingtaglink_1)
[Z-values](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/bs704_probability9.html)(not mentioned but are in texts that fully explain t-tests)
One-Sample t-tests come in 3 flavors: two-tailed test, one-tailed test (upper tail), and one-tailed test (lower tail). 
:::

```{r eval= FALSE} 
# One-sample t-test
t.test(x, #numeric vector of data 
       mu = 0, #True value of the mean. Defaults to 0
       alternative = "two.sided", # ("two.sided", "less", "greater") depending on which you want to use. Defaults to "two.sided
       conf.level = 0.95 #1-alpha. Defaults to 0.95
)
```

The `t.test()` function allows for multiple kinds of t-tests. To use this
function to perform a one sample t-test, you will need a numeric vector of the
data you want to run the data on, the mean you want to compare it to, your
confidence level (1 - alpha), and your alternative hypothesis. The t-test, like
other kinds of t-tests, will compare the means by calculating the t-statistic,
p-value and confidence intervals of your data’s mean.


_t_: The calculated t-statistic, uses formula : (mean(x) - mu)/(sd(x)*sqrt(n_samples))

_df_: Degrees of Freedom. Calculated by: n_samples-1

_p_: The p-value of the t-test as determined by the t-statistic and df on the [t-distribution table](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistest-means-proportions/t-table.pdf)

_(conf.level) percent confidence interval_: calculated interval where the true mean(x) is with conf.level % certainty.

_sample estimates, mean of x_: The calculated mean of x

Accepting or rejecting the null hypothesis is a matter of determining if mean(x) is outside the percent confidence interval range, if it is then the p-value will determine the significance of the results.

```{r}
#Example
set.seed(10)
x0 <- rnorm(100, 0, 1) #rnorm(n_samples, true_mean, standard_deviation)

ttestx0 <- t.test(x0, mu = 0, alternative = "two.sided", conf.level = 0.95) #actual running of the t-test
#t.test(x0) yeilds the same results bc I used default values for mu, alternative, and conf.level

tlessx0 <- t.test(x0, mu = 0, alternative = "less", conf.level = 0.90)

knitr::knit_print(ttestx0) #display t-test output via knitr
```

In `t.test_x0`, we are running a two-tailed t-test on a vector of 100 doubles
generated with `rnorm()` where the intended mean is 0. Since this is a
two-tailed test, the _alternate_ hypothesis is `mean(x0) != 0` while the _null_
hypothesis is `mean(x0) = 0`

Since mu=0 and 0 is within the range (-.323, 0.050), we have failed to reject
the null hypothesis with this set of data

#### __Unpaired T-Test__ {-}


_Dependent Variables_: 1, continuous [ratio and interval]

_Independent Variables_: 1

_Design_: 2 groups, 2 levels

_Parametric_: Yes

```{r eval= FALSE} 
# One-sample t-test
t.test(x, #numeric vector of data 1
       y, #numeric vector of data 2
       alternative = "two.sided", # ("two.sided", "less", "greater") depending on which you want to use. Defaults to "two.sided
       conf.level = 0.95 #1-alpha. Defaults to 0.95
)
```

The unpaired t-test functions similarly to the one-sample t-test except instead
of mu, it uses dataset y. Variance is assumed to be about equal between dataset
x and y.

::: {.box .readmore}
[Moer information about unpaired t-tests here](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module7-T-tests/Module7-ttests6.html)
[Another R guide for upaired t-tests](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module7-T-tests/Module7-ttests7.html)
:::

```{r}
#Unpaired t-test example
set.seed(10)
x <- rnorm(100, 0, 1)
y <- rnorm(100, 3, 1)

unpaired <- t.test(x, y)

knitr::knit_print(unpaired)
```

Output for an unpaired two sample t-test is similar to the output for the
one-sample. Mu is typically 0 (though one could change it with 'mu = n' in the
function call) and the test uses the confidence interval for (mean(x)-mean(y)),
which is (-3.308, -2.775). Since mu does not exist within the confidence
interval, the null hypothesis can be rejected.

#### __Paired T-Test__ {-}


_Dependent Variables_: 1, continuous [ratio and interval]

_Independent Variables_: 1

_Design_: 1 group, 2 levels

_Parametric_: Yes

```{r eval= FALSE} 
# One-sample t-test
t.test(x, #numeric vector of data 1
       y, #numeric vector of data 2
       paired = TRUE, #defaults to FALSE
       alternative = "two.sided", # ("two.sided", "less", "greater") depending on which you want to use. Defaults to "two.sided
       conf.level = 0.95 #1-alpha. Defaults to 0.95
)
```

The paired t-test works for matched and repeated-measures designs. Like the
unpaired t-test, it has a second data vector and does not need 'mu'. When
running a paired t-test make sure that you specify that `paired = TRUE` when
calling the function and that x and y have the same length.

[More information about paired t-tests
here](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module7-T-tests/Module7-ttests8.html#headingtaglink_2)

```{r}
#Unpaired t-test example
set.seed(10)
x <- rnorm(100, 0, 13)
y <- rnorm(100, 3, 1)

unpaired <- t.test(x, y, paired = TRUE)

knitr::knit_print(unpaired)
```

Similar to the unpaired t-test, the paired t-test looks at the differences
between x and y. Unlike the unpaired t-test, the statistic creates and runs its
test on a third set of data created from the differences of x and y. For an x
and y of length n, r would create a third data set of ` new = ((x1-y1),
(x2-y2),...(xn-1-yn-1), (xn-yn))` and run its testing with mean(new) and sd(new)


#### __Chi-Squared test__ {-}


_Dependent Variables_: 1, categorical

_Independent Variables_: 1

_Design_: 2 groups, 2 + levels

_Parametric_: No

[More Information on Chi-Squared tests](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_HypothesisTesting-ChiSquare/BS704_HypothesisTesting-ChiSquare3.html)

```{r eval=FALSE}
#How to Run a chi-squared test
chisq.test(x, # numeric vector, matrix, or factor
           y # numeric vector; ignored if x is a matrix; factor of the same length as x if x is a factor
)
```

#### __Chi-Square Goodness-Of-Fit_ _{-}


_Dependent Variables_: 1, categorical

_Independent Variables_: 0

_Design_: 1 group

_Parametric_: No

[More Information on Chi-Squared test on one sample ](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_HypothesisTesting-ChiSquare/BS704_HypothesisTesting-ChiSquare2.html#headingtaglink_1)

```{r eval=FALSE}
#How to Run a chi-square goodness of fit
chisq.test(x, # numeric vector
           p = #list of probabilities, like c(10, 10, 10, 70)/100), same length as x
)
```

The chi-squared goodness-of-fit can compare one set of outcomes to a given set of probabilities to determine the likelihood of the test probabilities differing from the given. The set of given probabilities constitutes the null hypothesis. In the below example, the dataset is the tested outcome frequencies: the number of times x[1], x[2], x[3], and x[4] were observed. If my null hypothesis says that each of the 4 should be observed an equal number of times, then the resulting chi-square test should will have a p-value of p<0.001

```{r}
x <- c(10,25, 18, 92)

chisq.test(x, p=c(25, 25, 25, 25)/100)
```
#### __Wilcoxon-Mann Whitney test__ {-}


_Dependent Variables_: 1, (ordinal, interval, or ratio)

_Independent Variables_: 1

_Design_: 2 groups, 2 levels

_Parametric_: No

[More information here](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_nonparametric/bs704_nonparametric4.html)
```{r eval=FALSE}
wilcox.test(x,#Numeric vector of data
            y #optional numeric vector of data)

```

#### __Wilcoxon Signed Ranks test__ {-}


_Dependent Variables_: 1, (ordinal, interval, or ratio)

_Independent Variables_: 1

_Design_: 1 groups, 2 levels

_Parametric_: No

[More information here](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_nonparametric/BS704_Nonparametric6.html)

```{r`eval=FALSE}

wilcox.test(x,#Numeric vector of data
            y, #numeric vector of data from 2nd level- same length
            paired = TRUE)

```

#### __One-Way ANOVA__ {-}


_Dependent Variables_: 1, continuous

_Independent Variables_: 1

_Design_: 2+ groups, 2+ levels

_Parametric_: Yes

[More information here](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_HypothesisTesting-ANOVA/BS704_HypothesisTesting-ANOVA4.html#headingtaglink_3)
```{r eval=FALSE}
summary(aov(formula, #formula specifying the model
            data #data frame containing the variables in the formula))

```


#### __Regressions__ {-}

#### __Factor Analysis__ {-}

### Choosing a Test

1. Look for your _Dependent Variable(s)._ How many do you have? (1 or 2+)
      + Which ones exist on a continuum, where any fraction within the valid range is technically possible, though the actual data might have rounded? (Continuous vs Categorical)
      + For Categorical: If values do not have a definite order then it’s Nominal. Else it’s probably Ordinal
2. Look for your _Independent Variable(s)._ How many do you have? (0, 1, or 2+)
      + Define their types and level of measurement as described in 1a and then do 2b and 2c if you have 1 or more independent variables
      + How many conditions does each independent variable have? (= # of levels)
      + Does the same subject or (equivalent subjects) undergo each condition/level? (‘Yes’ -> repeated-measures; ‘No’ -> between-groups
3. If you do not have nominal data, can you assume your data would have a normal distribution if you measured the general population? (‘Yes’-> parametric; ‘No’ -> non-parametric)



### Multiple Hypothesis Testing .

In statistics, the multiple comparisons, multiplicity or multiple testing
problem occurs when one considers a set of statistical inferences simultaneously
or infers a subset of parameters selected based on the observed values.
The more inferences are made, the more likely erroneous inferences become.
Several statistical techniques have been developed to address that problem,
typically by requiring a stricter significance threshold for individual
comparisons, so as to compensate for the number of inferences being made.
The goal of these techniques is to better control the number of false positives
produced by the test. The general term for identifying these stricter
significance thresholds is *multiple testing correction*. p-values that have
been subject to multiple testing correction are called *multiple hypothesis
adjusted p-values* or simply *adjusted p-values*, whereas the unadjusted
p-values are called *nominal p-values*.

There are two popular approaches for performing multiple testing correction: the
[Bonferroni correction, or familywise error
rate](https://en.wikipedia.org/wiki/Familywise_error_rate)(FWER) and
[Benjamini-Hochberg correction, or  false discovery
rate*](https://en.wikipedia.org/wiki/False_discovery_rate)(FDR). The FWER is the
more conservative approach, where adjusted p-values indicate the probability of
making one or more false positive inferences for all tests with that adjusted
p-value and smaller. FDR is more permissive, and is defined as the fraction of
results that are expected to be false positives for all tests with the given
threshold and below.

Both Bonferroni and Benjamini-Hochberg procedures assume all tests are
independent. In many types of biological data, e.g. genome wide gene expression,
this assumption may not hold, since many properties of a biological system may
be correlated. For this reason, FDR is more often applied to adjust p-values in
these types of experiments because it is more permissive.

### Statistical power 

__Statistical Power__ is the probability that the null hypothesis was actually false but the data did not have enough evidence to reject it. In short, it is the failure to reject the null hypothesis in the presence of a significant effect. This is a Type II error, also known as a false negative. Its counterpart, the Type I error, is the probability that a null hypothesis was rejected when there was no significant effect.

The chance of a Type I error is represented by alpha, the same alpha that we use to calculate significance level (1-alpha). Type II errors are represented by beta. The relationship between statistical power and beta are similar to the relationship between significance level and alpha: statistical power is calculated as 1-beta. 

[Read more about the relationship between Type II and Type I errors]( https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistest-means-proportions/BS704_HypothesisTest-Means-Proportions3.html#headingtaglink_3)

To calculate beta, find the area under the curve of the research hypothesis distribution on the opposite side of the critical value line where alpha is calculated. To find the area in the tail of the distribution, you would need to reference a Z-score chart, which will not be expected of you in this course. If you wish to read more about it [here is a link about finding the area under the normal curve with z-scores, same one as linked above]( https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/sas/sas4-onesamplettest/sas4-onesamplettest3.html) 

```{r echo=FALSE}
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 130   # expected value under H1
alpha <- 0.05  # probability of type I error
crit <- qnorm(1-alpha, mu0, sigma)
pow <- pnorm(crit, mu1, sigma, lower.tail=FALSE)
beta <- 1-pow
xLims <- c(50, 180)
left  <- seq(xLims[1],   crit, length.out=100)
right <- seq(crit, xLims[2],   length.out=100)
yH0r  <- dnorm(right, mu0, sigma)
yH1l  <- dnorm(left,  mu1, sigma)
yH1r  <- dnorm(right, mu1, sigma)

curve(dnorm(x, mu0, sigma), xlim=xLims, lwd=2, col="red", xlab="x", ylab="density",
      main="Relationship of Alpha and Beta Around Critical Value", ylim=c(0, 0.03), xaxs="i")
curve(dnorm(x, mu1, sigma), lwd=2, col="blue", add=TRUE)
polygon(c(right, rev(right)), c(yH0r, numeric(length(right))), border=NA,
        col=rgb(1, 0.3, 0.3, 0.6))
polygon(c(left,  rev(left)),  c(yH1l, numeric(length(left))),  border=NA,
        col=rgb(0.3, 0.3, 1, 0.6))
polygon(c(right, rev(right)), c(yH1r, numeric(length(right))), border=NA,
        density=5, lty=2, lwd=2, angle=45, col="darkgray")
abline(v=crit, lty=1, lwd=3, col="red")
text(crit+1,  0.03,  adj=0, label="Critical Value")
text(mu0-10,  0.025, adj=1, label="Null Hypothesis")
text(mu1+10,  0.025, adj=0, label="Alternative Hypothesis")
text(crit+8,  0.01,  adj=0, label="Statistical Power", cex=1.3)
text(crit-12, 0.004,  expression(beta),  cex=1.3)
text(crit-12, 0.002,  "Beta",  cex=1.3)
text(crit+5,  0.0015, expression(alpha), cex=1.3)
text(crit+5,  -0.0005, "Alpha", cex=1.3)

```
<small> Code sourced from:
caracal, "How do I find the probability of a type II error", stats.stackexchange.com, Feb 19, 2011, [link](https://stats.stackexchange.com/questions/7402/how-do-i-find-the-probability-of-a-type-ii-error), Date Accessed: Feb 28, 2022 </small>

The above graph shows the relationship of alpha and beta. Since the alternative hypothesis is looking for a higher mean, alpha is calculated by the area under the null hypothesis curve on right side of the critical value while beta is calculated by the area under the alternative hypothesis on the left of the critical value line. Should an experiment get a higher p-value than the critical value, then the Beta will increase in relationship to Alpha decreasing.

```{r echo=FALSE}
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 130   # expected value under H1
alpha <- 0.025  # probability of type I error
crit <- qnorm(1-alpha, mu0, sigma)
old <- qnorm(1-0.05, mu0, sigma)
pow <- pnorm(crit, mu1, sigma, lower.tail=FALSE)
beta <- 1-pow
xLims <- c(50, 180)
left  <- seq(xLims[1],   crit, length.out=100)
right <- seq(crit, xLims[2],   length.out=100)
yH0r  <- dnorm(right, mu0, sigma)
yH1l  <- dnorm(left,  mu1, sigma)
yH1r  <- dnorm(right, mu1, sigma)

curve(dnorm(x, mu0, sigma), xlim=xLims, lwd=2, col="red", xlab="x", ylab="density",
      main="Relationship of Alpha and Beta Around P<Critical", ylim=c(0, 0.03), xaxs="i")
curve(dnorm(x, mu1, sigma), lwd=2, col="blue", add=TRUE)
polygon(c(right, rev(right)), c(yH0r, numeric(length(right))), border=NA,
        col=rgb(1, 0.3, 0.3, 0.6))
polygon(c(left,  rev(left)),  c(yH1l, numeric(length(left))),  border=NA,
        col=rgb(0.3, 0.3, 1, 0.6))
polygon(c(right, rev(right)), c(yH1r, numeric(length(right))), border=NA,
        density=5, lty=2, lwd=2, angle=45, col="darkgray")
abline(v=crit, lty=1, lwd=3, col="red")
abline(v=old, lty=2, lwd=2, col="pink")
text(crit+1,  0.03,  adj=0, label="Critical Value ->")
text(mu0-10,  0.025, adj=1, label="Null Hypothesis")
text(mu1+10,  0.025, adj=0, label="Alternative Hypothesis")
text(crit+8,  0.01,  adj=0, label="Statistical Power", cex=1.3)
text(crit-12, 0.004,  expression(beta),  cex=1.3)
text(crit-12, 0.002,  "Beta",  cex=1.3)
text(crit+5,  0.0015, expression(alpha), cex=1.3)
text(crit+5,  -0.0005, "Alpha", cex=1.3)

```
In this graph, the critical value was moved to 0.025 while the null and alternative hypotheses stayed the same. As the chance of alpha decreased, the chances of beta increased. The opposite is also true, as alpha increases, beta decreases

##### Applications of Statistical Power {-}

One of the applications of statistical power, besides as a measure of Type II error probability, is to allow you to run a power analysis. A power analysis involves using the relationship between __Effect Size__, __Sample Size__, __Significance__, and __Statistical Power__ when you have three of the four parts in order to find the value of the fourth. This is typically used to find the effect size of a study, as that is often the hardest to estimate, or to find a sample size that corresponds with a desired effect size when designing a study.
[Further reading on statistical power and power analysis]( https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power6.html)

##### Effect Size {-}
The effect size is a measurement of the magnitude of experimental effects. There are a number of ways to calculate effect size including but not limited to Cohen’s D, Pearson’s R, and Odds Ratio. If you would like to read about what BU’s SPH has to say about it, [here is the SPH module link]( https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_power/BS704_Power8.html)




<small>
Citations:
Dorey FJ. Statistics in brief: Statistical power: what is it and when should it be used?. Clin Orthop Relat Res. 2011;469(2):619-620. doi:10.1007/s11999-010-1435-0

Parab S, Bhalerao S. Choosing statistical test. Int J Ayurveda Res. 2010;1(3):187-191. doi:10.4103/0974-7788.72494
Introduction to SAS. UCLA: Statistical Consulting Group. from https://stats.idre.ucla.edu/sas/modules/sas-learning-moduleintroduction-to-the-features-of-sas/ (accessed February 24, 2022)

Ranganathan P, Gogtay NJ. An Introduction to Statistics - Data Types, Distributions and Summarizing Data. Indian J Crit Care Med. 2019 Jun;23(Suppl 2):S169-S170. doi: 10.5005/jp-journals-10071-23198. PMID: 31485129; PMCID: PMC6707495.


</small>
