---
output: html_document
---

```{r data sci setup, include=FALSE}
library(tidyverse)
```

# Data Science

[Data science](https://en.wikipedia.org/wiki/Data_science) is an enormous and
rapidly growing field that incorporates elements of statistics, computer
science, software engineering, high performance and cloud computing, and big
data management, as well as syntheses with knowledge from other social and
physical science fields to model and predict phenomena captured by data
collected from the "real world". Many books have and will be written on this
subject, and this one does not pretend to even attempt to give this area an
adequate treatment. Like the rest of this book, the topics covered here are
opinionated and presented through the lens of a biological analyst practitioner
with enough high level conceptual details and a few general principles to
hopefully be useful in that context.

## Data Modeling

The goal of data modeling is to describe a dataset using a relatively small
number of mathematical relationships. Said differently, a model uses some parts
of a dataset to try to accurately predict other parts of the dataset in a way
that is useful to us.

Models are human inventions; they reflect our beliefs about the way the universe
works. The successful model identifies patterns within a dataset that are the
result of causal relationships in the universe that led to the phenomena that
were measured while accounting for noise in the data. However, *the model itself
does not identify or even accurately reflect those causal effects*. The model
merely summarizes patterns and we as scientists are left to interpret those
patterns and design follow up experiments to investigate the nature of those
causal relationships using our prior knowledge of the world.

There are several principles to keep in mind when modeling data:

1. **Data are never wrong.** All data are collected using processes and
devices designed and implemented by humans, who always have biases and make
assumptions. All data measure something about the universe, and so are "true" in
some sense of the word. If what we intended to measure and what we actually
measured were not the same thing, that is due to our errors in collection or
interpretation, not due to the data being wrong. If we approach our dataset with
a particular set of hypotheses and the data don't support those hypotheses, it
is our beliefs of the world and our understanding of the dataset that are wrong,
not the data itself.

2. **Not all data are useful.** Just because data isn't wrong, it doesn't mean
it is useful. There may have been systematic errors in the collection of the
data that makes interpreting them difficult. Data collected for one purpose may
not be useful for any other purposes. And sometimes, a dataset collected for a
particular purpose may simply not have the information needed to answer our
questions; if what we measure has no relationship to what we wish to predict,
the data itself is not useful - though the knowledge that what we measured has
no detectable effect on the thing we wish to predict may be very useful!

3. **"All models are wrong, but some are useful."** George Box, the renowned
British statistician, famously asserted this in a 1976 paper to the Journal of
the American Statistical Association. [@Box1976-rd]. By this he meant that every
model we create is a simplification of the system we are seeking to model, which
is by definition not identical to that system. To perfectly model a system, our
model would need to be precisely the system we are modeling, which is no longer
a model but the system itself. Fortunately, even though we know our models are
always wrong to some degree, they may nonetheless be useful because they are not
**too** wrong. Some models may indeed be too wrong, though.

4. **Data do not contain causal information.** "Correlation does not mean
causation." Data are measurements of the results of a process in the universe
that we wish to understand; the data are possibly reflective of that process,
but do not contain any information about the process itself. We cannot infer
causal relationships from a dataset alone. We must construct possible causal
models using our knowledge of the world first, then apply our data to our model
and other alternative models to compare their relative plausibility.

5. **All data have noise.** The usefulness of a model to describe a dataset is
related to the relative strength of the patterns and noise in the dataset when
viewed through the lens of the model; conceptually, the so-called "signal to
noise ratio" of the data. The fundamental concern of statistics is quantifying
uncertainty (i.e. noise), and separating it from real signal, though different
statistical approaches (e.g. frequentist vs Bayesian) reason about uncertainty
in different ways.

Modeling begins (or should begin) with posing one or more **scientific models**
of the process or phenomenon we wish to understand. The scientific model is
conceptual; it reflects our belief of the universe and proposes a causal
explanation for the phenomenon. We then decide how to map that scientific model
onto a **statistical model**, which is a mechanical procedure that quantifies
how well our scientific model explains a dataset. *The scientific model and
statistical model are related but independent choices we make.* There may be
many valid statistical models that represent a given scientific model. However,
sometimes in practice we lack sufficient knowledge about the process to propose
scientific models first, requiring data exploration and summarization first to
suggest reasonable starting points.

::: {.box .important}
This section pertains primarily to models specified explicitly by humans. There
is another class of models, namely those created by certain machine learning
algorithms like neural networks and deep learning, that *discover models from
data*. These models are fundamentally different than those designed by human
minds, in that they are often accurate and therefore useful, but it can be very
difficult if not impossible to understand how they work. While these are
important types of models that fall under the umbrella of data science, we limit
the content of this chapter to human designed statistical models.
:::

### A Worked Modeling Example

As an example, let's consider a scenario where we wish to assess whether any of
three genes can help us distinguish between patients who have Parkinson's
Disease and those who don't by measuring the relative activity of those genes in
blood samples. We have the following made-up dataset:

```{r data modeling pd, echo=FALSE}
set.seed(1337)
gene_exp <- tibble(
  sample_name = c(str_c("P",1:100), str_c("C",1:100)),
  `Disease Status` = factor(c(rep("Parkinson's",100),rep("Control",100))),
  `Gene 1` = c(rnorm(100,255,10), rnorm(100,250,10)),
  `Gene 2` = c(rnorm(100,520,100), rnorm(100,600,60)),
  `Gene 3` = c(rnorm(100,500,40), rnorm(100,330,40))
)
```

```{r}
gene_exp
```

Our imaginary dataset has 100 Parkinson's and 100 control subjects. For each of
our samples, we have a sample ID, Disease Status of `Parkinson's` or `Control`,
and numeric measurements of each of three genes. Below are [violin
plots](#violin-plot) of our (made-up) data set for these three genes:

```{r data modeling violin, echo=FALSE}
pivot_longer(
  gene_exp,
  c(`Gene 1`, `Gene 2`, `Gene 3`),
  names_to="Gene",
  values_to="Expression"
) %>% ggplot(
    aes(x=`Disease Status`,y=Expression,fill=`Disease Status`)
  ) +
  facet_wrap(vars(Gene)) +
  geom_violin()
```

By inspection, it appears that Gene 1 has no relationship with disease; we may
safely eliminate this gene from further consideration. Gene 2 appears to have a
different profile depending on disease status, where control individuals have
a higher average expression and a lower variance. Unfortunately, despite this
qualitative difference, this gene may not be useful for telling whether someone
has disease or not - the ranges completely overlap. Gene 3 appears to
discriminate between disease and control. There is some overlap in the two
expression distributions, but above a certain expression value these data
suggest a high degree of predictive accuracy may be obtained with this gene.
Measuring this gene may therefore be useful, if the results from this dataset
generalize to all people with Parkinson's Disease.

So far, we have not done any modeling, but instead relied on plotting and our
eyes. A more quantitative question might be: how much higher is Gene 3
expression in Parkinson's Disease than control? Another way of posing this
question is: if I know a patient has Parkinson's Disease, what Gene 3 expression
value do I expect them to have? Written this way, we have turned our question
into a prediction problem: if we only had information that a patient had
Parkinson's Disease, what is the predicted expression value of their Gene 3?

Another way to pose this prediction question is in the opposite (and arguably
more useful) direction: if all we knew about a person was their Gene 3 gene
expression, how likely is it that the person has Parkinson's Disease? If this
gene expression is predictive enough of a person's disease status, it may be a
viable [biomarker](https://en.wikipedia.org/wiki/Biomarker) of disease and thus
might be useful in a clinical setting, for example when identifying
presymptomatic individuals or assessing the efficacy of a pharmacological
treatment.

Although it may seems obvious, before beginning to model a dataset, we must
start by posing the **scientific question** as concisely as possible, as we have
done above. These questions will help us identify which modeling techniques are
appropriate and help us ensure we interpret our results correctly.

We will use this example dataset throughout this chapter to illustrate some key
concepts.

### Data Summarization

Broadly speaking, *data summarization* is the process of finding a
lower-dimensional representation of a larger dataset. There are many ways to
summarize a set of data; each approach will emphasize different aspects of the
dataset, and have varying degrees of accuracy. Consider the gene expression of
Gene 1 for all individuals in our example above, plotted as a distribution with
a [histogram](#histogram):

```{r data sci summ hist}
ggplot(gene_exp, aes(x=`Gene 1`)) +
  geom_histogram(bins=30,fill="#a93c13")
```

#### Point Estimates

The data are concentrated around the value 250, and become less common for
larger and smaller values. Since the extents to the left and right of the middle
of the distribution appear to be equally distant, perhaps the arithmetic mean
is a good way to identify the middle of the distribution:

```{r data sci summ hist2}
ggplot(gene_exp, aes(x=`Gene 1`)) +
  geom_histogram(bins=30,fill="#a93c13") +
  geom_vline(xintercept=mean(gene_exp$`Gene 1`))
```

By eye, the mean does seem to correspond well to the value that is among the
most frequent, and successfully captures an important aspect of the data: its
*central tendency*. Summaries that compute a single number are called *point
estimates*. Point estimates collapse the data into one singular point, one
value.

::: {.box}
The arithmetic mean is just one measure of central tendency, computed by taking
the sum of all the values and dividing by the number of values. The mean may be
a good point estimate of the central tendency of a dataset, but it is sensitive
to outlier samples. Consider the following examples:

```{r fig.align="center", fig.height=4}
library(patchwork)
well_behaved_data <- tibble(data = rnorm(1000))
data_w_outliers <- tibble(data = c(rnorm(800), rep(5, 200))) # oops I add some outliers :^)

g_no_outlier <- ggplot(well_behaved_data, aes(x = data)) +
  geom_histogram(fill = "#56CBF9", color = "grey", bins = 30) +
  geom_vline(xintercept = mean(well_behaved_data$data)) +
  ggtitle("Mean example, no outliers")

g_outlier <- ggplot(data_w_outliers, aes(x = data)) +
  geom_histogram(fill = "#7FBEEB", color = "grey", bins = 30) +
  geom_vline(xintercept = mean(data_w_outliers$data)) +
  ggtitle("Mean example, big outliers")

g_no_outlier | g_outlier
```

The median is another measure of central tendency, which is found by identifying
the value that divides the samples into equal halves when sorted from smallest
to largest. The median is more robust in the presence of outliers.

```{r fig.align="center", fig.height=4}
g_no_outlier <- ggplot(well_behaved_data, aes(x = data)) +
  geom_histogram(fill = "#AFBED1", color = "grey", bins = 30) +
  geom_vline(xintercept = median(well_behaved_data$data)) +
  ggtitle("Median example")

g_outlier <- ggplot(data_w_outliers, aes(x = data)) +
  geom_histogram(fill = "#7FBEEB", color = "grey", bins = 30) +
  geom_vline(xintercept = median(data_w_outliers$data)) +
  ggtitle("Median example, big outliers")

g_no_outlier | g_outlier
```
:::

#### Dispersion

Central tendencies are important aspects of the data but don't describe what the
data do for values outside this point estimate of central tendency; in other
words, we have not expressed the spread, or *dispersion* of the data.

We decide that perhaps computing the [standard
deviation](https://en.wikipedia.org/wiki/Standard_deviation) of the data may
characterize the spread well, since it appears to be symmetric around the mean.
We can layer this information on the plot as well to inspect it:

```{r data sci summ sd}
g1_mean <- mean(gene_exp$`Gene 1`)
g1_sd <- sd(gene_exp$`Gene 1`)
ggplot(gene_exp, aes(x=`Gene 1`)) +
  geom_histogram(bins=30,fill="#a93c13") +
  geom_vline(xintercept=g1_mean) +
  geom_segment(x=g1_mean-g1_sd, y=10, xend=g1_mean+g1_sd, yend=10)
```

The width of the horizontal line is proportional to the mean +/- one standard
deviation around the mean, and has been placed arbitrarily on the y axis at `y =
10` to show how this range covers the data in the histogram. The +/- 1 standard
deviation around the mean visually describes the spread of the data reasonably
well.

::: {.box}
Measures the spread of the data, typically around its perceived center (a mean).
Often related to the distribution of the data.

**Standard deviation**: A measure of how close values are to the mean. Bigger
standard deviations mean data is more spread out.

```{r taylor sd, fig.align="center"}
data <- tibble(data = c(rnorm(1000, sd=1.75)))
ggplot(data, aes(x = data)) +
  geom_histogram(fill = "#EAC5D8", color = "white", bins = 30) +
  geom_vline(xintercept = c(-2, -1, 0, 1, 2) * sd(data$data)) +
  xlim(c(-6, 6)) +
  ggtitle("Standard deviations aplenty", paste("SD:", sd(data$data)))
```

**Variance**:
Similar to SD (it's the square of SD), variance measures how far a random value
is from the mean.
```{r taylor var, fig.align="center"}
data <- tibble(data = c(rnorm(1000, sd=0.5)))
ggplot(data, aes(x = data)) +
  geom_histogram(fill = "#DBD8F0", color = "white", bins = 30) +
  geom_vline(xintercept = mean(data$data)) +
  xlim(c(-6, 6)) +
  ggtitle("Same mean as SD plot, but different variance",
          paste("Variance:", sd(data$data)))
```
:::

#### Distributions

With these two pieces of knowledge - the mean accurately describes the center of
the data and the standard deviation describes the spread - we now recognize that
these data may be [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution), and therefore
we can potentially describe the dataset mathematically. We decide to visually
inspect this possibility by layering a normal distribution on top of our data
using
[`stat_function`](https://ggplot2.tidyverse.org/reference/geom_function.html):

```{r}
g1_mean <- mean(gene_exp$`Gene 1`)
g1_sd <- sd(gene_exp$`Gene 1`)
ggplot(gene_exp, aes(x=`Gene 1`)) +
  geom_histogram(
    aes(y=after_stat(density)),
    bins=30,
    fill="#a93c13"
  ) +
  stat_function(fun=dnorm, args = list(mean=g1_mean, sd=g1_sd), size=2)
```

Note the histogram bars are scaled with
`aes(y=[after_stat](https://ggplot2.tidyverse.org/reference/aes_eval.html)(density))`
to the density of values in each bin to make all the bar heights sum to 1 so
that the y scale matches that of a normal distribution.

We have now created our first model: we chose to express the dataset as a normal
distribution parameterized by the mean and standard deviation and standard
deviation of the data. Using the values of `r round(g1_mean)` and `r round(g1_sd)`
as our mean and standard deviation, respectively, we can express our model
mathematically as follows:

$$
Gene\;1 \sim \mathcal{N}(254, 11)
$$

Here the $\sim$ symbol means "distributed as" and the $\mathcal{N}(\mu,\sigma)$
represents a normal distribution with mean of $\mu$ and standard deviation of
$\sigma$. This is mathematical formulation means the same thing as saying we are
modeling Gene 1 expression as a normal distribution with mean of 254 and
standard deviation of 11. Without any additional information about a new sample,
we would expect the expression of that gene to be 254, although it may vary from
this value.

::: {.box}
The normal distribution is the most common distribution observed in nature, but
it is hardly the only one. We could have proposed other distributions to instead
summarize our data:

```{r fig.align="center"}
g_norm <- ggplot(tibble(data = rnorm(5000)), aes(x = data)) +
  geom_histogram(fill = "#D0FCB3", bins = 50, color = "gray") +
  ggtitle("Normal distribution", "rnorm(n = 1000)")

g_unif <- ggplot(tibble(data = runif(5000)), aes(x = data)) +
  geom_histogram(fill = "#271F30", bins = 50, color = "white") +
  ggtitle("Uniform distribution", "runif(n = 1000)")

g_logistic <- ggplot(tibble(data = rlogis(5000)), aes(x = data)) +
  geom_histogram(fill = "#9BC59D", bins = 50, color = "black") +
  ggtitle("Logistic distribution", "rlogis(n = 1000)")

g_exp <- ggplot(tibble(data = rexp(5000, rate = 1)), aes(x = data)) +
  geom_histogram(fill = "#6C5A49", bins = 50, color = "white") +
  ggtitle("Exponential distribution", "rexp(n = 1000, rate = 1)")

(g_norm | g_unif) / (g_logistic | g_exp)
```
In addition to the normal distribution, we have also plotted samples drawn from
a [continuous uniform
distribution](ihttps://en.wikipedia.org/wiki/Continuous_uniform_distribution)
between 0 and 1, a [logistic
distribution](https://en.wikipedia.org/wiki/Logistic_distribution) which is
similar to the normal distribution but has heavier "tails", and an [exponential
distribution](https://en.wikipedia.org/wiki/Exponential_distribution).
There are many more distributions than these, and many of them were discovered
to arise in nature and encode different types of processes and relationships.
:::

A few notes on our data modeling example before we move on:

1. **Our model choice was totally subjective.** We looked at the data and
decided that a normal distribution was a reasonable choice. There were many
other choices we could have made, and all of them would be equally valid, though
they may not all describe the data equally well.

2. **We can't know if this is the "correct" model for the data.** By eye, it
appears to be a reasonably accurate summary. However, there is no such thing as
a correct model; some models are simply better at describing the data than
others. Recall that all models are wrong, and some models are useful. Our model
may be useful, but it is definitely wrong to some extent.

3. **We don't know how well our model describes the data yet.** So far we've
only used our eyes to choose our model which might be a good starting point
considering our data are so simple, but we have not yet quantified how well our
model describes the data, or compared it to alternative models to see which is
better. This will be discussed briefly in a later section.

### Linear Models

Our choice of a normal distribution to model our Gene 1 gene expression was only
*descriptive*; it was a low-dimensional summary of our dataset. However, it was
not very *informative*; it doesn't tell us anything useful about Gene 1
expression with respect to our scientific question of distinguishing between
Parkinson's Disease and Control individuals. To do that, we will need to find a
model that can make *predictions* that we may find useful if we receive new
data. To do that, we will introduce a new type of model: the linear model.

A linear model is any statistical model that relates one outcome variable as a
linear combination (i.e. sum) of one or more explanatory variables. This may be
expressed mathematically as follows:

$$
Y_i = \beta_0 + \beta_1 \phi_1 ( X_{i1} ) + \beta_2 \phi_2 ( X_{i2} ) + \ldots + \beta_p \phi_p ( X_{ip} ) + \epsilon_i
$$

Above, $Y_i$ is some outcome or response variable we wish to model, $X_{ij}$ is
our explanatory or predictor variable $j$ for observatoin $i$, and $\beta_j$ are
coefficients estimated to minimize the difference between the predicted outcome
$\hat{Y_i}$ and the observed $Y_i$ over all observations. $\phi_j$ is a possibly
non-linear transformation of the explanatory variable $X_ij$; note these
functions may be non-linear so long as the predicted outcome is modeled as a
linear combination of the transformed variables. The rest of this section is
dedicated to a worked example of a linear model for gene expression data.

Let us begin with a [beeswarm plot](#beeswarm-plot) plot of Gene 3:

```{r lin model gene 3 violin}
library(ggbeeswarm)
ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) +
  geom_beeswarm()
```

The expression values within each disease status look like they might be
normally distributed just like Gene 1, so let's summarize each group with the
arithmetic mean and standard deviation as before, and instead plot both
distributions as histograms:

```{r}
exp_summ <- pivot_longer(
  gene_exp,
  c(`Gene 3`)
) %>%
  group_by(`Disease Status`) %>%
  summarize(mean=mean(value),sd=sd(value))

pd_mean <- filter(exp_summ, `Disease Status` == "Parkinson's")$mean
c_mean <- filter(exp_summ, `Disease Status` == "Control")$mean

ggplot(gene_exp, aes(x=`Gene 3`, fill=`Disease Status`)) +
  geom_histogram(bins=20, alpha=0.6,position="identity") +
  annotate("segment", x=c_mean, xend=pd_mean, y=20, yend=20, arrow=arrow(ends="both", angle=90)) +
  annotate("text", x=mean(c(c_mean,pd_mean)), y=21, hjust=0.5, label="How different?")
```

We can make a point estimate of this difference by simply subtracting the means:

```{r lin model point diff}
pd_mean - c_mean
```

In other words, this point estimate suggests that on average Parkinson's
patients have `r round(pd_mean-c_mean,1)` greater expression than Controls. We can
plot this relationship relatively simply:

```{r lin model gene 3 fit}
ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) +
  geom_beeswarm() +
  annotate("segment", x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean)
```

However, this point estimate tells us nothing about how confident we are about
the difference. We can do this by using an [linear
regression](https://en.wikipedia.org/wiki/Least_squares) by modeling Gene 3 as a
function of disease status:

```{r lin model lm}
fit <- lm(`Gene 3` ~ `Disease Status`, data=gene_exp)
fit
```

The coefficient associated with having the disease status of Parkinson's disease
is almost exactly equal to our difference in means. We also note that the
coefficient labeled `(Intercept)` is nearly equal to the mean of our control
samples (`r round(c_mean,1)`). Under the hood, this simple linear model did the
same calculation we did by subtracting the means of each group but estimated the
means using all the data at once, instead of point estimates. Another advantage
of using `lm()` over the point estimate method is the model can estimate how
confident the model was that the difference in mean between Parkinson's and
controls subjects. Let's print some more information about the model than
before:

```{r lin model summary}
summary(fit)
```

We again see our coefficient estimates for the intercept (i.e. mean control
expression) and our increase in Parkinson's, but also a number of other terms,
in particular `Pr(>|t|)` and `Multiple R-squared`. The former is the
[p-value](#p-values) associated with each of the coefficient estimates, both of
which are very small, indicating to us that the model was very confident of the
estimated values. The latter, multiple R-squared or $R^2$, describes how much of
the variance in the data was explained by the model it found as a fraction
between 0 and 1. This model explains 77.7% of the variance of the data, which is
substantial. The $R^2$ value also has an associated p-value, which is also very
small. Overall, these statistics suggest this model fits the data very well.

We can plot the results of a linear model for each of our genes relatively
easily:

```{r lin model 3 genes}
pd_mean <- mean(filter(gene_exp,`Disease Status`=="Parkinson's")$`Gene 1`)
c_mean <- mean(filter(gene_exp,`Disease Status`=="Control")$`Gene 1`)
g1 <- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 1`, color=`Disease Status`)) +
  geom_beeswarm() +
  annotate("segment", x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) +
  theme(legend.position="none")

pd_mean <- mean(filter(gene_exp,`Disease Status`=="Parkinson's")$`Gene 2`)
c_mean <- mean(filter(gene_exp,`Disease Status`=="Control")$`Gene 2`)
g2 <- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 2`, color=`Disease Status`)) +
  geom_beeswarm() +
  annotate("segment", x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) +
  theme(legend.position="none")

pd_mean <- mean(filter(gene_exp,`Disease Status`=="Parkinson's")$`Gene 3`)
c_mean <- mean(filter(gene_exp,`Disease Status`=="Control")$`Gene 3`)
g3 <- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) +
  geom_beeswarm() +
  annotate("segment", x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) +
  theme(legend.position="none")
g1 | g2 | g3
```

We can also compute the corresponding linear model fits, and confirm that the
coefficients agree with the directions observed in the plot, as well as that all
the associations are significant FDR < 0.05:



```{r lin mod all fits, include=FALSE, cache=TRUE}
fit1 <- lm(`Gene 1` ~ `Disease Status`, data=gene_exp)
fit2 <- lm(`Gene 2` ~ `Disease Status`, data=gene_exp)
fit3 <- lm(`Gene 3` ~ `Disease Status`, data=gene_exp)

gene_stats <- bind_rows(
  c("Gene 1",coefficients(fit1),summary(fit1)$coefficients[2,4]),
  c("Gene 2",coefficients(fit2),summary(fit2)$coefficients[2,4]),
  c("Gene 3",coefficients(fit3),summary(fit3)$coefficients[2,4])
)
colnames(gene_stats) <- c("Gene","Intercept","Parkinson's","pvalue")
gene_stats$padj <- p.adjust(gene_stats$pvalue,method="fdr")
gene_stats
```

```r
fit1 <- lm(`Gene 1` ~ `Disease Status`, data=gene_exp)
fit2 <- lm(`Gene 2` ~ `Disease Status`, data=gene_exp)
fit3 <- lm(`Gene 3` ~ `Disease Status`, data=gene_exp)

gene_stats <- bind_rows(
  c("Gene 1",coefficients(fit1),summary(fit1)$coefficients[2,4]),
  c("Gene 2",coefficients(fit2),summary(fit2)$coefficients[2,4]),
  c("Gene 3",coefficients(fit3),summary(fit3)$coefficients[2,4])
)
colnames(gene_stats) <- c("Gene","Intercept","Parkinson's","pvalue")
gene_stats$padj <- p.adjust(gene_stats$pvalue,method="fdr")
gene_stats
```

```
## # A tibble: 3 x 5
##   Gene   Intercept        `Parkinson's`     pvalue                   padj
##   <chr>  <chr>            <chr>             <chr>                   <dbl>
## 1 Gene 1 250.410735540151 6.95982194659045  3.92131170913765e-06 3.92e- 6
## 2 Gene 2 597.763814010886 -93.6291659250254 2.37368730897756e-13 3.56e-13
## 3 Gene 3 334.57788193953  164.094204856583  1.72774902843408e-66 5.18e-66
```

We have just performed our first elementary differential expression analysis
using a linear model. Specifically, we examined each of our genes for a
statistical relationship with having Parkinson's disease. The mechanics of this
analysis are beyond the scope of this book, but later when we consider
differential expression analysis packages in [chapter
7](#biology-bioinformatics) this pattern should be familiar after this example.

::: {.box .note}
If you are familiar with [logistic
regression](https://en.wikipedia.org/wiki/Logistic_regression), you might have
wondered why we didn't model disease status, which is a binary variable, as a
function of gene expression like `Disease Status ~ Gene 3`. There are several
reasons for this, [complete
separation](https://en.wikipedia.org/wiki/Separation_(statistics)) principally
among them. In logistic regression, complete separation occurs when all the
predictor values (e.g. gene expression) for one outcome group are greater or
smaller than the other; i.e. there is no overlap in the values between groups.
This causes logistic regression to fail to converge, leaving these genes with no
statistics even though these genes are potentially the most interesting! There
are methods (e.g. [Firth's Logistic
Regression](https://cran.r-project.org/web/packages/logistf/index.html)) that
overcome this problem, but methods that model gene expression as a function of
other outcome variables were developed first and remain the most popular.
:::

::: {.box .readmore}
[Modeling Basics - R for Data Science](https://r4ds.had.co.nz/model-basics.html)
[Least squares regression](https://en.wikipedia.org/wiki/Least_squares)
:::

### Flavors of Linear Models

The linear model implemented above is termed [linear
regression](https://en.wikipedia.org/wiki/Linear_regression) due to the way it
models the relationship between the predictor variables and the outcome.
Specifically, linear regression makes some strong assumptions about that
relationship that may not always hold for all datasets. To address this
limitation, a more flexible class of linear models called [generalized linear
models](https://en.wikipedia.org/wiki/Generalized_linear_model) that allow these
assumptions to be relaxed by changing the mathematical relationship between the
predictors and outcome using a [link
function](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function),
and/or by mathematically transforming the predictors themselves. Some of the
more common generalized linear models are listed below:

* **Logistic regression** - models a binary outcome (e.g. disease vs control) as
a linear combination of predictor variables by using the [logistic
function](https://en.wikipedia.org/wiki/Logistic_function) as the link function.
* **Multinomial regression** - models a multinomial (i.e. categorical variable
with more than two categories) using a [multinomial logit function](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) link
function
* **Poisson regression** - models an outcome variable that are count data as
a linear combination of predictors using the logarithm as the link function;
this model is commonly used when modeling certain types of high throughput
sequencing data
* **Negative binomial regression** - also models an outcome variable that are
count data but relaxes some of the assumptions of Poisson regression, namely the
mean-variance equality constraint; negative binomial regression is commonly used
to model gene expression estimated from RNASeq data.

Generalized linear models are very flexible, and there are many other types of
these models used in biology and data science. It is important to be aware of
the characteristics of the outcome you wish to model and choose the modeling
method that is most suitable.

::: {.box .readmore}
[Generalized linear models](https://en.wikipedia.org/wiki/Generalized_linear_model)
:::

### Assessing Model Accuracy .


## Statistical Distributions & Tests .

### Statistical Distributions

By definition, a distribution is a function that shows 
**the possible values for a variable and how often they occur**. 
Generally, they are divided into two categories: discrete distributions and 
continuous distributions.  

#### Discrete distributions

#### Bernoulli random trail and more  
One of the examples for discrete random variable distribution is the **Bernoulli** 
function. A Bernoulli trail has only 2 outcomes with probability $p$ and $(1-p)$. 
Consider flipping a fair coin, and the random variable X can take value 0 or 1 
indicating you get a head or a tail. If it's a fair coin, we would expect the 
Pr{ X = 0 } = Pr{ X = 1 } = 0.5. Or, if we throw a die and we record X = 1 when
we get a six, and X = 0 otherwise, then Pr{ X = 0 } = 5/6 and Pr{ X = 1 } = 1/6.  

Now consider a slightly complicated situation: what if we are throwing the die 
$n$ times and we would like to analyze the total number of six, say $x$, we get 
during those $n$ throws? Now, this leads us to the **binomial distribution**. 
If it's a fair die, we would say our proportion parameter p = 1/6, which means 
the probability we are getting a six is 1/6 for each throw.  

$$\begin{equation}
f(x) = \frac {n!} {x!(n-x)!}p^x (1-p) ^{(n-x)}
\end{equation}$$

The **geometric** random variable, similar to the binomial, is also from a 
sequence of random Bernoulli trials with a constant probability parameter p. 
But this time, we define the random variable X as the number of consecutive 
failures before the first success. In this case, the probability of x 
consecutive failures followed by success on trial x+1 is:  

$$\begin{equation}
f(x) = p * (1-p)^x
\end{equation}$$

The **negative binomial** distribution goes one step forward. This time, we are 
still performing a sequence of independent Bernoulli random trials with a 
constant probability of success equal to p. But now we would like to record the 
random variable Z to be the total number of failures before we finally get to 
the $r^{th}$ success. In other words, when we get to the $r^{th}$ success, we 
had x+r Bernoulli random trails, in which x times failed and r times succeeded.  

$$\begin{equation}
f(x) = \frac {x+r-1} {r-1} p^r {(1-p)}^x
\end{equation}$$

#### Poisson  
The Poisson distribution is used to express the probability of a given number 
of events occurring in a fixed interval of time or space, and these events 
occur with a known constant mean rate and independently of the time since 
the last event. ~~But no one understands this definition.~~  

The formula for Poisson distribution is:  
$$\begin{equation}
f(k; \lambda) = Pr(X=k) = \frac {\lambda^k e^{-\lambda}} {k!} 
\end{equation}$$

- lambda is the expected value of the random variable X  
- k is the number of occurrences  
- e is Euler's number (e=2.71828)  
~~okay, the formula makes it even more confusing.~~  

Imagine you are working at a mail reception center, and your responsibility 
is to receive incoming letters. Assume the number of incoming letters is not 
affected by the day of the week or season of the year. You are expected to get 
20 letters on average in a day. But, the actual number of letters you receive 
each day will not be perfectly 20.  
You recorded the number of letters you receive each day in a month (30 days). 
In the following plot, each dot represents a day. The x-axis is calender day, 
and y-axis is the number of letters you receive on that day. Although on average 
you are receiving 20 letters each day, the actual number of letters each day vary a lot. 

```{r}
set.seed(2)
my_letter <- rpois(n = 30, lambda = 20)
plot(my_letter,
  main = "Letters received each day",
  xlab = "day of the month", ylab = "number of letters",
  pch = 19, col = "royalblue"
)
abline(a = 20, b = 0, lwd = 2, lty = 3, col = "salmon")
```

Now, let's plot the density plot of our data. The x-axis is the number of 
letters on a single day, and the y-axis is the probability.  
```{r}
plot(density(my_letter),
  lwd = 2, col = "royalblue",
  main = "Probability of number of letters each day",
  xlab = "number of letters"
)
```

Since we only have 30 data points, it doesn't look like a good curve. But, 
after we worked at the mail reception for 5000 days, it becomes much closer 
to the theoretical Poisson distribution with lambda = 20.  

```{r}
set.seed(3)
plot(density(rpois(n = 5000, lambda = 20)),
  lwd = 2, col = "royalblue",
  main = "Probability of number of letters each day",
  xlab = "number of letters"
)
```

Here is the theoretical Poisson distribution with lambda = 20.  
```{r}
plot(dpois(c(1:40), lambda = 20),
  lwd = 2, type = "l", col = "royalblue",
  ylab = "probability", main = "Poisson lambda=20"
)
```

If we want to know what's the probability to receive, for example, 18 letters,  
we can use `dpois()` function. 
```{r}
dpois(x = 18, lambda = 20)
```

If we want to know the probability of receiving 18 or less letters, 
use `ppois()` function. 
```{r}
ppois(q = 18, lambda = 20, lower.tail = T)
```
 
It is the cumulative area colored in the following plot:  
```{r}
plot(dpois(c(1:40), lambda = 20),
  lwd = 2, type = "l", col = "royalblue",
  ylab = "probability", main = "Poisson lambda=20"
)
polygon(
  x = c(1:18, 18),
  y = c(dpois(c(1:18), lambda = 20), 0),
  border = "royalblue", col = "lightblue1"
)
segments(x0 = 18, y0 = 0, y1 = 0.08439355, lwd = 2, lty = 2, col = "salmon")
```

`qpois()` is like the "reverse" of `ppois()`.  
```{r}
qpois(p = 0.3814219, lambda = 20)
```


Let's review the definition of Poisson distribution.  
- "these events occur in a fixed interval of time or space", which is a day;  
- "these events occur with a known constant mean rate", which is 20 letters;  
- "independently of the time since the last event", which means the number of 
letters you receive today is independent of the letter you receive tomorrow. 
~~more work today don't guarantee less work tomorrow, just like in real life~~  

Now let's re-visit the formula.  
$$\begin{equation}
f(k; \lambda) = Pr(X=k) = \frac {\lambda^k e^{-\lambda}} {k!} 
\end{equation}$$

- lambda is the expected value of X, which is 20 letters in this example.  
- k is the number of occurrences, which is the number of letters you get on a specific day.  
- e is Euler's number (e=2.71828)  


#### Continuous distributions
There are a number of different continuous distributions. Here we will focus 
on the **Normal distribution (Gaussian distribution)**. A lot of variables in 
real life are normally distributed, common examples include people's height, 
blood pressure, and students' exam score.  

This is what a normal distribution with mean = 0 and standard deviation = 1 looks like:  
```{r}
set.seed(2)
norm <- rnorm(n = 50000, mean = 0, sd = 1)
plot(density(norm),
  main = "A Normal distribution", xlab = "x",
  lwd = 2, col = "royalblue"
)
```


Similar as the Poisson distribution above, there are several functions to work 
with normal distribution, including `rnorm()`, `dnorm()`, `pnorm()`, and `qnorm()`.  

`rnorm()` is used to draw random data points from a normal distribution with 
a given mean and standard deviation. 
```{r}
set.seed(2)
rnorm(
  n = 6, # number of data points to draw
  mean = 0, # mean
  sd = 1
) # standard deviation
```

`dnorm()` is the density at a given quantile. For instance, in the normal 
distribution (mean=0, sd=1) above, the probability density at 0.5 is roughly 0.35. 
```{r}
dnorm(x = 0.5, mean = 0, sd = 1)
```

```{r}
plot(density(norm),
  main = "A Normal distribution", xlab = "x",
  lwd = 2, col = "royalblue"
)
segments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 3, col = "salmon")
segments(x0 = -5, y0 = 0.3520653, x1 = 0.5, lwd = 2, lty = 3, col = "salmon")
points(x = 0.5, y = 0.3520653, cex = 1.5, lwd = 2, col = "red")
text(x = 1.1, y = 0.36, labels = "0.3521", col = "red2")
```


`pnorm()` gives the distribution function. Or, you can think of it as the 
cumulative of the left side of the density function until a given value, 
which is the area colored in light blue in the following plot.  

```{r}
pnorm(q = 0.5, mean = 0, sd = 1)
```

```{r}
plot(density(norm),
  main = "A Normal distribution", xlab = "x",
  lwd = 2, col = "royalblue"
)
polygon(
  x = c(density(norm)$x[density(norm)$x <= 0.5], 0.5),
  y = c(density(norm)$y[density(norm)$x <= 0.5], 0),
  border = "royalblue", col = "lightblue1"
)
segments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 2, col = "salmon")
```

`qnorm()` gives the quantile function. 
You can think of it as the "reverse" of `pnorm()`. For instance:  
```{r}
pnorm(q = 0.5, mean = 0, sd = 1)
qnorm(p = 0.6914625, mean = 0, sd = 1)
```

Similarly, other distributions such as chi-square distribution, they all have 
the same set of functions: `rchisq()`, `dchisq()`, `pchisq()` and `qchisq()` etc.  


### Statistical Tests .

What is a statistical test, and when is it appropriate to run one? How do we
know which tests are appropriate in a given situation? What is the difference
between a parametric and non-parametric test? What are some common tests (e.g.
t-test, Chi-sq, hypergeometric, etc), and how do we run them in R?

### p-values .

What is a p-value? What is the relationship between a p-value and a distribution?
What does a p-value look like when plotted with a distribution? How do we
interpret p-values? How do we compute a p-value using a distribution and a
statistic using R? (Should we mention critical values?)

### Multiple Hypothesis Testing .

What is multiply hypothesis testing adjustment? Why is it important? What are
some common adjustment methods (Bonferroni (FWER) and Benjamini-Hochberg (FDR))?
How do we interpret adjusted p-values (depends on the adjustment method)? How
do we compute adjusted p-values in R?

### Statistical power .

Conceptually, what is statistical power, and what does it mean? Why is it
important? What is the relationship between a dataset, an analysis, and power?
What aspects of an analysis influence the statistical power of a test? (I don't
think it's reasonable to ask students to perform power calculations, just to be
aware that power is a thing and generally what it is).

## Cluster Analysis

Cluster analysis or simple clustering is the task of grouping objects together
such that objects within the same group, or *cluster*, are more similar to each
other than to objects in other clusters. It is a type of [exploratory data
analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) that seeks to
identify structure or organization in a dataset without making strong
assumptions about the data or testing a specific hypothesis. Many different
[clustering algorithms](https://en.wikipedia.org/wiki/Category:Cluster_analysis_algorithms)
have been developed that employ different computational strategies and are
designed for data with different types and properties. The choice of algorithm
is often dependent upon not only the type of data being clustered but also the
comparative performance of different clustering algorithms applied to the same
data. Different algorithms may identify different types of patterns, and so no
one clustering method is better than any other in general.

The goal of clustering may be easily illustrated with the following plot:

```{r include=FALSE}
# data drawn from three bivariate normal distributions
set.seed(1337)
n <- 100
well_clustered_data <- tibble(
  ID=c(stringr::str_c("A",1:n),stringr::str_c("B",1:n),stringr::str_c("C",1:n)),
  cluster=c(rep("A",n),rep("B",n),rep("C",n)),
  f1=c(rnorm(n,0,1),rnorm(n,5,1),rnorm(n,0,1)),
  f2=c(rnorm(n,0,1),rnorm(n,5,1),rnorm(n,8,1))
)
```

```{r clustering goal}
unclustered <- ggplot(well_clustered_data, aes(x=f1,y=f2)) +
  geom_point() +
  labs(title="Unclustered")
clustered <- ggplot(well_clustered_data, aes(x=f1,y=f2,color=cluster,shape=cluster)) +
  geom_point() +
  labs(title="Clustered")
unclustered | clustered
```

That is, the goal of clustering is to take unlabeled data that has some
structure like on the left and label data points that are similar to one another
to be in the same cluster. A full treatment of cluster analysis is beyond the
scope of this book, but below we discuss a few of the most common and general
clustering algorithms used in biology and bioinformatics.

### Hierarchical Clustering .

Hierarchical clustering is a form of clustering that clusters data points
together in nested, or *hierarchical*, groups based on their dissimilarity from
one another. There are two broad strategies to accomplish this:

* **Agglomerative**: all data points start in their own groups, and groups are
iteratively merged hierarchically into larger groups until all data points have
been added to a group
* **Divisive**: all data points start in the same group, and are recursively
split into smaller groups based on their dissimilarity




### k-means .

### Others

## Network Analysis .

What is a network? What is network analysis? What are some example network
analysis applications in biology and bioinformatics (there is a full ksection on
[Biological Networks] in the bio chapter, these are just illustrative examples)?
How do we represent networks in R, and how do we analyze them? ([Network
visualization] is covered in the data viz chapter, might be helpful to write
these two sections together)
