[["index.html", "BF591 - R for Biological Sciences Syllabus", " BF591 - R for Biological Sciences Syllabus Semester: Spring 2022 Location: EPC 209 and zoom Time: M/W 8:00AM - 9:45AM Contents: Course Schedule Instructors Office Hours Course Values and Policies This course introduces the R programming language through the lens of practitioners in the biological sciences, particularly biology and bioinformatics. Key concepts and patterns of the language are covered, including: RStudio Data wrangling with tidyverse Data visualization with ggplot Essential biological data shapes and formats Core bioconductor packages Basic data exploration, including elementary statistical modeling and summarization Elementary Data Science concepts Toolifying R scripts Communicating R code and results with RMarkdown Buidling R packages and unit testing strategies Building interactive tools with RShiny About 1/3 of the materials are inspired by the online textbook R for Data Science, while the rest has been developed by practicing bioinformaticians based on their experiences. Weekly programming assignments will help students apply these techniques to realistic problems involving analysis and visualization of biological data. Students will be introduced to a unit testing paradigm that will help them write correct code and deposit all their code into github for evaluation. Students will implement an end-to-end project that begins with one of a set of provided datasets, implements a set of data summarization and exploration operations on it, and allows interaction with an RShiny app. The course materials are aligned with BF528 Applications in Translational Bioinformatics and are intended to be taken in tandem, but the materials also stand alone as an independent class. "],["course-schedule.html", "Course Schedule", " Course Schedule Key: bold - class held over zoom struckout - class not held, reserved for project work Follow Week N links for detailed list of topic sections. Assignments are assigned and due on the first day of class each week unless mentioned otherwise Week Dates Topics Assignment Week 0 Prior to class Github configuration [Assignment 0] Week 1 1/24 &amp; 1/26 Preliminaries Data in Biology R Programming Basics Data Wrangling &amp; tidyverse Basics Assignment 1 Week 2 1/31 &amp; 2/2 R in Biology Bio: Bioconductor Basics Data Viz: Grammar of Graphics Assignment 1 Week 3 2/7 &amp; 2/9 Data Sci: Data Modeling Bio: Gene Expression Bio: Microarrays Bio: Differential Expression: Microarrays Assignment 2 Week 4 2/14 &amp; 2/16 Data Sci: PCA &amp; Clustering Data Vis: Heatmaps &amp; Dendrograms R Programming: Structures &amp; Iteration Assignment 3 Week 5 2/22 &amp; 2/23 Note: 2/22 is a Tuesday Bio: High Throughput Sequencing Bio: RNASeq Bio: Count Data Bio: Differential Expression: RNASeq Bio: Gene Set Enrichment Analysis Assignment 4 Week 6 2/28 &amp; 3/2 Data Sci: Distributions Data Sci: Statistical Tests R Programming: Style &amp; Conventions Assignment 5 3/7 &amp; 3/9 Spring Break Week 7 3/14 &amp; 3/16 Data Viz: Responsible Plotting [Assignment 6] Week 8 3/21 &amp; 3/23 RShiny [Assignment 7] Week 9 3/28 &amp; 3/30 Bio: Single Cell Sequencing [Assignment 7] Week 10 4/4 &amp; 4/6 Bio: Biological Networks Data Sci: Network Analysis Data Viz: Network Viz [Project] Week 11 4/11 &amp; 4/13 Toolification R Programming: Parallel Processing R Programming: Pipelines &amp; Workflows Week 12 4/20 EngineeRing: Object Orientation EngineeRing: Building R Packages Week 13 4/25 &amp; 4/27 Communicating with R Reproducible Analyses in R Week 14 5/2 &amp; 5/4 No class - Project work 5/9 First day of exam period Project due "],["instructors.html", "Instructors", " Instructors Primary instructor: Adam Labadorf (labadorf AT bu DOT edu) We have three staff members helping with the course, in no particular order (ok, I guess its alphabetical): Taylor Falk (BU BF MS Alumnus 21) is a Bioinformatics Developer working with the VA PTSD Brain Bank, developing infrastructure to support data generated out of our brains. He is championing the assignment strategy and will be available to help with issues. Mae Rose Gott (BU BF MS Alumna 21) is a Research Staff member working on a number of different projects across many different areas. She will be helping organize the course materials as we go forward. Vanessa Li (BU BF PhD Candidate) is a PhD candidate in Dr. Stefano Montis lab in Computational Biomedicine. She will primarily be helping with grading of your assignments. Joey Orofino (BU BF MS Alumnus 18) is a Research Scientist working on identifying small RNA based biomarkers of Parkinsons Disease "],["office-hours.html", "Office Hours", " Office Hours We hold the following office hours via Zoom: Joey - Mondays 3:30-4:30 Vanessa - Tuesdays 10-11AM Mae Rose - Wednesdays 8-9PM Taylor - Fridays 2-3PM The zoom link is pinned in the #announcements channel in slack. "],["course-values-and-policies.html", "Course Values and Policies", " Course Values and Policies Everyone is welcome. Every background, race, color, creed, religion, ethnic origin, age, sex, sexual orientation, gender identity, nationality is welcome and celebrated in this course. Everyone deserves respect, patience, and kindness. Disrespectful language, discrimination, or harassment of any kind are not tolerated, and may result in removal from class or the University. This is not merely BU policy. The instructors deem these principles to be inviolable human rights. Students should feel safe reporting any and all instances of discrimination or harassment to the instructor, to any of the Bioinformatics Program leadership, or the BU Equal Opportunity Office. Everyone brings value. Each of us brings unique experiences, skills, and creativity to this course. Our diversity is our greatest asset. Collaboration is highly encouraged. All students are encouraged to work together and seek out any and all available resources when completing projects in all aspects of the course, including sharing coding ideas and strategies with each other as well as those found on the internet. Any and all available resources may be brought to bear. However, consistent with BU policy, the bulk of your code and your final reports should be written in your own words and represent your own work and understanding of the material. Copying/pasting large sections of code is not acceptable and will be investigated as cheating (we check). A safe space for dissent. For complex topics such as those covered in this class, there is seldom one correct answer, approach, or solution. Disagreement fosters innovation. All in the course, including students and TAs, are encouraged to express constructive criticism and alternative ideas on any aspect of the content. We are always learning. Our knowledge and understanding is always incomplete. Even experts are fallible. The bioinformatics field evolves rapidly, and Rome was not built in a day. Be kind to yourself and to others. You are always smarter and more knowledgable today than you were yesterday. "],["introduction.html", "1 Introduction", " 1 Introduction Since the publication of the first draft human genome in 2001, data has driven biological discovery at an exponential rate. Rapid technological innovations in data-generating biochemical instruments, computational resource availability, data storage, and analytical approaches including artificial intelligence, machine learning, and data science more generally have combined and synergized to enable advances in our understanding of biological systems by orders of magnitude. As the rate of development of these technologies has increased, so are practitioners of biological inquiry expected to keep up with the rapidly expanding set of knowledge, skills, and tools required to use them. Modern biological data analysis entails combining knowledge and skills from many domains, including not only biological concepts like molecular biology, genetics, genomics, and biochemistry, but also in computational and quantitative skills including statistics, mathematics, programming and software engineering, high performance and cloud computing, data visualization, and computer science. No one person can be expert in all of these areas, but modern software tools and packages made available by subject matter experts enable us to perform cutting edge analysis with a conceptual understanding of the topics. One such tool is the R programming language, a statistical programming language and environment specifically designed to run statistical analyses and visualize data. Today R is one of the two most popular programming languages in biological data analysis and bioinformatics (the other being python). A major innovation in the R language came with the introduction of the tidyverse, as set of open-source data manipulation and visualization packages, first developed by Hadley Wickham and now improved, supported, and maintained by his team of data scientists and software engineers and other individuals. The tidyverse is a collection of packages that specialize in different aspects of data manipulation with the goal of enabling powerful, consistent, and accurate data operations in the broad field of data science. While not changing the structure of the language per se, the tidyverse packages define a set of consistent programming conventions and patterns that are tailored to the types of manipulations required to make data tidy and, therefore, easier and more consistent to work with. The tidyverse therefore is something of its own language that is compatible with but distinct in convention from the base R language. This book and accompanying course focus on how to use R and its related package ecosystems to analyze, visualize, and communicate biological data analyses. As noted above, effective biological data analysis employs skills that span several knowledge domains. This book covers many of these topics in relatively shallow depth, but with the intent of presenting just enough in each to enable the learner to become proficient in most day-to-day biological analysis tasks. "],["who-this-book-is-for.html", "1.1 Who This Book Is For", " 1.1 Who This Book Is For This book was written for the practicing biologist wishing to learn how to use R to analyze biological data. A basic working knowledge of genetics, genomics, molecular biology, and biochemistry is assumed, but we endeavored to include enough pertinent background to understand the analysis concepts presented in the text. Basic knowledge of statistics is assumed, but again some background is provided as necessary to understand the analyses and concepts in the text. No further knowledge is assumed. "],["a-note-about-reinventing-the-wheel.html", "1.2 A Note About Reinventing the Wheel", " 1.2 A Note About Reinventing the Wheel Many topics in this book are covered elsewhere in greater detail and depth. The content in each section is intended to stand alone, but may not provide a high level of detail that has been done better by others in online materials. These sections provide links to these other resources that provide more information, in case the instructions in this book are too terse or unclear. Wikipedia - Reinventing the Wheel "],["sources-and-references.html", "1.3 Sources and References", " 1.3 Sources and References The materials of this book were inspired and informed by a large number of sources, including books and freely available online materials. The authors would like to thank the creators and maintainers of these resources for their generosity in making their valuable contributions: R Materials * Hands-On Programming with R, by Garrett Grolemund * R for Data Science, by Hadley Wickam, Garrett Grolemund, et al * Advanced R, by Hadley Wickam * STAT 545 - Data wrangling, exploration, and analysis with R * What They Forgot to Teach You About R * Reproducible Analysis with R, by State of Alaskas Salmon and People Project, NCEAS * Data Science for Psychologists, by Hansjörg Neth Data visualization * How Charts Lie: Getting Smarter about Visual Information, by Alberto Cairo * The Functional Art - An Introduction to Information Graphics and Visualization, by Alberto Cairo * The Truthful Art - Data, Charts, and Maps for Communication, by Alberto Cairo "],["data-bio.html", "2 Data in Biology ", " 2 Data in Biology "],["a-brief-history-of-data-in-molecular-biology.html", "2.1 A Brief History of Data in Molecular Biology", " 2.1 A Brief History of Data in Molecular Biology Molecular biology became a data science in 1953 when the structure of DNA was determined. Prior to this advance, biochemical assays of biological systems could make general statements about the characteristics and composition of biological macromolecules (e.g. there are two types of nucleic acids - those made of ribose (RNA) and deoxyribose (DNA)) and some quantitative statements about those compositions (e.g. there are roughly equal concentrations of purines - adenine, guanine - and pyrimidines - cytosine, thymine - in any single chromosome). However, once it was shown that each nucleic acid molecule had a specific (and eventually measurable) sequence, this opened the possibility of defining the genetic signature of every living thing on Earth which, in principle, would enable us to understand how life works from its most basic components. A tantalizing prospect, to say the least. It is perhaps a happy coincidence that our computational and data storage technologies began developing around the same time these molecular biology advances were being made. While mechanical computers had existed for more than a hundred years and arguably longer, the first modern computer, the Atanasoff-Berry Computer (ABC), was invented by John Vincent Atanasoff and Clifford Berry in 1942 at what is now Iowa State University. Over the following decades, the speed, sophistication, and reliability of computing machines increased exponentially, enabling ever larger and faster computations to be performed. The development of computational capabilities necessitated technologies that stored information that these machines could use, both for instructions to tell the computers what operations they should perform and data they should use to perform them. Until the 1950s, the most commonly available mechanical data storage technologies like writing, phonographic cylinders and disks (a.k.a. records), and punch cards were impractical or unsuitable to create and store the amount of data needed by these computers. A newer technology, magnetic storage, originally proposed in 1888 by Oberlin Smith quickly became the standard way digital computers read and stored information. With these technological advances, the second half of the 20th century saw rapid advances in our ability to determine and study the properties and function of biological molecular sequences, primarily DNA and RNA (although the first biological sequences scientists determined were proteins composed of amino acids using methods independently invented by Frederick Sanger and Pehr Edman). In 1970, Pauline Hogeweg and Ben Hesper defined the new discipline of bioinformatics as the study of informatic processes in biotic systems (in fact, the original term was proposed in Dutch, Hogeweg and Hespers native language). This early form of bioinformatics was a subfield of theoretical biology, studied by those who recognized that biological systems, much like our computer systems, can be viewed as information storage and processing systems themselves. This broad definition of bioinformatics began narrowing in practice to the study of genetic information as the amount of molecular sequence data we collected grew. By the early 1980s, biological sequence data stored on magnetic tape were being created and studied using new pattern recognition algorithms on computers, which were becoming more widely available at academic and research institutions. At the same time, the idea of determining the complete sequence of the human genome was born, leading to the inception of the The Human Genome Project and the present modern post genomic era. Biological Data Timeline - Setting the Stage "],["biology-as-a-mature-data-science.html", "2.2 Biology as a Mature Data Science", " 2.2 Biology as a Mature Data Science The completion of the first draft human genome ushered in a revolution in how we understand ourselves as humans, from our evolutionary history, our ancestry, our traits, and our health. It provided fundamentally new and empirical tools and approaches to human genetic and biomedical research, and the technologies and techniques that were developed in the completion of the draft sequence formed the foundation for genetic research in non-human systems as well. Biological Data Timeline - Human Genome Era While the focus of the human genome project was on determining the DNA sequence of the human genome, this sequence and the technologies used to ascertain it provide us with opportunities to learn many other properties of genomes and biological systems by analyzing the data with different approaches. For example, knowing the complete sequence of a genome also provides information on the number of genes it contains, how repetitive the sequence is, and when combined with genetic sequences of other individuals or organisms, how closely related genes or even organisms as a whole are. Thanks to the central dogma of molecular biology, the gene sequences also give us information about the intermediate RNA molecule and resultant proteins encoded by a genome, creating opportunities for new ideas, hypotheses, experiments, and even new data-generating assays and approaches. These advances are causing exponential growth of different types of biological data and its volume, necessitating ever more powerful and sophisticated computational resources and analytical methods with no signs of slowing. The biochemical instruments used to produce these data are continually improving the precision, accuracy, throughput, and cost of their output and operations. The Biologists Tools "],["preliminaries.html", "3 Preliminaries ", " 3 Preliminaries "],["prelim-r.html", "3.1 The R Language", " 3.1 The R Language R is a free programming language and environment where that language can be used. More specifically, R is a statistical programming language, designed for the express and exclusive purpose of conducting statistical analyses and visualizing data. Said differently, R is not a general purpose programming language unlike other languages such as python, Java, C, etc. As such, the languages real strengths are in the manipulation, analysis, and visualization of data and statistical procedures, though it is often used for other purposes (for example, web applications with RShiny, or writing books like this one with bookdown). You may download R for free from the Comprehensive R Archive Network. The effective biological analysis practitioner knows how to use multiple tools for their appropriate purposes. The most common programming languages in this field are python, R, and scripting languages like The Bourne Again Shell - bash. While it is beyond the scope of this book to cover which tools are best used where, R is appropriate wherever data analysis and visualization are needed. Any operations that do not involve these aspects (e.g. manipulating text files, programming web servers, etc) are likely more suitable for other languages and software. R Project Home Page Hands-On Programming with R - Installing R and RStudio Section on R from R for Data Science "],["prelim-rstudio.html", "3.2 RStudio", " 3.2 RStudio This course assumes the learner is using the RStudio software platform for all analyses, unless otherwise noted. RStudio is a freely available and fully featured integrated development environment (IDE) for R, and has many convenient capabilities when learning R. RStudio may be downloaded and installed for free from the site above. All the examples and instructions in this book assume you have installed R are using RStudio. Be sure to turn off automatic environment saving in RStudio! Because this is so important, here it is again: By default, RStudio preserves your R environment when you shut it down and restores it when you start it again. This is very bad practice! The state of your R environment, which includes the values stored in variables, the R packages loaded, etc. from previously executed code is transient and may not reflect the results your code produces when run alone. Open the Tools &gt; Global Options menu and: Uncheck Restore .RData into workspace at startup Set Save workspace to .RData on exit: to Never Never save workspace The book R for Data Science has an excellent chapter on why this is a problem and how to change the RStudio setting to avoid it. Hands-On Programming with R - Installing R and RStudio RStudio Education - Beginners Guide ModernDrive R Series - Getting Started R for Data Science - What is real, and where does your analysis live? What They Forgot To Teach You About R - Always start R with a blank slate R for Data Science - RStudio "],["prog-the-r-script.html", "3.3 The R Script", " 3.3 The R Script Before we cover the R language itself, we should talk about how you should run your code and where it should live. As mentioned, R is both a programming language and an environment where you can run code written in that language. The environment is a program (confusingly also called R) that allows you to interact with it and run simple lines of code one at a time. This environment is very useful for learning how the language works and troubleshooting, but it is not suitable for recording and running large, complex analyses that require many lines of code. Therefore, all important R code should be written and saved in a file before you run it! The code may not be correct, and the interactive R environment is helpful for debugging and troubleshooting, but as soon as the code works it should be saved to the file and rerun from there. With this in mind, the basic unit of an R analysis is the R script. An R script is a file that contains lines of R code that run sequentially as a unit to complete one or more tasks. Every R script file has a name, which you choose and should be descriptive but concise about what the script does; script.R, do_it.R, and a_script_that_implements_my_very_cool_but_complicated_analysis_and_plots.R are generally poor names for scripts, whereas analyze_gene_expression.R might be more suitable. In RStudio, you can create a new script file in the current directory using the File -&gt; New File -&gt; R Script menu item or the new R Script button at the top of the screen: New R Script Your RStudio configuration should now enable you to write R code into the (currently unsaved) file in the top left portion of the screen (labeled in the figure as File Editor). Basic RStudio Interface You are now nearly ready to start coding in R! How to name files Some useful and advanced tips on how to name files "],["prog-workflow.html", "3.4 The Scripting Workflow", " 3.4 The Scripting Workflow But hold on, were still not quite ready to start coding. As mentioned above, all important R code should be written and saved in a file before you run it! Your scripts will very quickly contain many lines of code that are meant to be run in sequential order. While developing your code it is very helpful to run each individual line separately, building up your script incrementally over time. To illustrate how to do this, we will begin with a simple R code that stores the result of an arithmetic expression to a new variable: # stores the result of 1+1 into a variable named &#39;a&#39; a &lt;- 1+1 The concepts in this line of code will be covered in greater depth later, but for now an intuitive understanding will suffice to explain the development workflow in RStudio. When developing, this is the suggested sequence of operations: Save your file (naming if necessary on the first save) with Ctrl-s on Windows or Cmd-s on Mac Execute the line or lines of code in your script you wish to evaluate using Ctrl-Enter on Windows or Cmd-Enter on Mac. By default only the line with the cursor is executed; you may click and drag with the mouse to select multiple lines to execute if needed. NB: you can press the up arrow key to recall previously run commands on the console. The executed code will be evaluated in the Console window, where you may inspect the result and modify the code if necessary. You may inspect the definitions of any variables you have declared in the Environment tab at the upper right. When you have verified that the code you executed does what you intend, ensure the code in the file you started from is updated appropriately. Go to step 1 The above steps are depicted in the following figure: RStudio workflow Over time, you will gain comfort with this workflow and become more flexible with how you use RStudio. If you followed the instructions above and prevented RStudio from saving your environment when you exit the program (which you should! Did I mention you should?!), none of the results of code you previously ran will be available upon starting a new RStudio session. Although this may seem inconvenient, this is an excellent opportunity to verify that your script in its current state does what you intend for it to do. It is extremely easy to ask R to do things you dont mean for it to do! Rerunning your scripts from the beginning in a new RStudio session is an excellent way to guard against this kind of error. This short page summarizes this very well, you should read it: What They Forgot To Teach You About R - Always start R with a blank slate R for Data Science - Workflow: scripts RStudio IDE cheatsheet (scroll down the page to find the cheatsheet entitled RStudio IDE cheatsheet) "],["prelim-git.html", "3.5 git + github", " 3.5 git + github 3.5.1 Motivation Biological analysis entails writing code, which changes over time as you develop it, gain insight from your data, and identify new questions and hypotheses. A common pattern when developing scripts is to make copies of older code files to preserve them before making new changes to them. While it is a good idea to maintain a record of all the code you previously ran, over time this practice often leads to disorganized, cluttered, untidy analysis directories. For example, say you are working on a script named my_R_script.R and decide you want to add a new analysis that substantially changes the code. You might be tempted to make a copy of the current version of the code into a new file named my_R_script_v2.R that you then make changes to, leaving your original script intact and untouched going forward. You make your changes to your new script, produce some stunning and fascinating plots, present the analysis at a group meeting, only to discover later there was a critical bug in your code that made the plots misleading and requires substantial redevelopment. Bugs happen. There are two types of bugs: Syntax bugs: bugs due to incorrect language usage, which R will tell you about and can (usually) be easily identified and fixed Logic bugs: the code you write is syntactically correct, but does something other than what you intend Bugs are normal. The scenario described above, where you present results only to discover your code wasnt doing what you thought it was doing, is extremely common and it will happen to you. This is normal, and finding a bug in your code does not mean you are a bad programmer. Rather than edit your version 2 of your script directly, you decide it is sensible to copy the file to my_R_script_v2_BAD.R and edit the version 2 script to fix the bug. You are satisfied with your new version 2 script, and so make a new copy my_R_script_v2_final.R. Upon review of your analysis, you are asked to implement new changes to the script based on reviewer feedback. You make a new copy of your script to my_R_script_v2_final_revision.R and make the requested changes. Perhaps now your script is final, but in your directory you now have five different versions of your analysis: my_R_script.R my_R_script_v2.R my_R_script_v2_BAD.R my_R_script_v2_final.R my_R_script_v2_final_revision.R When you write your code, you may know which scripts are which, and if you follow good programming practice and carefully commented your code you or your successors may be able to sleuth what was done. However, as time passes, the intimate knowledge you thought you had about your code will be replaced by other more immediately important things; eventually you may not even understand or even recognize your own code, let alone someone else trying to understand it. Not an ideal situation in any case. A better solution involves recording changes to code over time in such a way that you can recover old code if needed, but dont clutter your analytical workspace with unneeded files. git provides an efficient solution to this problem. 3.5.2 git git is a free, open source version control software program. Version control software is used to track and record changes to code over time, potentially by many developers working on the same software project concurrently from different parts of the world. The base git software can be used on the command line, or with graphical user interface applications for popular operating systems. There are many excellent tutorials online (some linked below) that teach how to use git but the basic concepts are described below. The command line commands are listed, but the same operations apply in the graphical clients. A repository (or repo) is a collection of files in a directory that you have asked git to track (run git init in a new directory) Each file you wish to track must be explicitly added to the repo (run git add &lt;filename&gt; from within the git repo directory) When you modify a tracked file, git will notice those differences and show them to you with git status You may tell git to track the changes to the explicit files that changed (also run git add &lt;filename&gt; to record changes) A set of tracked changes is stored in the repo by making a commit. A commit takes a snapshot of all the tracked files in the repo at the time the commit is made (run git commit -c &lt;commit message&gt; with a concise commit message that describes what was done) Each commit has a date and time associated with it. The files in the repo can be reset to exactly the state they were in at any commit, thus preserving all previous versions of code. For the vast majority of use cases, the git init, git status, git add, and git commit operations are all you will need to use git effectively. Two more commands, git push and git pull are needed when sharing your code with others as described in the next section. Official git tutorial videos Official git book Git Immersion - a guided tour through git commands DataCamp - Git for data scientists 3.5.3 Git hosting platforms (GitHub) The basic git software only works on your local computer with local repositories. To share this code with others, and receive others contributions, a copy of the repo must be made available in a centralized location that everyone can access. One such place is github.com, which is a free web application that hosts git repos. bitbucket.org is another popular free git repo hosting service. These two services are practically the same, so we will focus on GitHub. There is no formal relationship between git and GitHub. git is an open source software project maintained by hundreds of developers around the world (and is hosted on GitHub). GitHub is an independently provided web service and application. The only connection between GitHub and git is that GitHub hosts git repos. As with git, there are many excellent tutorials on how to use GitHub, but the basic concepts are described below. First you must create an account on GitHub if you dont have one already Then, create a new repo on GitHub that you wish to contain your code The next step depends on whether you have an existing local repo or not: If you do not already have a local git repo: Follow the instructions on GitHub to clone your GitHub repo and create a local copy that is connected to the one on GitHub If you already have a local git repo: Follow the instructions on the GitHub to connect your local repo to the GitHub one (this is called adding a remote) Now, your local repo is connected to the same repo on GitHub, and the changes you make to your local files can be sent, or pushed to the repo on GitHub: Make changes to your local files, and git add and git commit them as above Update the remote repo on GitHub by pushing your local commits with git push Running git status will indicate whether your local repo is up to date with your remote GitHub repo When you are working on a team of contributors to a GitHub repo, your local files will become out of date as others push their changes. To ensure your local repo is up to date with the GitHub repo, you must pull your changes from GitHub with git pull. git was designed to automatically combine changes made to a code base by different developers whenever possible. However, if two people make changes to the same parts of the same file, git may not be able to resolve those changes on its own and the developers must communicate and decide what the code should be. These instances are called merge conflicts and can be challenging to resolve. Dealing with merge conflicts is beyond the scope of this book, but some resources are linked below for further reading. All the content and code for this book are stored and available on GitHub, as are the assignment code templates. Official GitHub Tutorial FreeCodeCamp - Git and GitHub For Beginners Official GitHub Tutorial on Merge Conflicts "],["prog-basics.html", "4 R Programming ", " 4 R Programming "],["before-you-begin.html", "4.1 Before you begin", " 4.1 Before you begin If you have not done so already, be sure to follow the R Language, RStudio, and The R Script, and The Scripting Workflow sections before working through this chapter. The guidance in these sections will set you up for success! "],["introduction-1.html", "4.2 Introduction", " 4.2 Introduction As with other subjects covered so far, the basic syntax of R is covered very well in other free online materials. Some of those excellent resources are linked at the end of this section, but a brief overview of the concepts and syntax are covered here. The code examples below can be written into a script and evaluated as described above or entered on the R Console directly and run by pressing Enter. "],["prog-r-syntax.html", "4.3 R Syntax Basics", " 4.3 R Syntax Basics At its core, R (like all programming languages) is basically a fancy calculator. The syntax of most basic arithmetic operations in R should be familiar to you: 1 + 2 # addition [1] 3 3 - 2 # subtraction [1] 1 4 * 2 # multiplication [1] 8 4 / 2 # division [1] 2 1.234 + 2.345 - 3.5*4.9 # numbers can have decimals [1] -13.571 1.234 + (2.345 - 3.5)*4.9 # expressions can contain parentheses [1] -4.4255 2**2 # exponentiation [1] 4 4**(1/2) # square root [1] 2 9**(1/3) # cube root [1] 3 The [1] lines above are the output given by R when the preceding expression is executed. Any portion of a line starting with a # is a comment and ignored by R. R also supports storing values into symbolic placeholders called variables, or objects. An expression like those above can be assigned into a variable with a name using the &lt;- operator: new_var &lt;- 1 + 2 Variables that have been assigned a value can be placed in subsequent expressions anywhere where their value is evaluated: new_var - 2 [1] 1 another_var &lt;- new_var * 4 The correct way to assign a value to a variable in R is with the &lt;- syntax, unlike many other programming languages which use =. However, although the = assignment syntax does work in R: new_var = 2 # works, but is not common convention! this is considered bad practice and may cause confusion later. You should always use the &lt;- syntax when assigning values to variables! In R, the period . does not have a special meaning like it does in many other languages like python, C, javascript, etc. Therefore, new.var is a valid variable name just like new_var, even though it may look strange to those familiar with these other languages. While including . in your R variable names is valid, the results that you will use in programs written in other languages that do have a meaning for this character. Therefore, it is good practice to avoid using . characters in your variable names to reduce the chances of conflicts later. Hands-On Programming with R R for Data Science - Workflow basics "],["prog-types.html", "4.4 Basic Types of Values", " 4.4 Basic Types of Values The most common type of value in R is the number, e.g. 1.0 or 1e-5 for \\(10^{-5}\\). For most practical purposes, R does not distinguish between numbers with fractional parts (e.g. 1.123) and integers (e.g. 1); a number is a number. In addition to numbers, there are some other types of values that are special in R: logical or boolean values - TRUE or FALSE. Internally, R stores TRUE as the number 1 and FALSE as the number 0. Generally, R interprets non-zero numbers as TRUE and 0 as FALSE, but it is good practice to supply the tokens TRUE or FALSE when an argument expects a logical value. missing values - NA. NA is a special value that indicates a value is missing. missing vectors - NULL. Similar to NA, NULL indicates that a vector, rather than a value, is missing. Vectors will be described in the next section on data strutures. factors - Factors are a complex type used in statistical models and are covered in greater detail later infinity - Inf and -Inf. These values encode what R understands to be positive or negative infinity, or any number divided by 0. impossible values - NaN. This value corresponds to the mathematically impossible or undefined value of 0/0. character data - \"value\". R can store character data in the form of strings. Note R does not interpret string values by default, so \"1\" and 1 are distinct. dates and times - R has a basic type to store dates and times (together termed a datetime, which includes both components). Internally, R stores datetimes as the fractional number of days since January 1, 1970, using negative numbers for earlier dates. complex numbers - R can store complex numbers using the complex function. Unsurprisingly, R cannot perform computations on NA, NaN, or Inf values. Each of these values have an infectious quality to them, where if they are mixed in with other values, the result of the computation reverts to the first of these values encountered: # this how to create a vector of 4 values in R x &lt;- c(1,2,3,NA) mean(x) # compute the mean of values that includes NA [1] NA mean(x,na.rm=TRUE) # remove NA values prior to computing mean [1] 2 mean(c(1,2,3,NaN)) [1] NaN mean(c(NA,NaN,1)) [1] NA If your code produces values that are not numbers as you expect, this suggests there are one of these values in your input, and need to be handled explicitly. The difference between NA and NaN in R R for Data Science - Dates and date-times Dates and times in R Complex numbers in R 4.4.1 Factors Factors are objects that R uses to handle categorical variables, i.e. variables that can take one of a distinct set of values for each sample. For example, a variable indicating whether a subject had a disease or was a control could be encoded using a factor with values Disease or Control. Consider an example dataset with six subjects where three are disease and three are control, and we create a factor from a corresponding variable of character strings using the factor() function: case_status &lt;- factor( c(&#39;Disease&#39;,&#39;Disease&#39;,&#39;Disease&#39;, &#39;Control&#39;,&#39;Control&#39;,&#39;Control&#39; ) ) case_status [1] Disease Disease Disease Control Control Control Levels: Control Disease The factor case_status prints as a vector of labels, either Disease or Control. The distinct values in the factor are called levels, and this factor has two: Control and Disease. Internally, a factor is stored as a vector of integers where each level has the same value: as.numeric(case_status) [1] 2 2 2 1 1 1 str(case_status) Factor w/ 2 levels &quot;Control&quot;,&quot;Disease&quot;: 2 2 2 1 1 1 By default, R assigns integers to levels in alphanumeric order; since Control comes lexicographically before Disease, the Control level is assigned the integer 1 and Disease is assigned 2. Each value of the factor corresponds to these integers, and since Disease came before Control, the numeric values of the factor are (2, 2, 2, 1, 1, 1). The integer values assigned to each level allow the factor to be sorted: sort(case_status) [1] Control Control Control Disease Disease Disease Levels: Control Disease Note the order of the factor levels has changed so that controls, which have a value of 1, precede disease, which have a value of 2. The integers assigned to each level can be specified explicitly when creating the factor: case_status &lt;- factor( c(&#39;Disease&#39;,&#39;Disease&#39;,&#39;Disease&#39;,&#39;Control&#39;,&#39;Control&#39;,&#39;Control&#39;), levels=c(&#39;Disease&#39;,&#39;Control&#39;) ) case_status [1] Disease Disease Disease Control Control Control Levels: Control Disease str(case_status) Factor w/ 2 levels &quot;Disease&quot;,&quot;Control&quot;: 1 1 1 2 2 2 The base R functions for reading in CSV Files load columns with character values as factors by default (you may turn this off with stringsAsFactors=FALSE to read.csv()), and in other situations you may have factors created by other functions that need to have their integer values changed. This process is called releveling the factor, and may be accomplished by passing a factor into the factor() function and specifying new levels: str(case_status) Factor w/ 2 levels &quot;Disease&quot;,&quot;Control&quot;: 1 1 1 2 2 2 factor(case_status, levels=c(&quot;Control&quot;,&quot;Disease&quot;)) Factor w/ 2 levels &quot;Control&quot;,&quot;Disease&quot;: 2 2 2 1 1 1 Controlling the order of levels in a factor is important in a number of situations. One is when specifying the reference category for categorical variables when constructing model matrices to pass to statistical models, the details of which are beyond the scope of this book. A second is when the order of categorical variables when passed to ggplot, which is covered in greater detail in [Reordering 1-D Data Elements] in the Grammar of Graphics chapter. The forcats tidyverse package provides more powerful functions for working with categorical variables stored in factors. "],["prog-struct.html", "4.5 Data Structures", " 4.5 Data Structures 4.5.1 Vectors Data structures in R (and other languages) are ways of storing and organizing more than one value together. The most basic data structure in R is a one dimensional sequence of values called a vector: # the c() function creates a vector x &lt;- c(1,2,3) [1] 1 2 3 The vector in R has a special property that all values contained in the vector must have the same type, from the list described above. When constructing a vector, R will coerce values to the most general type if it encounters values of different types: c(1,2,&quot;3&quot;) [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; c(1,2,TRUE,FALSE) [1] 1 2 1 0 c(1,2,NA) # note missing values stay missing [1] 1 2 NA c(&quot;1&quot;,2,NA,NaN) # NA stays, NaN is cast to a character type [1] &quot;1&quot; &quot;2&quot; NA &quot;NaN&quot; In addition to having a single type, vectors also have a length, which is defined as the number of elements in the vector: x &lt;- c(1,2,3) length(x) [1] 3 Internally, R is much more efficient at operating on vectors than individual elements separately. With numeric vectors, you can perform arithmetic operations on vectors of compatible size just as easily as individual values: c(1,2) + c(3,4) [1] 4 6 c(1,2) * c(3,4) [1] 3 8 c(1,2) * c(3,4,5) # operating on vectors of different lengths raises warning, but still works [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length In the example above, we multiplied a vector of length 2 with a vector of length 3: c(1,2) * c(3,4,5) # operating on vectors of different lengths raises warning, but still works [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length Rather than raise an error and aborting, R merely emits a warning message about the vectors not having divisible lengths. So how did R decide the third value should be 5? Because R cycles through each vector and multiplies the values element-wise until the longest vector has had an operation performed on all its values: c(1,2) * c(3,4,5) # yields: 1*3 2*4 1*5 [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length c(1,2) * c(3,4,5,6) # yields: 1*3 2*4 1*5 2*6 [1] 3 8 5 12 R will sometimes work in ways you dont expect. Be careful to read warnings and check that your code does what you expect! 4.5.2 Matrices A matrix in R is simply the 2 dimensional version of a vector. That is, it is a rectangle of values that all have the same type, e.g. number, character, logical, etc. A matrix may be constructed using the vector notation described above and specifying the number of rows and columns the matrix should have, and Instead of having a length like a vector, it has \\(m \\times n\\) dimensions: # create a matrix with two rows and three columns containing integers A &lt;- matrix(c(1,2,3,4,5,6) nrow = 2, ncol = 3, byrow=1 ) A [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 dim(A) # the dim function prints out the dimensions of the matrix, rows first [1] 2 3 Because a matrix is 2 dimensional, it can be transposed from \\(m \\times n\\) to be \\(n \\times m\\) using the t() function: # A defined above as a 2 x 3 matrix t(A) [,1] [,2] [1,] 1 4 [2,] 2 5 [3,] 3 6 dim(t(A)) [1] 3 2 Hands-On Programming with R - Atomic Vectors R for Data Science - Vectors Advanced R - Vectors 4.5.3 Lists and data frames Vectors and matrices have the special property that all items must be of the same type, e.g. numbers. Lists and data frames are data structures that do not have this requirement. Similar to vectors, lists and data frames are both one dimensional sequences of values, but the values can be of mixed types. For instance, the first item of a list may be a vector of numbers, while the second is a vector of character strings. These are the most flexible data structures in R, and are among the most commonly used. Lists can be created using the list() function: my_list &lt;- list( c(1,2,3), c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) my_list [[1]] [1] 1 2 3 [[2]] [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; my_list[[1]] # access the first item of the list [1] 1 2 3 my_list[[2]] # access the second item of the list [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; The arguments passed to list() define the values and their order of the list. In the above example, the list has two elements: one vector of 3 numbers and one vector of 4 character strings. Note you can access individual items of the list using the [[N]] syntax, where N is the 1-based index of the element. Lists can also be defined and indexed by name: my_list &lt;- list( numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) my_list $numbers [1] 1 2 3 $categories [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; my_list$numbers # access the first item of the list [1] 1 2 3 my_list$categories # access the second item of the list [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; The elements of the list have been assigned the names numbers and categories when creating the list, though any valid R identifier names can be used. When elements are associated with names they can be accessed using the list$name syntax. Lists and data frames are the same underlying data structure, however differ in one important respect: the elements of a data frame must all have the same length, while the elements of a list do not. You may create a data frame with the data.frame() function: my_df &lt;- data.frame( # recall &#39;.&#39; has no special meaning in R numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) Error in data.frame(c(1, 2, 3), c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) : arguments imply differing number of rows: 3, 4 my_df &lt;- data.frame( numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) ) my_df numbers categories 1 1 A 2 2 B 3 3 C my_df$numbers [1] 1 2 3 my_df[1] # numeric indexing also works, and returns a subset data frame numbers 1 1 2 2 3 3 my_df[1]$numbers [1] 1 2 3 # this syntax is [&lt;row&gt;,&lt;column&gt;], and if either is omitted return all my_df[,1] # return all rows of the first column as a vector [1] 1 2 3 my_df$categories [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; Note the data frame is printed as a matrix with element names as columns and automatically numbered rows. You may access specific elements of a data frame in a number of ways: my_df$numbers[1] # extract the first value of the numbers column [1] 1 my_df[1,1] # same as above, recall the [&lt;row&gt;,&lt;column&gt;] syntax [1] 1 my_df$categories[3] # extract the third value of the categories column [1] &quot;C&quot; In the examples above, the operation of extracting out different parts of a vector, matrix, list, or data frame is called subsetting. R provides many different ways to subset a data structure and discussing all of them is beyond the scope of this book. However, mastering subsetting will help your code be more concise and correct. See the Read More link on Subsetting below: Advanced R - Subsetting Advanced R - Data Structures Advanced R - Subsetting "],["logical-tests-and-comparators.html", "4.6 Logical Tests and Comparators", " 4.6 Logical Tests and Comparators As mentioned above, R recognizes logical values as a distinct type. R provides all the conventional infix logical operators: 1 == 1 # equality [1] TRUE 1 != 1 # inequality [1] FALSE 1 &lt; 2 # less than [1] TRUE 1 &gt; 2 # greater than [1] FALSE 1 &lt;= 2 # less than or equal to [1] TRUE 1 &gt;= 2 # greater than or equal to These operators also work on vectors, albeit with the same caveats about vector length as noted earlier: x &lt;- c(1,2,3) x == 2 [1] FALSE TRUE FALSE x &lt; 1 [1] FALSE FALSE FALSE x &lt; 3 [1] TRUE TRUE FALSE c(1,2) == c(1,3) [1] TRUE FALSE c(1,2) != c(1,3) [1] FALSE TRUE c(1,2) == c(1,2,3) [1] TRUE TRUE FALSE Warning message: In c(1, 2, 3) == c(1, 2) : longer object length is not a multiple of shorter object length R also provides many functions of the form is.X where X is some type or condition (recall that . is not a special character in R): is.numeric(1) # is the argument numeric? [1] TRUE is.character(1) # is the argument a string? [1] FALSE is.character(&quot;ABC&quot;) [1] TRUE is.numeric(c(1,2,3)) # recall a vector has exactly one type [1] TRUE is.numeric(c(1,2,&quot;3&quot;)) [1] FALSE is.na(c(1,2,NA)) [1] FALSE FALSE TRUE Quick-R - Operators "],["functions.html", "4.7 Functions", " 4.7 Functions Just as a variable is a symbolic representation of a value, a function is a symbolic representation of code. In other words, a function allows you to substitute a short name, e.g. mean, for a set of operations on a given input, e.g. the sum of a set of numbers divided by the number of numbers. R provides a very large number of functions for common operations in its default environment, and more functions are provided by packages you can install separately. Encapsulating many lines of code into a function is useful for (at least) five distinct reasons: Make your code more concise and readable Allow you to avoid writing the same code over and over (i.e. reuse it) Allow you to systematically test pieces of your code to make sure they do what you intend Allow you to share your code easily with others Program using a functional programming style (see note box below) At its core, R is a functional programming language. The details of what this means are outside the scope of this book, but as the name implies this refers to the language being structured around the use of functions. While this property has technical implications on the structure of the language, a more important consequence is the style of programming it entails. The functional programming style (or paradigm) has many advantages, including generally producing programs that are more concise, predictable, provably correct, and performant. provides a good starting point for learning about functional programming. In order to do anything useful, a function must generally be able to accept and execute on different inputs; e.g. the mean function wouldnt be very useful if it didnt accept a value! The terminology used in R and many other programming languages for this is the function must accept or allow you to pass it arguments. In R, functions accept arguments using the following *pattern: # a basic function signature function_name(arg1, arg2) # function accepts exactly 2 arguments Here, arg1 and arg2 are formal arguments or named arguments, indicating this function accepts two arguments. The name of the function (i.e. function_name) and the pattern of arguments it accepts is called the functions signature. Every function has at least one signature, and it is critical to understand it in order to use the function properly. In the above example, arg1 and arg2 are required arguments. This means that the function will not execute without exactly two arguments provided and will raise an error if you try otherwise: mean() # compute the arithmetic mean, but of what? Error in mean.default() : argument &quot;x&quot; is missing, with no default How do you know what arguments a function requires? All functions provided by base R and many other packages include detailed documentation that can be accessed directly through RStudio using either the ? or help(): RStudio - help and function signatures The second signature of the mean function introduces two new types of syntax: Default argument values - e.g. trim = 0. These are formal arguments that have a default value if not provided explicitly. Variable arguments - .... This means the mean function can accept arguments that are not explicitly listed in the signature. This syntax is called dynamic dots. With these definitions, we can now understand the Arguments section of the help documentation: Arguments to the mean function In other words: x is the vector of values (more on this in the next section on data structures) we wish to compute the arithmetic mean for trim is a fraction (i.e. a number between 0 and 0.5) that instructs R to remove a portion of the largest and smallest values from x prior to computing the mean. na.rm is a logical value (i.e. either TRUE or FALSE) that instructs R to remove NA values from x before computing the mean. All function arguments can be specified by name, regardless of whether there is a default value or not. For instance, the following two mean calls are equivalent: # this generates 100 normally distributed samples with mean 0 and standard deviation 1 my_vals &lt;- rnorm(100,mean=0,sd=1) mean(my_vals) [1] -0.05826857 mean(x=my_vals) [1] -0.05826857 To borrow from the Zen of Python, Explicit is better than implicit. Being explicit about which variables are being passed as which arguments will almost always make your code easier to read and more likely to do what you intend. The ... argument catchall can be very dangerous. It allows you to provide arguments to a function that have no meaning, and R will not raise an error. Consider the following call to mean: # this generates 100 normally distributed samples with mean 0 and standard deviation 1 my_vals &lt;- rnorm(100,mean=0,sd=1) mean(x=my_vals,tirm=0.1) [1] -0.05826857 Did you spot the mistake? The trim argument name has been misspelled as tirm, but R did not report an error. Compare the value of mean without the typo: mean(x=my_vals,trim=0.1) [1]-0.02139839 The value we get is different, because R recognizes trim but not tirm and changes its behavior accordingly. Not all functions have the ... catchall in their signatures, but many do and so you must be diligent when supplying arguments to function calls! R for Data Science - Functions rdrr.io - Base R Function Reference This tutorial Advanced R - Functional Programming 4.7.1 DRY: Dont Repeat Yourself Sometimes you will find yourself writing the same code more than once to perform the same operation on different data. For example, one common data transformation is standardization or normalization which entails taking a series of numbers, subtracting the mean of all the numbers from them, and dividing each by the standard deviation of the numbers: # 100 normally distributed samples with mean 20 and standard deviation 10 my_vals &lt;- rnorm(100,mean=20,sd=10) my_vals_norm &lt;- (my_vals - mean(my_vals))/sd(my_vals) mean(my_vals_norm) [1] 0 sd(my_vals_norm) [1] 1 Later in your code, you may need to standardize a different set of values, so you decide to copy and paste your code from above and replace the variable name to reflect the new data: # new samples with mean 40 and standard deviation 5 my_other_vals &lt;- rnorm(100,mean=40,sd=5) my_other_vals_norm &lt;- (my_other_vals - mean(my_other_vals))/sd(my_vals) mean(my_other_vals_norm) [1] 0 sd(my_other_vals_norm) # this should be 1! [1] 0.52351 Notice the mistake? We forgot to change the variable name my_vals to my_other_vals in our pasted code, which produced an incorrect result. Good thing we checked! In general, if you are copying and pasting code from one part of your script to another, you are repeating yourself and have to do a lot of work to be sure you have modified your copy correctly. Copying and pasting code is tempting from an efficiency standpoint, but introduces may opportunities for (often undetected!) errors. Dont Repeat Yourself (DRY) is a principle of software development that emphasizes recognizing and avoiding writing the same code over and over by encapsulating code. In R, this is most easily done with functions. If you notice yourself copying and pasting code, or writing the same pattern of code more than once, this is an excellent opportunity to write your own function and avoid repeating yourself! 4.7.2 Writing your own functions R allows you to define your own function using the following syntax: function_name &lt;- function(arg1, arg2, ...) { # code that does something with arg1, arg2, etc return(some_result) } You define the name of your function, the number of arguments it accepts and their names, and the code within the function, which is also called the function body. Taking the example above, I would define a function named standardize that accepts a vector of numbers, subtracts the mean from all the values, and divides them by the standard deviation: standardize &lt;- function(x) { res &lt;- (x - mean(x))/sd(x) return(res) } my_vals &lt;- rnorm(100,mean=20,sd=10) my_vals_std &lt;- standardize(my_vals) mean(my_vals_std) [1] 0 sd(my_vals_std) [1] 1 my_other_vals &lt;- rnorm(100,mean=40,sd=5) my_other_vals_std &lt;- standardize(my_other_vals) mean(my_other_vals_std) [1] 0 sd(my_other_vals_std) [1] 1 Notice above we are assigning the value of the standardize function to new variables. In R and other languages, the result of a function is returned when the function is called; the value returned is called the return value. The return() function makes it clear what the function is returning. The return() function is not strictly necessary in R; the result of the last line of code in the body of a function is returned by default. However, to again to borrow from the Zen of Python, Explicit is better than implicit. Being explicit about what a function returns by using the return() function will make your code less error prone and easier to understand. 4.7.3 Scope In programming, there is a critically important concept called scope. Every variable and function you define when you program has a scope, which defines where in the rest of your code the variable can be accessed. In R, variables defined outside of a function have universal or top level scope, i.e. they can be accessed from anywhere in your script. However, variables defined inside functions can only be accessed from within that function. For example: x &lt;- 3 multiply_x_by_two &lt;- function() { y &lt;- x*2 # x is not defined as a parameter to the function, but is defined outside the function return(y) } x [1] 3 multiply_x_by_two() [1] 6 y Error: object &#39;y&#39; not found Notice that the variable x is accessible within the function multiply_x_by_two, but the variable y is not accessible outside that function. The reason that x is accessible within the function is that multiply_x_by_two inherits the scope where it is defined, which in this case is the top level scope of your script, which includes x. The scope of y is limited to the body of the function between the { } curly braces defining the function. Accessing variables within functions from outside the functions scope is very bad practice! Functions should be as self contained as possible, and any values they need should be passed as parameters. A better way to write the function above would be as follows: x &lt;- 3 multiply_by_two &lt;- function(x) { y &lt;- x*2 # x here is defined as whatever is passed to the function! y } x [1] 3 multiply_by_two(6) [1] 12 x # the value of x in the outer scope remains the same, because the function scope does not modify it [1] 3 Every variable and function you define is subject to the same scope rules above. Scope is a critical concept to understand when programming, and grasping how it works will make your code more predictable and less error prone. Scope in R "],["iteration.html", "4.8 Iteration", " 4.8 Iteration In programming, iteration refers to stepping sequentially through a set or collection of objects, be it a vector of numbers, the columns of a matrix, etc. In non-functional languages like python, C, etc. there are particular control structures that implement iteration, commonly called loops. If you have worked with these languages, you may be familiar with for and while loops, which are some of these iteration control structures. However, R was designed to execute iteration in a different way than these other languages, and provides two forms of iteration: vectorized operations, and functional programming with apply(). Note that R does have for and while loop support in the language. However, these loop structures often have poor performance, and should generally be avoided in favor of the functional style of iteration described below. How To Avoid For Loops in R If you really, really want to learn how to use for loops in R, read this, but dont say I didnt warn you when your code slows to a crawl for unknown reasons: R for Data Science - for loops 4.8.1 Vectorized operations The simplest form of iteration in R comes in vectorized computation. This sounds fancy, but it just means R intrinsically knows how to perform many operations on vectors and matrices as well as individual values. We have already seen examples of this above when performing arithmetic operations on vectors: x &lt;- c(1,2,3,4,5) x + 3 # add 3 to every element of vector x [1] 4 5 6 7 8 x * x # elementwise multiplication, 1*1 2*2 etc [1] 1 4 9 16 25 x_mat &lt;- matrix(c(1,2,3,4,5,6),nrow=2,ncol=3) x_mat + 3 # add 3 to every element of matrix x_mat [,1] [,2] [,3] [1,] 4 6 8 [2,] 5 7 9 # the * operator always means element-wise x_mat * x_mat [,1] [,2] [,3] [1,] 1 9 25 [2,] 4 16 36 In addition to simple arithmetic operations, R also has syntax for vector-vector, matrix-vector, and matrix-matrix operations, like matrix multiplication and dot products: # the %*% operator stands for matrix multiplication x_mat %*% c(1,2,3) # [ 2x3 ] * [ 3 ] [,1] [1,] 22 [2,] 28 x_mat %*% t(x_mat) # recall t() is the transpose function, making [ 2x3 ] * [ 3x2 ] [,1] [,2] [1,] 35 44 [2,] 44 56 These forms of implicit iteration are very powerful, and the R program has been highly optimized to perform these operations very quickly. If you can cast your iteration into a vector or matrix multiplication, it is a good idea to do so. For other more complex or custom iteration, we must first talk briefly about functional programming. 4.8.2 Functional programming R is a functional programming language at its core, which means it is designed around the use of functions. In the previous section, we saw that functions are defined and assigned to names just like variables. This means that functions can be passed to other functions just like variables! Consider the following example. Lets consider a general formulation of vector transformation: \\[ \\bar{\\mathbf{x}} = \\frac{\\mathbf{x} - t_r(\\mathbf{x})}{s(\\mathbf{x})} \\] Here, \\(\\mathbf{x}\\) is a vector of real numbers, and \\(\\bar{\\mathbf{x}}\\) is defined as a vector of the same length where each value has had some average or central value \\(t_r(\\mathbf{x})\\) subtracted from it, and is divided by a scaling factor \\(s(\\mathbf{x})\\) to control the range of resulting values. Both \\(t_r(\\mathbf{x})\\) and \\(s(\\mathbf{x})\\) are scalars (i.e. individual numbers) and dependent upon the values of \\(\\mathbf{x}\\). If \\(t_r\\) is arithmetic mean and \\(s\\) is standard deviation, we have defined the standardization transformation mentioned in earlier examples: x &lt;- rnorm(100, mean=20, sd=10) x_zscore &lt;- (x - mean(x))/sd(x) However, there are many different ways to define the central value of a set of numbers: arithmetic mean geometric mean median mode and many more Each of these central value methods accepts a vector of numbers, but their behaviors are different, and are appropriate in different situations. Likewise, there are many possible scaling strategies we might consider: standard deviation rescaling factor (e.g. set data range to be between -1 and 1) scaling to unit length (all values sum to 1) and others We may wish to explore these different methods without writing entirely new code for each combination when trying out different transformation techniques. In R and other functional languages, we can easily accomplish this by passing functions as arguments to other functions. Consider the following R function: # note R already has a built in function named &quot;transform&quot; my_transform &lt;- function(x, t_r, s) { return((x - t_r(x))/s(x)) } This should look familiar to the equation presented earlier, except now in code the arguments t_r and s are passed as arguments. If we wished to transform using a Z-score normalization, we could call my_transform as follows: x &lt;- rnorm(100,mean=20,sd=10) x_zscore &lt;- my_transform(x, mean, sd) mean(x_zscore) [1] 0 sd(x_zscore) [1] 1 In the my_transform function call, the second and third arguments are the names of the mean and sd functions, respectively. In the definition of my_transform we use the syntax t_r(x) and s(x) to indicate that these arguments should be treated as functions. Using this strategy, we could just as easily define a transformation using median and sum for t_r and s if we wished to: x &lt;- rnorm(100,mean=20,sd=10) x_transform &lt;- my_transform(x, median, sum) median(x_transform) [1] 0 sum(x_transform) # this quantity does not have an a priori known value (or meaning for that matter, it&#39;s just an example) [1] 0.013 We can also write our own functions and pass them to get the my_transform function to have desired behavior. The following scales the values of x to have a range of \\([0,1]\\): data_range &lt;- function(x) { return(max(x) - min(x)) } # my_transform computes: (x - min(x))/(max(x) - min(x)) x_rescaled &lt;- my_transform(x, min, data_range) min(x_rescaled) [1] 0 max(x_rescaled) [1] 1 The data_range function simply subtracts the minimum value of x from the maximum value and returns the result. This feature of passing functions as arguments to other functions is a fundamental property of functional programming languages. Now we are ready to finally talk about how iteration is performed in R. Advanced R - Functional Programming Functional Programming Tutorial 4.8.3 apply() and friends When working with lists and matrices in R, there are often times when you want to perform a computation on every row or every column separately. A common example of this in data science mentioned above is feature standardization. Earlier we wrote a Z-score transformation that accepts a vector, subtracts the mean from each element, and divides the result by the standard deviation of the data. This ensures the data has a mean and standard deviation of 0 and 1, respectively. However, this function only operates on a single vector of numbers. Large datasets have many features, each of which may be individual vectors, that we desire to perform this same Z-score transformation on separately. In other words, we have one function that we wish to execute on either every row or every column of a matrix and return the result. This is a form of iteration that can be implemented in a functional style using the apply function. This is the signature of the apply function, from the RStudio help(apply) page: apply(X, MARGIN, FUN, ..., simplify = TRUE) Here, X is a matrix (i.e. a rectangle of numbers) that we wish to perform a computation on for either each row or each column. MARGIN indicates whether the matrix should be traversed by rows (MARGIN=1) or columns (MARGIN=2). FUN is the name of a function that accepts a vector and returns either a vector or a scalar value that we wish to execute on either the rows or columns. apply() then executes FUN on each row or column of X and returns the result. For example: zscore &lt;- function(x) { return((x-mean(x))/sd(x)) } # construct a matrix of 50 rows by 100 columns with samples drawn from a normal distribution x_mat &lt;- matrix( rnorm(100*50, mean=20, sd=5), nrow=50, ncol=100 ) # z-transform the rows of x_mat, so that each column has mean,sd of 0,1 x_mat_zscore &lt;- apply(x_mat, 2, zscore) # we can check that all the columns of x_mat_zscore have mean close to zero with apply too x_mat_zscore_means &lt;- apply(x_mat_zscore, 2, mean) # note: due to machine precision errors, these results will not be exactly zero, but are very close # note: the all() function returns true if all of its arguments are TRUE all(x_mat_zscore_means&lt;1e-15) [1] TRUE The same approach can be used when X is a list or data frame rather than a matrix using the lapply() function (hint: the l in lapply stands for list). Here is the function signature for lapply: lapply(X, FUN, ...) Recall that lists and data frames can be thought of as vectors where each element can be its own vector. Therefore, there is only one axis along which to iterate on the elements and there is not MARGIN argument as in apply. This function returns a new list of the same dimension as the original list with elements returned by FUN: x &lt;- list( feature1=rnorm(100,mean=20,sd=10), feature2=rnorm(100,mean=50,sd=5) ) x_zscore &lt;- lapply(x, zscore) # check that the means are close to zero x_zscore_means &lt;- lapply(x_zscore, mean) all(x_zscore_means &lt; 1e-15) [1] TRUE This functional programming pattern might be counter intuitive at first, but it is well worth your while to learn. R for Data Science - Iteration "],["installing-packages.html", "4.9 Installing Packages", " 4.9 Installing Packages Advanced functionality in R is provided through packages written and supported by R community members. With the exception of bioconductor packages, all R packages are hosted on the Comprehensive R Archive Network (CRAN) web site. At the time of writing, there are more than 18,000 packages hosted on CRAN that you can install. To install a package from CRAN, use the install.packages function in the R console: # install one package install.packages(&quot;tidyverse&quot;) # install multiple packages install.packages(c(&quot;readr&quot;,&quot;dplyr&quot;)) As mentioned above, many packages used in biological data analysis are not hosted on CRAN, but in Bioconductor. The Bioconductor projects mission is to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assays. Practically, this means Bioconductor packages are subject to stricter standards for documentation, coding conventions and structure, and standards compliance compared with the relatively more lax CRAN package submission process. "],["saving-and-loading-r-data.html", "4.10 Saving and Loading R Data", " 4.10 Saving and Loading R Data While it is always a good idea to save results in tabular form in CSV Files, sometimes it is convenient to save complicated R objects and data structures like lists to a file that can be read back into R easily. This can be done with the saveRDS() and readRDS() functions: a_complicated_list &lt;- list( categories = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), data_matrix = matrix(c(1,2,3,4,5,6),nrows=2,ncols=3), nested_list = list( a = c(1,2,3), b = c(4,5,6) ) ) saveRDS(a_complicated_list, &quot;a_complicated_list.rda&quot;) # later, possibly in a different script a_complicated_list &lt;- readRDS(&quot;a_complicated_list.rda&quot;) These functions are very convenient for saving results of complicated analyses and reading them back in later, especially if those analyses were time consuming. R also has the functions save() and load(). These functions are similar to saveRDS and readRDS, except they do not allow loading individual objects into new variables. Instead, save accepts one or more variable names that are in the global namespace and saves them all to a file: var_a &lt;- &quot;a string&quot; var_b &lt;- 1.234 save(var_a, var_b, file=&quot;saved_variables.rda&quot;) Then later when the file saved_variables.rda is load()ed, the all the variables saved into the file are loaded into the namespace with their saved values: # assume var_a and var_b are not defined yet load(&quot;saved_variables.rda&quot;) var_a [1] &quot;a string&quot; var_b [1] 1.234 This requires the programmer to remember the names of the variables that were saved into the file. Also, if there are variables that already exist in the current environment that are also saved in the file, those variable values will be overridden possibly without the programmers knowledge or intent. There is no way to change the variable names of variable saved in this way. For these reasons, saveRDS and loadRDS are generally safer to use as you can be more explicit about what you are saving and loading. saveRDS() and readRDS() R manual readRDS() and saveRDS() official documentation "],["prog-debugging.html", "4.11 Troubleshooting and Debugging", " 4.11 Troubleshooting and Debugging Bugs in code are normal. You are not a bad programmer if your code has bugs (thank goodness!). However, some bugs can be very difficult to fix, and some are even difficult to find. You will spend a substantial amount of time debugging your code in R, especially as you are learning the language and its many quirks. You will encounter R error and warning messages routinely during development, and not all of them are straightforward to understand. It is important that you learn how to seek the answers to the problems R reports on your own; your colleagues (and instructors!) will thank you for it. There is no standard approach to debugging, but here we borrow ideas from Hadley Wickams excellent section on debugging in his Advanced R book: Google! - copy and paste the error into google and see what comes back. Especially when starting out, the errors you receive have been encountered countless times by others before you, and solutions/explanations of them are already out there. If you arent already familiar with Stack Overflow, you will be very soon. Make it repeatable - When you encounter an error, dont change anything in your code and try again to make sure you get the same error again. This may require you to isolate the code with the error in a different setting to make it more easy to run. If you do, this means the error is repeatable, or replicable, and you can now try modifying the code in question to see if and how the error changes. Find out where the bug is* - Most bugs involve multiple lines of code, only a subset of which contains the actual error. Sometimes the exact line where the error occurs is obvious, but other times the error is a consequence of a mistake assumption made earlier in the code. Fix it and test it - When you have identified the specific issue causing the bug, modify the code so it produces the correct result and then rigorously test your fix to make sure it is correct. Sometimes making one change to code causes side effects elsewhere in your code in ways that are difficult to predict. Ideally, you have already written unit tests that explicitly test parts of your code, but if not you will need to use other means of convincing yourself that your fix worked. This debugging process will become second nature as you work more in R. Practically speaking, the most basic debugging method is to run code that isnt working the way it should, print out intermediate results to inspect the state of your variables, and make adjustments accordingly. In RStudio, the Environment Inspector in the top right of the interface makes inspecting the current values of your variables very easy. You can also easily execute lines of code from your script in the interpreter at the bottom right using Cntl-Enter and test out modifications there. Sometimes you will be working with highly nested data structures like lists of lists. These objects can be difficult to inspect due to their size. The str() function, which stands for structure, will pretty print an object with its values and its structure: nested_lists &lt;- list( a=list( item1=c(1,2,3), item2=c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) ), b=list( var1=1:10, var2=100:110, var3=1000:1010 ), c=c(10,9,8,7,6,5) ) nested_lists $a $a$item1 [1] 1 2 3 $a$item2 [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; $b $b$var1 [1] 1 2 3 4 5 6 7 8 9 10 $b$var2 [1] 100 101 102 103 104 105 106 107 108 109 110 $b$var3 [1] 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 str(nested_lists) List of 3 $ a:List of 2 ..$ item1: num [1:3] 1 2 3 ..$ item2: chr [1:3] &quot;A&quot; &quot;B&quot; &quot;C&quot; $ b:List of 3 ..$ var1: int [1:10] 1 2 3 4 5 6 7 8 9 10 ..$ var2: int [1:11] 100 101 102 103 104 105 106 107 108 109 ... ..$ var3: int [1:11] 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 ... $ c: num [1:6] 10 9 8 7 6 5 The output str is more concise and descriptive than simply printing out the object. RStudio has many more debugging tools you can use. Check out the section on debugging in Hadley Wickams Advanced R book in the Read More box for description of these tools. Hands-On Programming with R - Debugging R Code Advanced R - Debugging "],["prog-style.html", "4.12 Coding Style and Conventions", " 4.12 Coding Style and Conventions Some very common worries among new programmers is: Is my code terrible? How do I write good code? There is no gold standard for what makes code good, but there are some questions you can ask of your code as a guide: 4.12.1 Is my code correct? Does it produce the desired output? This is pretty obviously important in principle, but it can be difficult to be sure that your code is correct. This is especially difficult if your codebase is large and complicated as it tends to become over time. While simple trial and error is an effective first approach, a more reliable albeit time- and thought-intensive strategy is to write explicit tests for your code and run them regularly. 4.12.2 Does my code follow the DRY principle? Dont Repeat Yourself (DRY) is a powerful and helpful strategy to make your code more reliable. This typically involves identifying common patterns in your code and moving them to functions or objects. 4.12.3 Did I choose concise but descriptive variable and function names? Variable and function names should be descriptive when necessary and not too long. Try to put yourself in the shoes of someone who is reading your code for the first time and see if you can figure out what it does. Better yet, offer to buy a friend a coffee in return for them looking at it! 4.12.4 Did I use indentation and naming conventions consistently throughout my code? Consistently formatted code is much easier to read (and possibly understand) than inconsistent code. Consider the following code example: calcVal &lt;- function(x, a, arg=2) { return(sum(x*a)**2)} calc_val_2 &lt;- function(x, a, b, arg) { res &lt;- sum(b+a*x)**arg return(res)} This code is inconsistent in several ways: naming conventions - calcVal is camel case, calc_val_2 is snake case new lines and whitespace - calcVal is all on one line, calc_val_2 is on multiple lines unhelpful indentation - calc_val_2 has a function body that is not indented, and the close curly brace is appended to the last line of the body unhelpful function and argument names - the function names describe very little about what the functions do, and the argument names x, a, etc are not very descriptive about what they represent unused function arguments - the arg argument in calcVal isnt used anywhere in the function the two functions appear to do something very similar and could be made simpler using a default argument A more consistent version of this code might look like: exponent_product &lt;- function(x, a, offset=0, arg=2) { return(sum(offset+a*x)**arg) } This code is much cleaner, more consistent, and easier to read. 4.12.5 Did I write comments, especially when what the code does is not obvious? Sometimes what a piece of code does is obvious from looking at it: x &lt;- x + 1 Clearly this line of code takes the value of x, whatever it is, and adds 1 to it. However, it may not be obvious why a piece of code does what it does. In these cases, it may be very helpful to record your thinking about a line of code as a comment: # add 1 as a pseudocount x &lt;- x + 1 Then when you or someone else reads the code, it will be obvious what you were thinking when you wrote it. In your career, you will encounter situations where you need to figure out what you were thinking when you wrote a piece of code. Endeavor to make future you proud of current you! 4.12.6 How easy would it be for someone else to understand my code? If someone else who has never seen my code before is asked to run and understand it, how easy would it be for them to do so? 4.12.7 Is my code easy to maintain/change? This is related to the previous question, but is distinct in that understanding what code does is just the first step in being able to make desired changes to it. 4.12.8 The styler package . "],["data-wrangling.html", "5 Data Wrangling ", " 5 Data Wrangling "],["the-tidyverse.html", "5.1 The Tidyverse", " 5.1 The Tidyverse The tidyverse is an opinionated collection of R packages designed for data science. The packages are all designed to work together with a unified approach that helps code look consistent and neat. In the opinion of this author, the tidyverse practically changes the R language from a principally statistical programming language into an efficient and expressive data science language. While it is still important to understand the language fundamentals presented in our chapter on the R programming language, the tidyverse uses a distinct set of coding conventions that lets it achieve greater expressiveness, conciseness, and correctness relative to the base R language. As a data science language, R+tidyverse (referred to as simply tidyverse in this book) is strongly focused on operations related to loading, manipulating, visualizing, summarizing, and analyzing data sets from many domains. While this is a major strength of tidyverse and its community, it means that many educational materials are written for this general use case, and not for those practicing biological data analysis. While the general data manipulation operations are often the same between biological data analysis and these general case examples, biological analysis practitioners must nonetheless translate concepts from these general cases to the common data analysis tasks they must perform. Some analytical patterns are more common in biological data analysis than others, so these materials focus on that subset of operations in this book to aid the learning in applying the concepts to their problems as directly as possible. R for Data Science - Data Wrangling Introduction ModernDive - Data Wrangling "],["dw-basics.html", "5.2 Tidyverse Basics", " 5.2 Tidyverse Basics Since tidyverse is a set of packages that work together, you will often want to load multiple packages at the same time. The tidyverse authors recognize this, and defined a set of reasonable packages to load at once when loading the metapackage (i.e. a package that contains multiple packages): library(tidyverse) -- Attaching packages --------------------------------------------- tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.6 v dplyr 1.0.7 v tidyr 1.1.4 v stringr 1.4.0 v readr 2.1.1 v forcats 0.5.1 -- Conflicts ------------------------------------------------ tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() The packages in the Attaching packages section are those loaded by default, and each of these packages adds a unique set of functions to your R environment. We will mention functions from many of these packages as we go through this chapter, but for now here is a table of these packages and briefly what they do: Package Description ggplot2 Plotting using the grammar of graphics tibble Simple and sane data frames tidyr Operations for making data tidy readr Read rectangular text data into tidyverse purrr Functional programming tools for tidyverse dplyr A Grammar of Data Manipulation stringr Makes working with strings in R easier forcats Operations for using categorical variables Notice the dplyr::filter() syntax in the Conflicts section. filter is defined as a function in both the dplyr package as well as the base R stats package. The stats package is loaded by default when you run R, and thus the filter function is defined (specifically, it performs linear filtering on time series data). However, when dplyr is loaded, it also has a filter function which overrides the definition from the stats package. This is why the tidyverse package reports this as a conflict when loaded: -- Conflicts ------------------------------------------------ tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() This is tidyverse telling you that the filter function has been redefined and you should make sure you are aware of that. However, if you did want to use the original stats defined filter function, you may still access it using the :: namespace operator. This operator lets you look inside a loaded package, for example dplyr, and access a function within the namespace of that package: library(dplyr) # the filter() function definition is now from the dplyr package, and not from the stats package # the following two lines of code execute the same function filter(df) dplyr::filter(df) # to access the stats definition of the filter function, use the namespace operator stats::filter(df) Most functions defined by a package can be accessed using the :: operator, and it is often a good idea to do so, to ensure you are calling the right function. "],["importing-data.html", "5.3 Importing Data", " 5.3 Importing Data The first operation we typically must perform when analyzing data is reading our data from a file into R. The readr package in the default tidyverse packages contains the following similar functions that import data from delimited text files: Function Brief description/use read_csv Delimiter: , - Decimal separator: . read_csv2 Delimiter: ; - Decimal separator: , read_tsv Delimiter: &lt;tab&gt; - Decimal separator: . read_delim Delimiter: set by user - Decimal separator: . Some CSV files can be very large and may be compressed to save space. There are many different file compression algorithms, but the most common in data science and biology are gzip and bzip. All the readr file reading functions can work with compressed files directly, so you do not need to decompress them first. Each of these functions returns a special data frame called a tibble, which is explained in the next section. Note that readr also has functions for writing delimited files. These functions behave similarly to the read_X functions but instead of creating a tibble from a file, they create a file from a tibble. You will frequently need to export the results of your analysis to share with collaborators and also as part of larger workflows that use tools other than R. R for Data Science - Data Import readr - read_delim reference readr - write_delim reference "],["data-tibble.html", "5.4 The tibble", " 5.4 The tibble Data in tidyverse is organized primarily in a special data frame object called a tibble. The tibble() function is defined in the tibble package of the tidyverse: library(tibble) # or library(tidyverse) tbl &lt;- tibble( x = rnorm(100, mean=20, sd=10), y = rnorm(100, mean=50, sd=5) ) tbl # A tibble: 100 x 2 x y &lt;dbl&gt; &lt;dbl&gt; 1 16.5 54.6 2 14.4 54.3 3 7.87 53.7 4 8.06 50.8 5 37.2 57.1 6 16.5 51.9 7 15.8 50.1 8 40.3 44.3 9 12.0 49.8 10 23.8 50.1 # ... with 90 more rows A tibble stores rectangular data, i.e. a grid of data elements with where every column has the same number of rows. You can access individual columns in the same way as with base R data frames: tbl$x [1] 29.572549 12.015877 15.235536 23.071761 32.254703 48.048651 21.905756 [8] 15.511768 34.872685 21.352433 12.515230 23.608096 6.778630 12.342237 ... tbl[1,&quot;x&quot;] # access the first element of x # A tibble: 1 x 1 x &lt;dbl&gt; 1 29.6 tbl$x[1] [1] 29.57255 tibbles (and regular data frames) typically have names for their columns. In the above example, the column names are x and y, accessed using the colnames function: colnames(tbl) [1] &quot;x&quot; &quot;y&quot; Column names may be changed using this same function: colnames(tbl) &lt;- c(&quot;a&quot;,&quot;b&quot;) tbl # A tibble: 100 x 2 a b &lt;dbl&gt; &lt;dbl&gt; 1 16.5 54.6 2 14.4 54.3 3 7.87 53.7 4 8.06 50.8 5 37.2 57.1 6 16.5 51.9 7 15.8 50.1 8 40.3 44.3 9 12.0 49.8 10 23.8 50.1 # ... with 90 more rows As we will see again later, we can also use dplyr::rename to rename columns as well: dplyr::rename(tbl, a = x, b = y ) tibbles and dataframes also have row names as well as column names: rownames(tbl) [1] &quot;1&quot; &quot;2&quot; &quot;3&quot;... However, the tibble support for row names is only included for compatibility with base R data frames and should generally be avoided. The reason is that row names are basically a character column that has different semantics than every other column, and the authors of tidyverse believe row names are better stored as a normal column. tibble - working with row names The tibble package provides a convenient way to construct simple tibbles manually with the tribble() function, which stands for transposed tibble: gene_stats &lt;- tribble( ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p, &quot;apoe&quot;, 12.509293, 0.1032, 34.239521, 1.3e-5, &quot;hoxd1&quot;, 4.399211, 0.6323, 16.332318, 0.0421, &quot;snca&quot;, 45.748431, 4.2e-9, 0.757188, 0.9146, ) gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 ## 2 hoxd1 4.40 0.632 16.3 0.0421 ## 3 snca 45.7 0.0000000042 0.757 0.915 This made-up dataset includes statistics and p-values from two different statistical tests (again, made up) for three human genes. We will use this example below in the Arranging Data section. tibble documentation R for Data Science - tibbles "],["tidy-data.html", "5.5 Tidy Data", " 5.5 Tidy Data The tidyverse packages are designed to operate with so-called tidy data. From the tidy data section of the R for Data Science book, the following rules make data tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Here, a variable is a quantity or property that every observation in our dataset has, and each observation is a separate instance of those variable (e.g. a different sample, subject, etc). In our gene_stats example tibble above, the columns referring to different test statistics are the variables, and each gene in each row is an observation, and each cell has a value; we can therefore say that the dataset is tidy: gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 ## 2 hoxd1 4.40 0.632 16.3 0.0421 ## 3 snca 45.7 0.0000000042 0.757 0.915 Each row being an observation is somewhat abstract in this case; we could say that we observed the same test for all the genes in the tibble. Depending on what dataset we are working with, we sometimes have to be flexible in our conceptualization of what constitutes variables and observations. The R for Data Science book depicts these rules in the following illustration: Tidy data - from R for Data Science These rules are pretty generic, and in general a dataset might require some work to manipulate it into tidy form. Fortunately for those of us working in biology and bioinformatics, very many of our datasets are already provided in a format that is very close to being tidy, or the tools we use to process biological data do so for us. For this reason, details about the tidying operations that might be needed for data in the general case are left for reading in the tidy data section of R for Data Science book. There is one very big and common exception to the claim above that biological data is usually already tidy that. Briefly, from the illustration above, tidy data has observations as rows and variables as columns. However, the datasets that we often use, e.g. gene expression matrices, are organized to have variables (e.g. genes) as rows and observations (e.g. samples) as columns. This means some operations to summarize variables across observations, which are very common to compute, are not easily done with the tidyverse functions like mutate(). We describe how to work around this difference in the section Biological data is NOT Tidy!. Tidy data - R for Data Science "],["pipes.html", "5.6 pipes", " 5.6 pipes One of the key tidyverse programming patterns is chaining manipulations of tibbles together using the %&gt;% operator. We very often want to perform serial operations on a data frame, for example read in a file, rename one of the columns, subset the rows based on some criteria, and compute summary statistics on the result. We might implement such operations using a variable and assignment: # data_file.csv has two columns: bad_cOlumn_name and numeric_column data &lt;- readr::read_csv(&quot;data_file.csv&quot;) data &lt;- dplyr::rename(data, &quot;better_column_name&quot;=bad_cOlumn_name) data &lt;- dplyr::filter(data, better_column_name %in% c(&quot;condA&quot;,&quot;condB&quot;)) data_grouped &lt;- dplyr::group_by(data, better_column_name) summarized &lt;- dplyr::summarize(data_grouped, mean(numeric_column)) The repeated use of data and the intermediate data_grouped variable may be unnecessary if youre only interested in the summarized result. The code is also not very straightforward to read. Using the %&gt;% operator, we can write the same sequence of operations in a much more concise manner: data &lt;- readr::read_csv(&quot;data_file.csv&quot;) %&gt;% dplyr::rename(&quot;better_column_name&quot;=bad_cOlumn_name) %&gt;% dplyr::filter(better_column_name %in% c(&quot;condA&quot;,&quot;condB&quot;)) %&gt;% dplyr::group_by(better_column_name) %&gt;% dplyr::summarize(mean(numeric_column)) Note that the function calls in the piped example do not have the data variable passed in explicitly. This is because the %&gt;% operator passes the result of the function immediately preceding it as the first argument to the next function automatically. This convention allows us to focus on writing only the important parts of the code that perform the logic of our analysis, and avoid unnecessary and potentially distracting additional characters that make the code less readable. R for Data Science - Pipes %&gt;% operator documentation in the magrittr package "],["arranging-data.html", "5.7 Arranging Data", " 5.7 Arranging Data After we have loaded our data from a file into a tibble, we often need to manipulate it in various ways to make the values amenable to our desired analysis. Such manipulations might include renaming poorly named columns, filtering out certain records, deriving new columns using the values in others, changing the order of rows etc. These operations may collectively be termed arranging the data and many are provided in the *dplyr package. We will cover some of the most common data arranging functions here, but there are many more in the dplyr package worth knowing. In the examples below, we will make use of the following made-up tibble that contains fake statistics and p-values for three human genes: gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 ## 2 hoxd1 4.40 0.632 16.3 0.0421 ## 3 snca 45.7 0.0000000042 0.757 0.915 The gene_stats tibble above is a simple example of a very common type of data we work with in biology; namely instead of raw data, we work with statistics that have been computed using raw data that help us interpret the results. While these statistics may not be data per se, we can still use all the functions and strategies in the tidyverse to work with them. The tidyverse is a very big place. RStudio created many helpful cheatsheets to aid in looking up how do to certain operations. The cheatsheet on dplyr has lots of useful information on how to use the many functions in the package. 5.7.1 dplyr::mutate() - Create new columns using other columns Many biological analysis procedures perform some kind of statistical test on a collection of features (e.g. genes) and produce p-values that indicate how surprising each feature is according to the test. The p-values in our tibble are nominal, i.e. they have not been adjusted for multiple hypotheses. Briefly, when we run multiple tests like we are on each of our three genes, there is a chance that some of the tests will have a small p-value simply by chance. Multiple testing correction procedures adjust nominal p-values to account for this possibility in a number of different ways, but the most common procedure in biological analysis is the Benjamini-Hochberg or False Discovery Rate (FDR) procedure. In R, the p.adjust function can perform several of these procedures, including FDR: dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;) ) ## # A tibble: 3 x 6 ## gene test1_stat test1_p test2_stat test2_p test1_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 0.155 ## 2 hoxd1 4.40 0.632 16.3 0.0421 0.632 ## 3 snca 45.7 0.0000000042 0.757 0.915 0.0000000126 Notice how the adjusted p-values are larger than the nominal ones; this is the effect of the multiple testing procedure. Since we have two sets of p-values, we must compute the FDR on each of them, which we can do in the same call to mutate(): gene_stats_mutated &lt;- dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;) ) gene_stats_mutated ## # A tibble: 3 x 7 ## gene test1_stat test1_p test2_stat test2_p test1_padj test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 0.155 0.000039 ## 2 hoxd1 4.40 0.632 16.3 0.0421 0.632 0.0632 ## 3 snca 45.7 0.0000000042 0.757 0.915 0.0000000126 0.915 Another common operation is to create new columns derived from the values in multiple other columns. We (or our wetlab colleagues) might decide it is convenient to have a new column with TRUE or FALSE based on whether the gene was significant in either test. Such a column would make it easy to filter genes down to just ones that might be interesting in tools like Excel. We can create new columns from multiple columns just as easily using the mutate() function: dplyr::mutate(gene_stats_mutated, signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05) ) ## # A tibble: 3 x 9 ## gene test1_stat test1_p test2_stat test2_p test1_padj test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 0.155 0.000039 ## 2 hoxd1 4.40 0.632 16.3 0.0421 0.632 0.0632 ## 3 snca 45.7 0.0000000042 0.757 0.915 0.0000000126 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; Recall that the | and &amp; operators execute or and and logic, respectively. The above example required the creation of a new variable gene_stats_mutated because the columns test1_padj and test2_padj need to be in the tibble before computing the new fields. However, in mutate(), columns created first in the function call are available to later columns. In the following example, note that test1_padj is created first and then used to create the signif columns: dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), # test1_padj created test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), #test1_padj used signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05) ) ## # A tibble: 3 x 9 ## gene test1_stat test1_p test2_stat test2_p test1_padj test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 0.155 0.000039 ## 2 hoxd1 4.40 0.632 16.3 0.0421 0.632 0.0632 ## 3 snca 45.7 0.0000000042 0.757 0.915 0.0000000126 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; The alternative would be to split this into two mutate() commands, the first creating the adjusted p-value columns and the second creating the significance columns. dplyr recognizes how common it is to build new variables off of other new variables in a mutate() command, and therefore provides this convenient behavior. mutate() can also be used to modify columns in place. The official convention for human gene symbols is that they are upper case, but for some reason our tibble contains lower case gene symbols. We can correct this using mutate() but first we should talk about the stringr package which makes working with strings much easier than with base R functions. R for Data Science - Add new variables with mutate() dplyr mutate() reference 5.7.2 stringr - Working with character values Base R does not have very convenient functions for working with character strings (or just strings). This is due to its original intent a statistical programming language, where string manipulation is not (in principle) a common operation. However, in practice, we must frequently manipulate strings while loading, cleaning, and analyzing datasets. The stringr package aims to make working with strings as easy as possible. The package includes many useful functions for operating on strings, including searching for patterns, mutating strings, lexicographical sorting, combining multiple strings together (i.e. concatenation), and performing complex search/replace operations. There are far too many useful functions to cover here and you should become comfortable reading the stringr documentation and the very helpful stringr cheatsheet. In the previous section, we noted that the gene symbols in our tibble were lower case while official gene symbols are in upper case. We can use the stringr function stringr::str_to_upper() with the dplyr::mutate() function to perform this adjustment easily: dplyr::mutate(gene_stats, gene=stringr::str_to_upper(gene) ) ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ## 2 HOXD1 4.40 0.632 16.3 0.0421 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 Now are gene symbols have the appropriate case, and our wetlab colleagues wont complain about it. :) 5.7.2.1 Regular expressions Many of the string operations in the stringr package use regular expression syntax. A regular expression is a sequence of characters that describes patterns in text. Regular expressions are written in a sort of mini programming language where certain characters have special meaning that help in defining search patterns that identifies the location of sequences of characters in text that follow a particular pattern specified by the regular expression. This is similar to the Find functionality in many word processors, but is more powerful due to the flexibility of the patterns that can be found. A simple example will be helpful. Lets say we have a tibble containing the result of a (made-up) statistical test for all the genes in a genome: de_genes ## # A tibble: 39,535 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MT-TF 5799 0.000910 0.00941 ## 2 MT-RNR1 153 0.0272 0.0342 ## 3 MT-TV 115 0.0228 0.0301 ## 4 MT-RNR2 495 0.00318 0.0123 ## 5 MT-TL1 20201 0.000377 0.00841 ## 6 MT-ND1 160 0.0179 0.0258 ## 7 MT-TI 3511 0.00247 0.0115 ## 8 MT-TQ 772 0.00376 0.0129 ## 9 MT-TM 301 0.00325 0.0124 ## 10 MT-ND2 12 0.107 0.111 ## # ... with 39,525 more rows Now lets say were interested in examining the results for the BRCA family of genes, BRCA1 and BRCA2. We can use filter() on the data frame to look for them individually: de_genes %&gt;% filter(hgnc_symbol == &quot;BRCA1&quot; | hgnc_symbol == &quot;BRCA2&quot;) ## # A tibble: 2 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BRCA1 41 0.0321 0.0386 ## 2 BRCA2 447 0.0140 0.0223 This isnt so bad, but we can do the same thing with stringr::str_detect() , which returns TRUE if the provided pattern matches the input and FALSE otherwise, a regular expression, and the [dplyr::filter() function], which is described in greater detail in a later section: dplyr::filter(de_genes, str_detect(hgnc_symbol,&quot;^BRCA[12]$&quot;)) ## # A tibble: 2 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BRCA1 41 0.0321 0.0386 ## 2 BRCA2 447 0.0140 0.0223 The argument \"^BRCA[12]$\" is a regular expression that searches for the following: Search for genes that start with the characters BRCA - the ^ at the beginning of the pattern stands for the start of the string For genes that start with BRCA, then look for genes where the next character is either 1 or 2 with [12] - the characters between the [] are searched for explicitly, and any character encountered that is not listed between them results in a non-match For genes that start with BRCA followed with either a 1 or a 2, match successfully if the number is at the end of the name - the $ at the end of the pattern stands for the end of the string We can use these principles to find genes with more complex naming conventions. The Homeobox (HOX) genes encode DNA binding proteins that regulate gene expression of genes involved in morphgenesis and cell differentiation in vertebrates. In humans, HOX genes are organized into 4 clusters of paralogs that were the result of three DNA duplication events in the distant evolutionary past(Abbasi 2015), where each cluster encodes a subset of 13 distinct HOX genes placed next to each other. Each of these clusters has been assigned a letter identifier A-D (e.g. HOXA, HOXB, HOXC, and HOXD) and each paralogous gene within each cluster is assigned the same number (e.g. HOXA4, HOXB4, HOXC4, and HOXD4 are paralogs). There are 13 HOX genes in total, though not all genes remain in all clusters (e.g. HOXA1, HOXB1, and HOXD1 exist but HOXC1 was lost over time). The following figure depicts the human HOX gene clusters: Human HOX gene clusters - Veraksa, A.; Del Campo, M.; McGinnis, W. Developmental Patterning Genes and their Conserved Functions: From Model Organisms to Humans. Mol. Genet. Metab. 2000, 69 (2), 85100. Lets say we want to extract out all the HOX genes from our gene statistics. We can write a regular expression that matches the pattern described above: dplyr::filter(de_genes, str_detect(hgnc_symbol,&quot;^HOX[A-D][0-9]+$&quot;)) %&gt;% dplyr::arrange(hgnc_symbol) ## # A tibble: 39 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HOXA1 8734 0.000858 0.00934 ## 2 HOXA10 4149 0.00152 0.0102 ## 3 HOXA11 567 0.0101 0.0188 ## 4 HOXA13 411 0.0105 0.0191 ## 5 HOXA2 554 0.00600 0.0151 ## 6 HOXA3 18 0.0919 0.0959 ## 7 HOXA4 475 0.0113 0.0199 ## 8 HOXA5 434 0.0127 0.0211 ## 9 HOXA6 3983 0.00252 0.0115 ## 10 HOXA7 897 0.00961 0.0183 ## # ... with 29 more rows In this query we used two new regular expression features: within the [] we specified a range of characters A-D and 0-9 which will match any of the characters between A and D (i.e. A, B, C, or D) and 0 and 9 respectively the + character means match one or more of the preceding expression, which in our case is the [0-9]. This allows us to match genes with only a single number (e.g. HOXA1) as well as double digit numbers (e.g. HOXA10). Since we know the cluster identifier part of the HOX gene names (i.e. the [A-D] part) is exactly one character long, we could alternatively write the regular expression as follows, using the special . character: dplyr::filter(de_genes, str_detect(hgnc_symbol,&quot;^HOX.[0-9]+$&quot;)) %&gt;% dplyr::arrange(hgnc_symbol) ## # A tibble: 39 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HOXA1 8734 0.000858 0.00934 ## 2 HOXA10 4149 0.00152 0.0102 ## 3 HOXA11 567 0.0101 0.0188 ## 4 HOXA13 411 0.0105 0.0191 ## 5 HOXA2 554 0.00600 0.0151 ## 6 HOXA3 18 0.0919 0.0959 ## 7 HOXA4 475 0.0113 0.0199 ## 8 HOXA5 434 0.0127 0.0211 ## 9 HOXA6 3983 0.00252 0.0115 ## 10 HOXA7 897 0.00961 0.0183 ## # ... with 29 more rows Here, the . character is interpreted by the regex to match any single character, regardless of what it is, between the HOX part and the number part. This also requires that there exist exactly one character between the two parts; a gene symbol HOX1 would not be matched, because the 1 would match to the ., but no number remains to match to the [0-9]+ part. Sometimes you want to search text for characters that are considered special in the regular expression language. For example, if you had a list of filenames: filenames &lt;- tribble( ~name, &quot;annotation.csv&quot;, &quot;file1.txt&quot;, &quot;file2.txt&quot;, &quot;results.csv&quot; ) and wanted to limit to just those with the .txt extension, you need to match using a literal . character: filter(filenames, stringr::str_detect(name,&quot;[.]txt$&quot;)) ## # A tibble: 2 x 1 ## name ## &lt;chr&gt; ## 1 file1.txt ## 2 file2.txt Inside a [], characters do not have their usual regular expression meaning, and therefore [.] will match a literal . character. Instead of using the [] syntax, you may also escape these literal characters using two back slashes: filter(filenames, stringr::str_detect(name,&quot;\\\\.txt$&quot;)) ## # A tibble: 2 x 1 ## name ## &lt;chr&gt; ## 1 file1.txt ## 2 file2.txt Regular expressions are very powerful, and can do much more than what is described here. See the regular expression tutorial linked in the readmore box to learn more details. R for Data Science - Strings stringr documentation stringr cheatsheet RegexOne - regular expression tutorial 5.7.3 dplyr::select() - Subset Columns by Name Our mutate() operations above created a number of new columns in our tibble, but we did not specify where in the tibble the new columns should go. Lets consider the mutated tibble we created with all four new columns: dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05), gene=stringr::str_to_upper(gene) ) ## # A tibble: 3 x 9 ## gene test1_stat test1_p test2_stat test2_p test1_padj test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 0.155 0.000039 ## 2 HOXD1 4.40 0.632 16.3 0.0421 0.632 0.0632 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 0.0000000126 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; From a readability standpoint, it might be helpful if all the columns that are about each test were grouped together, rather than having to look at the end of the tibble to find them. The dplyr::select() function allows you to pick specific columns out of a larger tibble in whatever order you choose: stats &lt;- dplyr::select(gene_stats, test1_stat, test2_stat) stats ## # A tibble: 3 x 2 ## test1_stat test2_stat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 34.2 ## 2 4.40 16.3 ## 3 45.7 0.757 Here we have explicitly selected the statistics columns. dplyr also has helper functions that allow for more flexible selection of columns. For example, if all of the columns we wished to select ended with _stat, we could use the ends_with() helper function: stats &lt;- dplyr::select(gene_stats, ends_with(&quot;_stat&quot;)) stats ## # A tibble: 3 x 2 ## test1_stat test2_stat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 34.2 ## 2 4.40 16.3 ## 3 45.7 0.757 If you so desire, select() allows for the renaming of selected columns: stats &lt;- dplyr::select(gene_stats, t=test1_stat, chisq=test2_stat ) stats ## # A tibble: 3 x 2 ## t chisq ## &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 34.2 ## 2 4.40 16.3 ## 3 45.7 0.757 If we knew that these test statistics actually corresponded to some kind of t-test and a \\(\\chi\\)-squared test, naming the columns of the tibble appropriately may help others (and possibly you) understand your code better. We can use the dplyr::select() function to obtain our desired column order: dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05), gene=stringr::str_to_upper(gene) ) %&gt;% dplyr::select( gene, test1_stat, test1_p, test1_padj, test2_stat, test2_p, test2_padj, signif_either, signif_both ) ## # A tibble: 3 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 0.155 34.2 0.000013 0.000039 ## 2 HOXD1 4.40 0.632 0.632 16.3 0.0421 0.0632 ## 3 SNCA 45.7 0.0000000042 0.0000000126 0.757 0.915 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; Now the order of our columns is clear and convenient. It is not necessary to list the columns for each test statistic on the same line, but the author thinks this makes the code easier to read and understand. R for Data Science - Select columns with select() dplyr reference select helper functions 5.7.4 dplyr::filter() - Pick rows out of a data set Often, the first step in interpreting an analysis is to identify the features that are significant at some adjusted p-value threshold. First we will save our mutated tibble to another variable, to aid in demonstration: gene_stats_mutated &lt;- dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05), gene=stringr::str_to_upper(gene) ) %&gt;% dplyr::select( gene, test1_stat, test1_p, test1_padj, test2_stat, test2_p, test2_padj, signif_either, signif_both ) Now we can use the dplyr::filter() function to select rows based on whether they are significant in either test this with our above example. dplyr::filter(gene_stats_mutated, test1_padj &lt; 0.05) ## # A tibble: 1 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SNCA 45.7 0.0000000042 0.0000000126 0.757 0.915 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; dplyr::filter(gene_stats_mutated, test2_padj &lt; 0.05) ## # A tibble: 1 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 0.155 34.2 0.000013 0.000039 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; Here we are filtering the result so that only genes with nominal p-value less than 0.05 remain. Note we filter on the two tests separately, but we can also combine these tests using logical operators to achieve different results: # | means &quot;logical or&quot;, meaning the row is retained if either condition is true dplyr::filter(gene_stats_mutated, test1_padj &lt; 0.05 | test2_padj &lt; 0.05) ## # A tibble: 2 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 0.155 34.2 0.000013 0.000039 ## 2 SNCA 45.7 0.0000000042 0.0000000126 0.757 0.915 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; Only APOE and SCNA are significant in at least one of the tests. # &amp; means &quot;logical and&quot;, meaning the row is retained only if both conditions are true dplyr::filter(gene_stats_mutated, test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05) ## # A tibble: 0 x 9 ## # ... with 9 variables: gene &lt;chr&gt;, test1_stat &lt;dbl&gt;, test1_p &lt;dbl&gt;, ## # test1_padj &lt;dbl&gt;, test2_stat &lt;dbl&gt;, test2_p &lt;dbl&gt;, test2_padj &lt;dbl&gt;, ## # signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; It looks like we dont have any genes that are significant by both tests. Filtering results like this is one of the most common operations we do on the results of biological analyses. R for Data Science - Filter rows with filter() dplyr - filter() reference 5.7.5 dplyr::arrange() - Order rows based on their values Another common operation when working with biological analysis results is ordering them by some meaningful value. Like above, p-values are often used to prioritize results by simply sorting them in ascending order. The arrange() function is how to perform this sorting in tidyverse: stats_sorted_by_test1_p &lt;- dplyr::arrange(gene_stats, test1_p) stats_sorted_by_test1_p ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 snca 45.7 0.0000000042 0.757 0.915 ## 2 apoe 12.5 0.103 34.2 0.000013 ## 3 hoxd1 4.40 0.632 16.3 0.0421 Note we are sorting by nominal p-value here, not adjusted p-value. In general, sorting by nominal or adjusted p-value results in the same order of results. The only exception is when, due to the way the FDR procedure works, some adjusted p-values will be identical, making the relative order of those tests with the same FDR meaningless. In contrast, it is very rare that nominal p-values will be identical, and since they induce the same ordering of results, when sorting analysis results there are advantages to using nominal p-value, rather than adjusted p-value. In general, the larger the magnitude of the statistic, the smaller the p-value (for two-tailed tests), so if we so desired we could induce a similar ranking by arranging the data by the statistic in descending order: # desc() is a helper function that causes the results to be sorted in descending # order for the given column dplyr::arrange(gene_stats, desc(abs(test1_stat))) ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 snca 45.7 0.0000000042 0.757 0.915 ## 2 apoe 12.5 0.103 34.2 0.000013 ## 3 hoxd1 4.40 0.632 16.3 0.0421 Here we first apply the base R abs() function to compute the absolute value of the test 1 statistic and then specify that we want to sort largest first. Note although we dont have any negative values in our dataset, we should not assume that in general, so it is safer for us to be complete and add the absolute value call in case later we decide to copy and paste this code into another analysis. Thats pretty much all there is to arrange(). R for Data Science - Arrange rows witharrange() dplyr arrange() reference 5.7.6 Putting it all together In the previous sections, we performed the following operations: Created new columns by computing false discovery rate on the nominal p-values using the dplyr::mutate() and p.adjust functions Created new columns that indicate the patterns of significance for each gene using dplyr::mutate() Mutated the gene symbol case using stringr::str_to_upper and dplyr::mutate() Reordered the columns to group related variables with select() Filtered genes based on whether they have an adjusted p-value less than 0.05 for either and both statistical tests using dplyr::filter() Sorted the results by p-value using dplyr::arrange() For the sake of illustration, these steps were presented separately, but together they represent a single unit of data processing and thus might profitably be done in the same R command using %&gt;%: gene_stats &lt;- dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05), gene=stringr::str_to_upper(gene) ) %&gt;% dplyr::select( gene, test1_stat, test1_p, test1_padj, test2_stat, test2_p, test2_padj, signif_either, signif_both ) %&gt;% dplyr::filter( test1_padj &lt; 0.05 | test2_padj &lt; 0.05 ) %&gt;% dplyr::arrange( test1_p ) gene_stats ## # A tibble: 2 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SNCA 45.7 0.0000000042 0.0000000126 0.757 0.915 0.915 ## 2 APOE 12.5 0.103 0.155 34.2 0.000013 0.000039 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; This complete pipeline now contains all of our manipulations and our mutated tibble can be passed on to downstream analysis or collaborators. References "],["grouping-data.html", "5.8 Grouping Data", " 5.8 Grouping Data Sometimes we are interested in summarizing subsets of our data defined by some grouping variable. Consider the following made-up sample metadata for a set of individuals in an Alzheimers disease (AD) study: metadata &lt;- tribble( ~ID, ~condition, ~age_at_death, ~Braak_stage, ~APOE_genotype, &quot;A01&quot;, &quot;AD&quot;, 78, 5, &quot;e4/e4&quot;, &quot;A02&quot;, &quot;AD&quot;, 81, 6, &quot;e3/e4&quot;, &quot;A03&quot;, &quot;AD&quot;, 90, 5, &quot;e4/e4&quot;, &quot;A04&quot;, &quot;Control&quot;, 80, 1, &quot;e3/e4&quot;, &quot;A05&quot;, &quot;Control&quot;, 79, 0, &quot;e3/e3&quot;, &quot;A06&quot;, &quot;Control&quot;, 81, 0, &quot;e2/e3&quot; ) This is a typical setup for metadata in these types of experiments. There is a sample ID column which uniquely identifies each subject, a condition variable indicating which group each subject is in, and clinical information like age at death, Braak stage (a measure of Alzheimers disease pathology in the brain), and diploid APOE genotype (e2 is associated with reduced risk of AD, e3 is baseline, and e4 confers increased risk). An important experimental design consideration is to match sample attributes between groups as well as possible to avoid confounding our comparison of interest. In this case, age at death is one such variable that we wish to match between groups. Although these values look pretty well matched between AD and Control groups, it would be better to check explicitly. We can do this using dplyr::group_by() to group the rows together based on condition and dplyr::summarize() to compute the mean age at death for each group: dplyr::group_by(metadata, condition ) %&gt;% dplyr::summarize(mean_age_at_death = mean(age_at_death)) ## # A tibble: 2 x 2 ## condition mean_age_at_death ## &lt;chr&gt; &lt;dbl&gt; ## 1 AD 83 ## 2 Control 80 The dplyr::group_by() accepts a tibble and a column name for a column that contains a categorical variable (i.e. a variable with discrete values like AD and Control) and separates the rows in the tibble into groups according to the distinct values of the column. The dplyr::summarize() function accepts the grouped tibble and creates a new tibble with contents defined as a function of values for columns in for each group. From the example above, we see that the mean age at death is indeed different between the two groups, but not by much. We can go one step further and compute the standard deviation age range to further investigate: dplyr::group_by(metadata, condition ) %&gt;% dplyr::summarize( mean_age_at_death = mean(age_at_death), sd_age_at_death = sd(age_at_death), lower_age = mean_age_at_death-sd_age_at_death, upper_age = mean_age_at_death+sd_age_at_death, ) ## # A tibble: 2 x 5 ## condition mean_age_at_death sd_age_at_death lower_age upper_age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 83 6.24 76.8 89.2 ## 2 Control 80 1 79 81 Note the use of summarized variables defined first being used in variables defined later. Now we can see that the age ranges defined by +/- one standard deviation clearly overlap, which gives us more confidence that our average age at death for AD and Control are not significantly different. We used +/- one standard deviation to define the likely mean age range above and below the arithmetic mean for simplicity in the example above. The proper way to assess whether these distributions are significantly different is to use an appropriate statistical test like a t-test. Like other functions in dplyr, dplyr::summarize() has some helper functions that give it additional functionality. One useful helper function is n(), which is defined as the number of records within each group. We will add one more column to our summarized sample metadata from above that reports the number of subjects within each condition: dplyr::group_by(metadata, condition ) %&gt;% dplyr::summarize( num_subjects = n(), mean_age_at_death = mean(age_at_death), sd_age_at_death = sd(age_at_death), lower_age = mean_age_at_death-sd_age_at_death, upper_age = mean_age_at_death+sd_age_at_death ) ## # A tibble: 2 x 6 ## condition num_subjects mean_age_at_death sd_age_at_death lower_age upper_age ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 3 83 6.24 76.8 89.2 ## 2 Control 3 80 1 79 81 We now have a column with the number of subjects in each condition. Hadley Wickham is from New Zealand, which uses British, rather than American, English. Therefore, in many places, both spellings are supported in the tidyverse; e.g. both summarise() and summarize() are supported. R for Data Science - Grouped summaries with summarise() dplyr summarise() reference "],["rearranging-data.html", "5.9 Rearranging Data", " 5.9 Rearranging Data Sometimes the shape and format of our data is not the most convenient for performing certain operations on it, even if it is tidy. Lets say we are considering the range of statistics that were computed for all of our genes in the gene_stats tibble, and wanted to compute the average statistic over all genes for both tests. Recall our tibble has separate columns for each test: gene_stats &lt;- tribble( ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p, &quot;APOE&quot;, 12.509293, 0.1032, 34.239521, 1.3e-5, &quot;HOXD1&quot;, 4.399211, 0.6323, 16.332318, 0.0421, &quot;SNCA&quot;, 45.748431, 4.2e-9, 0.757188, 0.9146, ) gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ## 2 HOXD1 4.40 0.632 16.3 0.0421 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 For convenience, we desire our output to be in table form, with one row per test and the statistics for each test as columns. We could do this manually like so: tribble( ~test_name, ~min, ~mean, ~max, &quot;test1_stat&quot;, min(gene_stats$test1_stat), mean(gene_stats$test1_stat), max(gene_stats$test1_stat), &quot;test2_stat&quot;, min(gene_stats$test2_stat), mean(gene_stats$test2_stat), max(gene_stats$test2_stat), ) ## # A tibble: 2 x 4 ## test_name min mean max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 test1_stat 4.40 20.9 45.7 ## 2 test2_stat 0.757 17.1 34.2 This gets the job done, but is clearly very ugly, error prone, and would require significant work if we later added more statistics columns. Instead of typing out the values we desire manually, we can pivot our tibble using the tidyr::pivot_longer() function, so that the column values are placed in a new column and the corresponding values are placed in yet another column. This process is illustrated in the following figure: Pivot longer moves columns and values to two new columns In the figure, the original tibble has genes along rows and samples as columns. When the sample columns are pivoted, the value of each column name is placed in a new column named Sample and repeated for as many rows there are in the tibble. A second new column Value is populated with the corresponding values that were in each column. The gene associated with each value is preserved and repeated vertically until all the table columns and values have been pivoted. This process of pivoting transforms the tibble into so called long form. Returning to our gene_stats example, we can apply some operations to the tibble to easily perform the summarization we did above in a much more expressive manner: long_gene_stats &lt;- tidyr::pivot_longer( gene_stats, ends_with(&quot;_stat&quot;), names_to=&quot;test&quot;, values_to=&quot;stat&quot; ) long_gene_stats ## # A tibble: 6 x 5 ## gene test1_p test2_p test stat ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 APOE 0.103 0.000013 test1_stat 12.5 ## 2 APOE 0.103 0.000013 test2_stat 34.2 ## 3 HOXD1 0.632 0.0421 test1_stat 4.40 ## 4 HOXD1 0.632 0.0421 test2_stat 16.3 ## 5 SNCA 0.0000000042 0.915 test1_stat 45.7 ## 6 SNCA 0.0000000042 0.915 test2_stat 0.757 We see that now instead of having X_stat columns, the column names and their values have been put into the test and stat columns, respectively. Now to summarize the statistics for each test, we simply do a group_by() on the test column and compute summaries on the stat column: long_gene_stats %&gt;% dplyr::group_by(test) %&gt;% dplyr::summarize(min = min(stat), mean = mean(stat), max = max(stat)) ## # A tibble: 2 x 4 ## test min mean max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 test1_stat 4.40 20.9 45.7 ## 2 test2_stat 0.757 17.1 34.2 You may verify that the numbers are identical in this pivoted tibble as the one we manually created earlier. This pivoting method will produce the desired output regardless of the number of tests we include the table, so long as the column names end in \"_test\". The inverse of pivot_longer() is pivot_wider(). If you have variables gathered in single columns like that produced by pivot_longer() you can reverse the process with this function to create a tibble with those variables as columns. Pivoting - R for Data Science pivot_longer() reference "],["relational-data.html", "5.10 Relational Data", " 5.10 Relational Data As mentioned in our section on types of biological data, we often need to combine different sources of data together to aid in interpretation of our results. Below we redefine the tibble of gene statistics from above to have properly capitalized gene symbols: gene_stats &lt;- tribble( ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p, &quot;APOE&quot;, 12.509293, 0.1032, 34.239521, 1.3e-5, &quot;HOXD1&quot;, 4.399211, 0.6323, 16.332318, 0.0421, &quot;SNCA&quot;, 45.748431, 4.2e-9, 0.757188, 0.9146, ) gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ## 2 HOXD1 4.40 0.632 16.3 0.0421 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 Our gene identifiers in this data frame are gene symbols which, while convenient for our human brains to remember, can change over time and have many aliases (e.g. the APOE gene has also been called AD2, LDLCQ5, APO-E, and ApoE4 as listed on its genecard). This can make writing code that refers to a specific gene difficult, since there are so many possible names to look for. Fortunately, there are alternative gene identifier systems that do a better job of maintaining stable, consistent gene identifiers, one of the most popular being Ensembl. Ensembl gene IDs always take the form ENSGNNNNNNNNNNN where Ns are digits. These IDs are much more stable and predictable than gene symbols, and are preferable when working with genes in code. We wish to add Ensembl IDs for the genes in our gene_stats result to the tibble as a new column. Now suppose we have obtained another file with cross referenced gene identifiers like the following: gene_map &lt;- tribble( ~symbol, ~ENSGID, ~gene_name, &quot;APOE&quot;, &quot;ENSG00000130203&quot;, &quot;apolipoprotein E&quot;, &quot;BRCA1&quot;, &quot;ENSG00000012048&quot;, &quot;BRCA1 DNA repair associated&quot;, &quot;HOXD1&quot;, &quot;ENSG00000128645&quot;, &quot;homeobox D1&quot;, &quot;SNCA&quot;, &quot;ENSG00000145335&quot;, &quot;synuclein alpha&quot;, ) gene_map ## # A tibble: 4 x 3 ## symbol ENSGID gene_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE ENSG00000130203 apolipoprotein E ## 2 BRCA1 ENSG00000012048 BRCA1 DNA repair associated ## 3 HOXD1 ENSG00000128645 homeobox D1 ## 4 SNCA ENSG00000145335 synuclein alpha Imagine that this file contains mappings for all ~60,000 genes in the human genome. While it might be simple to look up our three genes in this file and annotate manually, it is easier to ask dplyr to do it for us. We can do this using the dplyr::left_join() function which accepts two data frames and the names of columns in each that share common values: dplyr::left_join( x=gene_stats, y=gene_map, by=c(&quot;gene&quot; = &quot;symbol&quot;) ) ## # A tibble: 3 x 7 ## gene test1_stat test1_p test2_stat test2_p ENSGID gene_name ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipoprot~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox D1 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein a~ Notice that the additional columns in gene_map that were not involved in the join (i.e. ENSG and gene_name) are appended to the gene_stats tibble. If we wanted to use pipes, we could implement the same join above as follows: gene_stats %&gt;% dplyr::left_join( gene_map, by=c(&quot;gene&quot; = &quot;symbol&quot;) ) ## # A tibble: 3 x 7 ## gene test1_stat test1_p test2_stat test2_p ENSGID gene_name ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipoprot~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox D1 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein a~ Under the hood, the dplyr::left_join() function took the values in gene_stats$gene and looked for the corresponding row in gene_map with the same value in in the symbol column. It then appends all the additional columns of gene_map for the matching rows and returns the result. We have added the identifier mapping we desired with only a few lines of code. And whats more, this code will work no matter how many genes we had in gene_stats as long as gene_map contains a mapping value for the values in gene_stats$gene. But what happens if we have genes in gene_stats that dont exist in our mapping? In the above example, we use a left join because we want to include all the rows in gene_stats regardless of whether a mapping exists in gene_map. In this case this was fine because all of our genes in gene_stats had a corresponding row in the mapping. However, notice that in the mapping there is an additional gene, BRCA1 that is not in our gene statistics tibble. If we reverse the order of the join, observe what happens: gene_map %&gt;% dplyr::left_join( gene_stats, by=c(&quot;symbol&quot; = &quot;gene&quot;) ) ## # A tibble: 4 x 7 ## symbol ENSGID gene_name test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE ENSG00000130203 apolipoprotein~ 12.5 1.03e-1 34.2 1.3 e-5 ## 2 BRCA1 ENSG00000012048 BRCA1 DNA repa~ NA NA NA NA ## 3 HOXD1 ENSG00000128645 homeobox D1 4.40 6.32e-1 16.3 4.21e-2 ## 4 SNCA ENSG00000145335 synuclein alpha 45.7 4.2 e-9 0.757 9.15e-1 Now the order of the rows is the same as in gene_map, and the columns for our missing gene BRCA1 are filled with NA. This is the left join at work, where the record from gene_map is included regardless of whether a corresponding value was found in gene_stats. There are additional types of joins besides left joins. Right joins are simply the opposite of left joins: gene_stats %&gt;% dplyr::right_join( gene_map, by=c(&quot;gene&quot; = &quot;symbol&quot;) ) ## # A tibble: 4 x 7 ## gene test1_stat test1_p test2_stat test2_p ENSGID gene_name ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipopr~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox ~ ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein~ ## 4 BRCA1 NA NA NA NA ENSG00000012048 BRCA1 DNA~ The result is the same as the left join on gene_map except the order of the resulting columns is different. Inner joins return results for rows that have a match between the two tibbles. Recall our left join on gene_map included BRCA1 even though it was not found in gene_stats. An inner join will not include this row, because no match in gene_stats was found: gene_map %&gt;% dplyr::inner_join( gene_stats, by=c(&quot;symbol&quot; = &quot;gene&quot;) ) ## # A tibble: 3 x 7 ## symbol ENSGID gene_name test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE ENSG00000130203 apolipoprotein E 12.5 1.03e-1 34.2 1.3 e-5 ## 2 HOXD1 ENSG00000128645 homeobox D1 4.40 6.32e-1 16.3 4.21e-2 ## 3 SNCA ENSG00000145335 synuclein alpha 45.7 4.2 e-9 0.757 9.15e-1 One last type of join is the dplyr::full_join() (also sometimes called an outer join). As you may expect, a full join will return all rows from both tibbles whether a match in the other table was found or not. 5.10.1 Dealing with multiple matches In the example above, there was a one-to-one relationship between the gene symbols in both tibbles. This made the joined tibble tidy. However, when a one-to-many relationship exists, i.e. one gene symbol in one tibble has multiple rows in the other, this can lead to what appears to be duplicate rows in the joined result. Due to the relative instability of gene symbols, it is very common to have multiple Ensembl genes associated with a single gene symbol. The following takes a gene mapping of Ensembl IDs to gene symbols and identifies cases where multiple Ensembl IDs map to a single gene symbol: readr::read_tsv(&quot;mart_export.tsv&quot;) %&gt;% dplyr::filter( `HGNC symbol` != &quot;NA&quot; &amp; # many unstudied genes have Ensembl IDs but no official symbol `HGNC symbol` %in% `HGNC symbol`[duplicated(`HGNC symbol`)]) %&gt;% dplyr::arrange(`HGNC symbol`) ## # A tibble: 7,268 x 3 ## `Gene stable ID` `HGNC symbol` `Gene name` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ENSG00000261846 AADACL2 arylacetamide deacetylase like 2 ## 2 ENSG00000197953 AADACL2 arylacetamide deacetylase like 2 ## 3 ENSG00000262466 AADACL2-AS1 &lt;NA&gt; ## 4 ENSG00000242908 AADACL2-AS1 &lt;NA&gt; ## 5 ENSG00000276072 AATF apoptosis antagonizing transcription factor ## 6 ENSG00000275700 AATF apoptosis antagonizing transcription factor ## 7 ENSG00000281173 ABCB10P1 &lt;NA&gt; ## 8 ENSG00000280461 ABCB10P1 &lt;NA&gt; ## 9 ENSG00000274099 ABCB10P1 &lt;NA&gt; ## 10 ENSG00000282479 ABCB10P3 &lt;NA&gt; ## # ... with 7,258 more rows There are over 7,000 Ensembl IDs that map to the same gene symbol as another Ensembl ID. That is more than 10% of all Ensembl IDs. So now lets create a new gene_stats tibble with one of these gene symbols and join with the map to see what happens: gene_map &lt;- readr::read_tsv(&quot;mart_export.tsv&quot;) gene_stats &lt;- tribble( ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p, &quot;APOE&quot;, 12.509293, 0.1032, 34.239521, 1.3e-5, &quot;HOXD1&quot;, 4.399211, 0.6323, 16.332318, 0.0421, &quot;SNCA&quot;, 45.748431, 4.2e-9, 0.757188, 0.9146, &quot;DEAF1&quot;, 0.000000, 1.0, 0, 1.0 ) %&gt;% left_join(gene_map, by=c(&quot;gene&quot; = &quot;HGNC symbol&quot;)) gene_stats ## # A tibble: 5 x 7 ## gene test1_stat test1_p test2_stat test2_p `Gene stable ID` `Gene name` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipopro~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox D1 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein ~ ## 4 DEAF1 0 1 0 1 ENSG00000282712 DEAF1 tran~ ## 5 DEAF1 0 1 0 1 ENSG00000177030 DEAF1 tran~ Notice how there are two rows for the DEAF1 gene that have identical values except for the Stable Gene ID column. This is a very common problem when mapping gene symbols to other gene identifiers and there is no general solution to picking the best mapping, short of manually inspecting all of the duplicates and choosing which one is the most appropriate yourself (which obviously is a huge amount of work). However, we do desire to remove the duplicated rows. In this case, since all the values besides the Ensembl ID are the same, effectively it doesnt matter which duplicate rows we eliminate. We can do this using the duplicated() function, which returns TRUE for all but the first row of a set of duplicated values: filter(gene_stats, !duplicated(gene)) ## # A tibble: 4 x 7 ## gene test1_stat test1_p test2_stat test2_p `Gene stable ID` `Gene name` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipopro~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox D1 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein ~ ## 4 DEAF1 0 1 0 1 ENSG00000282712 DEAF1 tran~ However, in general, you must be careful about identifying these types of one-to-many mapping issues and also about how you mitigate them. R for Data Science - Relational Data dplyr - mutate joins "],["data-science.html", "6 Data Science", " 6 Data Science Data science is an enormous and rapidly growing field that incorporates elements of statistics, computer science, software engineering, high performance and cloud computing, and big data management, as well as syntheses with knowledge from other social and physical science fields to model and predict phenomena captured by data collected from the real world. Many books have and will be written on this subject, and this one does not pretend to even attempt to give this area an adequate treatment. Like the rest of this book, the topics covered here are opinionated and presented through the lens of a biological analyst practitioner with enough high level conceptual details and a few general principles to hopefully be useful in that context. "],["data-modeling.html", "6.1 Data Modeling", " 6.1 Data Modeling The goal of data modeling is to describe a dataset using a relatively small number of mathematical relationships. Said differently, a model uses some parts of a dataset to try to accurately predict other parts of the dataset in a way that is useful to us. Models are human inventions; they reflect our beliefs about the way the universe works. The successful model identifies patterns within a dataset that are the result of causal relationships in the universe that led to the phenomena that were measured while accounting for noise in the data. However, the model itself does not identify or even accurately reflect those causal effects. The model merely summarizes patterns and we as scientists are left to interpret those patterns and design follow up experiments to investigate the nature of those causal relationships using our prior knowledge of the world. There are several principles to keep in mind when modeling data: Data are never wrong. All data are collected using processes and devices designed and implemented by humans, who always have biases and make assumptions. All data measure something about the universe, and so are true in some sense of the word. If what we intended to measure and what we actually measured were not the same thing, that is due to our errors in collection or interpretation, not due to the data being wrong. If we approach our dataset with a particular set of hypotheses and the data dont support those hypotheses, it is our beliefs of the world and our understanding of the dataset that are wrong, not the data itself. Not all data are useful. Just because data isnt wrong, it doesnt mean it is useful. There may have been systematic errors in the collection of the data that makes interpreting them difficult. Data collected for one purpose may not be useful for any other purposes. And sometimes, a dataset collected for a particular purpose may simply not have the information needed to answer our questions; if what we measure has no relationship to what we wish to predict, the data itself is not useful - though the knowledge that what we measured has no detectable effect on the thing we wish to predict may be very useful! All models are wrong, but some are useful. George Box, the renowned British statistician, famously asserted this in a 1976 paper to the Journal of the American Statistical Association. (Box 1976). By this he meant that every model we create is a simplification of the system we are seeking to model, which is by definition not identical to that system. To perfectly model a system, our model would need to be precisely the system we are modeling, which is no longer a model but the system itself. Fortunately, even though we know our models are always wrong to some degree, they may nonetheless be useful because they are not too wrong. Some models may indeed be too wrong, though. Data do not contain causal information. Correlation does not mean causation. Data are measurements of the results of a process in the universe that we wish to understand; the data are possibly reflective of that process, but do not contain any information about the process itself. We cannot infer causal relationships from a dataset alone. We must construct possible causal models using our knowledge of the world first, then apply our data to our model and other alternative models to compare their relative plausibility. All data have noise. The usefulness of a model to describe a dataset is related to the relative strength of the patterns and noise in the dataset when viewed through the lens of the model; conceptually, the so-called signal to noise ratio of the data. The fundamental concern of statistics is quantifying uncertainty (i.e. noise), and separating it from real signal, though different statistical approaches (e.g. frequentist vs Bayesian) reason about uncertainty in different ways. Modeling begins (or should begin) with posing one or more scientific models of the process or phenomenon we wish to understand. The scientific model is conceptual; it reflects our belief of the universe and proposes a causal explanation for the phenomenon. We then decide how to map that scientific model onto a statistical model, which is a mechanical procedure that quantifies how well our scientific model explains a dataset. The scientific model and statistical model are related but independent choices we make. There may be many valid statistical models that represent a given scientific model. However, sometimes in practice we lack sufficient knowledge about the process to propose scientific models first, requiring data exploration and summarization first to suggest reasonable starting points. This section pertains primarily to models specified explicitly by humans. There is another class of models, namely those created by certain machine learning algorithms like neural networks and deep learning, that discover models from data. These models are fundamentally different than those designed by human minds, in that they are often accurate and therefore useful, but it can be very difficult if not impossible to understand how they work. While these are important types of models that fall under the umbrella of data science, we limit the content of this chapter to human designed statistical models. 6.1.1 A Worked Modeling Example As an example, lets consider a scenario where we wish to assess whether any of three genes can help us distinguish between patients who have Parkinsons Disease and those who dont by measuring the relative activity of those genes in blood samples. We have the following made-up dataset: gene_exp ## # A tibble: 200 x 5 ## sample_name `Disease Status` `Gene 1` `Gene 2` `Gene 3` ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 P1 Parkinson&#39;s 257. 636. 504. ## 2 P2 Parkinson&#39;s 241. 556. 484. ## 3 P3 Parkinson&#39;s 252. 426. 476. ## 4 P4 Parkinson&#39;s 271. 405. 458. ## 5 P5 Parkinson&#39;s 248. 482. 520. ## 6 P6 Parkinson&#39;s 275. 521. 460. ## 7 P7 Parkinson&#39;s 264. 415. 568. ## 8 P8 Parkinson&#39;s 276. 450. 495. ## 9 P9 Parkinson&#39;s 274. 490. 496. ## 10 P10 Parkinson&#39;s 251. 584. 573. ## # ... with 190 more rows Our imaginary dataset has 100 Parkinsons and 100 control subjects. For each of our samples, we have a sample ID, Disease Status of Parkinson's or Control, and numeric measurements of each of three genes. Below are violin plots of our (made-up) data set for these three genes: By inspection, it appears that Gene 1 has no relationship with disease; we may safely eliminate this gene from further consideration. Gene 2 appears to have a different profile depending on disease status, where control individuals have a higher average expression and a lower variance. Unfortunately, despite this qualitative difference, this gene may not be useful for telling whether someone has disease or not - the ranges completely overlap. Gene 3 appears to discriminate between disease and control. There is some overlap in the two expression distributions, but above a certain expression value these data suggest a high degree of predictive accuracy may be obtained with this gene. Measuring this gene may therefore be useful, if the results from this dataset generalize to all people with Parkinsons Disease. So far, we have not done any modeling, but instead relied on plotting and our eyes. A more quantitative question might be: how much higher is Gene 3 expression in Parkinsons Disease than control? Another way of posing this question is: if I know a patient has Parkinsons Disease, what Gene 3 expression value do I expect them to have? Written this way, we have turned our question into a prediction problem: if we only had information that a patient had Parkinsons Disease, what is the predicted expression value of their Gene 3? Another way to pose this prediction question is in the opposite (and arguably more useful) direction: if all we knew about a person was their Gene 3 gene expression, how likely is it that the person has Parkinsons Disease? If this gene expression is predictive enough of a persons disease status, it may be a viable biomarker of disease and thus might be useful in a clinical setting, for example when identifying presymptomatic individuals or assessing the efficacy of a pharmacological treatment. Although it may seems obvious, before beginning to model a dataset, we must start by posing the scientific question as concisely as possible, as we have done above. These questions will help us identify which modeling techniques are appropriate and help us ensure we interpret our results correctly. We will use this example dataset throughout this chapter to illustrate some key concepts. 6.1.2 Data Summarization Broadly speaking, data summarization is the process of finding a lower-dimensional representation of a larger dataset. There are many ways to summarize a set of data; each approach will emphasize different aspects of the dataset, and have varying degrees of accuracy. Consider the gene expression of Gene 1 for all individuals in our example above, plotted as a distribution with a histogram: ggplot(gene_exp, aes(x=`Gene 1`)) + geom_histogram(bins=30,fill=&quot;#a93c13&quot;) 6.1.2.1 Point Estimates The data are concentrated around the value 250, and become less common for larger and smaller values. Since the extents to the left and right of the middle of the distribution appear to be equally distant, perhaps the arithmetic mean is a good way to identify the middle of the distribution: ggplot(gene_exp, aes(x=`Gene 1`)) + geom_histogram(bins=30,fill=&quot;#a93c13&quot;) + geom_vline(xintercept=mean(gene_exp$`Gene 1`)) By eye, the mean does seem to correspond well to the value that is among the most frequent, and successfully captures an important aspect of the data: its central tendency. Summaries that compute a single number are called point estimates. Point estimates collapse the data into one singular point, one value. The arithmetic mean is just one measure of central tendency, computed by taking the sum of all the values and dividing by the number of values. The mean may be a good point estimate of the central tendency of a dataset, but it is sensitive to outlier samples. Consider the following examples: library(patchwork) well_behaved_data &lt;- tibble(data = rnorm(1000)) data_w_outliers &lt;- tibble(data = c(rnorm(800), rep(5, 200))) # oops I add some outliers :^) g_no_outlier &lt;- ggplot(well_behaved_data, aes(x = data)) + geom_histogram(fill = &quot;#56CBF9&quot;, color = &quot;grey&quot;, bins = 30) + geom_vline(xintercept = mean(well_behaved_data$data)) + ggtitle(&quot;Mean example, no outliers&quot;) g_outlier &lt;- ggplot(data_w_outliers, aes(x = data)) + geom_histogram(fill = &quot;#7FBEEB&quot;, color = &quot;grey&quot;, bins = 30) + geom_vline(xintercept = mean(data_w_outliers$data)) + ggtitle(&quot;Mean example, big outliers&quot;) g_no_outlier | g_outlier The median is another measure of central tendency, which is found by identifying the value that divides the samples into equal halves when sorted from smallest to largest. The median is more robust in the presence of outliers. g_no_outlier &lt;- ggplot(well_behaved_data, aes(x = data)) + geom_histogram(fill = &quot;#AFBED1&quot;, color = &quot;grey&quot;, bins = 30) + geom_vline(xintercept = median(well_behaved_data$data)) + ggtitle(&quot;Median example&quot;) g_outlier &lt;- ggplot(data_w_outliers, aes(x = data)) + geom_histogram(fill = &quot;#7FBEEB&quot;, color = &quot;grey&quot;, bins = 30) + geom_vline(xintercept = median(data_w_outliers$data)) + ggtitle(&quot;Median example, big outliers&quot;) g_no_outlier | g_outlier 6.1.2.2 Dispersion Central tendencies are important aspects of the data but dont describe what the data do for values outside this point estimate of central tendency; in other words, we have not expressed the spread, or dispersion of the data. We decide that perhaps computing the standard deviation of the data may characterize the spread well, since it appears to be symmetric around the mean. We can layer this information on the plot as well to inspect it: g1_mean &lt;- mean(gene_exp$`Gene 1`) g1_sd &lt;- sd(gene_exp$`Gene 1`) ggplot(gene_exp, aes(x=`Gene 1`)) + geom_histogram(bins=30,fill=&quot;#a93c13&quot;) + geom_vline(xintercept=g1_mean) + geom_segment(x=g1_mean-g1_sd, y=10, xend=g1_mean+g1_sd, yend=10) The width of the horizontal line is proportional to the mean +/- one standard deviation around the mean, and has been placed arbitrarily on the y axis at y = 10 to show how this range covers the data in the histogram. The +/- 1 standard deviation around the mean visually describes the spread of the data reasonably well. Measures the spread of the data, typically around its perceived center (a mean). Often related to the distribution of the data. Standard deviation: A measure of how close values are to the mean. Bigger standard deviations mean data is more spread out. data &lt;- tibble(data = c(rnorm(1000, sd=1.75))) ggplot(data, aes(x = data)) + geom_histogram(fill = &quot;#EAC5D8&quot;, color = &quot;white&quot;, bins = 30) + geom_vline(xintercept = c(-2, -1, 0, 1, 2) * sd(data$data)) + xlim(c(-6, 6)) + ggtitle(&quot;Standard deviations aplenty&quot;, paste(&quot;SD:&quot;, sd(data$data))) Variance: Similar to SD (its the square of SD), variance measures how far a random value is from the mean. data &lt;- tibble(data = c(rnorm(1000, sd=0.5))) ggplot(data, aes(x = data)) + geom_histogram(fill = &quot;#DBD8F0&quot;, color = &quot;white&quot;, bins = 30) + geom_vline(xintercept = mean(data$data)) + xlim(c(-6, 6)) + ggtitle(&quot;Same mean as SD plot, but different variance&quot;, paste(&quot;Variance:&quot;, sd(data$data))) 6.1.2.3 Distributions With these two pieces of knowledge - the mean accurately describes the center of the data and the standard deviation describes the spread - we now recognize that these data may be normally distributed, and therefore we can potentially describe the dataset mathematically. We decide to visually inspect this possibility by layering a normal distribution on top of our data using stat_function: g1_mean &lt;- mean(gene_exp$`Gene 1`) g1_sd &lt;- sd(gene_exp$`Gene 1`) ggplot(gene_exp, aes(x=`Gene 1`)) + geom_histogram( aes(y=after_stat(density)), bins=30, fill=&quot;#a93c13&quot; ) + stat_function(fun=dnorm, args = list(mean=g1_mean, sd=g1_sd), size=2) Note the histogram bars are scaled with aes(y=[after_stat](https://ggplot2.tidyverse.org/reference/aes_eval.html)(density)) to the density of values in each bin to make all the bar heights sum to 1 so that the y scale matches that of a normal distribution. We have now created our first model: we chose to express the dataset as a normal distribution parameterized by the mean and standard deviation and standard deviation of the data. Using the values of 254 and 11 as our mean and standard deviation, respectively, we can express our model mathematically as follows: \\[ Gene\\;1 \\sim \\mathcal{N}(254, 11) \\] Here the \\(\\sim\\) symbol means distributed as and the \\(\\mathcal{N}(\\mu,\\sigma)\\) represents a normal distribution with mean of \\(\\mu\\) and standard deviation of \\(\\sigma\\). This is mathematical formulation means the same thing as saying we are modeling Gene 1 expression as a normal distribution with mean of 254 and standard deviation of 11. Without any additional information about a new sample, we would expect the expression of that gene to be 254, although it may vary from this value. The normal distribution is the most common distribution observed in nature, but it is hardly the only one. We could have proposed other distributions to instead summarize our data: g_norm &lt;- ggplot(tibble(data = rnorm(5000)), aes(x = data)) + geom_histogram(fill = &quot;#D0FCB3&quot;, bins = 50, color = &quot;gray&quot;) + ggtitle(&quot;Normal distribution&quot;, &quot;rnorm(n = 1000)&quot;) g_unif &lt;- ggplot(tibble(data = runif(5000)), aes(x = data)) + geom_histogram(fill = &quot;#271F30&quot;, bins = 50, color = &quot;white&quot;) + ggtitle(&quot;Uniform distribution&quot;, &quot;runif(n = 1000)&quot;) g_logistic &lt;- ggplot(tibble(data = rlogis(5000)), aes(x = data)) + geom_histogram(fill = &quot;#9BC59D&quot;, bins = 50, color = &quot;black&quot;) + ggtitle(&quot;Logistic distribution&quot;, &quot;rlogis(n = 1000)&quot;) g_exp &lt;- ggplot(tibble(data = rexp(5000, rate = 1)), aes(x = data)) + geom_histogram(fill = &quot;#6C5A49&quot;, bins = 50, color = &quot;white&quot;) + ggtitle(&quot;Exponential distribution&quot;, &quot;rexp(n = 1000, rate = 1)&quot;) (g_norm | g_unif) / (g_logistic | g_exp) In addition to the normal distribution, we have also plotted samples drawn from a continuous uniform distribution between 0 and 1, a logistic distribution which is similar to the normal distribution but has heavier tails, and an exponential distribution. There are many more distributions than these, and many of them were discovered to arise in nature and encode different types of processes and relationships. A few notes on our data modeling example before we move on: Our model choice was totally subjective. We looked at the data and decided that a normal distribution was a reasonable choice. There were many other choices we could have made, and all of them would be equally valid, though they may not all describe the data equally well. We cant know if this is the correct model for the data. By eye, it appears to be a reasonably accurate summary. However, there is no such thing as a correct model; some models are simply better at describing the data than others. Recall that all models are wrong, and some models are useful. Our model may be useful, but it is definitely wrong to some extent. We dont know how well our model describes the data yet. So far weve only used our eyes to choose our model which might be a good starting point considering our data are so simple, but we have not yet quantified how well our model describes the data, or compared it to alternative models to see which is better. This will be discussed briefly in a later section. 6.1.3 Linear Models Our choice of a normal distribution to model our Gene 1 gene expression was only descriptive; it was a low-dimensional summary of our dataset. However, it was not very informative; it doesnt tell us anything useful about Gene 1 expression with respect to our scientific question of distinguishing between Parkinsons Disease and Control individuals. To do that, we will need to find a model that can make predictions that we may find useful if we receive new data. To do that, we will introduce a new type of model: the linear model. A linear model is any statistical model that relates one outcome variable as a linear combination (i.e. sum) of one or more explanatory variables. This may be expressed mathematically as follows: \\[ Y_i = \\beta_0 + \\beta_1 \\phi_1 ( X_{i1} ) + \\beta_2 \\phi_2 ( X_{i2} ) + \\ldots + \\beta_p \\phi_p ( X_{ip} ) + \\epsilon_i \\] Above, \\(Y_i\\) is some outcome or response variable we wish to model, \\(X_{ij}\\) is our explanatory or predictor variable \\(j\\) for observatoin \\(i\\), and \\(\\beta_j\\) are coefficients estimated to minimize the difference between the predicted outcome \\(\\hat{Y_i}\\) and the observed \\(Y_i\\) over all observations. \\(\\phi_j\\) is a possibly non-linear transformation of the explanatory variable \\(X_ij\\); note these functions may be non-linear so long as the predicted outcome is modeled as a linear combination of the transformed variables. The rest of this section is dedicated to a worked example of a linear model for gene expression data. Let us begin with a beeswarm plot plot of Gene 3: library(ggbeeswarm) ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) + geom_beeswarm() The expression values within each disease status look like they might be normally distributed just like Gene 1, so lets summarize each group with the arithmetic mean and standard deviation as before, and instead plot both distributions as histograms: exp_summ &lt;- pivot_longer( gene_exp, c(`Gene 3`) ) %&gt;% group_by(`Disease Status`) %&gt;% summarize(mean=mean(value),sd=sd(value)) pd_mean &lt;- filter(exp_summ, `Disease Status` == &quot;Parkinson&#39;s&quot;)$mean c_mean &lt;- filter(exp_summ, `Disease Status` == &quot;Control&quot;)$mean ggplot(gene_exp, aes(x=`Gene 3`, fill=`Disease Status`)) + geom_histogram(bins=20, alpha=0.6,position=&quot;identity&quot;) + annotate(&quot;segment&quot;, x=c_mean, xend=pd_mean, y=20, yend=20, arrow=arrow(ends=&quot;both&quot;, angle=90)) + annotate(&quot;text&quot;, x=mean(c(c_mean,pd_mean)), y=21, hjust=0.5, label=&quot;How different?&quot;) We can make a point estimate of this difference by simply subtracting the means: pd_mean - c_mean ## [1] 164.0942 In other words, this point estimate suggests that on average Parkinsons patients have 164.1 greater expression than Controls. We can plot this relationship relatively simply: ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) + geom_beeswarm() + annotate(&quot;segment&quot;, x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) However, this point estimate tells us nothing about how confident we are about the difference. We can do this by using an linear regression by modeling Gene 3 as a function of disease status: fit &lt;- lm(`Gene 3` ~ `Disease Status`, data=gene_exp) fit ## ## Call: ## lm(formula = `Gene 3` ~ `Disease Status`, data = gene_exp) ## ## Coefficients: ## (Intercept) `Disease Status`Parkinson&#39;s ## 334.6 164.1 The coefficient associated with having the disease status of Parkinsons disease is almost exactly equal to our difference in means. We also note that the coefficient labeled (Intercept) is nearly equal to the mean of our control samples (334.6). Under the hood, this simple linear model did the same calculation we did by subtracting the means of each group but estimated the means using all the data at once, instead of point estimates. Another advantage of using lm() over the point estimate method is the model can estimate how confident the model was that the difference in mean between Parkinsons and controls subjects. Lets print some more information about the model than before: summary(fit) ## ## Call: ## lm(formula = `Gene 3` ~ `Disease Status`, data = gene_exp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -124.303 -25.758 -2.434 30.518 119.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 334.578 4.414 75.80 &lt;2e-16 *** ## `Disease Status`Parkinson&#39;s 164.094 6.243 26.29 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 44.14 on 198 degrees of freedom ## Multiple R-squared: 0.7773, Adjusted R-squared: 0.7761 ## F-statistic: 691 on 1 and 198 DF, p-value: &lt; 2.2e-16 We again see our coefficient estimates for the intercept (i.e. mean control expression) and our increase in Parkinsons, but also a number of other terms, in particular Pr(&gt;|t|) and Multiple R-squared. The former is the p-value associated with each of the coefficient estimates, both of which are very small, indicating to us that the model was very confident of the estimated values. The latter, multiple R-squared or \\(R^2\\), describes how much of the variance in the data was explained by the model it found as a fraction between 0 and 1. This model explains 77.7% of the variance of the data, which is substantial. The \\(R^2\\) value also has an associated p-value, which is also very small. Overall, these statistics suggest this model fits the data very well. We can plot the results of a linear model for each of our genes relatively easily: pd_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Parkinson&#39;s&quot;)$`Gene 1`) c_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Control&quot;)$`Gene 1`) g1 &lt;- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 1`, color=`Disease Status`)) + geom_beeswarm() + annotate(&quot;segment&quot;, x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) + theme(legend.position=&quot;none&quot;) pd_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Parkinson&#39;s&quot;)$`Gene 2`) c_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Control&quot;)$`Gene 2`) g2 &lt;- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 2`, color=`Disease Status`)) + geom_beeswarm() + annotate(&quot;segment&quot;, x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) + theme(legend.position=&quot;none&quot;) pd_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Parkinson&#39;s&quot;)$`Gene 3`) c_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Control&quot;)$`Gene 3`) g3 &lt;- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) + geom_beeswarm() + annotate(&quot;segment&quot;, x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) + theme(legend.position=&quot;none&quot;) g1 | g2 | g3 We can also compute the corresponding linear model fits, and confirm that the coefficients agree with the directions observed in the plot, as well as that all the associations are significant FDR &lt; 0.05: fit1 &lt;- lm(`Gene 1` ~ `Disease Status`, data=gene_exp) fit2 &lt;- lm(`Gene 2` ~ `Disease Status`, data=gene_exp) fit3 &lt;- lm(`Gene 3` ~ `Disease Status`, data=gene_exp) gene_stats &lt;- bind_rows( c(&quot;Gene 1&quot;,coefficients(fit1),summary(fit1)$coefficients[2,4]), c(&quot;Gene 2&quot;,coefficients(fit2),summary(fit2)$coefficients[2,4]), c(&quot;Gene 3&quot;,coefficients(fit3),summary(fit3)$coefficients[2,4]) ) colnames(gene_stats) &lt;- c(&quot;Gene&quot;,&quot;Intercept&quot;,&quot;Parkinson&#39;s&quot;,&quot;pvalue&quot;) gene_stats$padj &lt;- p.adjust(gene_stats$pvalue,method=&quot;fdr&quot;) gene_stats ## # A tibble: 3 x 5 ## Gene Intercept `Parkinson&#39;s` pvalue padj ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Gene 1 250.410735540151 6.95982194659045 3.92131170913765e-06 3.92e- 6 ## 2 Gene 2 597.763814010886 -93.6291659250254 2.37368730897756e-13 3.56e-13 ## 3 Gene 3 334.57788193953 164.094204856583 1.72774902843408e-66 5.18e-66 We have just performed our first elementary differential expression analysis using a linear model. Specifically, we examined each of our genes for a statistical relationship with having Parkinsons disease. The mechanics of this analysis are beyond the scope of this book, but later when we consider differential expression analysis packages in chapter 7 this pattern should be familiar after this example. If you are familiar with logistic regression, you might have wondered why we didnt model disease status, which is a binary variable, as a function of gene expression like Disease Status ~ Gene 3. There are several reasons for this, complete separation principally among them. In logistic regression, complete separation occurs when all the predictor values (e.g. gene expression) for one outcome group are greater or smaller than the other; i.e. there is no overlap in the values between groups. This causes logistic regression to fail to converge, leaving these genes with no statistics even though these genes are potentially the most interesting! There are methods (e.g. Firths Logistic Regression) that overcome this problem, but methods that model gene expression as a function of other outcome variables were developed first and remain the most popular. Modeling Basics - R for Data Science Least squares regression 6.1.4 Flavors of Linear Models The linear model implemented above is termed linear regression due to the way it models the relationship between the predictor variables and the outcome. Specifically, linear regression makes some strong assumptions about that relationship that may not always hold for all datasets. To address this limitation, a more flexible class of linear models called generalized linear models that allow these assumptions to be relaxed by changing the mathematical relationship between the predictors and outcome using a link function, and/or by mathematically transforming the predictors themselves. Some of the more common generalized linear models are listed below: Logistic regression - models a binary outcome (e.g. disease vs control) as a linear combination of predictor variables by using the logistic function as the link function. Multinomial regression - models a multinomial (i.e. categorical variable with more than two categories) using a multinomial logit function link function Poisson regression - models an outcome variable that are count data as a linear combination of predictors using the logarithm as the link function; this model is commonly used when modeling certain types of high throughput sequencing data Negative binomial regression - also models an outcome variable that are count data but relaxes some of the assumptions of Poisson regression, namely the mean-variance equality constraint; negative binomial regression is commonly used to model gene expression estimated from RNASeq data. Generalized linear models are very flexible, and there are many other types of these models used in biology and data science. It is important to be aware of the characteristics of the outcome you wish to model and choose the modeling method that is most suitable. Generalized linear models 6.1.5 Assessing Model Accuracy . References "],["statistical-distributions.html", "6.2 Statistical Distributions", " 6.2 Statistical Distributions Biological data, like all data, is uncertain. Measurements always contain noise, but a collection of measurements do not always contain signal. The field of statistics grew from the recognition that mathematics can be used to quantify uncertainty and help us reason about whether a signal exists within a dataset, and how certain we are of that signal. At its core, statistics is about separating the signal from the noise of a dataset in a rigorous and precise way. One of the fundamental statistical tools used when estimating uncertainty is the statistical distribution, or probability distribution, or simply distribution. There are two broad classes of distributions: statistical, or theoretical, distributions and empirical distributions. In this section we will discuss some general properties of distributions, briefly describe some common probability distributions, and explain how to specify and use these distributions in R. 6.2.1 Random Variables A random variable is an object or quantity which depends upon random events and that can have samples drawn from it. For example, a six-sided die is a random variable with six possible outcomes, and a single roll of the die will yield one of those outcomes with some probability (equal probability, if the die is fair). A coin is also a random variable, with heads or tails as the outcome. A genes expression in a RNASeq experiment is a random variable, where the possible outcomes are any non-negative integer corresponding to the number of reads that map to it. In these examples, the outcome is a simple category or real number, but random variables can be associated with more complex outcomes as well, including trees, sets, shapes, sequences, etc. The probability of each outcome is specified by the distribution associated with the random variable. Random variables are usually notated as capital letters, like \\(X,Y,Z,\\) etc. A sample drawn from a random variable is usually notated as a lowercase of the same letter, e.g. \\(x\\) is a sample drawn from the random variable \\(X\\). The distribution of a random variable is usually described like \\(X\\) follows a binomial distribution or \\(Y\\) is a normally distributed random variable. The probability of a random variable taking one of its possible values is usually written \\(P(X = x)\\). The value of \\(P(X = x)\\) is defined by the distribution of \\(X\\). As we will see later in the p-values section, sometimes we are also interested in the probability of observing a value of \\(x\\) or larger (or smaller). These probabilities are written \\(P(X &gt; x)\\) (or \\(P(X &lt; x)\\)). How these probabilities are computed is described in the next section. 6.2.2 Statistical Distribution Basics By definition, a statistical distribution is a function that maps the possible values for a variable to how often they occur. Said differently, a statistical distribution is used to compute the probability of seeing a single value, or a range of values, relative to all other possible values, assuming the random variable in question follows the statistical distribution. The following is a visualization of the theoretical distribution of a normally distributed random variable: tibble( x = seq(-4,4,by=0.1), `Probability Density`=dnorm(x,0,1) ) %&gt;% ggplot(aes(x=x,y=`Probability Density`)) + geom_line() + labs(title=&quot;Probability Density Function for a Normal Distribution&quot;) The plot above depicts to the probability density function (PDF) of a normal distribution with mean of zero and a standard deviation of one. The PDF defines the probability associated with every possible value of \\(x\\), which for the normal distribution is all real numbers. All PDFs have a closed mathematical form. The PDF for the normal distribution is: \\[ P(X = x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma}} \\] The notation \\(P(X = x|\\mu,\\sigma)\\) is read like the probability that the random variable \\(X\\) takes the value \\(x\\), given mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The normal distribution is an example of a parametric distribution because its PDF requires two parameters - mean and standard deviation - to compute probabilities. The choice of parameter values \\(\\mu\\) and \\(\\sigma\\) in the normal distribution determine the probability of the value of \\(x\\). Probability density functions are defined for continuous distributions only, as described later in this section. Discrete distributions have a probability mass function (PMF) instead of a PDF, since the probability distribution is subdivided into a set of different categories. PMFs have different mathematical characteristics than PDFs (e.g. they are not continuous and therefore are not differentiable), but serve the same purpose. In probability theory, if a plausible event has a probability of zero, this does not mean that event can never occur. In fact, every specific value in a distribution that supports all real numbers has a probability of zero (i.e. one specific value out of an infinite number of values). Instead, the probability distribution function allows us to reason about the relative likelihood of observing values in one range of the distribution compared with the others. While most values have extremely small relative probabilities, they never are equal to zero. This is due to the asymptotic properties of probability distributions, where every supported value of the distribution has a non-zero value by definition, though many values may be very close to zero. The PDF provides the probability density of specific values within the distribution, but sometimes we are interested in the probability of a value being less than or equal to a particular value. We compute this using the cumulative distribution function (CDF) or sometimes just distribution function. In the plot below, both the PDF and CDF are plotted for a normal distribution with mean zero and standard deviation of 1: tibble( x = seq(-4,4,by=0.1), PDF=dnorm(x,0,1), CDF=pnorm(x,0,1) ) %&gt;% ggplot() + geom_line(aes(x=x,y=PDF,color=&quot;PDF&quot;)) + geom_line(aes(x=x,y=CDF,color=&quot;CDF&quot;),linetype=&quot;dashed&quot;) The value of the CDF corresponds to the area under the density curve up to the corresponding value of \\(x\\); 1 minus that value is the area under the curve greater than that value. The following figures illustrate this: 91% of the probability density is less than the (arbitrary) value of \\(x=1.37\\), and likewise 9% is greater. This is how p-values are calculated, as described later. The CDF is also useful for generating samples from the distribution. In the following plot, 1000 random samples were drawn from a uniform distribution in the interval \\((0,1)\\) and plotted using the inverse CDF function: The histograms on the margins show the distributions of the \\(p\\) and \\(x\\) values for the scatter points. The \\(p\\) coordinates are uniform, while the \\(x\\) distribution is normal with mean of zero and standard deviation of one. In this way, we can draw samples from a normal distribution, or any other distribution that has an invertible CDF. 6.2.3 Distributions in R There are four key operations we performed with the normal distribution in the previous section: Calculate probabilities using the PDF Calculate cumulative probabilities using the CDF Calculate the value associated with a cumulative probability Sample values from a parameterized distribution In R, each of these operations has a dedicated function for each different distribution, prefixed by d, p, q, and r. For the normal distribution, these functions are dnorm, pnorm, qnorm, and rnorm: dnorm(x, mean=0, sd=1) - PDF of the normal distribution pnorm(q, mean=0, sd=1) - CDF of the normal distribution qnorm(p, mean=0, sd=1) - inverse CDF; accepts quantiles between 0 and 1 and returns the value of the distribution for those quantiles rnorm(n, mean=0, sd=1) - generate n samples from a normal distribution R uses this scheme for all of its base distributions, which include: Distribution Probability Density Function Normal dnorm(x,mean,sd) t Distribution dt(x,df) Poisson dpois(n,lambda) Binomial dbinom(x, size, prob) Negative Binomial dnbinom(x, size, prob, mu) Exponential dexp(x, rate) \\(\\chi^2\\) dchisq(x, df) There are many more distributions implemented in R beyond these, and they all follow the same scheme. The next two sections will cover examples of some of these distributions. Generally, statistical distributions are divided into two categories: discrete distributions and continuous distributions. 6.2.4 Discrete Distributions Discrete distributions are defined over countable, possibly infinite sets. Common discrete distributions include binomial (e.g. coin flips), multinomial (e.g. dice rolls), Poisson (e.g. number of WGS sequencing reads that map to a specific locus), etc. Below we describe a few of these in detail. 6.2.4.1 Bernoulli random trail and more One of the examples for discrete random variable distribution is the Bernoulli function. A Bernoulli trail has only 2 outcomes with probability \\(p\\) and \\((1-p)\\). Consider flipping a fair coin, and the random variable X can take value 0 or 1 indicating you get a head or a tail. If its a fair coin, we would expect the \\(Pr(X = 0) = Pr(X = 1) = 0.5\\). Or, if we throw a die and we record X = 1 when we get a six, and X = 0 otherwise, then \\(Pr(X = 0) = 5/6\\) and \\(Pr(X = 1) = 1/6\\). Now consider a slightly complicated situation: what if we are throwing the die \\(n\\) times and we would like to analyze the total number of six, say \\(x\\), we get during those \\(n\\) throws? Now, this leads us to the binomial distribution. If its a fair die, we would say our proportion parameter \\(p = 1/6\\), which means the probability we are getting a six is 1/6 for each throw. \\[ \\begin{equation} f(x) = \\frac {n!} {x!(n-x)!}p^x (1-p) ^{(n-x)} \\end{equation} \\] The geometric random variable, similar to the binomial, is also from a sequence of random Bernoulli trials with a constant probability parameter \\(p\\). But this time, we define the random variable \\(X\\) as the number of consecutive failures before the first success. In this case, the probability of \\(x\\) consecutive failures followed by success on trial \\(x+1\\) is: \\[ \\begin{equation} f(x) = p * (1-p)^x \\end{equation} \\] The negative binomial distribution goes one step forward. This time, we are still performing a sequence of independent Bernoulli random trials with a constant probability of success equal to \\(p\\). But now we would like to record the random variable \\(Z\\) to be the total number of failures before we finally get to the \\(r^{th}\\) success. In other words, when we get to the \\(r^{th}\\) success, we had \\(x+r\\) Bernoulli random trails, in which \\(x\\) times failed and \\(r\\) times succeeded. \\[ \\begin{equation} f(x) = \\frac {x+r-1} {r-1} p^r {(1-p)}^x \\end{equation} \\] 6.2.4.2 Poisson The Poisson distribution is used to express the probability of a given number of events occurring in a fixed interval of time or space, and these events occur with a known constant mean rate and independently of the time since the last event. But no one understands this definition. The formula for Poisson distribution is: \\[ \\begin{equation} f(k; \\lambda) = Pr(X=k) = \\frac {\\lambda^k e^{-\\lambda}} {k!} \\end{equation} \\] lambda is the expected value of the random variable X k is the number of occurrences e is Eulers number (e=2.71828) okay, the formula makes it even more confusing. Imagine you are working at a mail reception center, and your responsibility is to receive incoming letters. Assume the number of incoming letters is not affected by the day of the week or season of the year. You are expected to get 20 letters on average in a day. But, the actual number of letters you receive each day will not be perfectly 20. You recorded the number of letters you receive each day in a month (30 days). In the following plot, each dot represents a day. The x-axis is calender day, and y-axis is the number of letters you receive on that day. Although on average you are receiving 20 letters each day, the actual number of letters each day vary a lot. set.seed(2) my_letter &lt;- rpois(n = 30, lambda = 20) plot(my_letter, main = &quot;Letters received each day&quot;, xlab = &quot;day of the month&quot;, ylab = &quot;number of letters&quot;, pch = 19, col = &quot;royalblue&quot; ) abline(a = 20, b = 0, lwd = 2, lty = 3, col = &quot;salmon&quot;) Now, lets plot the density plot of our data. The \\(x\\)-axis is the number of letters on a single day, and the \\(y\\)-axis is the probability. plot(density(my_letter), lwd = 2, col = &quot;royalblue&quot;, main = &quot;Probability of number of letters each day&quot;, xlab = &quot;number of letters&quot; ) Since we only have 30 data points, it doesnt look like a good curve. But, after we worked at the mail reception for 5000 days, it becomes much closer to the theoretical Poisson distribution with lambda = 20. set.seed(3) plot(density(rpois(n = 5000, lambda = 20)), lwd = 2, col = &quot;royalblue&quot;, main = &quot;Probability of number of letters each day&quot;, xlab = &quot;number of letters&quot; ) Here is the theoretical Poisson distribution with lambda = 20: plot(dpois(c(1:40), lambda = 20), lwd = 2, type = &quot;l&quot;, col = &quot;royalblue&quot;, ylab = &quot;probability&quot;, main = &quot;Poisson lambda=20&quot; ) If we want to know whats the probability to receive, for example, 18 letters, we can use dpois() function. dpois(x = 18, lambda = 20) ## [1] 0.08439355 If we want to know the probability of receiving 18 or less letters, use ppois() function. ppois(q = 18, lambda = 20, lower.tail = T) ## [1] 0.3814219 It is the cumulative area colored in the following plot: plot(dpois(c(1:40), lambda = 20), lwd = 2, type = &quot;l&quot;, col = &quot;royalblue&quot;, ylab = &quot;probability&quot;, main = &quot;Poisson lambda=20&quot; ) polygon( x = c(1:18, 18), y = c(dpois(c(1:18), lambda = 20), 0), border = &quot;royalblue&quot;, col = &quot;lightblue1&quot; ) segments(x0 = 18, y0 = 0, y1 = 0.08439355, lwd = 2, lty = 2, col = &quot;salmon&quot;) qpois() is like the reverse of ppois(). qpois(p = 0.3814219, lambda = 20) ## [1] 18 Lets review the definition of Poisson distribution. these events occur in a fixed interval of time or space, which is a day; these events occur with a known constant mean rate, which is 20 letters; independently of the time since the last event, which means the number of letters you receive today is independent of the letter you receive tomorrow. more work today dont guarantee less work tomorrow, just like in real life Now lets re-visit the formula. \\[ \\begin{equation} f(k; \\lambda) = Pr(X=k) = \\frac {\\lambda^k e^{-\\lambda}} {k!} \\end{equation} \\] lambda is the expected value of \\(X\\), which is 20 letters in this example. \\(k\\) is the number of occurrences, which is the number of letters you get on a specific day. e is Eulers number (e=2.71828) 6.2.5 Continuous Distributions In contrast with discrete distributions, continuous distributions are defined over infinite, possibly bounded domains, e.g. all real numbers. There are a number of different continuous distributions. Here we will focus on the Normal distribution (Gaussian distribution). A lot of variables in real life are normally distributed, common examples include peoples height, blood pressure, and students exam score. This is what a normal distribution with mean = 0 and standard deviation = 1 looks like: set.seed(2) norm &lt;- rnorm(n = 50000, mean = 0, sd = 1) plot(density(norm), main = &quot;A Normal distribution&quot;, xlab = &quot;x&quot;, lwd = 2, col = &quot;royalblue&quot; ) Similar as the Poisson distribution above, there are several functions to work with normal distribution, including rnorm(), dnorm(), pnorm(), and qnorm(). rnorm() is used to draw random data points from a normal distribution with a given mean and standard deviation. set.seed(2) rnorm( n = 6, # number of data points to draw mean = 0, # mean sd = 1 # standard deviation ) ## [1] -0.89691455 0.18484918 1.58784533 -1.13037567 -0.08025176 0.13242028 dnorm() is the density at a given quantile. For instance, in the normal distribution (mean=0, standard deviation=1) above, the probability density at 0.5 is roughly 0.35. dnorm(x = 0.5, mean = 0, sd = 1) ## [1] 0.3520653 plot(density(norm), main = &quot;A Normal distribution&quot;, xlab = &quot;x&quot;, lwd = 2, col = &quot;royalblue&quot; ) segments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 3, col = &quot;salmon&quot;) segments(x0 = -5, y0 = 0.3520653, x1 = 0.5, lwd = 2, lty = 3, col = &quot;salmon&quot;) points(x = 0.5, y = 0.3520653, cex = 1.5, lwd = 2, col = &quot;red&quot;) text(x = 1.1, y = 0.36, labels = &quot;0.3521&quot;, col = &quot;red2&quot;) pnorm() gives the distribution function. Or, you can think of it as the cumulative of the left side of the density function until a given value, which is the area colored in light blue in the following plot. pnorm(q = 0.5, mean = 0, sd = 1) ## [1] 0.6914625 plot(density(norm), main = &quot;A Normal distribution&quot;, xlab = &quot;x&quot;, lwd = 2, col = &quot;royalblue&quot; ) polygon( x = c(density(norm)$x[density(norm)$x &lt;= 0.5], 0.5), y = c(density(norm)$y[density(norm)$x &lt;= 0.5], 0), border = &quot;royalblue&quot;, col = &quot;lightblue1&quot; ) segments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 2, col = &quot;salmon&quot;) qnorm() gives the quantile function. You can think of it as the reverse of pnorm(). For instance: pnorm(q = 0.5, mean = 0, sd = 1) ## [1] 0.6914625 qnorm(p = 0.6914625, mean = 0, sd = 1) ## [1] 0.5000001 Similarly, other distributions such as chi-square distribution, they all have the same set of functions: rchisq(), dchisq(), pchisq() and qchisq() etc. 6.2.6 Empirical Distributions Empirical distributions describe the relative frequency of observed values in a dataset. Empirical distributions may have any shape, and may be visualized using any of the methods described in the Visualizing Distributions section, e.g. a density plot. The following beeswarm and density plot both visualize the same made-up dataset: Note the \\(y\\) axis in the plot on the right is scaled as a density, rather than a count. A distribution plotted as a density ensures the values sum to 1, thus making a probability distribution. This empirical distribution looks quite complicated, and cannot be easily captured with a single mean and confidence interval. "],["statistical-tests.html", "6.3 Statistical Tests", " 6.3 Statistical Tests 6.3.1 What is a statistical test A statistical test is an application of mathematics that we used to analyze quantitative data. They can be described as a way to understand how the independent variable(s) of the study affect the outcome of the dependent variable. The kind of statistical test appropriate for a study depends on a number of factors including variables characteristics, study design, and data distribution. The independent variable is (are) the variable(s) being controlled in the study to determine its effects on the dependent variable(s). Not all studies have independent variable(s). Study design The number of conditions a study has corresponds to the number of levels the independent variable has, for example 1 condition = 1 level, 2 conditions = 2 levels, and so on. This is different from the number of independent variables a study has. If a study has multiple independent variables, each will have its own number of levels. Statistical tests can be categorized as to whether they can handle 1, 2, or 2+ levels. If you have 2 or more levels, then the type of grouping will need to be considered: are they between-subjects* or repeated-measures**. A between-subject design means that each participant undergoes one condition whereas a repeated-measures design has each participant undergo all conditions. A third type exists called matched groups, where different subjects undergo different conditions but are matched to each other in some important way, and may be treated as a repeated-measure design when selecting for a statistical test *Independent design = between-subjects = between-groups; not to be confused with the variable. **Dependent design = repeated-measures = within-subjects = within-groups; also not to be confused with the variable. Variables The characteristics of your variables, both independent (predictor) and dependent (descriptor), will help you choose which statistical test to use. Specifically, the number of each, their type, and the level of measurement, will help you narrow your choices to select the appropriate test. Number of Variables The number of both independent and dependent variables will affect which test you select. Test selection will differ whether you have 1 or 2+ dependent variables and 0, 1, or 2+ independent variables. Types of Data (Continuous and Categorical) Typically in bioinformatics, data is subdivided into 3 categories: Continuous, Categorical, and Binary. Continuous data is data that can take real number values within either a finite or infinite range. For example, height is a continuous variable: 152.853288cm (60.1783 in), 182.9704cm (72.0356 in), 172.7cm(68in), 163cm(64.2in) and 145.647cm (57.3413 in) are all technically valid heights. Categorical data is data that can be divided to categories or groups such as (high, medium, and low) or (under10, 10-29, and 30+). Binary data, data that can either be one thing or another (T/D, Y/N, 0/1, etc) falls under the Categorical data umbrella though may get mentioned separately elsewhere. Data that exists in sequential, positive integer form- such as number of siblings- is called Discrete data (bonus 4th category!) but typically ends up being treated as categorical data. Levels of Measurement The main levels of measurement in we use in statistics are Ratio, Interval, Nominal, and Ordinal. Both Ratio and Interval levels have distance between measurements defined; the biggest difference between the two is that Ratio measurements have a meaningful zero value (and no negative numbers). Height in inches or cm, Kelvin, and number of students in a class are all Ratio measurements. Interval measurements do not have a meaningful zero. Celsius and Fahrenheit both have arbitrary 0s- the freezing point of pure and (a specific kind of) ocean water- making most standard temperature measurements type Interval. Ordinal measurements have a meaningful order to their values but have variable/imprecise distances between measurements, like socioeconomic status (upper, middle, and lower) and days since diagnosis (under 10, 10-39, 40-99, 100+). Nominal measurements do not have meaningful order to their values, like country of origin and yes/no. Ratio and Interval measurements are continuous variables while Nominal and Ordinal measurements are categorical variables. Data Distribution (Parametric vs Nonparametric) This is the third factor to keep in mind for test selection and only applicable for NON-nominal dependent variables. Parametric tests make the assumption that the population the data is sourced from has a normal or Gaussian distribution. These are powerful tests because of their data distribution assumption, with the downside of only being able to be used in select cases. Nonparametric tests do not assume a normal distribution and therefore can be used in a wide range of cases, thought they are less likely to find significance in the results. 6.3.2 Common Statistical Tests Here are some common statistical tests and a quick overview as to how to run them in R. If you would like more information about a specific test, links are included in the descriptions These are only some statistical tests. Heres a link to where you can find a few more And heres a link to Wikipedias list of statistical tests if you want to overload on statistical tests because its your new hobby One-Sample T-Test Dependent Variables: 1, continuous [ratio and interval] Independent Variables: 0 variables Design: 1 group, 1 level Parametric: Yes More t-test information shamelessly linked from SPH Null and Alternate (Research) Hypotheses Z-values(not mentioned but are in texts that fully explain t-tests) One-Sample t-tests come in 3 flavors: two-tailed test, one-tailed test (upper tail), and one-tailed test (lower tail). # One-sample t-test t.test(x, #numeric vector of data mu = 0, #True value of the mean. Defaults to 0 alternative = &quot;two.sided&quot;, # (&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;) depending on which you want to use. Defaults to &quot;two.sided conf.level = 0.95 #1-alpha. Defaults to 0.95 ) The t.test() function allows for multiple kinds of t-tests. To use this function to perform a one sample t-test, you will need a numeric vector of the data you want to run the data on, the mean you want to compare it to, your confidence level (1 - alpha), and your alternative hypothesis. The t-test, like other kinds of t-tests, will compare the means by calculating the t-statistic, p-value and confidence intervals of your datas mean. t: The calculated t-statistic, uses formula : (mean(x) - mu)/(sd(x)*sqrt(n_samples)) df: Degrees of Freedom. Calculated by: n_samples-1 p: The p-value of the t-test as determined by the t-statistic and df on the t-distribution table (conf.level) percent confidence interval: calculated interval where the true mean(x) is with conf.level % certainty. sample estimates, mean of x: The calculated mean of x Accepting or rejecting the null hypothesis is a matter of determining if mean(x) is outside the percent confidence interval range, if it is then the p-value will determine the significance of the results. #Example set.seed(10) x0 &lt;- rnorm(100, 0, 1) #rnorm(n_samples, true_mean, standard_deviation) ttestx0 &lt;- t.test(x0, mu = 0, alternative = &quot;two.sided&quot;, conf.level = 0.95) #actual running of the t-test #t.test(x0) yeilds the same results bc I used default values for mu, alternative, and conf.level tlessx0 &lt;- t.test(x0, mu = 0, alternative = &quot;less&quot;, conf.level = 0.90) knitr::knit_print(ttestx0) #display t-test output via knitr ## ## One Sample t-test ## ## data: x0 ## t = -1.4507, df = 99, p-value = 0.15 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.32331056 0.05021268 ## sample estimates: ## mean of x ## -0.1365489 In t.test_x0, we are running a two-tailed t-test on a vector of 100 doubles generated with rnorm() where the intended mean is 0. Since this is a two-tailed test, the alternate hypothesis is mean(x0) != 0 while the null hypothesis is mean(x0) = 0 Since mu=0 and 0 is within the range (-.323, 0.050), we have failed to reject the null hypothesis with this set of data Unpaired T-Test Dependent Variables: 1, continuous [ratio and interval] Independent Variables: 1 Design: 2 groups, 2 levels Parametric: Yes # One-sample t-test t.test(x, #numeric vector of data 1 y, #numeric vector of data 2 alternative = &quot;two.sided&quot;, # (&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;) depending on which you want to use. Defaults to &quot;two.sided conf.level = 0.95 #1-alpha. Defaults to 0.95 ) The unpaired t-test functions similarly to the one-sample t-test except instead of mu, it uses dataset y. Variance is assumed to be about equal between dataset x and y. Moer information about unpaired t-tests here Another R guide for upaired t-tests #Unpaired t-test example set.seed(10) x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 3, 1) unpaired &lt;- t.test(x, y) knitr::knit_print(unpaired) ## ## Welch Two Sample t-test ## ## data: x and y ## t = -22.51, df = 197.83, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.308051 -2.775122 ## sample estimates: ## mean of x mean of y ## -0.1365489 2.9050374 Output for an unpaired two sample t-test is similar to the output for the one-sample. Mu is typically 0 (though one could change it with mu = n in the function call) and the test uses the confidence interval for (mean(x)-mean(y)), which is (-3.308, -2.775). Since mu does not exist within the confidence interval, the null hypothesis can be rejected. Paired T-Test Dependent Variables: 1, continuous [ratio and interval] Independent Variables: 1 Design: 1 group, 2 levels Parametric: Yes # One-sample t-test t.test(x, #numeric vector of data 1 y, #numeric vector of data 2 paired = TRUE, #defaults to FALSE alternative = &quot;two.sided&quot;, # (&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;) depending on which you want to use. Defaults to &quot;two.sided conf.level = 0.95 #1-alpha. Defaults to 0.95 ) The paired t-test works for matched and repeated-measures designs. Like the unpaired t-test, it has a second data vector and does not need mu. When running a paired t-test make sure that you specify that paired = TRUE when calling the function and that x and y have the same length. More information about paired t-tests here #Unpaired t-test example set.seed(10) x &lt;- rnorm(100, 0, 13) y &lt;- rnorm(100, 3, 1) unpaired &lt;- t.test(x, y, paired = TRUE) knitr::knit_print(unpaired) ## ## Paired t-test ## ## data: x and y ## t = -3.7956, df = 99, p-value = 0.0002539 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.126787 -2.233560 ## sample estimates: ## mean of the differences ## -4.680174 Similar to the unpaired t-test, the paired t-test looks at the differences between x and y. Unlike the unpaired t-test, the statistic creates and runs its test on a third set of data created from the differences of x and y. For an x and y of length n, r would create a third data set of new = ((x1-y1), (x2-y2),...(xn-1-yn-1), (xn-yn)) and run its testing with mean(new) and sd(new) Chi-Squared test Dependent Variables: 1, categorical Independent Variables: 1 Design: 2 groups, 2 + levels Parametric: No More Information on Chi-Squared tests #How to Run a chi-squared test chisq.test(x, # numeric vector, matrix, or factor y # numeric vector; ignored if x is a matrix; factor of the same length as x if x is a factor ) 6.3.2.1 __Chi-Square Goodness-Of-Fit_ _{-} Dependent Variables: 1, categorical Independent Variables: 0 Design: 1 group Parametric: No More Information on Chi-Squared test on one sample #How to Run a chi-square goodness of fit chisq.test(x, # numeric vector p = #list of probabilities, like c(10, 10, 10, 70)/100), same length as x ) The chi-squared goodness-of-fit can compare one set of outcomes to a given set of probabilities to determine the likelihood of the test probabilities differing from the given. The set of given probabilities constitutes the null hypothesis. In the below example, the dataset is the tested outcome frequencies: the number of times x[1], x[2], x[3], and x[4] were observed. If my null hypothesis says that each of the 4 should be observed an equal number of times, then the resulting chi-square test should will have a p-value of p&lt;0.001 x &lt;- c(10,25, 18, 92) chisq.test(x, p=c(25, 25, 25, 25)/100) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 117.43, df = 3, p-value &lt; 2.2e-16 Wilcoxon-Mann Whitney test Dependent Variables: 1, (ordinal, interval, or ratio) Independent Variables: 1 Design: 2 groups, 2 levels Parametric: No More information here wilcox.test(x,#Numeric vector of data y #optional numeric vector of data) Wilcoxon Signed Ranks test Dependent Variables: 1, (ordinal, interval, or ratio) Independent Variables: 1 Design: 1 groups, 2 levels Parametric: No More information here wilcox.test(x,#Numeric vector of data y, #numeric vector of data from 2nd level- same length paired = TRUE) One-Way ANOVA Dependent Variables: 1, continuous Independent Variables: 1 Design: 2+ groups, 2+ levels Parametric: Yes More information here summary(aov(formula, #formula specifying the model data #data frame containing the variables in the formula)) Regressions Factor Analysis 6.3.3 Choosing a Test Look for your Dependent Variable(s). How many do you have? (1 or 2+) Which ones exist on a continuum, where any fraction within the valid range is technically possible, though the actual data might have rounded? (Continuous vs Categorical) For Categorical: If values do not have a definite order then its Nominal. Else its probably Ordinal Look for your Independent Variable(s). How many do you have? (0, 1, or 2+) Define their types and level of measurement as described in 1a and then do 2b and 2c if you have 1 or more independent variables How many conditions does each independent variable have? (= # of levels) Does the same subject or (equivalent subjects) undergo each condition/level? (Yes -&gt; repeated-measures; No -&gt; between-groups If you do not have nominal data, can you assume your data would have a normal distribution if you measured the general population? (Yes-&gt; parametric; No -&gt; non-parametric) 6.3.4 P-values In statistics, the p-value is the probability of obtaining the observed results assuming that the null hypothesis is correct. In other words, the p-value is the probability of the same results happening by chance selection from the normal population rather than the test condition. For instance, imagine there is a game that randomly picks one out of four colors. The game developer decides to add a 5th color, yellow, so that each color now has 1/5 chance of winning and a 4/5 chance of losing. How many of its first games would yellow have to lose in a row before you suspect the dev forgot to code in the ability for yellow to win? If you want to wait to until there is a &lt; 5% chance of yellow losing every single game in a row under normal circumstances before pointing this out to the dev, then yellow would have to lose 14/14 games (a 4.4% likelihood). To visualize how 14 losses in a row in a game of chance with 1/5 success rate would occur in less than 5% of all instances, we can run a simulation of 1000 instances of 14 games with 1/5 chance success and displaying the results with their frequencies of success: roll &lt;- function(nrolls, noptions){ outcomes &lt;- sample(1:noptions, nrolls, TRUE) return(length(outcomes[outcomes == 1])) } dataset &lt;- function(sample, nrolls, noptions){ simulations &lt;- c() for(i in 1:sample){simulations[i] &lt;- roll(nrolls, noptions)} return(simulations) } set.seed(18) Yellow_Wins &lt;- dataset(1000, 14, 5) hist(Yellow_Wins, freq=FALSE, right=FALSE) Here you see that slightly less than 5% of all run instances had 0 wins. In this situation, we are comparing if yellows win rate is significantly lower than if yellow had an equal chance at winning. The research hypothesis or alternative hypothesis in this case is that yellows chance of winning is significantly lower than the rest of the colors while the null hypothesis would be that yellows chance of winning is the same as the rest of the colors. The p-value of this test would be 0.043 because 43 out of 1000 instances of 14 rolls with a 1/5 chance of success totaled 0 successes, the number of successes we saw. If we instead were looking for 1 success or less out of 14 rolls, the p-value would be 0.19, because our simulation ran 147 instances of 1 success and 42 instances of 0 success. Since p-values are calculated by the percentage of outcomes that are higher than the one observed, on a normal distribution they may look something like this: set.seed(2) norm &lt;- rnorm(n = 50000, mean = 4, sd = 1) plot(density(norm), main = &quot;P-Values on a Normal Distribution&quot;, xlab = &quot;X-Values&quot;, lwd = 2, col = &quot;royalblue&quot; ) polygon( x = c(density(norm)$x[density(norm)$x &gt;= 6], 6), y = c(density(norm)$y[density(norm)$x &gt;= 6], 0), border = &quot;royalblue&quot;, col = &quot;lightblue1&quot; ) segments(x0 = 6, y0 = 0, y1 = 0.4, lwd = 2, lty = 2, col = &quot;salmon&quot;) text(7, 0.2, &quot;Observed Value (@ 2sd)&quot;) text(3.9, 0.01, &quot;Instances higher than crit. val -&gt;&quot;) Here the observed value is 6, which happens to be exactly 2 standard deviations above the mean. To compute the p-value of this instance, the area under the curve would need to be calculated which can be easily done by looking up the appropriate values on a z-table. Here is a link about finding the area under the normal curve SPHs information about p-values here 6.3.5 Multiple Hypothesis Testing . What is multiply hypothesis testing adjustment? Why is it important? What are some common adjustment methods (Bonferroni (FWER) and Benjamini-Hochberg (FDR))? How do we interpret adjusted p-values (depends on the adjustment method)? How do we compute adjusted p-values in R? 6.3.6 Statistical power Statistical Power is the probability that the null hypothesis was actually false but the data did not have enough evidence to reject it. In short, it is the failure to reject the null hypothesis in the presence of a significant effect. This is a Type II error, also known as a false negative. Its counterpart, the Type I error, is the probability that a null hypothesis was rejected when there was no significant effect. The chance of a Type I error is represented by alpha, the same alpha that we use to calculate significance level (1-alpha). Type II errors are represented by beta. The relationship between statistical power and beta are similar to the relationship between significance level and alpha: statistical power is calculated as 1-beta. Read more about the relationship between Type II and Type I errors To calculate beta, find the area under the curve of the research hypothesis distribution on the opposite side of the critical value line where alpha is calculated. To find the area in the tail of the distribution, you would need to reference a Z-score chart, which will not be expected of you in this course. If you wish to read more about it here is a link about finding the area under the normal curve with z-scores, same one as linked above Code sourced from: caracal, How do I find the probability of a type II error, stats.stackexchange.com, Feb 19, 2011, link, Date Accessed: Feb 28, 2022 The above graph shows the relationship of alpha and beta. Since the alternative hypothesis is looking for a higher mean, alpha is calculated by the area under the null hypothesis curve on right side of the critical value while beta is calculated by the area under the alternative hypothesis on the left of the critical value line. Should an experiment get a higher p-value than the critical value, then the Beta will increase in relationship to Alpha decreasing. In this graph, the critical value was moved to 0.025 while the null and alternative hypotheses stayed the same. As the chance of alpha decreased, the chances of beta increased. The opposite is also true, as alpha increases, beta decreases Applications of Statistical Power One of the applications of statistical power, besides as a measure of Type II error probability, is to allow you to run a power analysis. A power analysis involves using the relationship between Effect Size, Sample Size, Significance, and Statistical Power when you have three of the four parts in order to find the value of the fourth. This is typically used to find the effect size of a study, as that is often the hardest to estimate, or to find a sample size that corresponds with a desired effect size when designing a study. Further reading on statistical power and power analysis Effect Size The effect size is a measurement of the magnitude of experimental effects. There are a number of ways to calculate effect size including but not limited to Cohens D, Pearsons R, and Odds Ratio. If you would like to read about what BUs SPH has to say about it, here is the SPH module link Citations: Dorey FJ. Statistics in brief: Statistical power: what is it and when should it be used?. Clin Orthop Relat Res. 2011;469(2):619-620. doi:10.1007/s11999-010-1435-0 Parab S, Bhalerao S. Choosing statistical test. Int J Ayurveda Res. 2010;1(3):187-191. doi:10.4103/0974-7788.72494 Introduction to SAS. UCLA: Statistical Consulting Group. from https://stats.idre.ucla.edu/sas/modules/sas-learning-moduleintroduction-to-the-features-of-sas/ (accessed February 24, 2022) Ranganathan P, Gogtay NJ. An Introduction to Statistics - Data Types, Distributions and Summarizing Data. Indian J Crit Care Med. 2019 Jun;23(Suppl 2):S169-S170. doi: 10.5005/jp-journals-10071-23198. PMID: 31485129; PMCID: PMC6707495. "],["exploratory-data-analysis.html", "6.4 Exploratory Data Analysis", " 6.4 Exploratory Data Analysis So far, we have discussed methods where we chose an explicit model to summarize our data. However, sometimes we dont have enough information about our data to propose reasonable models, as we did earlier when exploring the distribution of our imaginary gene expression dataset. There may be patterns in the data that emerge when we compute different summaries and ask whether there is a non-random structure to how the individual samples or features compare with one another. The types of methods we use to take a dataset and examine it for structure without a prefigured hypothesis is called exploratory analysis and is a key approach to working with biological data. 6.4.1 Principal Component Analysis A very common method of exploratory analysis is principal component analysis or PCA. PCA is a statistical procedure that identifies so called directions of orthogonal variance that capture covariance between different dimensions in a dataset. Because the approach captures this covariance between features of arbitrary dimension, it is often used for so-called dimensionality reduction, where a large amount of variance in a dataset with a potentially large number of dimensions may be expressed in terms of a set of basis vectors of smaller dimension. The mathematical details of this approach are beyond the scope of this book, but below we explain in general terms the intuition behind what PCA does, and present an example of how it is used in a biological context. PCA decomposes a dataset into a set of orthonormal basis vectors that collectively capture all the variance in the dataset, where the first basis vector, called the first principal component explains the largest fraction of the variance, the second principal component explains the second largest fraction, and so on. There are always an equal number of principal components as there are dimensions in the dataset or the number of samples, whichever is smaller. Typically only a small number of these components are needed to explain most of the variance. Each principal component is a \\(p\\)-dimensional unit vector (i.e. a vector of magnitude 1), where \\(p\\) is the number of features in the dataset, and the values are weights that describe the components direction of variance. By multiplying this component with the values in each sample, we obtain the projection of each sample with respect to the basis of the component. The projections of each sample made with each principal component produces a rotation of the dataset in \\(p\\) dimensional space. The figure below presents a geometric intuition of PCA. Principal Component Analysis - Geometric Intuition Illustration Many biological datasets, especially those that make genome-wide measurements like with gene expression assays, have many thousands of features (e.g. genes) and comparatively few samples. Since PCA can only determine a maximum number of principal components as the smaller of the number of features or samples, we will almost always only have as many components as samples. To demonstrate this, we perform PCA using the stats::prcomp() function on an example microarray gene expression intensity dataset: # intensities contains microarray expression data for ~54k probesets x 20 samples # transpose expression values so samples are rows expr_mat &lt;- intensities %&gt;% pivot_longer(-c(probeset_id),names_to=&quot;sample&quot;) %&gt;% pivot_wider(names_from=probeset_id) # PCA expects all features (i.e. probesets) to be mean-centered, # convert to dataframe so we can use rownames expr_mat_centered &lt;- as.data.frame( lapply(dplyr::select(expr_mat,-c(sample)),function(x) x-mean(x)) ) rownames(expr_mat_centered) &lt;- expr_mat$sample # prcomp performs PCA pca &lt;- prcomp( expr_mat_centered, center=FALSE, # prcomp centers features to have mean zero by default, but we already did it scale=TRUE # prcomp scales each feature to have unit variance ) # the str() function prints the structure of its argument str(pca) ## List of 5 ## $ sdev : num [1:20] 101.5 81 71.2 61.9 57.9 ... ## $ rotation: num [1:54675, 1:20] 0.00219 0.00173 -0.00313 0.00465 0.0034 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:54675] &quot;X1007_s_at&quot; &quot;X1053_at&quot; &quot;X117_at&quot; &quot;X121_at&quot; ... ## .. ..$ : chr [1:20] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ... ## $ center : logi FALSE ## $ scale : Named num [1:54675] 0.379 0.46 0.628 0.272 0.196 ... ## ..- attr(*, &quot;names&quot;)= chr [1:54675] &quot;X1007_s_at&quot; &quot;X1053_at&quot; &quot;X117_at&quot; &quot;X121_at&quot; ... ## $ x : num [1:20, 1:20] -67.94 186.14 6.07 -70.72 9.58 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:20] &quot;A1&quot; &quot;A2&quot; &quot;A3&quot; &quot;A4&quot; ... ## .. ..$ : chr [1:20] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;prcomp&quot; The result of prcomp() is a list with five members: sdev - the standard deviation (i.e. the square root of the variance) for each component rotation - a matrix where the principal components are in columns x - the projections of the original data, aka the rotated data matrix center - if center=TRUE was passed, a vector of feature means scale - if scale=TRUE was passed, a vector of the feature variances Recall that each principal component explains a fraction of the overall variance in the dataset. The sdev variable returned by prcomp() may be used to first calculate the variance explained by each component by squaring it, then dividing by the sum: library(patchwork) pca_var &lt;- tibble( PC=factor(str_c(&quot;PC&quot;,1:20),str_c(&quot;PC&quot;,1:20)), Variance=pca$sdev**2, `% Explained Variance`=Variance/sum(Variance)*100, `Cumulative % Explained Variance`=cumsum(`% Explained Variance`) ) exp_g &lt;- pca_var %&gt;% ggplot(aes(x=PC,y=`% Explained Variance`,group=1)) + geom_point() + geom_line() + theme(axis.text.x=element_text(angle=90,hjust=0,vjust=0.5)) # make x labels rotate 90 degrees cum_g &lt;- pca_var %&gt;% ggplot(aes(x=PC,y=`Cumulative % Explained Variance`,group=1)) + geom_point() + geom_line() + ylim(0,100) + # set y limits to [0,100] theme(axis.text.x=element_text(angle=90,hjust=0,vjust=0.5)) exp_g | cum_g The first component explains nearly 20% of all variance in the dataset, followed by the second component with about 12%, the third component 9%, and so on. The cumulative variance plot on the right shows that the top 15 components are required to capture 90% of the variance in the dataset. This suggests that each sample contributes a significant amount of variance to the overall dataset, but that there are still some features that covary among them. An important use of PCA results is the identification of outlier samples. This is accomplished by plotting the projections of each sample and examining the result by eye to identify samples that are far away from the other samples. This is usually done by inspection and outlier samples chosen subjectively; there are no general rules to decide when a sample is an outlier by this method. Below the projections for components 1 and 2 are plotted against each other as a scatter plot: as_tibble(pca$x) %&gt;% ggplot(aes(x=PC1,y=PC2)) + geom_point() By eye, no samples appear to be obvious outliers. However, this plot is just one of many pairs of projections. We could plot all pairs of the first six components using the ggpairs() function in the GGally package: library(GGally) as_tibble(pca$x) %&gt;% dplyr::select(PC1:PC6) %&gt;% ggpairs() This is already nearly unintelligible and mostly uninformative. While it is very common for projections for pairs of components to be plotted, an alternative way to visualize projections across all samples is by plotting the distribution of projections for each component using a beeswarm plot: as_tibble(pca$x) %&gt;% pivot_longer(everything(),names_to=&quot;PC&quot;,values_to=&quot;projection&quot;) %&gt;% mutate(PC=fct_relevel(PC,str_c(&quot;PC&quot;,1:20))) %&gt;% ggplot(aes(x=PC,y=projection)) + geom_beeswarm() + labs(title=&quot;PCA Projection Plot&quot;) + theme(axis.text.x=element_text(angle=90,hjust=0,vjust=0.5)) Now we can see all the projections for all components in the same plot, although we cannot see how they relate to one another. These projection plots may become more useful if we layer on additional information about our samples. There are two types of samples in our dataset ( labelled A and C. We can color our pairwise scatter plot by type like so: as_tibble(pca$x) %&gt;% mutate(type=stringr::str_sub(rownames(pca$x),1,1)) %&gt;% ggplot(aes(x=PC1,y=PC2,color=type)) + geom_point() Little pattern is obvious from the plot, but we can plot pairs of components as before but now with type information: library(GGally) as_tibble(pca$x) %&gt;% mutate(type=stringr::str_sub(rownames(pca$x),1,1)) %&gt;% dplyr::select(c(type,PC1:PC6)) %&gt;% ggpairs(columns=1:6,mapping=aes(fill=type)) Examining PC3 and PC4, we now observe that there may indeed be some genes that distinguish between types based on the separation of projection scores for the two types. Finally, we can also color our beeswarm plot by type: as_tibble(pca$x) %&gt;% mutate( sample=rownames(pca$x), type=stringr::str_sub(sample,1,1) ) %&gt;% pivot_longer(PC1:PC20,names_to=&quot;PC&quot;,values_to=&quot;projection&quot;) %&gt;% mutate(PC=fct_relevel(PC,str_c(&quot;PC&quot;,1:20))) %&gt;% ggplot(aes(x=PC,y=projection,color=type)) + geom_beeswarm() + labs(title=&quot;PCA Projection Plot&quot;) + theme(axis.text.x=element_text(angle=90,hjust=0,vjust=0.5)) These approaches to plotting the results of a PCA are complementary, and all may be useful in understanding the structure of a dataset. 6.4.2 Cluster Analysis Cluster analysis or simple clustering is the task of grouping objects together such that objects within the same group, or cluster, are more similar to each other than to objects in other clusters. It is a type of exploratory data analysis that seeks to identify structure or organization in a dataset without making strong assumptions about the data or testing a specific hypothesis. Many different clustering algorithms have been developed that employ different computational strategies and are designed for data with different types and properties. The choice of algorithm is often dependent upon not only the type of data being clustered but also the comparative performance of different clustering algorithms applied to the same data. Different algorithms may identify different types of patterns, and so no one clustering method is better than any other in general. The goal of clustering may be easily illustrated with the following plot: unclustered &lt;- ggplot(well_clustered_data, aes(x=f1,y=f2)) + geom_point() + labs(title=&quot;Unclustered&quot;) clustered &lt;- ggplot(well_clustered_data, aes(x=f1,y=f2,color=cluster,shape=cluster)) + geom_point() + labs(title=&quot;Clustered&quot;) unclustered | clustered That is, the goal of clustering is to take unlabeled data that has some structure like on the left and label data points that are similar to one another to be in the same cluster. A full treatment of cluster analysis is beyond the scope of this book, but below we discuss a few of the most common and general clustering algorithms used in biology and bioinformatics. 6.4.2.1 Hierarchical Clustering Hierarchical clustering is a form of clustering that clusters data points together in nested, or hierarchical, groups based on their dissimilarity from one another. There are two broad strategies to accomplish this: Agglomerative: all data points start in their own groups, and groups are iteratively merged hierarchically into larger groups based on their similarity until all data points have been added to a group Divisive: all data points start in the same group, and are recursively split into smaller groups based on their dissimilarity Whichever approach is used, the critical step in clustering is choosing the distance function that quantifies how dissimilar two data points are. Distance functions, or metrics are non-negative real numbers whose magnitude is proportional to some notion of distance between a pair of points. There are many different distance functions to choose from, and the choice is made based on the type of data being clustered. For numeric data, euclidean distance, which corresponds to the length of a line segment drawn between two points in any \\(n\\)-dimensional Euclidean space is often a reasonable choice. Because the divisive strategy is conceptually the inverse of agglomerative clustering, we will limit our description to agglomerative clustering. Once the distance function has been chosen, distances between all pairs of points are computed and the two nearest points are merged into a group. The new group of points is used for computing distances to all other points as a set using a linkage function which defines how the group is summarized into a single point for the purposes of distance calculation. The linkage function is what distinguishes different clustering algorithms, which include: Single-linkage - distance between two groups is computed as the distance between the two nearest members of the groups Complete-linkage - distance between two groups is computed as the distance between the two farthest members of the groups Unweighted Pair Group Method with Arithmetic mean (UPGMA) - distance between two groups is the average distance of all pairs of points between groups Weighted Pair Group Method with Arithmetic mean (WPGMA) - similar to UPGMA, but weights distances from pairs of groups evenly when merging The choice of linkage function (and therefore clustering algorithm) should be determined based on knowledge of the dataset and assessment of clustering performance. In general, there is no one linkage function that will always perform well. The following figure illustrates a simple example of clustering a 1 dimensional set of points using WPGMA: Conceptual illustration of agglomerative hierarchical clustering Below we will cluster the synthetic dataset introduced above in R: ggplot(well_clustered_data, aes(x=f1,y=f2)) + geom_point() + labs(title=&quot;Unclustered&quot;) This synthetic dataset has two distinct groups of samples drawn from multivariate normal samples. To hierarchically cluster these samples, we use the dist() and hclust() functions: # compute all pairwise distances using euclidean distance as the distance metric euc_dist &lt;- dist(dplyr::select(well_clustered_data,-ID)) # produce a clustering of the data using the hclust for hierarchical clustering hc &lt;- hclust(euc_dist, method=&quot;ave&quot;) # add ID as labels to the clustering object hc$labels &lt;- well_clustered_data$ID str(hc) ## List of 7 ## $ merge : int [1:59, 1:2] -50 -33 -48 -46 -44 -21 -11 -4 -36 -3 ... ## $ height : num [1:59] 0.00429 0.08695 0.23536 0.24789 0.25588 ... ## $ order : int [1:60] 8 9 7 6 19 18 11 16 4 14 ... ## $ labels : chr [1:60] &quot;A1&quot; &quot;A2&quot; &quot;A3&quot; &quot;A4&quot; ... ## $ method : chr &quot;average&quot; ## $ call : language hclust(d = euc_dist, method = &quot;ave&quot;) ## $ dist.method: chr &quot;euclidean&quot; ## - attr(*, &quot;class&quot;)= chr &quot;hclust&quot; The hclust() return object describes the clustering as a tree that can be visualized using a dendrogram: library(ggdendro) ggdendrogram(hc) Our data do indeed cluster well, where all samples from the same group cluster together perfectly. See the dendrogram section in the data vizualization chapter for more detail on how to plot clustering results. It is sometimes desirable to use split hierarchical clustering into groups based on their pattern. In the above clustering, three discrete clusters corresponding sample groups are clearly visible. If we wished to separate these three groups, we can use the cutree to divide the tree into three groups using k=3: labels &lt;- cutree(hc,k=3) labels ## A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 A11 A12 A13 A14 A15 A16 A17 A18 A19 A20 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## B1 B2 B3 B4 B5 B6 B7 B8 B9 B10 B11 B12 B13 B14 B15 B16 B17 B18 B19 B20 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 ## 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 We can then use these samples and labels to color our original plot as desired: # we turn our labels into a tibble so we can join them with the original tibble well_clustered_data %&gt;% left_join( tibble( ID=names(labels), label=as.factor(labels) ) ) %&gt;% ggplot(aes(x=f1,y=f2,color=label)) + geom_point() + labs(title=&quot;Clustered&quot;) We were able to recover the correct clustering because this dataset was easy to cluster by construction. Real data are seldom so well behaved. 6.4.3 k-means . 6.4.4 Others "],["network-analysis-..html", "6.5 Network Analysis .", " 6.5 Network Analysis . What is a network? What is network analysis? What are some example network analysis applications in biology and bioinformatics (there is a full ksection on [Biological Networks] in the bio chapter, these are just illustrative examples)? How do we represent networks in R, and how do we analyze them? ([Network visualization] is covered in the data viz chapter, might be helpful to write these two sections together) "],["data-visualization.html", "7 Data Visualization", " 7 Data Visualization Data visualization is a core component of both exploring data and communicating results to others. The goal of data visualization is to present data in a graphical way that shows the reader patterns that would not otherwise be visible. Despite its ubiquity and importance, effective data visualization is challenging, and while many tools and approaches exist there is no gold standard to follow in any meaningful sense. Rather, an effective visualization has the following properties: Depicts accurate data Depicts data accurately Shows enough, but not too much, of the data for the viewer to gain insight Is self contained - no additional information (except a caption) is required to understand the contents of the figure Beyond these, a great visualization has some additional properties: Exposes patterns in the data not easily observable by other methods Invites the viewer to ask more questions about the data "],["dv-gg.html", "7.1 Grammar of Graphics", " 7.1 Grammar of Graphics The grammar of graphics is a system of rules that describes how data and graphical aesthetics (e.g. color, size, shape, etc) are combined to form graphics and plots. First popularized in the book The Grammar of Graphics by Leland Wilkinson and co-authors in 1999, this grammar is a major contribution to the structural theory of statistical graphics. In 2005, Hadley Wickam wrote an implementation of the grammar of graphics in R called ggplot2 (gg stands for grammar of graphics). Under the grammar of graphics, every plot is the combination of three types of information: data, geometry, and aesthetics. Data is the data we wish to plot. Geometry is the type of geometry we wish to use to depict the data (e.g. circles, squares, lines, etc). Aesthetics connect the data to the geometry and defines how the data controls the way the selected geometry looks. A simple example will help to explain. Consider the following made up sample metadata tibble for a study of subjects who died with Alzheimers Disease (AD) and neuropathologically normal controls: ad_metadata ## # A tibble: 20 x 8 ## ID age_at_death condition tau abeta iba1 gfap braak_stage ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 A1 71 AD 117177 199539 16054 63790 5 ## 2 A2 75 AD 52252 53749 28671 89520 2 ## 3 A3 78 AD 77392 140100 55759 121642 3 ## 4 A4 78 AD 127597 171722 111870 120153 5 ## 5 A5 78 AD 141357 377799 114246 127705 6 ## 6 A6 79 AD 57292 2295 1172 30151 2 ## 7 A7 78 AD 81819 185869 47385 137946 3 ## 8 A8 78 AD 40398 103569 78848 88832 2 ## 9 A9 80 AD 106133 106552 139167 20719 4 ## 10 A10 73 AD 75406 0 150470 47397 3 ## 11 C1 84 Control 53561 74010 45380 141297 2 ## 12 C2 78 Control 9632 5035 9194 129374 0 ## 13 C3 80 Control 22225 36310 20179 46846 1 ## 14 C4 68 Control 31281 38529 12532 112923 1 ## 15 C5 79 Control 9552 15199 5548 88043 0 ## 16 C6 67 Control 22852 28683 46462 55053 1 ## 17 C7 74 Control 61609 114650 44289 104788 3 ## 18 C8 67 Control 8472 6546 13876 59001 0 ## 19 C9 62 Control 667 253 318 42382 0 ## 20 C10 83 Control 10537 5516 180 133667 0 For context, tau protein and amyloid beta peptides from the amyloid precursor protein aggregate into neurofibrillary tangles and A-beta plaques, respectively, the brains of people with AD. Generally, the amount of both of these pathologies is associated with more severe disease. Braak stage is a neuropathological assessment of the amount of pathology in a brain that is associated with the severity of disease, where 0 indicates absence of pathology and 6 with widespread involvement in multiple brain regions. Aggregation of tau is also a consequence of normal aging, so must accompany neurological symptoms such as dementia to indicate an AD diagnosis post mortem. Note we have control samples as well as AD. Tauopathy: tau protein accumulates in the cell bodies of affected neurons - Wikipedia The histology measures tau, abeta, iba1, and gfap have been quantified using digital microscopy, where brain sections are stained with immunohistochemistry to identify the location and degree of pathology; the measures in the table are the number of pixels of a 400 x 400 pixel image of a piece of brain tissue that fluoresce when stained with the corresponding antibody. Tau and A-beta antibodies are specialized to the types of aggregated proteins mentioned above and provide a quantification of the level of overall AD pathology. Ionized calcium binding adaptor molecule 1 (IBA1) is a marker of activated microglia, the resident macrophages of the brain, which is an indication of neuroinflammation. Glial fibrillary acidic protein (GFAP) is a marker for activated astrocytes, specialized cells that derive from the neuron lineage, are critical for maintaining the blood brain barrier, and are also involved in the neuroinflammatory response. Lets say we wished to visualize the relationship between age at death and the amount of tau pathology. A scatter plot where each marker is a subject with \\(x\\) and \\(y\\) position corresponding to age_at_death and tau respectively. The following R code creates such a plot with ggplot2: ggplot(data=ad_metadata, mapping = aes(x = age_at_death, y=tau)) + geom_point() All ggplot2 plots begin with the ggplot() function call, which is passed a tibble with the data to be plotted. We then define the aesthetics are defined by mapping the x coordinate to the age_at_death column and the y coordinate to the tau column with aes(x = age_at_death, y=tau). Finally, the geometry as point with geom_point(), meaning marks will be made at pairs of x,y coordinates. The plot shows what we expect given our knowledge of the relationship between age and amount of tau; the two look to be positively correlated. However, we are not capturing the whole story: we know that there are both AD and Control subjects in this dataset. How does condition relate to this relationship we see? We can layer on an additional aesthetic of color to add this information to the plot: ggplot(data=ad_metadata, mapping = aes(x = age_at_death, y=tau, color=condition)) + geom_point() This looks a little clearer, showing that Control subjects generally have both an earlier age at death and a lower amount of tau pathology. This might be a problem, however, since if the age distributions of AD and Control groups are different that might pose a problem with confounding. We should investigate this. Instead of plotting age at death and tau against each other, we will examine the distributions of each of these variables for AD and Control samples separately. We will use the violin geometry with geom_violin() to look at the distributions of age_at_death: ggplot(data=ad_metadata, mapping = aes(x=condition, y=age_at_death)) + geom_violin() We can see immediately that there are big differences between the age distributions of the two groups. This is not ideal, but perhaps we can adjust for these effects in downstream analyses. Wed like to look at the tau distributions as well, but it would be nice to have these two plots side by side in the same plot. To do that, we will use another library called patchwork, which allows independent ggplot2 plots to be arranged together with a simple expressive syntax: library(patchwork) age_boxplot &lt;- ggplot(data=ad_metadata, mapping = aes(x=condition, y=age_at_death)) + geom_boxplot() tau_boxplot &lt;- ggplot(data=ad_metadata, mapping=aes(x=condition, y=tau)) + geom_boxplot() age_boxplot | tau_boxplot # this puts the plots side by side This confirms our suspicion, and also reveals a serious problem with our samples: we have strong confounding of tau and age at death between AD and Control samples. This means that if we look for differences between AD and Control, we wont know if the difference is due to the amount of tau pathology or due to age of the subjects. With this sample set, we simply cannot confidently answer that question. Just a few simple plots alerted us to this problem; hopefully more expensive datasets have not already been generated for these samples, so that hopefully different subjects are available that could avoid this confounding. This has been a biological data analysis oriented tutorial on plotting meant to illustrate the principles of the grammar of graphics. Namely, every plot has data, geometry, and aesthetics that can be independently controlled to produce many types of plots. Many of these plots have names, like scatter plots and boxplots, but as you compose different types of geometries and aesthetics together you may find yourself generating plots that arent so easily named. The next sections of this chapter are a kind of cook book of different kinds plots you can generate with data of different shapes. It is not intended to be comprehensive, but a helpful guide when you are trying to decide how to visualize your own datasets. If you want to go directly to the comprehensive documentation of the many types of ggplot2 plots, peruse the R Graph Gallery site. ggplot2 - Elegant Graphics for Data Analysis, by Hadley Wickam R for Data Science - Data Vizualization R Graph Gallery patchwork package "],["plotting-one-dimension.html", "7.2 Plotting One Dimension", " 7.2 Plotting One Dimension The simplest plots involve plotting a single vector of numbers, or several such vectors (e.g. for different samples). Each value in the vector typically corresponds to a category or fixed value, for example the tau column from the example above has pairs of (ID, tau value). The order of these numbers can be changed, but the vector remains one dimensional or 1-D. 7.2.1 Bar chart Bar charts map length (i.e. the height or width of a box) to the scalar value of a number. The difference in visual length can help the viewer notice consistent patterns in groups of bars, depending on how they are arranged: ggplot(ad_metadata, mapping = aes(x=ID,y=tau)) + geom_bar(stat=&quot;identity&quot;) Note the stat=\"identity\" argument is required because by default geom_bar counts the number of values for each value of x, which in our case is only ever one. This plot is not particularly helpful, so lets change the fill color of the bars based on condition: ggplot(ad_metadata, mapping = aes(x=ID,y=tau,fill=condition)) + geom_bar(stat=&quot;identity&quot;) Slightly better, but maybe we can see even more clearly if we sort our tibble by tau first. Sorting elements in these 1-D charts is somewhat complicated, and is explained in the [Reordering 1-D Data Elements] section below. Bar charts can also plot negative numbers. In the following example, we center the tau measurements by subtracting the mean from each value before plotting: mutate(ad_metadata, tau_centered=(tau - mean(tau))) %&gt;% ggplot(mapping = aes(x=ID, y=tau_centered, fill=condition)) + geom_bar(stat=&quot;identity&quot;) 7.2.2 Lollipop plots Similar to bar charts, so-called lollipop plots replace the bar with a line segment and a circle. The length of the line segment is proportional to the magnitude of the number, and the point marks the length of the segment as a height on the y or length on the x axis, depending on orientation. ggplot(ad_metadata) + geom_point(mapping=aes(x=ID, y=tau)) + geom_segment(mapping=aes(x=ID, xend=ID, y=0, yend=tau)) Note that aes() mappings can be made on the ggplot() object or on each individual geometry function call, to specify different mappings based on geometry. 7.2.3 Stacked Area charts Stacked area charts can visualize multiple 1D data that share a common categorical axis. The charts consist of one line per variable with vertices that correspond to x and y values similar to a bar or lollipop plots. Each variable is plotted using the previous one as a baseline, so that the height of the data points for each category is proportional to their sum. The space between the lines for each variable and the previous one are filled with a color. The following plot visualizes the amount of marker stain for each of the four genes for each individaul: pivot_longer( ad_metadata, c(tau,abeta,iba1,gfap), names_to=&#39;Marker&#39;, values_to=&#39;Intensity&#39; ) %&gt;% ggplot(aes(x=ID,y=Intensity,group=Marker,fill=Marker)) + geom_area() We notice that subject A4 has the highest overall level of marker intensity, followed by A1, A7, etc. The control samples overall have less intensity across all markers. Certain samples, A2 and C5, have little to no abeta aggregation, and C6 has little to no tau. Stacked area plots require three pieces of data: x - a numeric or categorical axis for vertical alignment y - a numeric axis to draw vertical proportions group - a categorical variable that indicates which (x,y) pairs correspond to the same line In the example above, we needed to pivot our tibble so that the different markers and their values were placed into columns Marker and Intensity, respectively. Data for stacked bar charts will usually need to be in this long format, as described in Rearranging Data. Sometimes it is more helpful to view the relative proportion of values in each category rather than the actual values. The result is called a proportional stacked area plots. While not a distinct plot type, we can create one by preprocessing our data by dividing each value by the column sum: pivot_longer( ad_metadata, c(tau,abeta,iba1,gfap), names_to=&#39;Marker&#39;, values_to=&#39;Intensity&#39; ) %&gt;% group_by(ID) %&gt;% # we want to divide each subjects intensity values by the sum of all four markers mutate( `Relative Intensity`=Intensity/sum(Intensity) ) %&gt;% ungroup() %&gt;% # ungroup restores the tibble to its original number of rows after the transformation ggplot(aes(x=ID,y=`Relative Intensity`,group=Marker,fill=Marker)) + geom_area() Now the values for each subject have been normalized to each sum to 1. In this way, we might note that the relative proportion of abeta seems to be greater in AD samples than Controls, but that may not be true of tau. These observations may inspire us to ask these questions more rigorously than we have done so far by inspection. 7.2.4 Parallel Coordinate plots "],["visualizing-distributions.html", "7.3 Visualizing Distributions", " 7.3 Visualizing Distributions The distribution is one of the most important properties of a set of numbers. A distribution describes the general shape of the numbers, i.e. what is the relative frequency of the values, or ranges of values, within the data. Understanding the distribution of a data set is critical when choosing methods to apply, since many methods are only appropriate when data is distributed in certain ways, e.g. linear regression assumes the response variable is normally distributed, otherwise the result of the model cannot be interpreted properly. Often, we dont know how our data are distributed when we obtain it and so we must examine the distribution empirically. The visualizations in this section are all used for the purpose of depicting the distribution of a set of numbers. 7.3.1 Histogram The most common way to plot the distribution of a 1-D set of data is the histogram. The histogram divides up the range of a dataset from minimum to maximum into bins usually of the same width and tabulates the number of values that fall within each bin. Below is a histogram of our age_at_death measurement for all samples: ggplot(ad_metadata) + geom_histogram(mapping=aes(x=age_at_death)) Note that the histogram does not look very complete, because there are only 20 values in our data. We can mitigate this somewhat by increasing the number of bins the data range is divided into: ggplot(ad_metadata) + geom_histogram(mapping=aes(x=age_at_death),bins=10) This is a little bit better, but there are still some bins (76-79, 84-87) that have no values. Compare this to the following synthetic dataset of 1000 normally distributed values: tibble( x=rnorm(1000) ) %&gt;% ggplot() + geom_histogram(aes(x=x)) For distributions with a small number of samples, histograms might not be the best visualization. We will continue with synthetic normally distributed dataset for the remaining examples. ggplot allows you to easily plot multiple distributions on the same plot: tibble( x=c(rnorm(1000),rnorm(1000,mean=4)), type=c(rep(&#39;A&#39;,1000),rep(&#39;B&#39;,1000)) ) %&gt;% ggplot(aes(x=x,fill=type)) + geom_histogram(bins=30, alpha=0.6, position=&quot;identity&quot;) The alpha=0.6, position=\"identity\" arguments makes the bars partially transparent so you can see the overlap more clearly. ggplot2 geom_histogram reference R Graph Gallery - histograms 7.3.2 Density Another way to describe a distribution is with a density plot. Instead of binning the values into intervals and drawing bars with height proportional to the number of values in each bin, a density plot draws a smoothly interpolated line that approximates the distribution instead. A key difference between a histogram and a density plot is the density plot is always normalized so the integral under the curve is approximately 1, whereas a histogram may be either counts or, if the counts in each bin are divided by the total number of data points, a proportion. Compare the histogram and density plots of the age_at_death variable from our example tibble: library(patchwork) hist_g &lt;- ggplot(ad_metadata) + geom_histogram(mapping=aes(x=age_at_death),bins=30) density_g &lt;- ggplot(ad_metadata) + geom_density(mapping=aes(x=age_at_death),fill=&quot;#c9a13daa&quot;) hist_g | density_g Notice the overall shape of the two distributions is similar, with the highest values in both around age 77. The density plot is a smoother representation of a histogram, but its accuracy is still highly sensitive to the number of measurements used to construct it. Compare the histogram and density plots of two sets of 1000 normally distributed samples with different means: library(patchwork) normal_samples &lt;- tibble( x=c(rnorm(1000),rnorm(1000,mean=4)), type=c(rep(&#39;A&#39;,1000),rep(&#39;B&#39;,1000)) ) hist_g &lt;- ggplot(normal_samples) + geom_histogram( mapping=aes(x=x,fill=type), alpha=0.6, position=&quot;identity&quot;, bins=30 ) density_g &lt;- ggplot(normal_samples) + geom_density( mapping=aes(x=x,fill=type), alpha=0.6, position=&quot;identity&quot; ) hist_g | density_g Again the two types of plots depict similar distributions, although they are different enough to possibly suggest different interpretations. In general, density plots might be preferable over histograms if the data are noisy or sparse in that they produce cleaner plots, but potentially at the expense of accuracy when the number of samples is low. ggplot2 geom_density reference R Graph Gallery - density plots 7.3.3 Boxplot Box plots, or box and whisker plots, are extremely common when used to describe distributions. Below is a boxplot of age at death divided by condition: ggplot(ad_metadata) + geom_boxplot(mapping=aes(x=condition,y=age_at_death)) Boxplots are drawn assuming the data are unimodal (i.e. shaped like a hill, possibly slanted to one side or the other), where the extents of the box represent the 1st and 3rd quartile of the data, the central line is the median, the whiskers are drawn as 1.5 times the value outside the 1st and 3rd quartiles. Sometimes individual values more extreme than the whiskers are drawn individually to identify them as outliers. Boxplot anatomy. IQR stands for inner quartile range, the distance between the 1st and 3rd quartile - Wikipedia However, boxplots have some significant shortcomings. Primarily, the rectangle of the inner quartile range does not describe the actual distribution of the samples within it. Although the median can give a sense of skewness, if the data are not unimodal this may be misleading. Consider the following distributions plotted as boxplots or as violin plots (described in the next section): library(patchwork) normal_samples &lt;- tibble( x=c(rnorm(1000),rnorm(1000,4),rnorm(1000,2,3)), type=c(rep(&#39;A&#39;,2000),rep(&#39;B&#39;,1000)) ) g &lt;- ggplot(normal_samples, aes(x=type,y=x,fill=type)) boxplot_g &lt;- g + geom_boxplot() violin_g &lt;- g + geom_violin() boxplot_g | violin_g The two distributions look almost identical in the boxplot figure; however they are dramatically different when visualized using a method like a violin plot where the contours of the entire distribution are depicted. Unless you are certain that your data are unimodal, one of the other distribution visualization methods in this section will likely more accurately depict your data than a boxplot. ggplot2 geom_boxplot reference R Graph Gallery - boxplots 7.3.4 Violin plot As seen in the last section, a violin plot is another way to depict a distribution by producing a shape where the width is proportional to the value along the x or y axis, depending on orientation. The violin shape is similar in principle to a histogram or a density plot, in that it describes the contour of all the data in the distribution, not just the quantiles and extents, as in a box plot. Below is a violin plot of the tau measures from our example tibble: ggplot(ad_metadata) + geom_violin(aes(x=condition,y=tau,fill=condition)) The violin plot is both more and less descriptive than a boxplot; it does depict the entire distribution of the data, but also doesnt include features like median by default. ggplot2 geom_violin reference R Graph Gallery - violin plots 7.3.5 Beeswarm plot The beeswarm plot is similar to a violin plot, but instead of plotting the contours of the data, it plots the data itself as points like in a scatter plot. The individual values of the distribution are organized vertically and spaced such that the points dont overlap. In this plot, the distribution of age at death is plotted for each kind of sample and the markers are colored by the amount of tau: library(ggbeeswarm) ggplot(ad_metadata) + geom_beeswarm(aes(x=condition,y=age_at_death,color=condition),cex=2,size=2) We may not have noticed before that our AD samples have a big gap in ages between 74 and 81; since the beeswarm plot displays all the data, we can see it easily here. Beeswarm plots are typically only useful when the number of values is within a range; not too many and not too few. The example above is close to having too few values per group for this plot to be useful, but consider the following with too many samples: normal_samples &lt;- tibble( x=c(rnorm(1000),rnorm(1000,4),rnorm(1000,2,3)), type=c(rep(&#39;A&#39;,2000),rep(&#39;B&#39;,1000)) ) ggplot(normal_samples, aes(x=type,y=x,color=type)) + geom_beeswarm() This plot likely has too many samples to be the right choice (its also ugly), but it does give an idea of the distribution of the data. In the previous examples the markers for each group also determined the color of the group. This makes the chart a bit easier to read and more pleasing to the eye, but is technically redundant. You can use however profitably however to color markers by some other value that might be of interest. Consider this final example where markers are colored by another randomly generated variable: normal_samples &lt;- tibble( x=c(rnorm(100),rnorm(100,4),rnorm(100,2,3)), type=c(rep(&#39;A&#39;,200),rep(&#39;B&#39;,100)), category=sample(c(&#39;healthy&#39;,&#39;disease&#39;),300,replace=TRUE) ) ggplot(normal_samples, aes(x=type,y=x,color=category)) + geom_beeswarm() We are now effectively visualizing three dimensions which may provide insight into the data. beeswarm package reference 7.3.6 Ridgeline If you have many non-trivial distributions that you would like the user to compare, a good option is a ridgeline chart. The ridgeline plot is simply multiple density plots drawn for different variables within the same plot. Like the beeswarm plot, ridgeline plots are provided by another package outside ggplot2. library(ggridges) tibble( x=c(rnorm(100),rnorm(100,4),rnorm(100,2,3)), type=c(rep(&#39;A&#39;,200),rep(&#39;B&#39;,100)), ) %&gt;% ggplot(aes(y=type,x=x,fill=type)) + geom_density_ridges() Many distributions may be plotted: tibble( x=rnorm(10000,mean=runif(10,1,10),sd=runif(2,1,4)), type=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;,&quot;I&quot;,&quot;J&quot;),1000) ) %&gt;% ggplot(aes(y=type,x=x,fill=type)) + geom_density_ridges(alpha=0.6,position=&quot;identity&quot;) R Graph Gallery - ridgeline plots ggridges package vignette ggridges package index on CRAN "],["plotting-two-or-more-dimensions-..html", "7.4 Plotting Two or More Dimensions .", " 7.4 Plotting Two or More Dimensions . 7.4.1 Scatter plot . 7.4.2 Line plot . ggplot(ad_metadata,mapping=aes(x=abeta, y=tau)) + geom_line() pivot_longer(ad_metadata, c(tau,abeta,iba1,gfap), names_to=&#39;Marker&#39;, values_to=&#39;Intensity&#39; ) %&gt;% ggplot(ad_metadata,mapping=aes(x=ID, y=Intensity, group=Marker, color=Marker)) + geom_line() 7.4.3 Bubble plot . 7.4.4 Heatmaps Heatmaps visualize values associated with a grid of points \\((x,y,z)\\) as a grid of colored rectangles, where \\(x\\) and \\(y\\) define the grid point coordinates and \\(z\\) is a continuous value. A common heatmap you might have seen is the weather map, which plots current or predicted weather patterns on top of a geographic map: Weather maps are heatmaps The color at each grid coordinate, which in this case are spaced closely enough so we cannot distinguish the boundaries between the rectangles without zooming in, is proportional to the type and intensity of weather at that location. In the weather map, the weather values are mapped to a color scales that describe the amount of precipitation as shown in the map legend. In biology, heatmaps are more typically used to visualize matrices. The concept is the same as with the weather map, except instead of geographic coordinates the grid corresponds to the rows and columns of a data matrix, and the color of each grid point is mapped to a color scale chosen by the designer. The grid of rectangles is typically not overlayed over anything, as in the weather map, but is instead the entire visualization. We can create heatmap in R using the base R heatmap() function. Below is a heatmap visualization of our histological markers from the AD example: # heatmap() requires a R matrix, and cannot accept a tibble or a dataframe marker_matrix &lt;- as.matrix( dplyr::select(ad_metadata,c(tau,abeta,iba1,gfap)) ) # rownames of the matrix become y labels rownames(marker_matrix) &lt;- ad_metadata$ID heatmap(marker_matrix) The lower right portion of the plot is the heatmap, where the matrix values are visualized on a color gradient from light yellow to dark red. More precisely, the heatmap() function creates a clustered heatmap, where the rows and columns have been hierarchically clustered separately and ordered according to how similar they are to one another. The left and top line diagrams are dendrograms, which depict the similarity between rows and columns as a tree, where the total branch length summed from one row/column to another is proportional to the dissimilarity (i.e. numeric distance) of the two. The base R heatmap() function performs many different operations on the input matrix than just draw a grid of rectangles in proportion to the values. By default, it also: performs hierarchical clustering of the rows and columns using a Euclidean distance function and orders them accordingly draws dendrograms on the rows and columns according to the clustering scales the data in the rows to have mean zero and standard deviation 1 Each of these defaults can be changed by passing arguments into the function call. The following turns off all of the extra functionality and produces only a heatmap of the matrix: heatmap( marker_matrix, Rowv=NA, Colv=NA, scale=&quot;none&quot;, ) Note how the colors in this heatmap are very different for many of the marker/sample pairs. This illustrates some of the dangers of using heatmaps, which are described more fully in the [How To Use Heatmaps Responsibly] section below. The base R heatmap function has the major drawback that no color key is provided to indicate how values map to colors. Another heatmap heatmap.2 function in the gplots package which has a similar interface to heatmap(), but allows provides more parameters to control the behavior of the plot and includes a color key: library(gplots) heatmap.2(marker_matrix) The extra decoration (dashed and solid vertical lines called traces by the package) provide another way to understand the magnitude of the value in each cell. Refer to the gplots documentation of heatmap2 for more information on how to interpret the trace (hint: you may turn it off with by passing the argument trace = \"none\" to the function call). Both heatmap() and heatmap.2() provide a useful method to annotate rows and columns with a categorical variable. Each of our subjects either has AD or is a control, and we can add a color bar along the margin of the plot to indicate the status of each subject: condition_colors &lt;- transmute( ad_metadata, color=if_else(condition == &quot;AD&quot;,&quot;red&quot;,&quot;blue&quot;) ) heatmap( marker_matrix, RowSideColors=condition_colors$color ) And with heatmap.2: heatmap.2( marker_matrix, RowSideColors=condition_colors$color ) Heatmaps may also be created using ggplot and the geom_tile geometry. This method of generating heatmaps is more manual than the other functions covered so far, and so are both flexible and require more work to obtain certain functionality (e.g. clustering, color bars on side margins, etc). This geometry requires the data to be in long format, with one column for x, y, and z values: pivot_longer( ad_metadata, c(tau,abeta,iba1,gfap), names_to=&quot;Marker&quot;, values_to=&quot;Intensity&quot; ) %&gt;% ggplot(aes(x=Marker,y=ID,fill=Intensity)) + geom_tile() Note this ggplot method does not scale or reorder rows or columns. 7.4.4.1 Specifying Heatmap Colors The colors of the heatmap may be changed by passing one of the native color palettes to the heatmap function with the col argument: # native R colors are: # - rainbow(n, start=.7, end=.1) # - heat.colors(n) # - terrain.colors(n) # - topo.colors(n) # - cm.colors(n) # the n argument specifies the number of colors (i.e. resolution) of the colormap to return heatmap(marker_matrix,col=cm.colors(256)) To change the color using ggplot and geom_tile(), use the scale_fill_gradientn function to specify a different color palette: pivot_longer( ad_metadata, c(tau,abeta,iba1,gfap), names_to=&quot;Marker&quot;, values_to=&quot;Intensity&quot; ) %&gt;% ggplot(aes(x=Marker,y=ID,fill=Intensity)) + geom_tile() + scale_fill_gradientn(colors=cm.colors(256)) The native color palettes in R look as follows: Instead of these color palettes, you may use one of the ColorBrewer palettes in the RColorBrewer package library(RColorBrewer) display.brewer.all() There are very many color palettes to choose from, and you may also specify your own palettes with as much detail and complexity as you desire. More discussion on how to choose colors is described in the next section. 7.4.4.2 How To Use Heatmaps Resonsibly . While heatmaps may seem intuitive, they are actually quite complicated and can be difficult to create in a way that accurately depicts the underlying matrix. More to come "],["other-kind-of-plots-..html", "7.5 Other Kind of Plots .", " 7.5 Other Kind of Plots . 7.5.1 Chord . 7.5.2 Sankey . 7.5.3 Dendrograms Dendrograms are visualizations of tree structures (dendron means tree in Greek). In the graph theory branch of mathematics, a tree is defined as a graph composed of nodes and edges in which any two nodes are connected by exactly one path. Such a graph is also called an acyclic graph, where no paths exist that can be followed from a node back to iself. A full treatment of trees and graph theory is beyond the scope of this book, but we will discuss trees to the extent they are necessary to understand how to read a dendrogram. The concepts of a dendrogram are illustrated in the following figure: Dendrogram Illustration The tree drawn in a typical dendrogram is created using the output of a clustering algorithm, e.g. hierarchical clustering that groups data points together based on their dissimilarity. Briefly, in the figure, the data are one dimensional scalars, so the distance between all points is simply the absolute value of the difference between them. The two closest points are then merged into a single group, and the pairwise distances are recomputed for all remaining data points with a summary of this new group (the group summarization method is specified by the user and is called the linkage critera). Then the process repeats, where the two nearest data points or summarized groups are merged into a new group, the new group is summarized, pairwise distances recomputed, etc. until all the data points have been included in a group. In this way, all data points are assigned to a hierarchy of groups. In general, any hierarchical grouping of data can be used to generate a dendrogram. Given a hierarchical clustering result, a dendrogram can be drawn in several ways in R. Using our AD marker example data, we hierarchically cluster the subjects based on the content of all their markers and visualize the result as follows: library(ggdendro) # produce a clustering of the data using the hclust for hierarchical clustering # and euclidean distance as the distance metric euc_dist &lt;- dist(dplyr::select(ad_metadata,c(tau,abeta,iba1,gfap))) hc &lt;- hclust(euc_dist, method=&quot;ave&quot;) # add ID as labels to the clustering object hc$labels &lt;- ad_metadata$ID ggdendrogram(hc) This dendrogram does not produce highly distinct clusters of AD and control samples with all the data. To illustrate what a dataset with strong clustering looks like, we draw multivariate samples from two normal distributions and cluster the results: library(patchwork) library(ggdendro) well_clustered_data &lt;- tibble( ID=c(stringr::str_c(&quot;A&quot;,1:10),stringr::str_c(&quot;B&quot;,1:10)), f1=c(rnorm(10,0,1),rnorm(10,10,1)), f2=c(rnorm(10,0,1),rnorm(10,10,1)) ) scatter_g &lt;- ggplot(well_clustered_data, aes(x=f1,y=f2)) + geom_point() # produce a clustering of the data using the hclust for hierarchical clustering # and euclidean distance as the distance metric euc_dist &lt;- dist(dplyr::select(well_clustered_data,-ID)) hc &lt;- hclust(euc_dist, method=&quot;ave&quot;) # add ID as labels to the clustering object hc$labels &lt;- well_clustered_data$ID dendro_g &lt;- ggdendrogram(hc) scatter_g | dendro_g Here, all the A and B samples strongly cluster together, with a large distance between clusters. Dendrograms Wikipedia page ggdendro package vignette "],["tips-and-tricks-..html", "7.6 Tips and Tricks .", " 7.6 Tips and Tricks . 7.6.1 Reordering 1-D Data Elements . How do we enforce an order on elements in a plot? Why might we want to do that? ad_metadata %&gt;% arrange(tau) %&gt;% ggplot(mapping = aes(x=ID,y=tau,fill=condition)) + geom_bar(stat=&quot;identity&quot;) But wait, our bars are in the same order as in the previous plot. Sorting our data prior to plotting did not have the desired effect. The reason is that although the ID column is a character string in our original tibble, ggplot converts it into a factor prior to plotting. Recall that a factor is a numeric type composed of sequential integers, each of which having a character label associated with it. 7.6.2 Confidence Intervals . What are some ways to depict uncertainty in a plot? 7.6.3 Annotations . How do we add text annotations to plots? Manually vs data-driven. "],["multiple-plots-..html", "7.7 Multiple Plots .", " 7.7 Multiple Plots . How do we add multiple datasets to plots? 7.7.1 Secondary Axes . How and when do we add series with different scales to the same plot? 7.7.2 Facet wrapping . What is facet wrapping and in which contexts is it useful (i.e. group_by)? 7.7.3 Multipanel Figures . How do we add multiple, unrelated subplots to the same larger plot? "],["responsible-plotting-..html", "7.8 Responsible Plotting .", " 7.8 Responsible Plotting . Good plots empower us to ask good questions. - Alberto Cairo, How Charts Lie "],["publication-ready-plots-..html", "7.9 Publication Ready Plots .", " 7.9 Publication Ready Plots . ggpubr - publication ready plotting with ggplot "],["network-visualization-..html", "7.10 Network visualization .", " 7.10 Network visualization . How do we visualize networks? "],["biology-bioinformatics.html", "8 Biology &amp; Bioinformatics ", " 8 Biology &amp; Bioinformatics "],["bio-r.html", "8.1 R in Biology", " 8.1 R in Biology R became popular in biological data analysis in the early to mid 2000s, when microarray technology came into widespread use enabling researchers to look for statistical differences in gene expression for thousands of genes across large numbers of samples. As a result of this popularity, a community of biological researchers and data analysts created a collection of software packages called Bioconductor, which made a vast array of cutting edge statistical and bioinformatic methodologies widely available. Due to its ease of use, widespread community support, rich ecosystem of biological data analysis packages, and to it being free, R remains one of the primary languages of biological data analysis. The languages early focus on statistical analysis, and later transition to data science in general, makes it very useful and accessible to perform the kinds of analyses the new data science of biology required. It is also a bridge between biologists without a computational background and statisticians and bioinformaticians, who invent new methods and implement them as R packages that are easily accessible by all. The language and the package ecosystems and communities it supports continue to be a major source of biological discovery and innovation. As a data science, biology benefits from the main strengths of R and the tidyverse when combined with the powerful analytical techniques available in Bioconductor packages, namely to manipulate, visualize, analyze, and communicate biological data. "],["biological-data-overview.html", "8.2 Biological Data Overview", " 8.2 Biological Data Overview 8.2.1 Types of Biological Data In very general terms, there are five types of data used in biological data analysis: raw/primary data, processed data, analysis results, metadata, and annotation data. Raw/primary data. These data are the primary observations made by whatever instruments/techniques we are using for our experiment. These may include high-throughput sequencing data, mass/charge ratio data from mass spectrometry, 16S rRNA sequencing data from metagenomic studies, SNPs from genotyping assays, etc. These data can be very large and are often not efficiently processed using R. Instead, specialized tools built outside of R are used to first process the primary data into a form that is amenable to analysis. The most common primary biological data types include Microarrays, High Throughput Sequencing data, and mass spectrometry data. Processed data. Processed data is the result of any analysis or transformation of primary data into an intermediate, more interpretable form. For example, when performing gene expression analysis with RNASeq, short reads that form the primary data are typically aligned against a genome and then counted against annotated genes, resulting in a count for each gene in the genome. Typically, these counts are then combined for many samples into a single matrix and subject to downstream analyses like differential expression. Analysis results. Analysis results arent data per se, but are the results of analysis of primary data or processed results. For example, in gene expression studies, a first analysis is often differential expression, where each gene in the genome is tested for expression associated with some outcome of interest across many samples. Each gene then has a number of statistics and related values, like log2 fold change, nominal and adjusted p-value, etc. These forms of data are usually what we use to form interpretations of our datasets and therefore we must manipulate them in much the same way as any other dataset. Metadata. In most experimental settings, multiple samples have been processed and had the same primary data type collected. These samples also have information associated with them, which is here termed metadata, or data about data. In our gene expression of post mortem brain experiments mentioned above, the information about the individuals themselves, including age at death, whether the person had a disease, the measurements of tissue quality, etc. is the metadata. The primary and processed data and metadata are usually stored in different files, where the metadata (or sample information or sample data, etc) will have one column indicating the unique identifier (ID) of each sample. The processed data will typically have columns named for each of the sample IDs. Annotation data. A tremendous amount of knowledge has been generated about biological entities, e.g. genes, especially since the publication of the human genome. Annotation data is publicly available information about the features we measure in our experiments. This may come in the form of the coordinates in a genome where genes exist, any known functions of those genes, the domains found in proteins and their relative sequence, gene identifier cross references across different gene naming systems (e.g. symbol vs Ensembl ID), single nucleotide polymorphism genomic locations and associations with traits or diseases, etc. This is information that we use to aid in interpretation of our experimental data but generally do not generate ourselves. Annotation data comes in many forms, some of which are in CSV format. The figure below contains a simple diagram of how these different types of data work together for a typical biological data analysis. Information flow in biological data analysis 8.2.2 CSV Files The most common, convenient, and flexible data file format in biology and bioinformatics is the character-delimited or character-separated text file. These files are plain text files (i.e. not the native file format of any specific program, like Excel) that generally contain rectangles of data. When formatted correctly, you can think of these files as containing a grid or matrix of data values with some number of columns, each of which has the same number of values. Each line of these files has some number of data values separated by a consistent character, most commonly the comma which are called comma-separated value, or CSV, files and filenames typically end with the extension .csv. Note that other characters, especially the character, may be used to create valid files in this format, and all the same general principles apply. This is an example of a simple CSV file: id,somevalue,category,genes 1,2.123,A,APOE 4,5.123,B,&quot;HOXA1,HOXB1&quot; 7,8.123,,SNCA Some properties and principles of CSV files: The first line often but not always contains the column names of each column Each value is delimited by the same character, in this case , Values can be any value, including numbers and characters When a value contains the delimiting character (e.g. HOXA1,HOXB1 contains a ,), the value is wrapped in double quotes Values can be missing, indicated by sequential delimiters (i.e. ,, or one , at the end of the line, if the last column value is missing) There is no delimiter at the end of the lines To be well-formatted every line must have the same number of delimited values These same properties and principles apply to all character-separated files, regardless of the specific delimiter used. If a file follows these principles, they can be loaded very easily into R or any other data analysis setting. ### Common Biological Data Matrices Typically the first data set you will work with in R is processed data as described in the previous section. This data has been transformed from primary data in some way such that it (usually) forms a numeric matrix with features as rows and samples as columns. The first column of these files usually contains a feature identifier, e.g. gene identifier, genomic locus, probe set ID, etc and the remaining columns have numerical values, one per sample. The first row is usually column names for all the columns in the file. Below is an example of one of these files from a microarray gene expression dataset loaded into R: intensities &lt;- read_csv(&quot;example_intensity_data.csv&quot;) intensities # A tibble: 54,675 x 36 probe GSM972389 GSM972390 GSM972396 GSM972401 GSM972409 GSM972412 GSM972413 GSM972422 GSM972429 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1007_s_at 9.54 10.2 9.72 9.68 9.35 9.89 9.70 9.67 9.87 2 1053_at 7.62 7.92 7.17 7.24 8.20 6.87 6.62 7.23 7.45 3 117_at 5.50 5.56 5.06 7.44 5.19 5.72 5.87 6.15 5.46 4 121_at 7.27 7.96 7.42 7.34 7.49 7.76 7.44 7.66 8.02 5 1255_g_at 2.79 3.10 2.78 2.91 3.02 2.73 2.78 3.56 2.83 6 1294_at 7.51 7.28 7.00 7.18 7.38 6.98 6.90 7.54 7.66 7 1316_at 3.89 4.36 4.24 3.94 4.20 4.34 4.06 4.24 4.11 8 1320_at 4.65 4.91 4.70 4.78 5.06 4.71 4.55 4.58 5.10 9 1405_i_at 8.03 7.47 5.42 7.21 9.48 6.79 6.57 8.50 6.36 10 1431_at 3.09 3.78 3.33 3.12 3.21 3.27 3.37 3.84 3.32 # ... with 54,665 more rows, and 26 more variables: GSM972433 &lt;dbl&gt;, GSM972438 &lt;dbl&gt;, GSM972440 &lt;dbl&gt;, # GSM972443 &lt;dbl&gt;, GSM972444 &lt;dbl&gt;, GSM972446 &lt;dbl&gt;, GSM972453 &lt;dbl&gt;, GSM972457 &lt;dbl&gt;, # GSM972459 &lt;dbl&gt;, GSM972463 &lt;dbl&gt;, GSM972467 &lt;dbl&gt;, GSM972472 &lt;dbl&gt;, GSM972473 &lt;dbl&gt;, # GSM972475 &lt;dbl&gt;, GSM972476 &lt;dbl&gt;, GSM972477 &lt;dbl&gt;, GSM972479 &lt;dbl&gt;, GSM972480 &lt;dbl&gt;, # GSM972487 &lt;dbl&gt;, GSM972488 &lt;dbl&gt;, GSM972489 &lt;dbl&gt;, GSM972496 &lt;dbl&gt;, GSM972502 &lt;dbl&gt;, # GSM972510 &lt;dbl&gt;, GSM972512 &lt;dbl&gt;, GSM972521 &lt;dbl&gt; The file has 54,676 rows, consisting of one header row which R loads in as the column names, and the remaining are probe sets, one per row. There are 36 columns, where the first contains the probe set ID (e.g. 1007_s_at) and the remaining 35 columns correspond to samples. 8.2.3 Biological data is NOT Tidy! As mentioned in the tidy data section, the tidyverse packages assume data to be in so-called tidy format, with variables as columns and observations as rows. Unfortunately, certain forms of biological data are typically available in the opposite orientation - variables are in rows and observations are in columns. This is primarily true in feature data matrices, e.g. gene expression counts matrices, where the number of variables (e.g. genes) is much larger than the number of samples, which tend to be small very small compared with the number of features. This format is convenient for humans to interact with using, e.g. spreadsheet programs like Microsoft Excel, but can unfortunately make performing certain operations on them challenging in tidyverse. For example, consider the microarray expression dataset in the previous section. Each of the 54,676 rows is a probeset, and each of the 35 numeric columns is a sample. This is a very large number of probesets to consider, especially if we plan to conduct a statistical test on each, which would impose a substantial multiple hypothesis testing burden on our results. We may therefore wish to eliminate probesets that have very low variance from the overall dataset, since these probesets are not likely to have a detectable statistical difference in our tests. However, computing the variance for each probeset is a computation across all columns, not on columns themselves, and this is not what tidyverse is designed to do well. Said differently, R and tidyverse do not operate by default on the rows of a data frame, tibble, or matrix. Both base R and tidyverse are optimized to perform computations on columns, not rows. The reasons for this are buried in the details of how the R program itself was written to organize data internally and are beyond the scope of this book. The consequence of this design choice is that, while we can perform operations on the rows rather than the columns of a data structure, our code may perform very poorly (i.e. take a very long time to run). When working with these datasets, we have a couple options to deal with this issue: Pivot into long format. As described in the Rearranging Data section, we can rearrange our tibble to be more amenable to certain computations. In our earlier example, we wish to group all of our measurements by probeset and compute the variance of each, then possibly filter out probesets based on low variance. We can therefore combine pivot_longer(), group_by(), summarize(), and finally left_join() to perform this operation. Exactly how to do this is left as an exercise in Assignment 1. Compute row-wise statistics using apply(). As described in Iteration, R is a functional programming language and implements iteration in a functional style using the apply() function. The apply() function accepts a MARGIN argument of 1 or 2 if the provided function is to be applied to the columns or rows, respectively. This method can be used to compute a summary statistic on each row of a tibble and the result saved back into the tibble using the column set operator: intensity_variance &lt;- apply(intensities, 2, var) intensities$variance &lt;- intensity_variance "],["bio-bioconductor.html", "8.3 Bioconductor", " 8.3 Bioconductor Bioconductor is an organized collection of strictly biological analysis methods packages for R. These packages are hosted and maintained outside of CRAN because the maintainers enforce a more rigorous set of coding quality, testing, and documentation standards than is required by normal R packages. These stricter requirements arose from a recognition that software packages are generally only as usable as their documentation allows them to be, and also that many if not most of the users of these packages are not statisticians or experienced computer programmers. Rather, they are people like us: biological analysis practitioners who may or may not have substantial coding experience but must analyze our data nonetheless. The excellent documentation and community support of the bioconductor ecosystem is a primary reason why R is such a popular language in biological analysis. Bioconductor is divided into roughly two sets of packages: core maintainer packages and user contributed packages. The core maintainer packages are among the most critical, because they define a set of common objects and classes (e.g. the ExpressionSet class in the Biobase package) that all Bioconductor packages know how to work with. This common base provides consistency among all Bioconductor packages thereby streamlining the learning process. User contributed and maintained packages provide specialized functionality for specific types of analysis. Bioconductor itself must be installed prior to installing other Bioconductor packages. To [install bioconductor], you can run the following: if (!require(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(version = &quot;3.14&quot;) Bioconductor undergoes regular releases indicated by a version, e.g. version = \"3.14\" at the time of writing. Every bioconductor package also has a version, and each of those versions may or may not be compatible with a specific version of Bioconductor. To make matters worse, Bioconductor versions are only compatible with certain versions of R. Bioconductor version 3.14 requires R version 4.1.0, and will not work with older R versions. These versions can cause major version compatibility issues when you are forced to use older versions of R, as may sometimes be the case on managed compute clusters. There is not a good general solution for ensuring all your packages work together, but the general best rule of thumb is to use the most current version of R and all packages at the time when you write your code. The BiocManager package is the only Bioconductor package installable using install.packages(). After installing the BiocManager package, you may then install bioconductor packages: # installs the affy bioconductor package for microarray analysis BiocManager::install(&quot;affy&quot;) One key aspect of Bioconductor packages is consistent and helpful documentation; every package page on the Bioconductor.org site lists a block of code that will install the package, e.g. for the affy package. In addition, Biconductor provides three types of documentation: Workflow tutorials on how to perform specific analysis use cases Package vignettes for every package, which provide a worked example of how to use the package that can be adapted by the user to their specific case Detailed, consistently formatted reference documentation that gives precise information on functionality and use of each package The thorough and useful documentation of packages in Bioconductor is one of the reasons why the package ecosystem, and R, is so widely used in biology and bioinformatics. The base Bioconductor packages define convenient data structures for storing and analyzing many types of data. Recall earlier in the Types of Biological Data section, we described five types of biological data: primary, processed, analysis, metadata, and annotation data. Bioconductor provides a convenient class for storing many of these different data types together in one place, specifically the SummarizedExperiment class in the package of the same name package. An illustration of what a SummarizedExperiment object stores is below, from the Bioconductor maintainers paper in Nature: SummarizedExperiment Schematic - Huber, et al. 2015. Orchestrating High-Throughput Genomic Analysis with Bioconductor. Nature Methods 12 (2): 11521. As you can see from the figure, this class stores processed data (assays), metadata (colData and exptData), and annotation data (rowData). The SummarizedExperiment class is used ubiquitously throughout the Bioconductor package ecosystem, as are other core classes some of which we will cover later in this chapter. "],["microarrays.html", "8.4 Microarrays", " 8.4 Microarrays A Microarray Device - Thermo Fisher Scientific Microarrays are devices that measure the relative abundance of thousands of distinct DNA sequences simultaneously. Short (~25 nucleotide) single-stranded DNA molecules called probes are deposited on a small glass slide in a grid of spots, where each spot contains many copies of a probe with identical sequence. The probe sequences are selected a priori based on the purpose of the array. For example, gene expression microarrays have probes that correspond to the coding regions of the genome for a species of interest, while genotyping microarrays use sequences with known variants found in a population of genomes, most often the human population. Microarrays of the same type (e.g. human gene expression) all have the same set of probes. The choice of DNA sequence probes therefore determines what the microarray measures and how to interpret the data. The design of a microarray is illustrated in the following figure. Illustration of Microarray Design A microarray device generates data by applying a specially prepared DNA sample to it; the sample usually corresponds to a single biological specimen, e.g. an individual patient. The preparation method for the sample depends on what is being measured: When measuring DNA directly, e.g. genetic variants, the DNA itself is biochemically extracted When measuring gene expression via RNA abundance, RNA is first extracted and then reverse transcribed to complementary DNA (cDNA) In either case, the molecules that will be applied to the microarray are DNA molecules. After extraction and preparation, the DNA molecules are then randomly cut up into shorter molecules (i.e. sheared) and each molecule has a molecular tag biochemically ligated to it that will emit fluorescence when excited by a specific wavelength of light. This tagged DNA sample is then washed over the microarray chip, where DNA molecules that share sequence complementarity with the probes on the array pair together. After this treatment, the microarray is washed to remove DNA molecules that did not have a match on the array, leaving only those molecules with a sequence match that remain bound to probes. The microarray chip is then loaded into a scanning device, where a laser with a specific wavelength of light is then shone onto the array, causing the spots with tagged DNA molecules associated with probes to fluoresce, and other spots remain dark. A high resolution image is taken of the fluorescent array, and the image is analyzed to map the intensity of the light on each spot to a numeric value proportional to its intensity. The reason for this is that, since each spot has many individual probe molecules contained within it, the more copies of the corresponding DNA molecule were in the sample, the more light the spot emits. In this way, the relative abundance of all probes on the array are measured simultaneously. The process of generating microarray data from a sample is illustrated in the following figure. Illustration of Microarray Data Generation Process After the microarray has been scanned, the relative copy number of the DNA in the sample matching the probes on the microarray are expressed as the intensity of fluorescence of each probe. The raw intensity data from the scan has been processed and analyzed by the scanner software to account for technical biases and artefacts of the scanning instrument and data generation process. The data from a single scan is processed and stored in a file in CEL format, a proprietary data format that stores the raw probe intensity data that can be loaded for downstream analysis. "],["high-throughput-sequencing.html", "8.5 High Throughput Sequencing", " 8.5 High Throughput Sequencing High throughput sequencing (HTS) technologies measure and digitize the nucleotide sequences of thousands to billions of DNA molecules simultaneously. In contrast to microarrays, which cannot identify new sequences due to their a priori probe based design, HTS instruments can in principle determine the sequence of any DNA molecule in a sample. For this reason, HTS datasets are sometimes called unbiased assays, because the sequences they identify are not predetermined. The most popular HTS technology at the time of writing is provided by the Illumina biotechnology company. This technology was first developed and commercialized by the company Solexa during the late 1990s and was instrumental in the completion of the human genome. The cost of generating data with this technology has reduced by several orders of magnitude since then, resulting in an exponential growth of biological data and fundamentally changed our understanding of biology and health. Because of this, the analysis of these data, and not the time and cost of generation, has become the primary bottleneck in biological and biomedical research. The Illumina HTS technology uses a biotechnological technique called sequencing by synthesis. Briefly, this technique entails biochemically ligating millions to billions of short (~300 nucleotide long) DNA molecules to a glass slide, denatured to become single stranded, and then used to synthesize the complementary strand by incorporating fluorescently tagged nucleotides. The tagged nucleotides that are incorporated on each nucleotide addition are excited with a laser and a high resolution photograph is taken of the flow cell. This process is repeated many times until the DNA molecules on the flow cell have been completely synthesized, and the collective images are computationally analyzed to build up the complete sequence of each molecule. Other HTS methods have been developed that use different strategies, some of which do not utilize the sequencing by synthesis method. These alternative technologies differ in the length, number, confidence and per base cost of the DNA molecules they sequence. At the time of writing, the two most common technologies include: Pacific Biosciences (PacBio) Long Read Sequencing - this technology uses an engineered polymerase that performs run-on amplification of circularized DNA molecules and records a signature of each different base as it is incorporated into the amplification product. PacBio sequencing can generate reads that are thousands of nucleotides in length. Oxford Nanopore Sequencing (ONS) - this technology uses engineered proteins to form pores in an electrically conductive membrane, through which single stranded DNA molecules pass. As the individual nucleotides of a DNA molecule pass through the pore, the electrical current induced by the different nucleotide molecular structures is recorded and analyzed with signal processing algorithms to recover the original sequence of the molecule. No synthesis occurs in ONS technology. Due to its widespread use, only Illumina sequencing data analysis is covered in this book, though additions in the future may be warranted. Any DNA molecules may be subjected to sequencing via this method; the meaning of the resulting datasets and the goal of subsequent analysis are therefore dependent upon the material being studied. Below are some of the most common types of HTS experiments: Whole genome sequencing (WGS) - the input to sequencing is randomly sheared genomic DNA molecules, from single organisms or a collection of organisms, with the goal of reconstructing or further refining the genome sequence of the constituent organisms RNA Sequencing (RNASeq) - the input is complementary DNA (cDNA) that has been reverse transcribed from RNA extracted from a sample, with the goal of detecting which RNA transcripts (and therefore genes) are expressed in that sample Whole genome bisulfite sequencing (WGBS) - the input to sequencing is randomly sheared genomic DNA that has undergone bisulfite treatment, which converts methylated cytosines to thymines, thereby enabling detection of epigenetic methylation marks in the DNA Chromatin immunoprecipitation followed by sequencing (ChIPSeq) - the input is genomic DNA that was bound by specific DNA-binding proteins (e.g. transcription factors), that allows identification of locations in the genome where those proteins were bound A complete coverage of the many different types of sequencing assays is beyond the scope of this book, but analysis techniques used for of some of these different types of experiments are covered later in this chapter. 8.5.1 Raw HTS Data The raw data produced by the sequencing instrument, or sequencer, are the digitized DNA sequences for each of the billions of molecules that were captured by the flow cell. The digitized nucleotide sequence for each molecule is called a read, and therefore every run of a sequencing instrument produces millions to billions of reads. The data is stored in a standardized format called the FASTQ file format. Data for a single read in FASTQ format is shown below: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !&#39;&#39;*((((***+))%%%++)(%%%%).1***-+*&#39;&#39;))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65 Each read has four lines in the FASTQ file. The first beginning with @ is the sequencing header or name, which provides a unique identifier for the read in the file. The second line is the nucleotide sequence as detected by the sequencer. The third line is a secondary header line that starts with + and may or may not include further information. The last line contains the phred quality scores for the base calls in the corresponding position of the read. Briefly, each character in the phred quality score line has a corresponding integer is proportional to the confidence that the base in the corresponding position of the read was called correctly. This information is used to assess the quality of the data. See this more in-depth explanation of phred score for more information. 8.5.2 Preliminary HTS Data Analysis Since all HTS experiments generate the same type of data (i.e. reads), all data analyses begin with processing of the sequences via bioinformatic algorithms; the choice of algorithm is determined by the experiment type and the biological question being asked. A full treatment of the variety of bioinformatic approaches to processing these data is beyond the scope of this book. We will focus on one analysis approach, alignment, that is commonly employed to produce derived data that is then further processed using R. It is very common for HTS data, also called short read sequencing data, to be generated from organisms that have already had their genome sequenced, e.g. human or mouse. The reads in a dataset can therefore be compared with the appropriate genome sequence to aid in interpretation. This comparison amounts to searching in the organisms genome, or the so-called reference genome, for the location or locations from which the molecule captured by each read originated. This search process is called alignment, where each short read sequence is aligned against the usually much larger reference genome to identify locations with identical or nearly identical sequence. Said differently, the alignment process attempts to assign each read to one or more locations in a reference genome, where only reads that have a match in the target genome will be assigned. The alignment process is computationally intensive and requires specialized software to perform efficiently. While it may be technically capable of performing alignment, R is not optimized to process character data in the volume typical of these datasets. Therefore, it is not common to work with short read data (i.e. FASTQ files) directly in R, and other processing steps are required to transform the sequencing data into other forms that are then amenable to analysis in R. The end result of aligning all the reads in a short read dataset is assignments of reads to positions across an entire reference genome. The distribution of those read alignment locations across the genome forms the basis of the data used to interpret the dataset. Specifically, each locus, or location spanning one or more bases, has zero or more reads aligned to it. The number of reads aligned to a locus, or the read count is commonly interpreted as being proportional to the number of those molecules found in the original sample. Therefore, the analysis and interpretation of HTS data commonly involves the analysis of count data, which we discuss in detail in the next section. The following figure illustrates the concepts described in this section. Illustration of WGS and RNASeq read alignment process 8.5.3 Count Data As mentioned above, the number of reads, or read counts that align to each locus of a genome form a distribution we can use to interpret the data. Count data have two important properties that require special attention: Count data are integers - the read counts assigned to a particular locus are whole numbers Counts are non-negative - counts can either be zero or positive; they cannot be negative These two properties have the important consequence that count data are not normally distributed. Unlike with microarray data, common statistical techniques like linear regression are not directly applicable to count data. We must therefore account for this property of the data prior to statistical analysis in one of two ways: Use statistical methods that model counts data - generalized linear models that model data using Poisson and Negative Binomial distributions Transform the data to have be normally distributed - certain statistical transformations, such as regularized log or rank transformations can make counts data more closely follow a normal distribution We will discuss these strategies in greater detail in the RNASeq Gene Expression Data section. "],["gene-identifiers.html", "8.6 Gene Identifiers", " 8.6 Gene Identifiers 8.6.1 Gene Identifier Systems Since the first genetic sequence for a gene was determined in 1965 - an alanine tRNA in yeast(Holley et al. 1965) - scientists have been trying to give them names. In 1979, formal guidelines for the nomenclature used for human genes was proposed(Shows et al. 1979) along with the founding of the Human Gene Nomenclature Committee (Bruford et al. 2020), so that researchers would have a common vocabulary to refer to genes in a consistent way. In 1989, this committee was placed under the auspices of the Human Genome Organisation (HUGO), becoming the HUGO Gene Nomenclature Committee (HGNC). The HGNC has been the official body providing guidelines and gene naming authority since, and as such HGNC gene names are often called gene symbols. As per Bruford et al. (2020), the official human gene symbol guidelines are as follows: Each gene is assigned a unique symbol, HGNC ID and descriptive name. Symbols contain only uppercase Latin letters and Arabic numerals. Symbols should not be the same as commonly used abbreviations Nomenclature should not contain reference to any species or G for gene. Nomenclature should not be offensive or pejorative. Gene symbols are the most human-readable system for naming genes. The gene symbols BRCA1, APOE, and ACE2 may be familiar to you as they are involved in common diseases, namely breast cancer, Alzheimers Disease risk, and SARS-COV-2, respectively. Typically, the gene symbol is an acronym that roughly represents a label of what the gene is or does (or was originally found to be or do, as many genes are subsequently discovered to be involved in entirely separate processes as well), e.g. APOE represents the gene apolipoprotein E. This convention is convenient for humans when reading and identifying genes. However, standardized though the symbol conventions may be, they are not always easy for computers to work with, and ambiguities can cause mapping problems. Other gene identifier systems developed along with other gene information databases. In 2000, the Ensembl genome browser was launched by the Wellcome Trust, a charitable foundation based in the United Kingdom, with the goal of providing automated annotation of the human genome. The Ensembl Project, which supports the genome browser, recognized even before the publication of the human genome that manual annotation and curation of genes is a slow and labor intensive process that would not provide researchers around the world timely access to information. The project therefore created a set of automated tools and pipelines to collect, process, and publish rapid and consistent annotation of genes in the human genome. Since its initial release, Ensembl now supports over 50,000 different genomes. Ensembl assigns a automatic, systematic ID called the Ensembl Gene ID to every gene in its database. Human Ensembl Gene IDs take the form ENSG plus an 11 digit number, optionally followed by a period delimited version number. For example, at the time of writing the BRCA1 gene has an Ensembl Gene ID of ENSG00000012048.23. The stable ID portion (i.e. ENSGNNNNNNNNNNN) will remain associated with the gene forever (unless the gene annotation changes dramatically in which case it is retired). The .23 is the version number of the gene annotation, meaning this gene has been updated 22 times (plus its initial version) since addition to the database. The additional version information is very important for reproducibility of biological analysis, since conclusions drawn by results of these analyses are usually based on the most current information about a gene which are continually updated over time. Ensembl maintains annotations for many different organisms, and the gene identifiers for each genome contain codes that indicate which organism the gene is for. Here are the codes for genes in several different species: Gene ID Prefix Organism Example ENSG Homo sapiens ENSG00000139618 ENSMUSG Mus musculus (mouse) ENSMUSG00000017167 ENSDARG Danio rerio (zebrafish) ENSDARG00000024771 ENSMMUG Macaca mulata (Macaque) ENSMMUG00000020312 ENSVPAG Vicugna pacos (Alpaca) ENSVPAG00000001990 Ensembl Gene IDs and gene symbols are the most commonly used gene identifiers. The GENCODE project which provides consistent and stable genome annotation releases for human and mouse genomes, uses these two types of identifiers with its official annotations. Ensembl provides stable identifiers for transcripts as well as genes. Transcript identifiers correspond to distinct patterns of exons/introns that have been identified for a gene, and each gene has one or more distinct transcripts. Ensembl Transcript IDs have the form ENSTNNNNNNNNNNN.NN similar to Gene IDs. Ensembl is not the only gene identifier system besides gene symbol. Several other databases have devised and maintain their own identifiers, most notably [Entrez Gene IDs](UCSC Gene IDs used by the NCBI Gene database which assigns a unique integer to each gene in each organism, and the Online Mendelian Inheritance in Man (OMIM) database, which has identifiers that look like OMIM:NNNNN, where each OMIM ID refers to a unique gene or human phenotype. However, the primary identifiers used in modern human biological research remain Ensembl IDs and official HGNC gene symbols. 8.6.2 Mapping Between Identifier Systems A very common operation in biological analysis is to map between gene identifier systems, e.g. you are given Ensembl Gene ID and want to map to the more human-recognizable gene symbols. Ensembl provides a service called BioMart that allows you to download annotation information for genes, transcripts, and other information maintained in their databases. It also provides limited access to external sources of information on their genes, including cross references to HGNC gene symbols and some other gene identifier systems. The Ensembl website has helpful documentation on how to use BioMart to download annotation data using the web interface. In addition to downloading annotations as a CSV file from the web interface, Ensembl also provides the biomaRt Bioconductor package to allow programmatic access to the same information directly from R. There is much more information in the Ensembl databases than gene annotation data that can be accessed via BioMart, but we will provide a brief example of how to extract gene information here: # this assumes biomaRt is already installed through bioconductor library(biomaRt) # the human biomaRt database is named &quot;hsapiens_gene_ensembl&quot; ensembl &lt;- useEnsembl(biomart=&quot;ensembl&quot;, dataset=&quot;hsapiens_gene_ensembl&quot;) # listAttributes() returns a data frame, turn into a tibble to help with formatting as_tibble(listAttributes(ensembl)) # A tibble: 3,143 x 3 name description page &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 ensembl_gene_id Gene stable ID feature_page 2 ensembl_gene_id_version Gene stable ID version feature_page 3 ensembl_transcript_id Transcript stable ID feature_page 4 ensembl_transcript_id_version Transcript stable ID version feature_page 5 ensembl_peptide_id Protein stable ID feature_page 6 ensembl_peptide_id_version Protein stable ID version feature_page 7 ensembl_exon_id Exon stable ID feature_page 8 description Gene description feature_page 9 chromosome_name Chromosome/scaffold name feature_page 10 start_position Gene start (bp) feature_page # ... with 3,133 more rows The name column provides the programmatic name associated with the attribute that can be used to retrieve that annotation information using the getBM() function: # create a tibble with ensembl gene ID, HGNC gene symbol, and gene description gene_map &lt;- as_tibble( getBM( attributes=c(&quot;ensembl_gene_id&quot;, &quot;hgnc_gene_symbol&quot;, &quot;description&quot;), mart=ensembl ) ) gene_map # A tibble: 68,012 x 3 ensembl_gene_id hgnc_symbol description &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 ENSG00000210049 MT-TF mitochondrially encoded tRNA-Phe (UUU/C) [Source:HGNC Symbol;Acc:HGNC:748~ 2 ENSG00000211459 MT-RNR1 mitochondrially encoded 12S rRNA [Source:HGNC Symbol;Acc:HGNC:7470] 3 ENSG00000210077 MT-TV mitochondrially encoded tRNA-Val (GUN) [Source:HGNC Symbol;Acc:HGNC:7500] 4 ENSG00000210082 MT-RNR2 mitochondrially encoded 16S rRNA [Source:HGNC Symbol;Acc:HGNC:7471] 5 ENSG00000209082 MT-TL1 mitochondrially encoded tRNA-Leu (UUA/G) 1 [Source:HGNC Symbol;Acc:HGNC:7~ 6 ENSG00000198888 MT-ND1 mitochondrially encoded NADH:ubiquinone oxidoreductase core subunit 1 [So~ 7 ENSG00000210100 MT-TI mitochondrially encoded tRNA-Ile (AUU/C) [Source:HGNC Symbol;Acc:HGNC:748~ 8 ENSG00000210107 MT-TQ mitochondrially encoded tRNA-Gln (CAA/G) [Source:HGNC Symbol;Acc:HGNC:749~ 9 ENSG00000210112 MT-TM mitochondrially encoded tRNA-Met (AUA/G) [Source:HGNC Symbol;Acc:HGNC:749~ 10 ENSG00000198763 MT-ND2 mitochondrially encoded NADH:ubiquinone oxidoreductase core subunit 2 [So~ # ... with 68,002 more rows With this Ensembl Gene ID to HGNC symbol mapping in hand, you can combine the tibble above with other tibbles containing gene information by joining them together. BioMart/biomaRt is not the only ways to map different gene identifiers. The Bioconductor package AnnotateDbi also provides this functionality in a flexible format independent of the Ensembl databases. This package includes not only gene identifier mapping and information, but also identifiers from technology platforms, e.g. probe set IDs from microarrays, that can help when working with these types of data. The package also allows comprehensive and flexible homolog mapping. As with all Bioconductor packages, the AnnotationDbi documentation is well written and comprehensive, though knowledge of relational databases is helpful in understanding how the packages work. Ensembl BioMart portal Ensembl BioMart web portal documentation biomaRt Bioconductor documentation 8.6.3 Mapping Homologs Sometimes it is necessary to link datasets from different organisms together by orthology. For example, in an experiment performed in mice, we might be interested in comparing gene expression patterns observed in the mouse samples to a publicly available human dataset. In these contexts, we must link gene identifiers from one organism to their corresponding homologs in the other. BioMart enables us to extract these linked identifiers by explicitly connecting different biomaRt databases with the getLDS() function: human_db &lt;- useEnsembl(&quot;ensembl&quot;, dataset = &quot;hsapiens_gene_ensembl&quot;) mouse_db &lt;- useEnsembl(&quot;ensembl&quot;, dataset = &quot;mmusculus_gene_ensembl&quot;) orth_map &lt;- as_tibble( getLDS(attributes = c(&quot;ensembl_gene_id&quot;, &quot;hgnc_symbol&quot;), mart = human_db, attributesL = c(&quot;ensembl_gene_id&quot;, &quot;mgi_symbol&quot;), martL = mouse_db ) ) orth_map # A tibble: 26,390 x 4 Gene.stable.ID HGNC.symbol Gene.stable.ID.1 MGI.symbol &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 ENSG00000198695 MT-ND6 ENSMUSG00000064368 &quot;mt-Nd6&quot; 2 ENSG00000212907 MT-ND4L ENSMUSG00000065947 &quot;mt-Nd4l&quot; 3 ENSG00000279169 PRAMEF13 ENSMUSG00000094303 &quot;&quot; 4 ENSG00000279169 PRAMEF13 ENSMUSG00000094722 &quot;&quot; 5 ENSG00000279169 PRAMEF13 ENSMUSG00000095666 &quot;&quot; 6 ENSG00000279169 PRAMEF13 ENSMUSG00000094741 &quot;&quot; 7 ENSG00000279169 PRAMEF13 ENSMUSG00000094836 &quot;&quot; 8 ENSG00000279169 PRAMEF13 ENSMUSG00000074720 &quot;&quot; 9 ENSG00000279169 PRAMEF13 ENSMUSG00000096236 &quot;&quot; 10 ENSG00000198763 MT-ND2 ENSMUSG00000064345 &quot;mt-Nd2&quot; # ... with 26,380 more rows The mgi_symbol field refers to the gene symbol assigned in the Mouse Genome Informatics database maintained by The Jackson Laboratory. References "],["gene-expression.html", "8.7 Gene Expression", " 8.7 Gene Expression Gene expression is the process by which information from a gene is used in the synthesis of functional gene products that affect a phenotype. While gene expression studies are often focused on protein coding genes, there are many other functional molecules, e.g. transfer RNAs, lncRNAs, etc, that are produced by this process. The gene expression process has many steps and intermediate products, as depicted in the following figure: The Gene Expression Process - An overview of the flow of information from DNA to protein in a eukaryote The specific parts of the genome that code for genes are copied, or transcribed into RNA molecules called transcripts. In lower organisms like bacteria, these RNA molecules are passed on directly to ribosomes, which translate them into proteins. In most higher organisms like eukaryotes, these initial transcripts, called pre-messenger RNA (pre-mRNA) transcripts are further processed such that certain parts of the transcript, called introns, are spliced out and the flanking sequences, called exons, are concatenated together. After this splicing process is complete, the pre-mRNA transcripts become mature messenger RNA (mRNA) transcripts which go on to be exported from the nucleus and loaded into ribosomes in the cytoplasm to undergo translation into proteins. In gene expression studies, the relative abundance, or number of copies, of RNA or mRNA transcripts in a sample is measured. The measurements are non-negative numbers that are proportional to the relative abundance of a transcript with respect to some reference, either another gene, or in the case of high throughput assays like microarrays or high throughput sequencing, relative to measurements of all distinct transcripts examined in the experiment. Conceptually, the larger the magnitude of a transcript abundance measurement, the more copies of that transcript were in the original sample. There are many ways to measure the abundance of RNA transcripts; the following are some of the common methods at the time of writing: Light absorbance - RNA absorbs ultraviolet light with wavelength 260 nm, which can be used to determine RNA concentration in a sample using specialized equipment Northern blot - measures relative abundance of an RNA with a specific sequence quantitative polymerase chain reaction (qPCR) - measures relative abundance of an RNA with a specific sequence using PCR amplification Oligonucleotide and microarrays - measures relative abundance of thousands of genes simultaneously using known DNA probe sequences and fluorescently tagged RNA molecules High throughput RNA sequencing (RNASeq) - measures thousands to millions of RNA fragments simultaneously in proportion to their relative abundance While any of the measurement methods above may be analyzed in R, the high throughput methods (i.e. microarray, high throughput sequencing) are the primary concern of this chapter. These methods generate measurements for thousands of transcripts or genes simultaneously, requiring the power and flexibility of programmatic analysis to process in a practical amount of time. The remainder of this chapter is devoted to understanding the specific technologies, data types, and analytical strategies involved in working with these data. Gene expression measurements are almost always inherently relative, either due to limitations of the measurement methods (e.g. microarrays, described below) or because measuring all molecules in a sample will almost always be prohibitively expensive, have diminishing returns, and it is very difficult if not impossible to determine if all the molecules have been measured. This means we cannot in general associate the numbers associated with the measurements with an absolute molecular copy number. An important implication of the inherent relativity of these measurements is: absence of evidence is not evidence of absence. In other words, if a transcript has an abundance measurement of zero, this does not necessarily imply that the gene is not expressed. It may be that the gene is indeed expressed, but the copy number is sufficiently small that it was not detected by the assay. 8.7.1 Gene Expression Data in Bioconductor The SummarizedExperiment container is the standard way to load and work with gene expression data in Bioconductor. This container requires the following information: assays - one or more measurement assays (e.g. gene expression) in the form of a feature by sample matrix colData - metadata associated with the samples (i.e. columns) of the assays rowData - metadata associated with the features (i.e. rows) of the assays exptData - additional metadata about the experiment itself, like protocol, project name, etc The figure below illustrates how the SummarizedExperiment container is structured and how the different data elements are accessed: SummarizedExperiment Schematic - Huber, et al. 2015. Orchestrating High-Throughput Genomic Analysis with Bioconductor. Nature Methods 12 (2): 11521. Many Bioconductor packages for specific types of data, e.g. limma create these SummarizedExperiment objects for you, but you may also create your own by assembling each of these data into data frames manually: # microarray expression dataset intensities intensities &lt;- readr::read_delim(&quot;example_intensity_data.csv&quot;,delim=&quot; &quot;) # the first column of intensities tibble is the probesetId, extract to pass as rowData rowData &lt;- intensities[&quot;probeset_id&quot;] # remove probeset IDs from tibble and turn into a R dataframe so that we can assign rownames # since tibbles don&#39;t support row names intensities &lt;- as.data.frame( select(intensities, -probeset_id) ) rownames(intensities) &lt;- rowData$probeset_id # these column data are made up, but you would have a sample metadata file to use colData &lt;- tibble( sample_name=colnames(intensities), condition=sample(c(&quot;case&quot;,&quot;control&quot;),ncol(intensities),replace=TRUE) ) se &lt;- SummarizedExperiment( assays=list(intensities=intensities), colData=colData, rowData=rowData ) se ## class: SummarizedExperiment ## dim: 54675 35 ## metadata(0): ## assays(1): intensities ## rownames(54675): 1007_s_at 1053_at ... AFFX-TrpnX-5_at AFFX-TrpnX-M_at ## rowData names(1): probeset_id ## colnames(35): GSM972389 GSM972390 ... GSM972512 GSM972521 ## colData names(2): sample_name condition Detailed documentation of how to create and use the SummarizedExperiment is available in the SummarizedExperiment vignette. SummarizedExperiment is the successor to the older ExpressionSet container. Both are still used by Bioconductor packages, but SummarizedExperiment is more modern and flexible, so it is suggested for use whenever possible. 8.7.2 Differential Expression Analysis Differential expression analysis seeks to identify to what extent gene expression is associated with one or more variables of interest. For example, we might be interested in genes that have higher expression in subjects with a disease, or which genes change in response to a treatment. Typically, gene expression analysis requires two types of data to run: an expression matrix and a design matrix. The expression matrix will generally have features (e.g. genes) as rows and samples as columns. The design matrix is a numeric matrix that contains the variables we wish to model and any covariates or confounders we wish to adjust for. The variables in the design matrix then must be encoded in a way that statistical procedures can understand. The full details of how design matrices are constructed is beyond the scope of this book. Fortunately, R makes it very easy to construct these matrices from a tibble with the model.matrix() function. Consider for the following imaginary sample information tibble that has 10 AD and 10 Control subjects with different clinical and protein histology measurements assayed on their brain tissue: ad_metadata ## # A tibble: 20 x 8 ## ID age_at_death condition tau abeta iba1 gfap braak_stage ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 A1 81 AD 98543 166078 77299 75277 4 ## 2 A2 80 AD 32711 56919 49571 117441 1 ## 3 A3 91 AD 134427 295407 106724 67719 6 ## 4 A4 90 AD 81515 221801 171212 56346 4 ## 5 A5 80 AD 42442 97918 8321 109406 2 ## 6 A6 81 AD 54802 49358 82374 42797 2 ## 7 A7 77 AD 127166 8417 62675 85377 6 ## 8 A8 74 AD 103015 28478 131168 133892 5 ## 9 A9 82 AD 114270 188208 194818 61935 5 ## 10 A10 83 AD 90785 46279 121555 131853 4 ## 11 C1 79 Control 991 237 1062 64752 0 ## 12 C2 76 Control 2804 4423 3441 23384 0 ## 13 C3 73 Control 3120 7073 1402 135049 0 ## 14 C4 73 Control 7167 798 8365 18961 0 ## 15 C5 69 Control 6591 2590 4885 65094 0 ## 16 C6 77 Control 38697 103664 33430 23136 2 ## 17 C7 73 Control 58381 72071 16864 17295 3 ## 18 C8 75 Control 5102 10533 5321 89479 0 ## 19 C9 76 Control 59906 47967 19103 124651 3 ## 20 C10 81 Control 46553 49670 90763 52684 2 We might be interested in identifying genes that are increased or decreased in people with AD compared with Controls. We can create a model matrix for this as follows: model.matrix(~ condition, data=ad_metadata) ## (Intercept) conditionAD ## 1 1 1 ## 2 1 1 ## 3 1 1 ## 4 1 1 ## 5 1 1 ## 6 1 1 ## 7 1 1 ## 8 1 1 ## 9 1 1 ## 10 1 1 ## 11 1 0 ## 12 1 0 ## 13 1 0 ## 14 1 0 ## 15 1 0 ## 16 1 0 ## 17 1 0 ## 18 1 0 ## 19 1 0 ## 20 1 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$condition ## [1] &quot;contr.treatment&quot; The ~ condition argument is an an R forumla, which is a concise description of how variable should be included in the model. The general format of a formula is as follows: # portions in [] are optional [&lt;outcome variable&gt;] ~ &lt;predictor variable 1&gt; [+ &lt;predictor variable 2&gt;]... # examples # model Gene 3 expression as a function of disease status `Gene 3` ~ condition # model the amount of tau pathology as a function of abeta pathology, # adjusting for age at death tau ~ age_at_death + abeta # create a model without an outcome variable that can be used to create the # model matrix to test for differences with disease status adjusting for age at death ~ age_at_death + condition For most models, the design matrix will have an intercept term of all ones and additional colums for the other variables in the model. We might wish to adjust out the effect of age by including age_at_death as a covariate in the model: model.matrix(~ age_at_death + condition, data=ad_metadata) ## (Intercept) age_at_death conditionAD ## 1 1 81 1 ## 2 1 80 1 ## 3 1 91 1 ## 4 1 90 1 ## 5 1 80 1 ## 6 1 81 1 ## 7 1 77 1 ## 8 1 74 1 ## 9 1 82 1 ## 10 1 83 1 ## 11 1 79 0 ## 12 1 76 0 ## 13 1 73 0 ## 14 1 73 0 ## 15 1 69 0 ## 16 1 77 0 ## 17 1 73 0 ## 18 1 75 0 ## 19 1 76 0 ## 20 1 81 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$condition ## [1] &quot;contr.treatment&quot; The model matrix now includes a column for age at death as well. This model model matrix is now suitable to pass to differential expression software packages to look for genes associated with disease status. As mentioned above, modern gene expression assays measure thousands of genes simultaneously. This means each gene must be tested for differential expression individually. In general, each gene is tested using the same statistical model, so a differential expression analysis package will perform something equivalent to the following: Gene 1 ~ age_at_death + condition Gene 2 ~ age_at_death + condition Gene 3 ~ age_at_death + condition ... Gene N ~ age_at_death + condition Each gene model will have its own statistics associated with it that we must process and interpret after the analysis is complete. 8.7.3 Microarray Gene Expression Data On a high level, there are four steps involved in analyzing gene expression microarray data: Summarization of probes to probesets. Each gene is represented by multiple probes with different sequences. Summarization is a statistical procedure that computes a single value for each probeset from its corresponding probes. Normalization. This includes removing background signal from individual arrays as well as adjusting probeset intensities so that they are comparable across multiple sample arrays. Quality control. Compare all normalized samples within a sample set to identify, mitigate, or eliminate potential outlier samples. Analysis. Using the quality controlled expression data, Implement statistical analysis to answer research questions. The full details of microarray analysis are beyond the scope of this book. However in the following sections we cover some of the basic entry points to performing these steps in R and Bioconductor. The CEL data files from a set of microarrays can be loaded into R for analysis using the affy Bioconductor package. This package provides all the functions necessary for loading the data and performing key preprocessing operations. Typically, two or more samples were processed in this way, resulting in a set of CEL files that should be processed together. These CEL files will typically be all stored in the same directory, and may be loaded using the affy::ReadAffy function: # read all CEL files in a single directory affy_batch &lt;- affy::ReadAffy(celfile.path=&quot;directory/of/CELfiles&quot;) # or individual files in different directories cel_filenames &lt;- c( list.files( path=&quot;CELfile_dir1&quot;, full.names=TRUE, pattern=&quot;.CEL$&quot; ), list.files( path=&quot;CELfile_dir2&quot;, full.names=TRUE, pattern=&quot;.CEL$&quot; ) ) affy_batch &lt;- affy::ReadAffy(filenames=cel_filenames) The affy_batch variable is a AffyBatch container, which stores information on the probe definitions based on the type of microarray, probe-level intensity for each sample, and other information about the experiment contained within the CEL files. The affy package provides functions to perform probe summarization and normalization. The most popular method to accomplish this at the time of writing is called Robust Multi-array Average or RMA (Irizarry et al. 2003), which performs summarization and normalization of multiple arrays simultaneously. Below, the RMA algorithm is used to normalize an example dataset provided by the affydata Bioconductor package. Certain Bioconductor packages provide example datasets for use with companion analysis packages. For example, the affydata package provides the Dilution dataset, which was generated using two concentrations of cDNA from human liver tissue and a central nervous system cell line. To load a data package into R, first run library(&lt;data package&gt;) and then data(&lt;dataset name&gt;). library(affy) library(affydata) ## Package LibPath Item ## [1,] &quot;affydata&quot; &quot;C:/Users/Adam/Documents/R/win-library/4.1&quot; &quot;Dilution&quot; ## Title ## [1,] &quot;AffyBatch instance Dilution&quot; data(Dilution) # normalize the Dilution microarray expression values # note Dilution is an AffyBatch object eset_rma &lt;- affy::rma(Dilution,verbose=FALSE) # plot distribution of non-normalized probes # note rma normalization takes the log2 of the expression values, # so we must do so on the raw data to compare raw_intensity &lt;- as_tibble(exprs(Dilution)) %&gt;% mutate(probeset_id=rownames(exprs(Dilution))) %&gt;% pivot_longer(-probeset_id, names_to=&quot;Sample&quot;, values_to = &quot;Intensity&quot;) %&gt;% mutate( `log2 Intensity`=log2(Intensity), Category=&quot;Before Normalization&quot; ) # plot distribution of normalized probes rma_intensity &lt;- as_tibble(exprs(eset_rma)) %&gt;% mutate(probesetid=featureNames(eset_rma)) %&gt;% pivot_longer(-probesetid, names_to=&quot;Sample&quot;, values_to = &quot;log2 Intensity&quot;) %&gt;% mutate(Category=&quot;RMA Normalized&quot;) dplyr::bind_rows(raw_intensity, rma_intensity) %&gt;% ggplot(aes(x=Sample, y=`log2 Intensity`)) + geom_boxplot() + facet_wrap(vars(Category)) Above, we first apply RMA to the Dilution microarray dataset using the affy::rma() function. Then we plot the log 2 intensities of the probes in each array, first using the raw intensities and then afysis seeking to identify how associated gene expression is with one or more variables of interest. For example, 8.7.4 Differential Expression: Microarrays (limma) limma, which is short for linear models of microarrays, is one of the top most downloaded Bioconductor packages. limma is utilized for analyzing microarray gene expression data, with a focus on analyses using linear models to integrate all of the data from an experiment. limma was developed for microarray analysis prior to the development of sequencing based gene expression methods (i.e. RNASeq) but has since added functionality to analyze other types of gene expression data. limma excels at analyzing these types of data as it can support arbitrarily complex experimental designs while maintaining strong statistical power. An experiment with a large number of conditions or predictors can still be analyze even with small sample sizes. The method accomplishes this by using an empirical Bayes approach that borrows information across all the genes in the dataset to better control error in individual gene models. See Ritchie et al. (2015) for more details on how limma works. limma requres an expression matrix like that stored in an ExpressionSet or SummarizedExperiment container and a design matrix: # intensities is an example gene expression matrix that corresponds to our # AD sample metadata of 10 AD and 10 Control individuals ad_se &lt;- SummarizedExperiment( assays=list(intensities=intensities), colData=ad_metadata, rowData=rownames(intensities) ) # define our design matrix for AD vs control, adjusting for age at death ad_vs_control_model &lt;- model.matrix(~ age_at_death + condition, data=ad_metadata) # now run limma # first fit all genes with the model fit &lt;- limma::lmFit( assays(se)$intensities, ad_vs_control_model ) # now better control fit errors with the empirical Bayes method fit &lt;- limma::eBayes(fit) We have now conducted a differential expression analysis with limma. We can extract out the results for our question of interest - which genes are associated with AD - using the limma::topTable() function: # the coefficient name conditionAD is the column name in the design matrix # adjust=&quot;BH&quot; means perform Benjamini-Hochberg multiple testing adjustment # on the nominal p-values from each gene test # topTable only returns the top 10 results sorted by ascending p-value by default topTable(fit, coef=&quot;conditionAD&quot;, adjust=&quot;BH&quot;) ## logFC AveExpr t P.Value adj.P.Val B ## 234942_s_at 1.1231942 8.397425 4.890779 9.613196e-05 0.770605 -2.987573 ## 202423_at -0.7499088 8.646687 -4.785281 1.221646e-04 0.770605 -3.025525 ## 1557538_at -0.8057310 4.348859 -4.697520 1.492072e-04 0.770605 -3.057737 ## 1558290_a_at -0.8580750 7.278878 -4.658725 1.630238e-04 0.770605 -3.072162 ## 234596_at 0.4925167 3.038448 4.564888 2.020432e-04 0.770605 -3.107523 ## 225986_x_at -0.6030385 6.958092 -4.502976 2.328354e-04 0.770605 -3.131216 ## 203124_s_at -1.2857166 8.565022 -4.488589 2.406451e-04 0.770605 -3.136763 ## 209346_s_at 0.6578475 6.010219 4.421869 2.804637e-04 0.770605 -3.162688 ## 224825_at 1.0266849 8.147104 4.421597 2.806391e-04 0.770605 -3.162795 ## 205647_at -0.8569475 4.399029 -4.399700 2.951151e-04 0.770605 -3.171376 From the table, none of the probesets in this experiment are significant at FDR &lt; 0.05. Homepage BioConductor Publication 8.7.5 RNASeq RNA sequencing (RNASeq) is a HTS technology that measures the abundance of RNA molecules in a sample. Briefly, RNA molecules are extracted from a sample using a biochemical protocol that isolates RNA from other molecules (i.e. DNA, proteins, etc) in the sample. Ribosomal RNA is removed from the sample (see Note box below), and the remaining RNA molecules are size selected to retain only short molecules (~15-300 nucleotides in length, depending on the protocol). The size selected molecules are then reverse transcribed into cDNA and converted to double stranded molecules. Sequencing libraries are prepared with these cDNA using proprietary biochemical protocols to make them suitable for sequencing on the sequencing instrument. After sequencing, FASTQ files containing millions of reads for one or more samples is produced. Except for the initial RNA extraction, all of these steps are typically performed by specialized instrumentation facilities called sequencing cores who provide the data in FASTQ format to the investigators. 80%-90% of the RNA mass in a cell is ribosomal RNA (rRNA), the RNA that makes up ribosomes and is responsible for maintaining protein translation processes in the cell. If total RNA was sequenced, the large majority of reads would correspond to rRNAs and would therefore be of little or no use. For this reason, rRNA molecules are removed from the RNA material that goes on to sequencing using either a poly-A tail enrichment or ribo-depletion strategy. These two methods maintain different populations of RNA and therefore sequencing datasets generated with different strategies require different interpretations. The more detail on this topic is beyond the scope of this book, but (Zhao et al. 2018) provide a good introduction and comparison of both approaches. The reads produced in an RNASeq experiment represent RNA fragments from transcripts. The number of reads that correspond to specific transcripts is assumed to be proportional to the copy number of that transcript in the original sample. The alignment process described in the Preliminary HTS Data Analysis section produces alignments of reads against the genome or transcriptome, and the number of reads that align against each gene or transcript is counted using a reference annotation that defines the locations of every known gene in the genome. The resulting counts are therefore estimates of the copy number of the molecules for each gene or transcript in the original sample. Conceptually, the more reads that map to a gene, the higher the abundance of transcripts for that gene, and therefore we infer higher the expression of that gene. Gene expression for different genes can vary by orders of magnitude within a single cell, where highly expressed genes may have billions of transcripts and others comparatively few. Therefore, the likelihood of obtaining a read for a given transcript is proportional to the relative abundance of that transcript compared with all other transcripts. Relatively rare transcripts have a low probability of being sampled by the sequencing procedure, and extremely lowly expressed genes may not be sampled at all. The library size (i.e. the total number of reads) in the sequencing dataset for a sample determines the range of gene expression the dataset is likely to detect, where datasets with few reads are unlikely to sample lowly expressed transcripts even though they are truly expressed. For this reason absence of evidence is not evidence of absence in RNASeq data. If a read is not observed for any given gene, we cannot say that the gene is not expressed; we can only say that the relative expression of that gene is below the detection threshold of the dataset we generated, given the number of sequenced reads. For a single sample, the output of this read counting procedure is a vector of read counts for each gene in the annotation. Genes with no reads mapping to them will have a count of zero, and all others will have a read count of one or more; as mentioned in the Count Data section, read counts are typically non-zero integers. This procedure is typically repeated separately for a set of samples that make up the experiment, and the counts for each sample are concatenated together into a counts matrix. This count matrix is the primary result of interest that is passed on to downstream analysis, most commonly differential expression analysis. 8.7.6 RNASeq Gene Expression Data The read counts for each gene or transcript in the genome are estimates of the relative abundance of molecules in the original sample. The number of genes counted depends on the organism being studied and the maturity of the gene annotation used. Complex multicellular organisms have on the order of thousands to tens of thousands of genes; for example, humans and mice have 20k-30k genes in their respective genomes. Each RNASeq experiment thus may yield tens of thousands of measurements for each sample. We shall consider an example RNASeq dataset generated by (OMeara et al. 2015) generated to study how mouse cardiac cells can regenerate up until one week of age, after which they lose the ability to do so. The dataset has eight RNASeq samples, two for each of four time points across the developmental window until adulthood. We will load in the expression data as a tibble: counts &lt;- read_tsv(&quot;verse_counts.tsv&quot;) counts ## # A tibble: 55,416 x 9 ## gene P0_1 P0_2 P4_1 P4_2 P7_1 P7_2 Ad_1 Ad_2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ENSMUSG00000102693.2 0 0 0 0 0 0 0 0 ## 2 ENSMUSG00000064842.3 0 0 0 0 0 0 0 0 ## 3 ENSMUSG00000051951.6 19 24 17 17 17 12 7 5 ## 4 ENSMUSG00000102851.2 0 0 0 0 1 0 0 0 ## 5 ENSMUSG00000103377.2 1 0 0 1 0 1 1 0 ## 6 ENSMUSG00000104017.2 0 3 0 0 0 1 0 0 ## 7 ENSMUSG00000103025.2 0 0 0 0 0 0 0 0 ## 8 ENSMUSG00000089699.2 0 0 0 0 0 0 0 0 ## 9 ENSMUSG00000103201.2 0 0 0 0 0 0 0 1 ## 10 ENSMUSG00000103147.2 0 0 0 0 0 0 0 0 ## # ... with 55,406 more rows The annotation counted reads for 55,416 annnotated Ensembl gene for the mouse genome, as described in the Gene Identifier Systems section. There are typically three steps when analyzing a RNASeq count matrix: Filter genes that are unlikely to be differentially expressed. Normalize filtered counts to make samples comparable to one another. Differential expression analysis of filtered, normalized counts. Each of these steps will be described in detail below. 8.7.7 Filtering Counts Filtering genes that are not likely to be differentially expressed is an important step in differential expression analysis. The primary reason for this is to reduce the number of genes tested for differential expression, thereby reducing the multiple testing burden on the results. There are many approaches and rationales for picking filters, and the choice is generally subjective and highly dependent upon the specific dataset. In this section we discuss some of the rationale and considerations for different filtering strategies. Genes that were not detected in any sample are the easiest to filter, and often eliminate many genes from consideration. Below we filter out all genes that have zero counts in all samples: nonzero_genes &lt;- rowSums(counts[-1])!=0 filtered_counts &lt;- counts[nonzero_genes,] filtered_counts ## # A tibble: 32,613 x 9 ## gene P0_1 P0_2 P4_1 P4_2 P7_1 P7_2 Ad_1 Ad_2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ENSMUSG00000051951.6 19 24 17 17 17 12 7 5 ## 2 ENSMUSG00000102851.2 0 0 0 0 1 0 0 0 ## 3 ENSMUSG00000103377.2 1 0 0 1 0 1 1 0 ## 4 ENSMUSG00000104017.2 0 3 0 0 0 1 0 0 ## 5 ENSMUSG00000103201.2 0 0 0 0 0 0 0 1 ## 6 ENSMUSG00000103161.2 0 0 1 0 0 0 0 0 ## 7 ENSMUSG00000102331.2 5 8 2 6 6 11 4 3 ## 8 ENSMUSG00000102343.2 0 0 0 1 0 0 0 0 ## 9 ENSMUSG00000025900.14 4 3 6 7 7 3 36 35 ## 10 ENSMUSG00000102948.2 1 0 0 0 0 0 0 1 ## # ... with 32,603 more rows Only 32,613 genes remain after filtering out genes with all zeros, eliminating nearly 23,000 genes, or more than 40% of the genes in the annotation. Filtering genes with zeros in all samples as we have done above is somewhat liberal, as there may be many genes with reads in only one sample. Genes with only a single sample with a non-zero count also cannot be differentially expressed, so we can easily decide to eliminate those as well: nonzero_genes &lt;- rowSums(counts[-1])&gt;=2 filtered_counts &lt;- counts[nonzero_genes,] filtered_counts ## # A tibble: 28,979 x 9 ## gene P0_1 P0_2 P4_1 P4_2 P7_1 P7_2 Ad_1 Ad_2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ENSMUSG00000051951.6 19 24 17 17 17 12 7 5 ## 2 ENSMUSG00000103377.2 1 0 0 1 0 1 1 0 ## 3 ENSMUSG00000104017.2 0 3 0 0 0 1 0 0 ## 4 ENSMUSG00000102331.2 5 8 2 6 6 11 4 3 ## 5 ENSMUSG00000025900.14 4 3 6 7 7 3 36 35 ## 6 ENSMUSG00000102948.2 1 0 0 0 0 0 0 1 ## 7 ENSMUSG00000025902.14 195 186 204 229 157 173 196 129 ## 8 ENSMUSG00000104238.2 0 4 0 2 4 1 3 3 ## 9 ENSMUSG00000102269.2 7 16 6 6 4 6 8 3 ## 10 ENSMUSG00000118917.1 0 1 0 0 1 1 0 0 ## # ... with 28,969 more rows We have now reduced our number of genes to 28,979 by including only genes with at least two non-zero counts. We may now wonder how many genes we would include if we required at least \\(n\\) samples to have non-zero counts. Consider the following bar chart of the number of genes detected in at least \\(n\\) samples: library(patchwork) nonzero_counts &lt;- mutate(counts,`Number of samples`=rowSums(counts[-1]!=0)) %&gt;% group_by(`Number of samples`) %&gt;% summarize(`Number of genes`=n() ) %&gt;% mutate(`Cumulative number of genes`=sum(`Number of genes`)-lag(cumsum(`Number of genes`),1,default=0)) sum_g &lt;- ggplot(nonzero_counts) + geom_bar(aes(x=`Number of samples`,y=`Cumulative number of genes`,fill=&quot;In at least n&quot;),stat=&quot;identity&quot;) + geom_bar(aes(x=`Number of samples`,y=`Number of genes`,fill=&quot;In exactly n&quot;),stat=&quot;identity&quot;) + labs(title=&quot;Number of genes detected in n samples&quot;) + scale_fill_discrete(name=&quot;Nonzero in:&quot;) sum_g The largest number of genes is detected in none of the samples, followed by genes detected in all samples, with many fewer in between. Based on the plot, there is no obvious choice for how many nonzero samples are sufficient to use as a filter. We may gain additional insight into this problem by considering the distribution of counts for genes present in at least \\(n\\) samples as above, rather than just the number of nonzero counts. Below is a plot similar to that above but, which plots the distribution of mean counts across samples within each number of nonzero samples: # calculate the mean count for non-zero counts in genes with exactly n non-zero counts mutate(counts,`Number of nonzero samples`=factor(rowSums(counts[-1]!=0))) %&gt;% pivot_longer(-c(gene,`Number of nonzero samples`),names_to=&quot;Sample&quot;,values_to=&quot;Count&quot;) %&gt;% group_by(`Number of nonzero samples`) %&gt;% group_by(gene, .add=TRUE) %&gt;% summarize(`Nonzero Count Mean`=mean(Count[Count!=0]),.groups=&quot;keep&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x=`Number of nonzero samples`,y=`Nonzero Count Mean`,fill=`Number of nonzero samples`)) + geom_violin(scale=&quot;width&quot;) + scale_y_log10() + labs(title=&quot;Nonzero count mean for genes with exactly n nonzero counts&quot;) From this plot, we see that the mean count for genes with even a single sample with a zero is substantially lower than most genes without any zeros. We might therefore decide to filter out genes that have any zeros, with the rationale that most genes of appreciable abundance would still be included. However, there are some genes with a high average count for samples that are not zero that would be eliminated if we chose to use only genes expressed in all 8 samples. Again, there is no clear choice of filtering threshold, and a subjective decision is necessary. One important property of RNASeq expression data is that genes with higher mean count also have higher variance. This mean-variance dependence means these count data are heteroskedastic, a term used to describe data that has inconstant variance across different values. We can visualize this dependence by calculating the mean and variance for each of our normalized count genes and plotting them against each other: Unlike in microarray data, where probesets can be filtered based on variance, using low variance as a filter for count data would result in filtering out genes with low mean, which would lead to undesirable bias toward highly expressed genes. Depending on the experiment, it may be meaningful if a gene is detected at all in any of the samples. In these cases, genes with only 1 read in a single sample might be kept, even though these genes would not result in confident statistical inference if subjected to differential expression. The scientific question asked by the experiment is an important consideration when deciding thresholds. When the goal of the RNASeq experiment is differential expression analysis, a good rule of thumb when choosing a filtering threshold is to choose genes that have sufficiently many non-zero counts such that the statistical method has enough variance to perform reliable inference. For example, a reasonable filtering threshold might be to include genes that have non-zero counts in at least 50% of the samples. This threshold depends only upon the number of samples being considered. In an experimental design with two or more sample groups, a reasonable strategy might be to require at least 50% of samples within at least one group or within all groups to be included. Taking the experimental design into account when filtering can ensure the most interesting genes, e.g. those expressed only in one condition, are captured. Regardless of the filtering strategy used, there is no biological meaning to any filtering threshold. The number of reads detected for each gene is dependent primarily upon the library size, where larger libraries are more likely to sample lowly abundant genes. Therefore, we cannot filter genes with the intent of removing lowly expressed genes; instead we can only filter out genes that are below the detection threshold afforded by the library size of the dataset. 8.7.8 Count Distributions The range of gene expression in a cell varies by orders of magnitude. The following histogram plots the distribution of read count values for one of the adult samples: dplyr::select(filtered_counts, Ad_1) %&gt;% mutate(`log10(counts+1)`=log10(Ad_1+1)) %&gt;% ggplot(aes(x=`log10(counts+1)`)) + geom_histogram(bins=50) + labs(title=&#39;log10(counts) distribution for Adult mouse&#39;) We add 1 count, called a pseudocount, to avoid taking the log of zero, therefore all the genes at \\(x=0\\) are genes that have no reads. From the plot, the largest count is nearly \\(10^6\\) while the smallest is 1, spanning nearly six orders of magnitude. There are relatively few of these highly abundant genes, however, and most genes are betwen the range of 100-10,000 reads. We see that this distribution is similar for all the samples, when displayed as a ridgeline plot: library(ggridges) pivot_longer(filtered_counts,-c(gene),names_to=&quot;Sample&quot;,values_to=&quot;Count&quot;) %&gt;% mutate(`log10(Count+1)`=log10(Count+1)) %&gt;% ggplot(aes(y=Sample,x=`log10(Count+1)`,fill=Sample)) + geom_density_ridges(bandwidth=0.132) + labs(title=&quot;log10(Count+1) Distributions&quot;) The general shape of the distributions is similar, with a large mode of counts at zero, representing genes that had zero counts in that sample, and a wider mode between 100 and 10,000 where most consistently expressed genes fall. However, we note that the peak count of these larger modes vary and also recall that though the difference appears small, the log scale of the x axis means the differences in counts may be substantial. We will discuss the implications of this in the next section. 8.7.8.1 Count Normalization The number of reads generated for each sample in an RNASeq dataset varies from sample to sample due to randomness in the biochemical process that generates the data. The relationship between the number of reads that maps to any given gene and the relative abundance of the molecule in the sample is dependent upon the library size of each sample. The direct number of counts, or the raw counts, is therefore not directly comparable between samples. A simple example should suffice to make this clear: Imagine two RNASeq datasets generated from the same RNA sample. One of the samples was sequenced to 1000 reads, and the other to 2000 reads. Now imagine that one gene makes up half of the RNA in the original sample. In the sample with 1000 reads, we find this gene has 500 reads. In the sample with 2000 reads, the gene has 1000 of those reads. In both cases the fraction of reads that map to the gene is 0.5, but the absolute number of reads differs greatly. This is why the raw counts for a gene are not directly comparable between samples with different numbers of reads. Since in general every sample will have a unique number of reads, these raw counts will never be directly comparable. The way we mitigate this problem is with a statistical procedure called count normalization. Count normalization is the process by which the number of raw counts in each sample is scaled by a factor to make multiple samples comparable. Many strategies have been proposed to do this, but the simplest is a family of methods that divide by some factor of the library size. A common method in this family is to compute counts per million reads or CPM normalization, which scales each gene count by the number of millions of reads in the library: \\[ cpm_{s,i} = \\frac{c_{s,i}}{l_s} * 10^6 \\] Here, \\(c_{s,i}\\) is the raw count of gene \\(i\\) in sample \\(s\\), and \\(l_s\\) is the number of reads in the dataset for sample \\(s\\). If each sample is scaled by their own library size, then in principle the proportion of read counts assigned to each gene out of all reads for that sample will be comparable across samples. When we have no additional knowledge of the distribution of genes in our system, library size normalization methods are the most general and make the fewest assumptions. Below is a ridgeline plot of the counts distribution after CPM normalization: The distributions look quite different after CPM normalization, but the modes now look more consistent than with raw counts. One drawback of library size normalization is it is sensitive to extreme values; individual genes with many more reads in one sample than in other samples. These outliers can lead to pathological effects when comparing gene count values across samples. However, if we are measuring gene expression in a single biological system like mouse or human, as is often the case, other methods have been developed to better use our understanding of the gene expression distribution to perform more robust and biologically meaningful normalization. A more robust approach is possible when we make the reasonable assumption that, for any set of biological conditions in an experiment, most genes are not differentially expressed. This is the assumption made by the DESeq2 normalization method. The DESeq2 normalization method is a statistical procedure that uses the median geometric mean computed across all samples to determine the scale factor for each sample. The procedure is somewhat complicated and we encourage the reader to examine it in detail in the DESeq2 publication (Love, Huber, and Anders 2014). The method has proven to be consistent and reliable compared with other strategies (Dillies et al. 2013) and is the default normalization for the populare DESeq2 differential expression method, which will be described in a later section. The DESeq2 normalization method may be implemented using the DESeq2 Bioconductor package: library(DESeq2) # DESeq2 requires a counts matrix, column data (sample information), and a formula # the counts matrix *must be raw counts* count_mat &lt;- as.matrix(filtered_counts[-1]) row.names(count_mat) &lt;- filtered_counts$gene dds &lt;- DESeqDataSetFromMatrix( countData=count_mat, colData=tibble(sample_name=colnames(filtered_counts[-1])), design=~1 # no formula is needed for normalization, ~1 produces a trivial design matrix ) # compute normalization factors dds &lt;- estimateSizeFactors(dds) # extract the normalized counts deseq_norm_counts &lt;- as_tibble(counts(dds,normalized=TRUE)) %&gt;% mutate(gene=filtered_counts$gene) %&gt;% relocate(gene) Using the DESeq2 package, we first construct a DESeq object using the raw counts matrix, a sample information tibble with only a sample name column, and a trivial design formula, which will be explained in more depth in the DESeq2/edgeR section. The estimateSizeFactors() function performs the normalization routine on the counts, and the counts(dds,normalized=TRUE) call extracts the normalized counts matrix. When plotted as a ridgeline plot as before, we see that the gene expression distribution modes are better behaved compared to raw counts, and the shape of the distribution is preserved unlike with CPM normalized counts: pivot_longer(deseq_norm_counts,-c(gene),names_to=&quot;Sample&quot;,values_to=&quot;Count&quot;) %&gt;% mutate(`log10(Count+1)`=log10(Count+1)) %&gt;% ggplot(aes(y=Sample,x=`log10(Count+1)`,fill=Sample)) + geom_density_ridges(bandwidth=0.132) + labs(title=&quot;log10(Count+1) Distributions&quot;) The DESeq2 normalization procedure has two important considerations to keep in mind: The procedure borrows information across all samples. The geometric mean of counts across all samples is computed as part of the normalization procedure. This means that the size factors computed depend on the samples in the dataset. The normalized counts for one sample will likely change if it is normalized with a different set of samples. The procedure does not use genes with any zero counts. The geometric mean calculation across all samples means that any sample with a zero count results in the entire geometric mean being zero. For this reason only genes with nonzero counts in every sample are used to calculate the normalization factors. If one of the samples has a large number of zeros, for example due to a small library size, this could dramatically influence the normalization factors for the entire experiment. The CPM normalization procedure does not borrow information across all samples, and therefore is not subject to these considerations. These are only two of many possible normalization methods, DESeq2 being the most popular at the time of writing. For a detailed explanation of many normalization methods, see (Dillies et al. 2013). 8.7.8.2 Count Transformation As mentioned in the Count Data section, one way to deal with the non-normality of count data is to perform a data transformation that makes the counts data follow a normal distribution. Transformation to normality allows common and powerful statistical methods like linear regression to be used. The most basic count transformation is to take the logarithm of the counts. However, this can be problematic for genes with low counts, causing the values to spread further apart than genes with high counts, increasing the likelihood of false positive results. To address this issue, the DESeq2 package provides the rlog() function that performs a regularized logarithmic transformation that adjusts the log transform based on the mean count of each gene. The rlog transformation also has the desirable effect of making the count data homoskedastic, meaning the variance of a gene is not dependent on the mean. Some statistical methods assume that a dataset has this property. The relationship of mean count vs variance will be discussed in more detail in the next section. We can clearlysee the effect of rlog on the counts distribution by plotting the rloged counts using a ridgeline plot as before: # the dds is the DESeq2 object from earlier rld &lt;- rlog(dds) # extract rlog values as a tibble rlog_counts &lt;- as_tibble(assay(rld)) rlog_counts$gene &lt;- filtered_counts$gene pivot_longer(rlog_counts,-c(gene),names_to=&quot;Sample&quot;,values_to=&quot;rlog count&quot;) %&gt;% ggplot(aes(y=Sample,x=`rlog count`,fill=Sample)) + geom_density_ridges(bandwidth=0.132) + labs(title=&quot;rlog Distributions&quot;) It is generally accepted that count transformations have significant drawbacks compared with statistical methods that model counts and errors explicitly, like generalized linear models (GLM) such as Poisson and Negative Binomial regressions (St-Pierre, Shikon, and Schneider 2018). Whenever possible, these GLM based methods should be used instead of transformations. However, the transformations may sometimes be necessary depending on the required analysis, so the rlog function in DESeq2 may be useful for these cases. DESeq2 package vignette on Count data transformations 8.7.9 Differential Expression: RNASeq The read counts for each gene or transcript form the primary data used to identify genes whose expression correlates with some condition of interest. The counts matrix created by concatenating read counts for all genes across a set of samples where rows are genes or transcripts and columns are samples is the most common form of expression data used for this purpose. As mentioned in the Count Data section, these counts are not normally distributed and therefore require a statistical approach that can accommodate the counts distribution. The strong relationship between the mean and variance is characteristic of RNASeq expression data, and motivates the use of a generalized linear model called negative binomial regression. Negative binomial regression models count data using the negative binomial distribution, which allows the relationship of mean and variance of a counts distribution to vary. The statistical details of the negative binomial distribution and negative binomial regression are beyond the scope of this book, but below we will discuss the aspects necessary to implement differential expression analysis. 8.7.9.1 DESeq2/EdgeR DESeq2 and edgeR are bioconductor packages that both implement negative binomial regression to perform differential expression on RNASeq data. Both perform raw counts normalization as part of their function, though they differ in the normalization method (DESeq2 method is described above, while edgeR uses trimmed mean of M-values (TMM), see (Robinson and Oshlack 2010)). The interface for using both packages is also similar, so we will focus on DESeq2 in this section. As mentioned briefly above, DESeq2 requires three pieces of information to perform differential expression: A raw counts matrix with genes as rows and samples as columns A data frame with metadata associated with the columns A design formula for the differential expression model DESeq2 requires raw counts due to its normalization procedure, which borrows information across samples when computing size factors, as described above in the Count Normalization section. With these three pieces of information, we construct a DESeq object: library(DESeq2) # filter counts to retain genes with at least 6 out of 8 samples with nonzero counts filtered_counts &lt;- counts[rowSums(counts[-1]!=0)&gt;=6,] # DESeq2 requires a counts matrix, column data (sample information), and a formula # the counts matrix *must be raw counts* count_mat &lt;- as.matrix(filtered_counts[-1]) row.names(count_mat) &lt;- filtered_counts$gene # create a sample matrix from sample names sample_info &lt;- tibble( sample_name=colnames(filtered_counts[-1]) ) %&gt;% separate(sample_name,c(&quot;timepoint&quot;,&quot;replicate&quot;),sep=&quot;_&quot;,remove=FALSE) %&gt;% mutate( timepoint=factor(timepoint,levels=c(&quot;Ad&quot;,&quot;P0&quot;,&quot;P4&quot;,&quot;P7&quot;)) ) design &lt;- formula(~ timepoint) sample_info ## # A tibble: 8 x 3 ## sample_name timepoint replicate ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 P0_1 P0 1 ## 2 P0_2 P0 2 ## 3 P4_1 P4 1 ## 4 P4_2 P4 2 ## 5 P7_1 P7 1 ## 6 P7_2 P7 2 ## 7 Ad_1 Ad 1 ## 8 Ad_2 Ad 2 The design formula ~ timepoint is similar to that supplied to the model.matrix() function as described in the Differential Expression Analysis section, where DESeq2 implicitly creates the model matrix by combining the column data with the formula. In this differential expression analysis, we wish to identify genes that are different between the timepoints P0, P4, P7, and Ad. With these three pieces of information prepared, we can create a DESeq object and perform differential expression analysis: dds &lt;- DESeqDataSetFromMatrix( countData=count_mat, colData=sample_info, design=design ) dds &lt;- DESeq(dds) resultsNames(dds) ## [1] &quot;Intercept&quot; &quot;timepoint_P0_vs_Ad&quot; &quot;timepoint_P4_vs_Ad&quot; ## [4] &quot;timepoint_P7_vs_Ad&quot; DESeq2 performed differential expression on each gene and estimated parameters for three different comparisons - P0 vs Ad, P4 vs Ad, and P7 vs Ad. Here, Ad is the reference group so that we may interpret the statistics as relative to the adult animals. The choice of reference group does not change the magnitude of the results, but may change the sign of the estimated coefficients, which are log2 fold changes. To extract out the differential expression statistics for the P0 vs Ad comparison, we can use the results() function: res &lt;- results(dds, name=&quot;timepoint_P0_vs_Ad&quot;) p0_vs_Ad_de &lt;- as_tibble(res) %&gt;% mutate(gene=rownames(res)) %&gt;% relocate(gene) %&gt;% arrange(pvalue) p0_vs_Ad_de ## # A tibble: 21,213 x 7 ## gene baseMean log2FoldChange lfcSE stat pvalue padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ENSMUSG00000000031.17 19624. 6.75 0.190 35.6 5.70e-278 1.07e-273 ## 2 ENSMUSG00000026418.17 9671. 9.84 0.285 34.5 2.29e-261 2.15e-257 ## 3 ENSMUSG00000051855.16 3462. 5.35 0.157 34.1 2.84e-255 1.77e-251 ## 4 ENSMUSG00000027750.17 9482. 4.05 0.125 32.4 2.26e-230 1.06e-226 ## 5 ENSMUSG00000042828.13 3906. -5.32 0.167 -31.8 1.67e-221 6.25e-218 ## 6 ENSMUSG00000046598.16 1154. -6.08 0.197 -30.8 4.77e-208 1.49e-204 ## 7 ENSMUSG00000019817.19 1952. 5.15 0.171 30.1 1.43e-198 3.83e-195 ## 8 ENSMUSG00000021622.4 17146. -4.57 0.157 -29.0 4.41e-185 1.03e-181 ## 9 ENSMUSG00000002500.16 2121. -6.20 0.217 -28.5 6.69e-179 1.39e-175 ## 10 ENSMUSG00000024598.10 1885. 5.90 0.209 28.2 5.56e-175 1.04e-171 ## # ... with 21,203 more rows These are the results of the differential expression comparing P0 with Ad; the other contrasts (P4 vs Ad and P7 vs Ad) may be extracted by specifying the appropriate result names to the name argument to the results() function. The columns in the DESeq2 results are defined as follows: baseMean - the mean normalized count of all samples for the gene log2FoldChange - the estimated coefficient (i.e. log2FoldChange) for the comparison of interest lfcSE - the standard error for the log2FoldChange estimate stat - the Wald statistic associated with the log2FoldChange estimate pvalue - the nominal p-value of the Wald test (i.e. the signifiance of the association) padj - the multiple testing adjusted p-value (i.e. false discovery rate) In the P0 vs Ad comparison, we can check the number of significant results easily: # number of genes significant at FDR &lt; 0.05 filter(p0_vs_Ad_de,padj&lt;0.05) ## # A tibble: 7,557 x 7 ## gene baseMean log2FoldChange lfcSE stat pvalue padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ENSMUSG00000000031.17 19624. 6.75 0.190 35.6 5.70e-278 1.07e-273 ## 2 ENSMUSG00000026418.17 9671. 9.84 0.285 34.5 2.29e-261 2.15e-257 ## 3 ENSMUSG00000051855.16 3462. 5.35 0.157 34.1 2.84e-255 1.77e-251 ## 4 ENSMUSG00000027750.17 9482. 4.05 0.125 32.4 2.26e-230 1.06e-226 ## 5 ENSMUSG00000042828.13 3906. -5.32 0.167 -31.8 1.67e-221 6.25e-218 ## 6 ENSMUSG00000046598.16 1154. -6.08 0.197 -30.8 4.77e-208 1.49e-204 ## 7 ENSMUSG00000019817.19 1952. 5.15 0.171 30.1 1.43e-198 3.83e-195 ## 8 ENSMUSG00000021622.4 17146. -4.57 0.157 -29.0 4.41e-185 1.03e-181 ## 9 ENSMUSG00000002500.16 2121. -6.20 0.217 -28.5 6.69e-179 1.39e-175 ## 10 ENSMUSG00000024598.10 1885. 5.90 0.209 28.2 5.56e-175 1.04e-171 ## # ... with 7,547 more rows There were 7,557 significantly differentially expressed genes associated with this comparison, which includes both up- and down-regulated genes (examine the signs of the log2FoldChange column). A common diagnostic plot of differenial expression results is the so-called volcano plot which plots the log2FoldChange on the x-axis against -log10 adjusted p-value on the y-axis: mutate( p0_vs_Ad_de, `-log10(adjusted p)`=-log10(padj), `FDR&lt;0.05`=padj&lt;0.05 ) %&gt;% ggplot(aes(x=log2FoldChange,y=`-log10(adjusted p)`,color=`FDR&lt;0.05`)) + geom_point() The plot shows that there are significant genes at FDR &lt; 0.05 that are increased and decreased in the comparison. DESeq2 has many capabilities beyond this basic differential expression functionality as described in its comprehensive documentation in its vignette. As mentioned, edgeR implements a very similar model to DESeq2 and also has excellent documentation. 8.7.9.2 limma/voom While the limma package was initially developed for microarray analysis, the authors added support for RNASeq soon after the HTS technique became available. limma accommodates count data by first performing a Count Transformation using its own voom transformation procedure. voom transforms counts by first performing a counts per million normalization, taking the logarithm of the CPM values, and finally estimates the mean-variance relationship across genes for use in its empirical Bayes statistical framework. The counts data being thus transformed, limma performs statistical inference using its usual linear model framework to produce differential expression results with arbitrarily complex statistical models. This brief example is from the limma User Guide chapter 15, and covers loading and processing data from an RNA-seq experiment. We go into more depth while working with limma in assignment 6. Without going into too much detail, the design variable is how we inform limma of our experimental conditions, and where limma draws from to construct its linear models correctly. This design is relatively simple, just four samples belonging to two different conditions (the swirls here refer to the swirl of zebra fish, you can just see them as a phenotypic difference) design &lt;- data.frame(swirl = c(&quot;swirl.1&quot;, &quot;swirl.2&quot;, &quot;swirl.3&quot;, &quot;swirl.4&quot;), condition = c(1, -1, 1, -1)) dge &lt;- DGEList(counts=counts) keep &lt;- filterByExpr(dge, design) dge &lt;- dge[keep,,keep.lib.sizes=FALSE] DGEList() is a function from edgeR, of which limma borrows some loading and filtering functions. This experiment filters by expression level, and uses square bracket notation ([]) to reduce the number of rows. Finally, the expression data is transformed into CPM, counts per million, and a linear model is applied to the data with lmFit(). topTable() is used to view the most differentially expressed data. # limma trend logCPM &lt;- cpm(dge, log=TRUE, prior.count=3) fit &lt;- lmFit(logCPM, design) fit &lt;- eBayes(fit, trend=TRUE) topTable(fit, coef=ncol(design)) References "],["gene-set-enrichment-analysis.html", "8.8 Gene Set Enrichment Analysis", " 8.8 Gene Set Enrichment Analysis With the constant evolution of high-throughput sequencing (HTS) technologies, the size and dimensionality of data generated has been ever increasing. The question of interest has shifted from how do we generate the data to how do we make meaningful biological interpretations on a genome-wide level. The simplest and most common output of HTS experiments is a list of interesting genes. In the specific case of differential gene expression analysis, it is possible and quite common to obtain hundreds or even thousands of differentially expressed genes in a single experiment that may be directly or indirectly related to the phenotype of interest. While it can be helpful and fruitful to research these genes individually, this form of personal inspection is limited by ones domain knowledge and by the size of the results. On a biological level, it is complicated by the fact that most processes are often coordinated by the actions of many genes/gene products working in concert. Cellular signaling pathways, phenotypic differences or responses to various stimuli are typically associated with changes in the expression pattern of many genes that share common biological functions or regulation. Gene set enrichment analysis is an umbrella term for methods designed to analyze expression data and capture changes in higher-level biological processes and pathways by organizing genes into biologically relevant groups or gene sets. We will discuss the background of two common gene set enrichment analyses (over-representation analysis and rank-based Gene Set Enrichment Analysis), their advantages and disadvantages, and walk-through an example of how they can each be implemented in R. Before this, we will briefly touch upon the definition of a gene set and describe how to construct or obtain gene sets. 8.8.1 Gene Sets Gene sets are curated lists of genes that are grouped together by some logical association or pre-existing domain knowledge. Their primary use is to facilitate the biological interpretation of expression data by capturing high-level interactions between biologically relevant groups of genes. These sets are highly flexible and may be constructed based on any a priori knowledge or classification. For example, one could define a gene set that includes all genes that are members of the same biochemical pathway or a gene set that consists of all genes that are induced upon treatment with a particular pharmacological agent. While it is perfectly valid to construct new gene sets, there are many existing and curated collections of gene sets that are maintained and contributed to by communities of scientists throughout the world. Below, we will highlight some of the major collections that are commonly used and cited in published work: KEGG Pathways The Kyoto Encyclopedia of Genes and Genomes (KEGG) is a repository for biological pathway information that the authors describe as a means to computerize functional interpretations as part of the pathway reconstruction process based on the hierarchically structured knowledge about the genomic, chemical, and network spaces. The KEGG Pathways database consists of maps displaying the functional and regulatory relationships of genes within various metabolic and cell signaling pathways. The KEGG offers a web service that allows for the extraction of the genes and other information belonging to specific pathways. GO Annotations The Gene Ontology provides a network representation of biological systems from the molecular-level to the organismal level by defining a graph based representation of connected terms using a controlled vocabulary. At a high level, GO annotations consist of three broad ontologies termed molecular function, cellular component, and biological process. The molecular function is the specific activity a gene product performs on a molecular level (i.e. a protein kinase would be annotated with protein kinase activity). The biological process defines the higher-level programs and pathways thats accomplished by the activities of the gene product (i.e. if we take our previous example of a protein kinase, its biological process might be assigned as signal transduction). The cellular component refers to the localization of the gene product within the cell (i.e. the cellular component of a protein kinase might be cytosol). A gene product can be annotated with zero or more terms from each ontology, and these annotations are based on multiple levels of evidence from published work. The GO annotations may be accessed or downloaded directly from their own website, and there exist a number of web services that use GO annotations in the background (DAVID, enrichR, etc.) Molecular Signatures Database The Molecular Signatures Database (MSigDB) is a collection of gene sets curated, maintained and provided by the Broad Institute and UC San Diego. Although intended and designed for specific use with the Gene Set Enrichment Analysis Methodology, they are freely available and only require proper attribution for other uses. The MSigDB consists of 9 major collections of gene sets: H (hallmark gene sets), C1 (positional gene sets), C2 (curated gene sets), c3 (regulatory target gene sets), c4 (computational gene sets), c5 (ontology gene sets), c6 (oncogenic signature gene sets), c7 (immunologic signature gene sets), c8 (cell type signature gene sets). They are available in formats ready for use in the GSEA methodology and other formats that are easily imported into various settings for custom use. The gene sets as well as the GSEA methodology are available directly from their website. There are a number of R-specific packages that have been developed for working directly with these gene sets such as GSEABase or fgsea, which we will discuss later. 8.8.2 Working with gene sets in R We will walk through two quick examples of how to read in an example collection of gene sets. We will utilize the Hallmarks gene set collection, downloaded directly from the MSigDB website, which consists of 50 gene sets representing well-defined biological processes and generated by aggregating together many pre-existing gene sets. The MSigDB provides these gene set collections in the GMT format. These files are tab-delimited and each row in the format represents one gene set with the first column being the name of the gene set, and the second column a short description. The remaining columns each represent a gene and unequal lengths of columns per row is allowed. As is, the GMT format is designed to work specifically with the GSEA methodology developed and provided by the Broad Institute. However, we will also show two ways to manually parse these gene collections for exploration and further use in R. The first way to parse these gene sets would be to use various tidyverse functions that you should be familiar with already to construct a tibble. Essentially, we read the file, rename the first two columns for convenience, and use a combination of pivot_longer() and group_by() to quickly access genes by pathway. Below, we have the results of these operations and used them to display a summary of the number of genes in all the pathways contained with the hallmark pathways gene collection. read_delim(&#39;h.all.v7.5.1.symbols.gmt&#39;, delim=&#39;\\t&#39;, col_names=FALSE) %&gt;% dplyr::rename(Pathway = X1, Desc=X2) %&gt;% dplyr::select(-Desc) %&gt;% pivot_longer(!Pathway, values_to=&#39;genes&#39;, values_drop_na=TRUE, names_to = NULL) %&gt;% group_by(Pathway) %&gt;% summarise(n=n()) ## # A tibble: 50 x 2 ## Pathway n ## &lt;chr&gt; &lt;int&gt; ## 1 HALLMARK_ADIPOGENESIS 200 ## 2 HALLMARK_ALLOGRAFT_REJECTION 200 ## 3 HALLMARK_ANDROGEN_RESPONSE 100 ## 4 HALLMARK_ANGIOGENESIS 36 ## 5 HALLMARK_APICAL_JUNCTION 200 ## 6 HALLMARK_APICAL_SURFACE 44 ## 7 HALLMARK_APOPTOSIS 161 ## 8 HALLMARK_BILE_ACID_METABOLISM 112 ## 9 HALLMARK_CHOLESTEROL_HOMEOSTASIS 74 ## 10 HALLMARK_COAGULATION 138 ## # ... with 40 more rows We can see that there are 50 total pathways in the hallmarks gene collection that contain varying numbers of genes. We discarded the description column and if we were to save the results after using pivot_longer() instead of piping them to group_by and summarise(), we would have a tibble in the long format with each row representing a single pathway and single gene. However, there do exist various packages that have been developed to specifically handle gene set data in R such as the previously mentioned GSEABase. This is a collection of functions and class-based objects that facilitate working with gene sets. The foundation of the GSEABase package is the GeneSet and GeneSetCollection classes which store gene sets and metadata or a collection of GeneSet objects, respectively. We will use the GSEABase package to read in the collection of gene sets we previously downloaded. library(&#39;GSEABase&#39;) hallmarks_gmt &lt;- getGmt(con=&#39;h.all.v7.5.1.symbols.gmt&#39;) hallmarks_gmt ## GeneSetCollection ## names: HALLMARK_TNFA_SIGNALING_VIA_NFKB, HALLMARK_HYPOXIA, ..., HALLMARK_PANCREAS_BETA_CELLS (50 total) ## unique identifiers: JUNB, CXCL2, ..., SRP14 (4383 total) ## types in collection: ## geneIdType: NullIdentifier (1 total) ## collectionType: NullCollection (1 total) If we simply access the hallmarks_gmt variable, we can see that it is a GeneSetCollection object containing 50 total gene sets which encompass 4383 unique identifiers or HGNC symbols. Although this package supports a range of functions, we will focus on the basics. For a more thorough description of the classes and methods, please read their extended documentation available here. The geneIds method will return a list with each pathway as a named vector of associated gene ids: head(geneIds(hallmarks_gmt), 2) ## $HALLMARK_TNFA_SIGNALING_VIA_NFKB ## [1] &quot;JUNB&quot; &quot;CXCL2&quot; &quot;ATF3&quot; &quot;NFKBIA&quot; &quot;TNFAIP3&quot; &quot;PTGS2&quot; ## [7] &quot;CXCL1&quot; &quot;IER3&quot; &quot;CD83&quot; &quot;CCL20&quot; &quot;CXCL3&quot; &quot;MAFF&quot; ## [13] &quot;NFKB2&quot; &quot;TNFAIP2&quot; &quot;HBEGF&quot; &quot;KLF6&quot; &quot;BIRC3&quot; &quot;PLAUR&quot; ## [19] &quot;ZFP36&quot; &quot;ICAM1&quot; &quot;JUN&quot; &quot;EGR3&quot; &quot;IL1B&quot; &quot;BCL2A1&quot; ## [25] &quot;PPP1R15A&quot; &quot;ZC3H12A&quot; &quot;SOD2&quot; &quot;NR4A2&quot; &quot;IL1A&quot; &quot;RELB&quot; ## [31] &quot;TRAF1&quot; &quot;BTG2&quot; &quot;DUSP1&quot; &quot;MAP3K8&quot; &quot;ETS2&quot; &quot;F3&quot; ## [37] &quot;SDC4&quot; &quot;EGR1&quot; &quot;IL6&quot; &quot;TNF&quot; &quot;KDM6B&quot; &quot;NFKB1&quot; ## [43] &quot;LIF&quot; &quot;PTX3&quot; &quot;FOSL1&quot; &quot;NR4A1&quot; &quot;JAG1&quot; &quot;CCL4&quot; ## [49] &quot;GCH1&quot; &quot;CCL2&quot; &quot;RCAN1&quot; &quot;DUSP2&quot; &quot;EHD1&quot; &quot;IER2&quot; ## [55] &quot;REL&quot; &quot;CFLAR&quot; &quot;RIPK2&quot; &quot;NFKBIE&quot; &quot;NR4A3&quot; &quot;PHLDA1&quot; ## [61] &quot;IER5&quot; &quot;TNFSF9&quot; &quot;GEM&quot; &quot;GADD45A&quot; &quot;CXCL10&quot; &quot;PLK2&quot; ## [67] &quot;BHLHE40&quot; &quot;EGR2&quot; &quot;SOCS3&quot; &quot;SLC2A6&quot; &quot;PTGER4&quot; &quot;DUSP5&quot; ## [73] &quot;SERPINB2&quot; &quot;NFIL3&quot; &quot;SERPINE1&quot; &quot;TRIB1&quot; &quot;TIPARP&quot; &quot;RELA&quot; ## [79] &quot;BIRC2&quot; &quot;CXCL6&quot; &quot;LITAF&quot; &quot;TNFAIP6&quot; &quot;CD44&quot; &quot;INHBA&quot; ## [85] &quot;PLAU&quot; &quot;MYC&quot; &quot;TNFRSF9&quot; &quot;SGK1&quot; &quot;TNIP1&quot; &quot;NAMPT&quot; ## [91] &quot;FOSL2&quot; &quot;PNRC1&quot; &quot;ID2&quot; &quot;CD69&quot; &quot;IL7R&quot; &quot;EFNA1&quot; ## [97] &quot;PHLDA2&quot; &quot;PFKFB3&quot; &quot;CCL5&quot; &quot;YRDC&quot; &quot;IFNGR2&quot; &quot;SQSTM1&quot; ## [103] &quot;BTG3&quot; &quot;GADD45B&quot; &quot;KYNU&quot; &quot;G0S2&quot; &quot;BTG1&quot; &quot;MCL1&quot; ## [109] &quot;VEGFA&quot; &quot;MAP2K3&quot; &quot;CDKN1A&quot; &quot;CCN1&quot; &quot;TANK&quot; &quot;IFIT2&quot; ## [115] &quot;IL18&quot; &quot;TUBB2A&quot; &quot;IRF1&quot; &quot;FOS&quot; &quot;OLR1&quot; &quot;RHOB&quot; ## [121] &quot;AREG&quot; &quot;NINJ1&quot; &quot;ZBTB10&quot; &quot;PLPP3&quot; &quot;KLF4&quot; &quot;CXCL11&quot; ## [127] &quot;SAT1&quot; &quot;CSF1&quot; &quot;GPR183&quot; &quot;PMEPA1&quot; &quot;PTPRE&quot; &quot;TLR2&quot; ## [133] &quot;ACKR3&quot; &quot;KLF10&quot; &quot;MARCKS&quot; &quot;LAMB3&quot; &quot;CEBPB&quot; &quot;TRIP10&quot; ## [139] &quot;F2RL1&quot; &quot;KLF9&quot; &quot;LDLR&quot; &quot;TGIF1&quot; &quot;RNF19B&quot; &quot;DRAM1&quot; ## [145] &quot;B4GALT1&quot; &quot;DNAJB4&quot; &quot;CSF2&quot; &quot;PDE4B&quot; &quot;SNN&quot; &quot;PLEK&quot; ## [151] &quot;STAT5A&quot; &quot;DENND5A&quot; &quot;CCND1&quot; &quot;DDX58&quot; &quot;SPHK1&quot; &quot;CD80&quot; ## [157] &quot;TNFAIP8&quot; &quot;CCNL1&quot; &quot;FUT4&quot; &quot;CCRL2&quot; &quot;SPSB1&quot; &quot;TSC22D1&quot; ## [163] &quot;B4GALT5&quot; &quot;SIK1&quot; &quot;CLCF1&quot; &quot;NFE2L2&quot; &quot;FOSB&quot; &quot;PER1&quot; ## [169] &quot;NFAT5&quot; &quot;ATP2B1&quot; &quot;IL12B&quot; &quot;IL6ST&quot; &quot;SLC16A6&quot; &quot;ABCA1&quot; ## [175] &quot;HES1&quot; &quot;BCL6&quot; &quot;IRS2&quot; &quot;SLC2A3&quot; &quot;CEBPD&quot; &quot;IL23A&quot; ## [181] &quot;SMAD3&quot; &quot;TAP1&quot; &quot;MSC&quot; &quot;IFIH1&quot; &quot;IL15RA&quot; &quot;TNIP2&quot; ## [187] &quot;BCL3&quot; &quot;PANX1&quot; &quot;FJX1&quot; &quot;EDN1&quot; &quot;EIF1&quot; &quot;BMP2&quot; ## [193] &quot;DUSP4&quot; &quot;PDLIM5&quot; &quot;ICOSLG&quot; &quot;GFPT2&quot; &quot;KLF2&quot; &quot;TNC&quot; ## [199] &quot;SERPINB8&quot; &quot;MXD1&quot; ## ## $HALLMARK_HYPOXIA ## [1] &quot;PGK1&quot; &quot;PDK1&quot; &quot;GBE1&quot; &quot;PFKL&quot; &quot;ALDOA&quot; &quot;ENO2&quot; ## [7] &quot;PGM1&quot; &quot;NDRG1&quot; &quot;HK2&quot; &quot;ALDOC&quot; &quot;GPI&quot; &quot;MXI1&quot; ## [13] &quot;SLC2A1&quot; &quot;P4HA1&quot; &quot;ADM&quot; &quot;P4HA2&quot; &quot;ENO1&quot; &quot;PFKP&quot; ## [19] &quot;AK4&quot; &quot;FAM162A&quot; &quot;PFKFB3&quot; &quot;VEGFA&quot; &quot;BNIP3L&quot; &quot;TPI1&quot; ## [25] &quot;ERO1A&quot; &quot;KDM3A&quot; &quot;CCNG2&quot; &quot;LDHA&quot; &quot;GYS1&quot; &quot;GAPDH&quot; ## [31] &quot;BHLHE40&quot; &quot;ANGPTL4&quot; &quot;JUN&quot; &quot;SERPINE1&quot; &quot;LOX&quot; &quot;GCK&quot; ## [37] &quot;PPFIA4&quot; &quot;MAFF&quot; &quot;DDIT4&quot; &quot;SLC2A3&quot; &quot;IGFBP3&quot; &quot;NFIL3&quot; ## [43] &quot;FOS&quot; &quot;RBPJ&quot; &quot;HK1&quot; &quot;CITED2&quot; &quot;ISG20&quot; &quot;GALK1&quot; ## [49] &quot;WSB1&quot; &quot;PYGM&quot; &quot;STC1&quot; &quot;ZNF292&quot; &quot;BTG1&quot; &quot;PLIN2&quot; ## [55] &quot;CSRP2&quot; &quot;VLDLR&quot; &quot;JMJD6&quot; &quot;EXT1&quot; &quot;F3&quot; &quot;PDK3&quot; ## [61] &quot;ANKZF1&quot; &quot;UGP2&quot; &quot;ALDOB&quot; &quot;STC2&quot; &quot;ERRFI1&quot; &quot;ENO3&quot; ## [67] &quot;PNRC1&quot; &quot;HMOX1&quot; &quot;PGF&quot; &quot;GAPDHS&quot; &quot;CHST2&quot; &quot;TMEM45A&quot; ## [73] &quot;BCAN&quot; &quot;ATF3&quot; &quot;CAV1&quot; &quot;AMPD3&quot; &quot;GPC3&quot; &quot;NDST1&quot; ## [79] &quot;IRS2&quot; &quot;SAP30&quot; &quot;GAA&quot; &quot;SDC4&quot; &quot;STBD1&quot; &quot;IER3&quot; ## [85] &quot;PKLR&quot; &quot;IGFBP1&quot; &quot;PLAUR&quot; &quot;CAVIN3&quot; &quot;CCN5&quot; &quot;LARGE1&quot; ## [91] &quot;NOCT&quot; &quot;S100A4&quot; &quot;RRAGD&quot; &quot;ZFP36&quot; &quot;EGFR&quot; &quot;EDN2&quot; ## [97] &quot;IDS&quot; &quot;CDKN1A&quot; &quot;RORA&quot; &quot;DUSP1&quot; &quot;MIF&quot; &quot;PPP1R3C&quot; ## [103] &quot;DPYSL4&quot; &quot;KDELR3&quot; &quot;DTNA&quot; &quot;ADORA2B&quot; &quot;HS3ST1&quot; &quot;CAVIN1&quot; ## [109] &quot;NR3C1&quot; &quot;KLF6&quot; &quot;GPC4&quot; &quot;CCN1&quot; &quot;TNFAIP3&quot; &quot;CA12&quot; ## [115] &quot;HEXA&quot; &quot;BGN&quot; &quot;PPP1R15A&quot; &quot;PGM2&quot; &quot;PIM1&quot; &quot;PRDX5&quot; ## [121] &quot;NAGK&quot; &quot;CDKN1B&quot; &quot;BRS3&quot; &quot;TKTL1&quot; &quot;MT1E&quot; &quot;ATP7A&quot; ## [127] &quot;MT2A&quot; &quot;SDC3&quot; &quot;TIPARP&quot; &quot;PKP1&quot; &quot;ANXA2&quot; &quot;PGAM2&quot; ## [133] &quot;DDIT3&quot; &quot;PRKCA&quot; &quot;SLC37A4&quot; &quot;CXCR4&quot; &quot;EFNA3&quot; &quot;CP&quot; ## [139] &quot;KLF7&quot; &quot;CCN2&quot; &quot;CHST3&quot; &quot;TPD52&quot; &quot;LXN&quot; &quot;B4GALNT2&quot; ## [145] &quot;PPARGC1A&quot; &quot;BCL2&quot; &quot;GCNT2&quot; &quot;HAS1&quot; &quot;KLHL24&quot; &quot;SCARB1&quot; ## [151] &quot;SLC25A1&quot; &quot;SDC2&quot; &quot;CASP6&quot; &quot;VHL&quot; &quot;FOXO3&quot; &quot;PDGFB&quot; ## [157] &quot;B3GALT6&quot; &quot;SLC2A5&quot; &quot;SRPX&quot; &quot;EFNA1&quot; &quot;GLRX&quot; &quot;ACKR3&quot; ## [163] &quot;PAM&quot; &quot;TGFBI&quot; &quot;DCN&quot; &quot;SIAH2&quot; &quot;PLAC8&quot; &quot;FBP1&quot; ## [169] &quot;TPST2&quot; &quot;PHKG1&quot; &quot;MYH9&quot; &quot;CDKN1C&quot; &quot;GRHPR&quot; &quot;PCK1&quot; ## [175] &quot;INHA&quot; &quot;HSPA5&quot; &quot;NDST2&quot; &quot;NEDD4L&quot; &quot;TPBG&quot; &quot;XPNPEP1&quot; ## [181] &quot;IL6&quot; &quot;SLC6A6&quot; &quot;MAP3K1&quot; &quot;LDHC&quot; &quot;AKAP12&quot; &quot;TES&quot; ## [187] &quot;KIF5A&quot; &quot;LALBA&quot; &quot;COL5A1&quot; &quot;GPC1&quot; &quot;HDLBP&quot; &quot;ILVBL&quot; ## [193] &quot;NCAN&quot; &quot;TGM2&quot; &quot;ETS1&quot; &quot;HOXB9&quot; &quot;SELENBP1&quot; &quot;FOSL2&quot; ## [199] &quot;SULT2B1&quot; &quot;TGFB3&quot; The names method will return all of the gene set names contained within a specific collection: head(names(hallmarks_gmt),2) ## [1] &quot;HALLMARK_TNFA_SIGNALING_VIA_NFKB&quot; &quot;HALLMARK_HYPOXIA&quot; We can access a specific gene set contained within this collection by referring to its name and using the following notation: hallmarks_gmt[[&#39;HALLMARK_ANGIOGENESIS&#39;]] ## setName: HALLMARK_ANGIOGENESIS ## geneIds: VCAN, POSTN, ..., CXCL6 (total: 36) ## geneIdType: Null ## collectionType: Null ## details: use &#39;details(object)&#39; Simply accessing the object will provide a high-level summary of the information contained within. To access a specific value of this GeneSet object, we would call one of the slots (a core concept in object-oriented programming). In our particular case, we could extract the gene names contained assigned to this GeneSet by calling the geneIds slot as shown below to return a vector of the gene names. We can see the first five below and also the length of the returned vector by using the base R length() function: head(hallmarks_gmt[[&#39;HALLMARK_ANGIOGENESIS&#39;]]@geneIds) ## [1] &quot;VCAN&quot; &quot;POSTN&quot; &quot;FSTL1&quot; &quot;LRPAP1&quot; &quot;STC1&quot; &quot;LPL&quot; length(hallmarks_gmt[[&#39;HALLMARK_ANGIOGENESIS&#39;]]@geneIds) ## [1] 36 GSEABase includes a number of built-in functions for reading gene sets in from various sources and performing common operations such as set intersections, set differences, and ID conversions. We will demonstrate the usage of some of these in the next section covering Over-representation analysis. 8.8.3 Over-representation Analysis One of the most common ways to utilize gene sets to evaluate gene expression results is to perform Over-representation Analysis (ORA). Let us assume that we have obtained a list of differentially expressed genes from an experiment. We are curious to know if within this list of differentially expressed genes, do we see an over-representation or enrichment of genes belonging to gene sets of interest? In more general terms, the goal of ORA is to determine how likely it is that there is a non-random association between a gene being differentially expressed and having membership in a chosen (and ideally relevant) gene set. In R, we can do a simple ORA by utilizing a Fishers exact test and a contingency table. For a purely hypothetical example, let us assume that we have performed differential gene expression analysis between two different cell lines. We obtain a list of 10,000 total genes (our background) discovered in the experiment and find that at our chosen statistical threshold, 1,000 of these are differentially expressed. To keep things simple, we will perform a single ORA test against the Hallmarks Angiogenesis gene set using a sample list of 1000 differentially expressed genes selected from data generated by Marisa et al. 2013. The Hallmarks Angiogenesis gene set consists of 36 genes and we find that 13 of these are also present in our list of differentially expressed genes. Please note that we are removing this list of genes from its original meaning and context found in the publication and simply using it to demonstrate the basic steps occurring during ORA. All of these numbers and lists were arbitrarily chosen and this experimental setup is purely hypothetical. Another issue to note is that typically the background list should represent the entire pool of genes from which any differentially expressed genes could have been selected. For expression experiments, it is typical to choose all of the detected genes (regardless of significance) as the background. The number of genes in the organisms genome could potentially also be an appropriate background Also, it is important to keep in mind that in reality, ORA is nearly always performed on a larger scale against a variety of different gene sets. This allows for the unbiased discovery of potentially novel and unexpected enrichment in other biological areas of interest. It also necessitates the need for multiple-testing correction, which we have discussed in multiple hypothesis testing. To begin, we would want to prepare a contingency table which describes the various overlaps between our sets of interest. For a 2x2 contingency table, these four values will be: Genes present in our list of differentially expressed genes and present in our gene set Genes present in our list of differentially expressed genes and not present in our gene set Genes not present in our list of differentially expressed genes and present in our gene set Genes not present in our list of differentially expressed genes and not present in our gene set To demonstrate what this would look like, we have manually constructed a contingency table with labels and totals added below. If you look at the margins of the table and recall the previously given values above, you can reconstruct the logic used to generate each of the values in all the cells. Differentially Expressed Not Differentially Expressed Total In Gene Set 13 23 36 Not in Gene Set 987 8977 9964 Total 1000 9000 10000 For the purposes of this example, we are reading in our differentially expressed genes from an external file, but this vector could be generated in any number of ways depending upon where and how your results are stored. Following good coding practices, we will write a small function that takes this list of DE genes and a GeneSet object to programmatically generate a contingency table: There are many ways to construct a contingency table. This is just one way that was chosen to make calculations of the values contained within the table transparent and easy to understand. #load and read our list of DE genes contained within a newline delimited txt file de_genes &lt;- scan(&#39;example_de_list.txt&#39;, what=character(), sep=&#39;\\n&#39;) #define a function that takes a list of DE genes, and a specific GeneSet from a GeneSetCollection to generate a contingency table #using set operations in GSEABase make_contingency &lt;- function(de_list, GeneSetobj) { #make our de list into a simple GeneSet object using GSEABase de_geneset &lt;- GeneSet(de_list, setName=&#39;1000 DE genes&#39;) #If we had the full results, we could determine this value without manually setting it background_len &lt;- 10000 #Calculate the values inside the contingency table using set operations de_in &lt;- length((de_geneset &amp; GeneSetobj)@geneIds) de_notin &lt;- length(setdiff(de_geneset, GeneSetobj)@geneIds) set_notde &lt;- length(setdiff(GeneSetobj, de_geneset)@geneIds) notin_notde &lt;- background_len - (de_in + de_notin + set_notde) #return a matrix of the contingency values return(matrix(c(de_in, de_notin, set_notde, notin_notde), nrow=2)) } contingency_table &lt;- make_contingency(de_genes, hallmarks_gmt[[&#39;HALLMARK_ANGIOGENESIS&#39;]]) contingency_table ## [,1] [,2] ## [1,] 13 23 ## [2,] 987 8977 We perform the Fishers Exact test using the built-in R function fisher.test() and view the summarized output by simply calling the variable where we stored the test results: fisher_results &lt;- fisher.test(contingency_table, alternative=&#39;greater&#39;) fisher_results ## ## Fisher&#39;s Exact Test for Count Data ## ## data: contingency_table ## p-value = 2.382e-05 ## alternative hypothesis: true odds ratio is greater than 1 ## 95 percent confidence interval: ## 2.685749 Inf ## sample estimates: ## odds ratio ## 5.139308 Specific values of the results can be accessed by the $notation (i.e. fisher_results$p.value). The full list of returned values may be found in the R documentation for fisher.test() Back to our hypothetical example and focusing on the p-value returned of r fisher_results$p.value, we can interpret this as the probability of randomly obtaining results as or more extreme than what we observed assuming the null hypothesis that there is no association between differential expression and gene set membership is true. Based on these results and if this p-value was below our pre-determined statistical threshold, we could make the conclusion that there is an enrichment or over-representation of our differentially expressed genes from this experiment in the Hallmark Angiogenesis gene set. Relating this back to the experiment, we might hypothesize that the differences between our two cell lines might be driving gene expression changes that result in alterations in genes involved in angiogenesis. This might motivate potential further in vitro experiments on these cell lines, including migration and proliferation assays, that could reveal if this enrichment of angiogenesis genes is reflected at a phenotypic or functional level. ORA is a quick and useful way to generate further hypotheses to investigate specific mechanisms of action or regulation. For example, after identifying a gene set as being enriched or over-represented, one could further test the specific genes in the set by examining their directionality of change or asking if dependent pathways/networks are also perturbed. One major limitation of ORA is that it relies on the choice of arbitrary statistical thresholds to define interesting or differentially expressed genes. To reiterate again, p-value thresholds hold no inherent biological meaning and are subjectively determined. Changing the p-value threshold may result in dramatic differences in the outcome of ORA. Additionally, expression datasets may measure tens of thousands of genes in a single experiment, and filtering by a p-value threshold may discard potentially useful information. ORA (though often modified with slightly different statistical methodologies) is implemented in a number of different R packages such as topGO or various web services including DAVID and enrichR 8.8.4 Rank-based Analysis We will refer to the specific method developed by the Broad Institute and UC San Diego as Gene Set Enrichment Analysis (note the capitalization). This specific methodology should not be confused with gene set enrichment analysis, which we use as an umbrella term covering the many statistical methodologies used to analyze gene sets. Gene Set Enrichment Analysis (https://www.gsea-msigdb.org/gsea/index.jsp) was a method developed to facilitate the biological analysis of genome-wide expression experiments. It is a rank-based method which utilizes all of the information from an expression dataset instead of relying on pre-determined statistical thresholds. In the simplest case, GSEA organizes expression data into two classes, and ranks all discovered genes by a chosen metric correlating their expression to these classes. Then, for a pre-defined set of genes, GSEA tests whether the members of this particular gene set occur more frequently at the top or bottom of the ranked list or are randomly distributed throughout. This ranking can be done by a number of different measures, but common ones include signal-to-noise ratio, signed log p-value or log fold change estimates. These pre-defined gene sets are flexible and may be manually constructed or drawn from the many curated databases of gene sets. Behind the scenes, GSEA functions by descending through the ranked list and increasing a cumulative score when a gene is encountered thats contained within the chosen gene set and decreasing that score when it encounters a gene that is not within that gene set. This incrementing is weighted to put more emphasis on genes that are found at the extremes of the ranked list and the final score taken as the enrichment score (ES) is the maximum of the deviation from zero. From this score, a p-value is determined by permuting the phenotype labels to generate a null distribution to compare against. To account for multiple hypothesis correction, the ES is normalized by the size of the gene set to create a normalized enrichment score (NES) and the p-values are subjected to Benjamini-Hochberg correction to generate FDR values for each NES. To make this explanation more transparent, let us assume that we have performed a RNAseq experiment to determine which genes are changed when we knock out our gene of interest in a model cell line. We have obtained our results, which consists of a list of all genes discovered in the experiment and associated statistical measures such as p-value and log fold change estimates. To perform a basic GSEA, we would sort this list of genes, without respect to significance, by descending fold change to generate a ranked list where genes that are upregulated (positive fold change) are at the top of our list and genes that are downregulated (negative fold change) are at the bottom of our list. We would use this ranked list and any number or combination of gene sets to perform GSEA using one of the available means. GSEA is available as a java application with a graphical user interface as well as a command-line utility for use in HPC cluster environments. There also exist several packages in R that also implement the core GSEA algorithm along with various changes and refinements to the underlying statistical methodology. In the following section, we will refer to the usage of one such package, fgsea. Under most circumstances, GSEA can be run with default parameters but please see the official documentation for a list of customizable parameters and situations in which to adjust them. GSEA will return a list of results for gene sets tested consisting of their associated enrichment scores, and various statistical measures for a given ranked input With GSEA results, it is typical to set a permissive FDR threshold to consider which gene sets are significant. Further inspection of individual gene sets may be performed by analyzing the leading edge subset for each gene set. This is the group of genes that occur before and contribute to the maximum deviation from zero score for a given gene set during the analysis. These genes are typically interpreted as being the more relevant subset of genes that likely contribute the most to the enrichment signal, and are typically used in further investigations comparing leading edge subsets from other gene sets or applying domain knowledge to predict their regulatory effects and functions with respect to their annotated biological pathway or gene set. GSEA is a highly flexible method that incorporates information from all of the data generated in HTS experiments. The choice of ranking methods and gene sets can all be tailored to the specific experiment or question at hand, and it is a simple yet fast method that can detect more subtle changes in gene networks than ORA. The interpretation of a GSEA result needs to be made with care. Lets assume that we have performed GSEA using fold Change as our ranking metric and we find that the KEGG Glycolysis pathway is found to be enriched with a positive normalized enrichment score and is statistically significant. Alone, this does not necessarily imply that glycolysis is more active between the conditions of interest. It simply allows for the conclusion that there is an enrichment of genes belonging to this gene set among the genes with a positive log fold change in the experiment. This might imply that your experimental conditions are affecting some underlying transcriptional regulation that results in these genes being up-regulated. However, it could also be explained by feedback mechanisms, direct or indirect, from related biological pathways or gene networks. It does not allow you to directly conclude that the glycolysis pathway itself is more active at a functional level. To make that assertion, you would need to perform further in vitro or in vivo functional assays that directly measure the activity and outputs of glycolysis given your experimental conditions. It is important to remember that unless specified, gene sets will often contain a mix of genes with different regulatory activities including both activators and inhibitors of that pathway. In addition, for many biological pathways, there are other factors that influence its regulation that may not be apparent at a gene expression level. In the specific example of metabolic pathways, it is common for the enzymatic products of subsequent reactions to feedback and regulate the activity and flux through the pathway or connected pathways. This regulation is not easily discerned at the gene expression level alone. All of this is to say that the results from GSEA must be interpreted with care and integrated with other analyses and knowledge to gain a holistic understanding of its meaning. 8.8.5 fgsea To perform GSEA analysis in R, we will be using the fgsea (http://bioconductor.org/packages/release/bioc/html/fgsea.html) package. fgsea uses the core algorithm behind rank-based GSEA but with additional statistical methodologies meant to provide better estimations of small p-values. For this example, we have generated simulated and completely artificial data for the ranked list. We have chosen 1000 genes to represent all of the genes discovered in our experiment, annotated them with random values for their fold change and sorted the list in descending order. The top of our list corresponds to genes with a positive fold change and the bottom those with a negative fold change. For this hypothetical experiment, let us assume that genes that have a positive fold change are upregulated in our condition of interest vs. the reference and genes that have a negative fold change are downregulated in our condition of interest vs. the reference. The genes that served as our ranked list input were taken from an experiment looking at differences in cancer subtypes. Although our fold change values associated with them were generated randomly, there is likely a strong residual signal / bias towards various cancer pathways as we will see in our results below library(&#39;fgsea&#39;) fgsea takes as input, a named vector of gene-level statistics (a ranked list) and a named list of gene sets. The gene names in the ranked list must exactly match how they appear in the gene sets (see the note below for additional considerations to keep in mind). By default, the fgsea() function will run a pre-ranked GSEA meaning that you must generate the ranked list manually beforehand. Importantly, it will not check your ranked list input and will assume it is sorted numerically in descending order of your ranking metric. :::{.box .warning} Gene sets downloaded from the MSigDB are sets of human genes and are represented by either NCBI Entrez IDs or HGNC symbols. For convenience, we have been working with gene sets containing HGNC symbols. It is important to remember to check what identifier system and what species the gene set is provided as. This also becomes relevant when performing rank-based GSEA on data generated in non-human species. In these situations, you will need to convert the gene IDs between species and assuming youre using a pre-defined gene set, match the identifiers from your ranked list to the format found in the gene set. This issue typically becomes relevant when trying to use the MSigDB gene sets to compare against a ranked list of genes generated from HTS experiments in Mus musculus. MGI symbols typically begin with an uppercase letter and are followed by all lowercase letters or numbers. While there are examples of exact concordance where true orthologous mouse and human genes have matching MGI and HGNC symbols only differing in case (i.e. Gzmb and GZMB), you should not simply convert MGI symbols to all uppercase. This will work for the subset of genes where the orthologs have the same symbol but will misidentify many genes where they do not share the same base name. One way to properly perform this kind of orthology mapping is to use biomaRt, which we have discussed previously. ::: For this example, we have already made simulated vectors containing the gene names and their associated fold change values. We will generate the appropriate format using the setNames function on our two vectors and we can use both head and tail to quickly check the sorting. For non-simulated data, you could generate these vectors in a number of ways depending upon the format of your results data. These are the top 5 genes in our ranked list: rnk_list &lt;- setNames(rnks, de_genes) head(rnk_list) ## RBMS1P1 RBMS1 GAS1 SFRP2 CCDC80 MGP ## 9.965527 9.907958 9.888917 9.828172 9.826594 9.802787 And these are the bottom 5 genes in the list: tail(rnk_list) ## KLF7 ANKDD1A RIMKLB RGS19 PLPPR4 TENM4 ## -9.865186 -9.922858 -9.930453 -9.931198 -9.959923 -9.984908 For our gene sets, we will use the same Hallmarks gene set provided by MSigDB. There are two ways we can load these gene sets in the appropriate format expected. The first is to use one of the built-in functions in fgsea, gmtPathways, which will read directly from the GMT file. We have displayed the contents of a randomly selected gene set below: hallmark_pathways_fgsea &lt;- fgsea::gmtPathways(&#39;h.all.v7.5.1.symbols.gmt&#39;) hallmark_pathways_fgsea$HALLMARK_TGF_BETA_SIGNALING ## [1] &quot;TGFBR1&quot; &quot;SMAD7&quot; &quot;TGFB1&quot; &quot;SMURF2&quot; &quot;SMURF1&quot; &quot;BMPR2&quot; ## [7] &quot;SKIL&quot; &quot;SKI&quot; &quot;ACVR1&quot; &quot;PMEPA1&quot; &quot;NCOR2&quot; &quot;SERPINE1&quot; ## [13] &quot;JUNB&quot; &quot;SMAD1&quot; &quot;SMAD6&quot; &quot;PPP1R15A&quot; &quot;TGIF1&quot; &quot;FURIN&quot; ## [19] &quot;SMAD3&quot; &quot;FKBP1A&quot; &quot;MAP3K7&quot; &quot;BMPR1A&quot; &quot;CTNNB1&quot; &quot;HIPK2&quot; ## [25] &quot;KLF10&quot; &quot;BMP2&quot; &quot;ENG&quot; &quot;APC&quot; &quot;PPM1A&quot; &quot;XIAP&quot; ## [31] &quot;CDH1&quot; &quot;ID1&quot; &quot;LEFTY2&quot; &quot;CDKN1C&quot; &quot;TRIM33&quot; &quot;RAB31&quot; ## [37] &quot;TJP1&quot; &quot;SLC20A1&quot; &quot;CDK9&quot; &quot;ID3&quot; &quot;NOG&quot; &quot;ARID4B&quot; ## [43] &quot;IFNGR2&quot; &quot;ID2&quot; &quot;PPP1CA&quot; &quot;SPTBN1&quot; &quot;WWTR1&quot; &quot;BCAR3&quot; ## [49] &quot;THBS1&quot; &quot;FNTA&quot; &quot;HDAC1&quot; &quot;UBE2D3&quot; &quot;LTBP2&quot; &quot;RHOA&quot; If we had previously loaded these gene sets in using GSEABase, we could also simply do the following: hallmark_pathways_GSEABase &lt;- geneIds(hallmarks_gmt) hallmark_pathways_GSEABase$HALLMARK_TGF_BETA_SIGNALING ## [1] &quot;TGFBR1&quot; &quot;SMAD7&quot; &quot;TGFB1&quot; &quot;SMURF2&quot; &quot;SMURF1&quot; &quot;BMPR2&quot; ## [7] &quot;SKIL&quot; &quot;SKI&quot; &quot;ACVR1&quot; &quot;PMEPA1&quot; &quot;NCOR2&quot; &quot;SERPINE1&quot; ## [13] &quot;JUNB&quot; &quot;SMAD1&quot; &quot;SMAD6&quot; &quot;PPP1R15A&quot; &quot;TGIF1&quot; &quot;FURIN&quot; ## [19] &quot;SMAD3&quot; &quot;FKBP1A&quot; &quot;MAP3K7&quot; &quot;BMPR1A&quot; &quot;CTNNB1&quot; &quot;HIPK2&quot; ## [25] &quot;KLF10&quot; &quot;BMP2&quot; &quot;ENG&quot; &quot;APC&quot; &quot;PPM1A&quot; &quot;XIAP&quot; ## [31] &quot;CDH1&quot; &quot;ID1&quot; &quot;LEFTY2&quot; &quot;CDKN1C&quot; &quot;TRIM33&quot; &quot;RAB31&quot; ## [37] &quot;TJP1&quot; &quot;SLC20A1&quot; &quot;CDK9&quot; &quot;ID3&quot; &quot;NOG&quot; &quot;ARID4B&quot; ## [43] &quot;IFNGR2&quot; &quot;ID2&quot; &quot;PPP1CA&quot; &quot;SPTBN1&quot; &quot;WWTR1&quot; &quot;BCAR3&quot; ## [49] &quot;THBS1&quot; &quot;FNTA&quot; &quot;HDAC1&quot; &quot;UBE2D3&quot; &quot;LTBP2&quot; &quot;RHOA&quot; Now to run fgsea, we will use default parameters besides manually setting thresholds to ignore gene sets in the analysis based on minimum and maximum values for their size: fgsea_results &lt;- fgsea(hallmark_pathways_GSEABase, rnk_list, minSize = 15, maxSize=500) fgsea_results &lt;- fgsea_results %&gt;% as_tibble() As noted in the official documentation for the original GSEA, very small gene sets or very large gene sets may lead to issues with artificially high scores or poor normalization, respectively. These values can be adjusted as needed, but we have used the range suggested by the official documentation. We will convert the fgsea results into a tibble, and perform some basic exploration of our results. We can see below the results sorted by ascending padj and the pathways with the lowest adjusted p-values are largely associated with cancer pathways including Epithelial Mesenchymal Transition, and Apical Junctions. fgsea_results %&gt;% arrange(padj) ## # A tibble: 15 x 8 ## pathway pval padj log2err ES NES size leadingEdge ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;list&gt; ## 1 HALLMARK_EPITHELIA~ 4.86e-11 7.28e-10 0.851 0.509 2.60 107 &lt;chr [56]&gt; ## 2 HALLMARK_APICAL_JU~ 1.50e- 3 1.12e- 2 0.455 0.479 1.87 32 &lt;chr [16]&gt; ## 3 HALLMARK_MYOGENESIS 3.61e- 3 1.81e- 2 0.432 0.428 1.76 40 &lt;chr [24]&gt; ## 4 HALLMARK_COAGULATI~ 5.16e- 2 1.55e- 1 0.277 0.383 1.46 30 &lt;chr [13]&gt; ## 5 HALLMARK_COMPLEMENT 5.15e- 2 1.55e- 1 0.277 0.380 1.49 33 &lt;chr [11]&gt; ## 6 HALLMARK_KRAS_SIGN~ 1.08e- 1 2.37e- 1 0.188 0.329 1.30 35 &lt;chr [13]&gt; ## 7 HALLMARK_UV_RESPON~ 1.11e- 1 2.37e- 1 0.186 0.325 1.31 37 &lt;chr [17]&gt; ## 8 HALLMARK_APOPTOSIS 1.28e- 1 2.39e- 1 0.171 0.378 1.31 21 &lt;chr [10]&gt; ## 9 HALLMARK_HYPOXIA 2.24e- 1 3.74e- 1 0.124 0.298 1.16 31 &lt;chr [12]&gt; ## 10 HALLMARK_ADIPOGENE~ 2.73e- 1 4.09e- 1 0.111 0.329 1.13 20 &lt;chr [11]&gt; ## 11 HALLMARK_IL2_STAT5~ 5.71e- 1 7.14e- 1 0.0747 -0.253 -0.925 24 &lt;chr [6]&gt; ## 12 HALLMARK_INTERFERO~ 5.63e- 1 7.14e- 1 0.0749 -0.254 -0.924 23 &lt;chr [7]&gt; ## 13 HALLMARK_ALLOGRAFT~ 6.92e- 1 7.99e- 1 0.0646 -0.236 -0.857 23 &lt;chr [4]&gt; ## 14 HALLMARK_INFLAMMAT~ 9.10e- 1 9.72e- 1 0.0479 0.184 0.683 26 &lt;chr [7]&gt; ## 15 HALLMARK_TNFA_SIGN~ 9.72e- 1 9.72e- 1 0.0501 -0.171 -0.591 20 &lt;chr [4]&gt; To begin to explore these results, we could filter by our FDR threshold and subset significant gene sets by the direction of their NES (positive indicating an enrichment at the top of our list and negative indicating an enrichment at the bottom). top_positive_nes &lt;- fgsea_results %&gt;% filter(padj &lt; .25 &amp; NES &gt; 0) %&gt;% slice_max(NES, n=5) top_positive_nes ## # A tibble: 5 x 8 ## pathway pval padj log2err ES NES size leadingEdge ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;list&gt; ## 1 HALLMARK_EPITHELIAL_M~ 4.86e-11 7.28e-10 0.851 0.509 2.60 107 &lt;chr [56]&gt; ## 2 HALLMARK_APICAL_JUNCT~ 1.50e- 3 1.12e- 2 0.455 0.479 1.87 32 &lt;chr [16]&gt; ## 3 HALLMARK_MYOGENESIS 3.61e- 3 1.81e- 2 0.432 0.428 1.76 40 &lt;chr [24]&gt; ## 4 HALLMARK_COMPLEMENT 5.15e- 2 1.55e- 1 0.277 0.380 1.49 33 &lt;chr [11]&gt; ## 5 HALLMARK_COAGULATION 5.16e- 2 1.55e- 1 0.277 0.383 1.46 30 &lt;chr [13]&gt; To provide a basic visualization of these results, we can display the normalized enrichment scores for all of our pathways in a bar chart and fill the bars by whether or not they meet our padj threshold. Rank-based GSEA is often used as a hypothesis-generating experiment as it can quickly capture potentially interesting global patterns of regulation among many biological pathways and processes while considering all the information generated in a HTS experiment. It is typical to set a permissive FDR for considering significant gene sets. The results of rank-based GSEA need to be further inspected and most often confirmed through complementary or follow-up experiments. Thus, we will choose to use a relaxed FDR threshold of &lt; .25. fgsea_results %&gt;% mutate(pathway = forcats::fct_reorder(pathway, NES)) %&gt;% ggplot() + geom_bar(aes(x=pathway, y=NES, fill = padj &lt; .25), stat=&#39;identity&#39;) + scale_fill_manual(values = c(&#39;TRUE&#39; = &#39;red&#39;, &#39;FALSE&#39; = &#39;blue&#39;)) + theme_minimal() + ggtitle(&#39;fgsea results for Hallmark MSigDB gene sets&#39;) + ylab(&#39;Normalized Enrichment Score (NES)&#39;) + xlab(&#39;&#39;) + coord_flip() (#fig:top level results plot)GSEA results suggest an enrichment of cancer-related pathways amongst upregulated genes. GSEA was performed using a list of genes generated through RNAseq and ranked by descending fold change. GSEA was run using default parameters with minSize = 15 and maxSize = 500. Gene sets with a Benjamini-Hochberg adjusted p-value &lt; .25 are considered significant. Given the artificial nature of our data and our choice of a small gene set collection, we have very few results and are able to display all of them intelligibly on one plot. In real experiments, there may be several hundred significant gene sets of interest. In cases like this, you may need to restrict the number of gene sets by taking the top ten ranked by NES (both positive and negative) and plotting those in a figure like the one above. This is only a suggestion and there are many ways to choose interesting gene sets to plot together depending upon the experimental context and future questions of interest. If we wished to investigate a single gene set from this list of results, we could create an enrichment plot displaying the Enrichment Score as its calculated through the ranked list. For convenience, we have wrapped the pre-built function, plotEnrichment from fgsea, in a user-defined function that allows us to specify a specific pathway. The plotEnrichment function expects at minimum the list of genes contained within the pathway to plot and the input ranked list of gene-level statistics. We have displayed the enrichment plot for one of the significantly enriched gene sets below: make_gsea_plot &lt;- function(pathway_name, rnks) { plotEnrichment(hallmark_pathways_fgsea[[pathway_name]], rnks) + labs(title=pathway_name) } make_gsea_plot(&#39;HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION&#39;, rnk_list) (#fig:enrichment plot)The Hallmarks EPITHELIAL_MESENCHYMAL_TRANSITION Pathway is significantly enriched amongst the upregulated genes. There are 56 genes in the leading edge subset including GAS1, POSTN, and LOX. The Normalized Enrichment Score calculated for this gene set was 2.59 and it has an adjusted p-value of 7.28e-10 As mentioned in the prior section, it is often useful for further investigation to inspect the leading edge subset of genes contained within interesting gene sets. fgsea stores these genes in the form of a list associated with each pathway in the returned table. We could convert and save these values in a named list for convenience. This can be accomplished by making use of the deframe() function. Below we have saved all of the genes contained within the leading edge subset of significant gene sets, and displayed those belonging to the HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION gene set as determined in this experiment: gather_leadingedge &lt;- function(results, threshold) { genes &lt;- results %&gt;% filter(padj &lt; threshold) %&gt;% dplyr::select(pathway, leadingEdge) %&gt;% deframe() return(genes) } leading_edge_genes &lt;- gather_leadingedge(fgsea_results, .25) leading_edge_genes$HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION ## [1] &quot;GAS1&quot; &quot;MGP&quot; &quot;SPOCK1&quot; &quot;EFEMP2&quot; &quot;FERMT2&quot; &quot;FBN1&quot; &quot;TAGLN&quot; &quot;FSTL1&quot; ## [9] &quot;TIMP3&quot; &quot;VIM&quot; &quot;VCAN&quot; &quot;THBS2&quot; &quot;ACTA2&quot; &quot;DCN&quot; &quot;BGN&quot; &quot;NNMT&quot; ## [17] &quot;CALD1&quot; &quot;NTM&quot; &quot;FBLN1&quot; &quot;SPARC&quot; &quot;COL5A2&quot; &quot;MYL9&quot; &quot;SLIT2&quot; &quot;DPYSL3&quot; ## [25] &quot;VEGFC&quot; &quot;COL3A1&quot; &quot;LGALS1&quot; &quot;THY1&quot; &quot;PCOLCE&quot; &quot;ADAM12&quot; &quot;SFRP4&quot; &quot;WIPF1&quot; ## [33] &quot;COL6A3&quot; &quot;CTHRC1&quot; &quot;TIMP1&quot; &quot;TPM2&quot; &quot;FN1&quot; &quot;COL5A1&quot; &quot;PMP22&quot; &quot;MMP2&quot; ## [41] &quot;HTRA1&quot; &quot;COL1A2&quot; &quot;POSTN&quot; &quot;PDGFRB&quot; &quot;EMP3&quot; &quot;MXRA5&quot; &quot;FLNA&quot; &quot;PRRX1&quot; ## [49] &quot;FAP&quot; &quot;LOX&quot; &quot;BASP1&quot; &quot;GREM1&quot; &quot;CCN2&quot; &quot;CDH11&quot; &quot;LUM&quot; &quot;LAMC1&quot; This named list contains all of the leading edge genes for gene sets meeting a certain padj threshold. We could import these as a GSEABase GeneSetCollection object for further analyses in R or write them to a file to pass on to collaborators. These examples are just some of the ways you can utilize and explore the GSEA results returned by fgsea. There are also other methods such as the one found in the built-in function, fgsea::plotGseaTable, that provide other ways to visualize these results for groups of different gene sets. You can also perform any number of different filtering and sorting options on the table of results depending upon your interests. "],["biological-networks-..html", "8.9 Biological Networks .", " 8.9 Biological Networks . What are biological networks? What do they represent (networks encode relationships between entities)? Why are biological networks useful and what can we learn from them? 8.9.1 Biological Pathways . What is a biological pathway? Why are they important, and how are they useful? 8.9.2 Gene Regulatory Networks . What are gene regulatory networks? Why are they important? How do we identify them (data driven correlation, wetlab experiments, others?)? How are they useful? 8.9.3 Protein-Protein Interaction Network . What are protein-protein interaction (PPI) networks? What information do they represent (direct association, functional association, etc)? Where does PPI information come from (datasets, databases, etc)? What are some ways we can use PPI information when interpreting other biological data (like differential expression? not sure)? 8.9.4 WGCNA . What is WGCNA? What problem does WGCNA attempt to solve, and how is it different than other classes of analysis methods (e.g. differential expression)? What data are appropriate for use in WGCNA? How do we run an interpret a WGCNA analysis in R? "],["single-cell-sequencing-analysis-..html", "8.10 Single Cell Sequencing Analysis .", " 8.10 Single Cell Sequencing Analysis . 8.10.1 Single Cell Sequencing . What is the goal of single cell sequencing? How is it generated, and what does it measure? What questions can we ask of single cell data that we cannot ask of other types of sequencing data? How is single cell data loaded into R? 8.10.2 Single Cell Analysis . What is the general analytical workflow for single cell analysis? Why do we perform each step along the analysis path? What are key parameters that we must choose that influence the results? How do we perform single cell analysis in R (Seurat)? 8.10.3 Single Cell Clustering . 8.10.4 Dimensionality Reduction &amp; Projection . PCA, tSNE, UMAP 8.10.5 Single Cell Data Visualization . 8.10.6 Single Cell Marker Analysis . "],["engineering.html", "9 EngineeRing ", " 9 EngineeRing "],["unit-testing.html", "9.1 Unit Testing", " 9.1 Unit Testing Writing code that does what you mean for it to do is often harder than it might seem, especially in R. Also, as your code grows in size and complexity, and you use good programming practice like writing functions, changing one part of your code may have unexpected effects on other parts that you didnt change. Unless you are using a programming language that has support for proof-based correctness guarantees, it may be impossible to determine if your code is always correct. As you might imagine, so-called total correctness is very difficult to attain, and often requires more time to implement than is practical (unless youre programming something where correctness is very important, e.g. for a self-driving car). However, there is a collection of approaches that can give us reasonable assurances that our code does what we mean for it to do. These approaches are called software testing frameworks that explicitly test our code for correctness. There are many different testing frameworks, but they all employ the general principle that we test our codes correctness by passing it inputs for which we we know what the output should be. For example, consider the following function that sums two numbers: add &lt;- function(x,y) { return(x+y) } We can test this function using a known set of inputs and explicitly comparing the result with the expected output: result &lt;- add(1,2) result == 3 [1] TRUE Our test instance in this case is input x=1,y=2 and the expected output is 3. By comparing the result of this input with the expected output, we have showed that at least for these specific inputs the function behaves as intended. The testing terminology used in this case is the test passed. If the result had been anything other than 3, the test would have failed. The above is an example of a test, but it is an informal test; it is not yet integrated into a framework since we have to manually inspect the result as passing or failing. In a testing framework, you as the developer of your code also write tests for your code and runs those tests frequently as your code evolves to make sure it continues to behave as you expect over time. The R package testthat provides such a testing framework that tries to make testing as fun as possible, so that you get a visceral satisfaction from writing tests. Its true that writing tests for your own code may feel tedious and very not fun, but the tradeoff is that tested code is more likely to be correct, saving you from potentially embarrassing (or worse) errors! Writing tests using testthat is very easy, using the example test written above (remember to install the package using install.packages(\"testthat\") first). library(testthat) test_that(&quot;add() correctly adds things&quot;, { expect_equal(add(1,2),3) expect_equal(add(5,6),11) } ) Test passed Test passed! How satisfying! The test_that function accepts two arguments: a concise, human readable description of the test one or more tests enclosed by {} written using expect_X functions from the testthat package In the example above, we are explicitly testing that the result of add(1,2) is equal to 3 and add(5,6) is equal to 11; specifically, we called expect_equal, which accepts two arguments that it uses to test equality. We have written two explicit test cases (i.e. 1+2 == 3 and 5+6 == 11) under the same test heading. If we had a mistake in our test such that the expected output was wrong, testthat would inform us not only of the failure, but more details about what happened compared to what we asked it to expect: test_that(&quot;add() correctly adds things&quot;, { expect_equal(add(1,2),3) expect_equal(add(5,6),10) } ) -- Failure (Line 3): add() correctly adds things ------------------------------- add(5, 6) not equal to 10. 1/1 mismatches [1] 11 - 10 == 1 Error: Test failed In this case, our test case was incorrect, but this would be very helpful information to have if we had correctly specified input and expected output and the test failed! It means we did something wrong, but now we are aware of it and can fix it. The general testing strategy usually involves writing an R script that only contains tests like the example above and not analysis code; the tests in your test script call the functions you have written in your other scripts to check for their correctness exactly like above. Then, whenever you make substantial changes to your analysis code, you can simply run your test script to check whether everything went ok. Of course, as you add more functions to your analysis script you need to add new tests for that code. If we had put our test above in a script file called test_functions.R we could run them on our analysis code like the following: add &lt;- function(x,y) { return(x+y) } testthat::test_file(&quot;test_functions.R&quot;) == Testing test_functions.R ======================================================= [ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ] Done! The ultimate testing strategy is called test driven development where you write your tests before developing any analysis code, even for functions that dont exist yet. Imagine we decide we need a new function that multiplies two numbers together and havent written it yet. testtthat can handle the situation where you call a function that isnt defined yet: test_that(&quot;mul() correctly multiplies things&quot;,{ expect_equal(mul(1,2),2) }) -- Error (Line 1): new function ------------------------------------------------ Error in `mul(1, 2)`: could not find function &quot;mul&quot; Backtrace: 1. testthat::expect_equal(mul(1, 2), 2) 2. testthat::quasi_label(enquo(object), label, arg = &quot;object&quot;) 3. rlang::eval_bare(expr, quo_get_env(quo)) Error: Test failed In this case, the test failed because mul() is not defined yet, but you have already done the hard part of writing the test! Now all you have to do is write the mul() function and keep working on it until the tests pass. Writing tests first and analysis code later is a great way to be thoughtful about how your code is structured, along with the usual benefit of testing that it means your code is more likely to be correct. Testing - R packages testthat Reference "],["object-orientation-in-r-..html", "9.2 Object Orientation in R .", " 9.2 Object Orientation in R . "],["toolification-..html", "9.3 Toolification .", " 9.3 Toolification . "],["pipelines-workflows-..html", "9.4 Pipelines &amp; Workflows .", " 9.4 Pipelines &amp; Workflows . "],["parallel-processing-..html", "9.5 Parallel Processing .", " 9.5 Parallel Processing . "],["r-packages-..html", "9.6 R Packages .", " 9.6 R Packages . R packages by Hadley Wickam "],["rshiny.html", "10 RShiny ", " 10 RShiny "],["overview.html", "10.1 Overview", " 10.1 Overview "],["learning-objectives.html", "10.2 Learning Objectives", " 10.2 Learning Objectives "],["skill-list.html", "10.3 Skill List", " 10.3 Skill List "],["communicating-with-r.html", "11 Communicating with R", " 11 Communicating with R No matter what your job is, you will at some point need to communicate the findings of your analysis with others. This will usually entail a combination of different forms of communication, including text, tables, and visualizations like plots. Depending on your audience, communicating may also include your code which describes very precisely (and ideally, reproducibly) what you did in your analysis. This chapter describes a few tools and techniques you can use to create static reports that help you communicate in all these ways. "],["rmarkdown-knitr.html", "11.1 RMarkdown &amp; knitr", " 11.1 RMarkdown &amp; knitr A markup language is a special kind of programming language used to annotate and decorate plain text with information about its intended formatting and structure. The syntax of a markup language is intended to be easy to read and write by humans and also machine readable, so that it may be processed by formatting programs into different formats, e.g. the same markup text might be converted into HTML or PDF. markdown is one such markup language. The markup is simple, and provides basic formatting syntax by default. The following contains some examples of markdown syntax: You can *emphasize* text, or **really emphasize it**. Lists are pretty easy to read as well: * item 1 * item 2 * item 3 If you need an enumerated list you can do that too: 1. item 1 2. item 2 3. item 3 You can easily include links to web sites like [Google](http://google.com) and images: ![an image of a master of the universe](https://upload.wikimedia.org/wikipedia/commons/b/bc/Juvenile_Ragdoll.jpg){width=50%} The markdown from above might be formatted as follows: You can emphasize text, or really emphasize it. Lists are pretty easy to read as well: item 1 item 2 item 3 If you need an enumerated list you can do that too: item 1 item 2 item 3 You can easily include links to web sites like Google and images: an image of a master of the universe Refer to the markdown documentation for a complete listing of supported markdown syntax. Some other markup languages you might recognize: ReStructured Text - the markup language used in most python documentation LaTeX - a markup language designed to help format mathematical expression HTML - the web markup language (HyperText Markup Language) wikitext - the markup language used by Wikipedia As the name suggests, RMarkdown is an extension of markdown that works in R. The most important extension is the ability to include code blocks in R in between markdown formatted text that can be executed. This enables writing executable reports that update their results whenever the document is rerun interspersed with explanatory text and other descriptive elements. For this reason, RMarkdown files are sometimes called RMarkdown notebooks, because they can be used to record both narrative text and results, similar to a traditional lab notebook. RMarkdown files typically end with .Rmd. RStudio has full RMarkdown integration to make writing RMarkdown notebooks very easy. Below is a screenshot of an RMarkdown document loaded in RStudio: RMarkdown notebook example The grey lines starting with ```{r} define a special code block that contains R code that should be executed and the output placed after the block. When run, RStudio will show you the output of the notebook within its interface: RMarkdown notebook example after running Notice how there is now a plot in the rendered document on the right. The code within the block was run, the plot generated, and inserted into the report. In this case, the notebook created an HTML document that RStudio knows how to display, but could also be opened in a standard web browser. These blocks are not standard R syntax, but are instead understood and processed by the knitr R package. As the name suggests, the process of generating a report from an RMarkdown document is called knitting; you would ask RStudio to knit your RMarkdown into a report. RMarkdown documents can be knitted into many different formats, including HTML, PDF, Microsoft Word, and even some slide presentation formats. When exporting to HTML, the report may include interactive elements including plots, collapsible sections, and code blocks, and maps. RMarkdown documents can also be written to accept parameters. This means you can write a single RMarkdown document that can be run on different inputs. Reports for standardized analytical pipelines, as are often implemented by high throughput sequencing cores, can thus be generated trivially for new datasets as they are generated without writing any additional code. RMarkdown - Getting Started RMarkdown package documentation and guide Literate programming "],["bookdown.html", "11.2 bookdown", " 11.2 bookdown Another R package that processed RMarkdown documents is bookdown. As the name suggests, this package is designed to write books using RMarkdown. This book you are reading was written in RMarkdown and generated using bookdown! You can look at all the source code on GitHub. "],["contribution-guide.html", "12 Contribution Guide", " 12 Contribution Guide This page contains some conventions and tips for writing material for this book. "],["custom-blocks.html", "12.1 Custom blocks", " 12.1 Custom blocks There are a number of custom blocks that can be used for highlighting salient information throughout the text. The code inside the corresponding block below can be used to insert different standout boxes: ::: {.box .tldr} This is a tl;dr ::: ::: {.box .note} This is a note ::: ::: {.box .hint} This is a hint ::: ::: {.box .important} This is an important ::: ::: {.box .warning} This is a warning ::: "],["assignments.html", "Assignments ", " Assignments "],["assignment-format.html", "Assignment Format", " Assignment Format GitHub repo templates here "],["starting-an-assignment.html", "Starting an Assignment", " Starting an Assignment :^) good luck When starting a project there are a small number of steps to get things set up just right in order to have a smooth and seamless experience. While it is absolutely possible to complete these projects on your own machine and your own installation of R and RStudio, it will be easier and faster to do your work on BUs SCC. The experience will be more straightforward and basically identical to doing it on your own. Starting your own git repository and cloning it In our development of these assignments we used git and github.com to manage our updates and allow for easy sharing. We can now pass these repositories (repos) along to you using GitHub classroom so you can follow the steps within. Contents Each assignment will have these following files within, roughly:  reference_report.html  main.R  README.md  report.Rmd  test_main.R reference_report.html - This is the completed report of the assignment. You will be endeavoring to replicate this while completing the assignment, with some room for creative differences in elements like plotting. main.R - This is the main R script in the assignment, where you will be doing a lot of your programming. This script contains function definitions and descriptions, but with functions that are empty. Complete the functions as described in the script and ensure they pass the tests. README.md - Every repository should have a README. Usually it contains useful information about installation, usage, and licensing. In this case, it will have helpful links back to assignment instructions. report.Rmd - Another empty file, this markdown script will source() your main.R script, loading the functions into its environment. There will be empty code blocks throughout this document, use the functions you developed in main.R to replicate the figures or match the captions seen in finalized_report.pdf. test_main.R - This is a completed testing script produced by us to help you quickly determine how will your code is running. This file sources your main.R file, meaning it runs it to completion and stores any functions and variables in the active environment. It then runs tests to ensure your functions are working in a predictable way. Tests are incredibly valuable tools, and potentially one of the strongest in a programmers toolkit. Most assignments beyond assignment 1 have a complementary guide hosted on the website page. These go into more background about how functions and tests work so use them if youre feeling stuck or would like more context. https://bu-bioinfo.github.io/r_for_biological_sciences/assignment-format.html GitHub tutorial Find your assignment link Clone your repository, you will see a new repo in the form: myusername/assignment-X Inside this repo, click the big g r e e n button and copy the HTTP link to your repository. Login to https://scc-ondemand.bu.edu/. Start an interactive R session (Interactive Apps &gt; RStudio Server), use the default settings but increase the hours to like 12 or something. Or if you like high pressure situations set it to like 20 minutes. Once your session is loaded and you are login in, select the New Project button below the Edit button. With the prompt open, select Version Control &gt; git, and paste your repos url into the first prompt. Feel free to rename the project, but also like dont? Thats confusing. You can change where this folder is setup on SCC, but I find ~/Documents or ~/Documents/BF591 to be a good home for organizing my various projects. You will need to login to complete this process, enter your GitHub username and password when prompted. R will download all of the repository contents, and add an .RProj file to the directory. This RProject file is useful for switching between R assignments in RStudio. This context menu is in the top right of the RStudio interface: Committing and Pushing with R (and without) With the RProject set up, you now have a directory on SCC that has cloned your remote repository in GitHub. At this moment all of the files are the same, since you have made no progress on your assignment (depressing, I know). In a burst of productive energy, you will start to make changes to main.R and write your beautiful, buggy functions. Once youre done for a little bit, what next? You could leave it on SCC and it would probably be pretty safe, but what if you want to share this code easily? Or if a cataclysm strikes Western Massachusetts? Your changes are gone and life will have lost all meaning. To prevent this cataclysm, you must add, commit, and push your code back to GitHub. Using RStudio Save the files you want to commit, and select the Git tab in the top right viewer (by default). Youll notice the files you changed are listed here, along with varying symbols. A question mark ? means the file is untracked, an M means a tracked file has been modified. A D means a tracked file has been deleted. If I select a file I want to add back to GitHub, I can tick the checkbox and it will switch to the left column. This is called staging a file. Once Ive staged all the files I want to add, I can click on the Commit button. This brings up another window, which will again list the files you are staging and actually show you the differences youre making (this is called the diff, based on the GNU utility of the same name). The most important part is the Commit box. A commit is a series of file updates made together, typically to solve a task. It is paramount that you include commit messages, so you can track what it is youre doing with your work! Just a sentence is usually enough to describe what the changes in this commit are doing. Once youve pressed the commit button, all thats left to do is to push your commit. This sends the commit to GitHub where it can safely live forever, duplicated across the world and safe from most cataclysms. In the same vein, the pull button is useful if your GitHub has been updated more recently than your current SCC directory. This can happen if youre editing these files from another computer or on GitHub.com directly. Using the command line That was kind of a lot of clicking and funny little menus, and my doctor says I am at high risk for a repetitive stress injury, so cant I just use my keyboard instead? Yes, of course. Well use the terminal, which is also built-in to RStudio but really just emulates the standard bash terminal that you would use to interact with SCC normally. Click on the terminal tab at the bottom of your layout (default). Marvel at the wonderful monospace font and rest your hands upon the keyboard. To see how the directory is doing, we can type git status to see what files have been changed. Wow! Looks like I have a lot of stuff to deal with. Whats nice is the git command offers a lot of helpful tips for getting me on my way. I want to add these files, and I really dont care how many I had so I am going to use git add *, which basically adds everything that can be found in the current directory. If I only wanted one or two things, I could specify git add file1.txt file2.txt and leave the other files alone. Once I have added (or removed git rm file1.txt) the files I want to commit, I simply use git commit -m \"Added a lot more content to tutorials! to create the commit and add my very important message. Messages are not mandatory but if you slack on being descriptive you will regret it when you come back later to figure out what in Gods name you were doing that day. Finally, with my commit done and files added, I simply say git push to send my wonderful code back to GitHub, and I can sit back and marvel at my handiwork. Put simply, the three commands git add *, git commit -m \"message\", and git push will save the changes to your code and keep your repository up to date. Easier than clicking around RStudio but both ways are super valid! "],["assignment-1.html", "Assignment 1", " Assignment 1 Problem Statement This assignment will focus on basic functions of R with an emphasis on tidyverse implementations. tidyverse is a collection of packages, pioneered by Hadley Wickham and RStudio, that looks to standardize procedures, functionality, and syntax in R. To gain familiarity with R, we will be working with a microarray dataset that contains gene probe expression data for various samples collected from cancer patients. In bioinformatics, it is common to have multiple datasets for your different modes of data (i.e. microarray expression data is kept separate from clinical data detailing the samples). You will get an opportunity to work with both of these datasets, and be required to cross reference between the two. Learning Objectives and Skill List Install various packages needed for analysis Load Data Gain familiarity with common tidyverse operations such as groupby(), mutate, and summarize. Create a small plot to display results Utilize R Markdown to create an attractive format for sharing data. Instructions Our main focus for this assignment are installing packages, manipulating data, and summarizing important statistics across both samples and features (genes). An empty project repo can be found here: https://github.com/bu-bioinfo/bf591-assignment-1 The project is laid out as such: main.R report.Rmd reference_report.html Each step of the assignment is explained in the R markdown file, report.Rmd. There you will find a list of tasks to explicity implement functions in your empty main.R script. The main.R script contains skeletons of each function youll need to implement, explaining what each function should do, the parameters it expects to receive, and what type of output is expected to be returned. A reference report, reference_report.html is also provided. Assuming you successfully implement all the functions in main.R, your generated report should look identical to the information displayed in reference_report.html. In this way, you can use reference_report.html as a guide to determine if you are correctly implementing your functions. Here is the suggested workflow for developing and checking your code in this assignment: main.R contains function definitions, including signature descriptions, for a number of functions, but the bodies of those functions are currently blank report.Rmd has code chunks that call functions defined in main.R - you do not need to write anything in the Rmd file (but you may) Your task is to read the function descriptions and the text in the Rmarkdown document and fill in the function bodies to produce the desired behavior in main.R You can test your work by executing individual code chunks in report.Rmd and comparing your output to the example compiled report in the repo In the workflow, you will go back and forth between developing code in main.R and running code chunks in report.Rmd When you have developed function bodies for all the functions and executed all the code chunks in the report successfully, you should be able to knit the entire report Hints When developing the period_to_underscore() function, you might find the stringr::str_replace_all() function helpful. The pattern argument to these functions is interpreted as a regular expression or regex for short. A regular expression is a sequence of characters (i.e. a string) written in a language that describes patterns in text, similar to Find and Replace operations in word processing software, but is more powerful and flexible in the kinds of patterns it can detect. Some characters have special meaning in regular expressions, one of which is the . character. In order to identify the literal period character like we are trying to do, we must instruct the regular expression to do so by either escaping the character with \\\\. or place it in a range with [.]. Either of these two methods will work to replace a literal . with _. See the section on Regular expressions for more information. If you are getting type conversion errors when loading in your expression CSV file, check to make sure you arent supplying your own column names. The CSV file has column headers already, and supplying your own will cause the first line to be read in as data. Since the first row are character values in this case, all of the other values in the columns will be coerced to characters as well, instead of reading them in as numbers. "],["assignment-2.html", "Assignment 2", " Assignment 2 Problem Statement Arranging the structure of our data inputs is vital to using R, as is creating the correct environment for analysis by installing packages. It is also important to share data and results easily using R markdown. Learning Objectives Install various packages needed for analysis Load data, filter that data, and retrieve HGNC ids for the data Create a small plot to display results Utilize R Markdown to create an attractive format for sharing data. Skill List How to utilize R markdown to create a report of data analysis Installing and loading packages in R Utilizing Bioconductor to equate affy ids to HGNC ids (gene names) Instructions Our main focus for this assignment are installing packages, manipulating data, and plotting our manipulated data. We will be borrowing from the same CSV of expression data from BF528s Project 1, available here. An empty project repo can be found here: https://github.com/bu-bioinfo/bf591-assignment-2 The project is laid out as such: main.R test_main.R report.Rmd A skeleton of the functions you need to complete is in main.R. Tests have been pre-written to test your code and help you ensure it is running correctly, these are in test_main.R. Finally, we are also introducing the concept of R Markdown, which for this assignment is report.Rmd. The document itself goes into greater detail, but you will: Complete the functions in main.R and use testthat:test_file('test_main.R') to ensure they work correctly. Read the R Markdown file and complete the section called Assignment. To do this, you can source('main.R') to bring over the functions you wrote in step one. Finally, annotate the functions you wrote and Knit the R Markdown report, complete with your additional comments and code execution. This page will go into detail on how the functions and their associated tests should work. Function Details 1. Bioconductor While many useful R packages can be loaded through CRAN using the install.packages() syntax, a lot of specifically bioinformatics packages are exclusively released on Bioconductor. For this assignment we only need the package called biomaRt. R programmers fancy themselves very clever, so Rs show up a lot. Naturally, we want to load our packages at the beginning of our script so all of the code we write beneath can access it as it runs. However, if a user or ourselves already has this package installed we dont want to waste their time installing it again. The function require() can help us avoid unnecessary installation time and will help us develop faster. The Bioconductor link above has an example of this method. This section is untested. 2. load_expression() Perhaps the most integral part of using the many data wrangling abilities of R is actually entering your data into the R environment. While there are many ways to do this in R, we ultimately want this data to be in a tibble, which means the current form of the CSV will make R very angry if you attempt to load it in. This is because tibbles dont support row names very well, and the first column of our data doesnt have a name. Try to use the load_expression() data to load data from a filename parameter and return a tibble of that information. I called my firs column probeids. Tests The tests for this function are: test_that(&quot;loading csv correctly&quot;, { result_tib &lt;- load_expression(&quot;/project/bf528/project_1/data/example_intensity_data.csv&quot;) expect_equal(dim(result_tib), c(54675, 36)) expect_true(is_tibble(result_tib)) }) This test uses the load_expression() you write to store the returned tibble in result_tib. While this test is using the same file for data input as you are, this may not always be the case. The test then compares the dimensions of that result, and expects 54,675 rows and 36 columns. These are the dimensions of the input CSV. It also checks to confirm it is a tibble object (because tibbles are better than dataframes). 3. filter_15() In order to filter the numerous rows we have for this data, we introduce a function that filters the probe IDs in the tibble our data is stored in. We want to capture probes that have a suitably high level of expression, so we are setting log2(15) as the cutoff for an expression level. We will keep a row if 15% of the values in that row exceed log2(15) (about 3.9). Since we may want to examine the probe IDs we find, the function simple returns the values of the probe IDs (column 1) instead of returning the entire tibble. This function presents an important concept in R: using built-ins to speed up our code. Built-ins are functions and packages that are optimized to process data in a certain way. Since were looking at each row of a table, we could simply use a for loop to iterate one row at a time. This is slow, though, and for this function might take 5-10 seconds to run (a long time for a program like this!). Instead, you could use a function like apply() or lapply() to filter every row at once. This solution takes mere moments. Tests library(tibble) test_that(&quot;the correct rows are filtered out in filter_15()&quot;, { test_tib &lt;- tibble(probeids=c(&#39;1_s_at&#39;, &#39;2_s_at&#39;, &#39;3_s_at&#39;, &#39;4_s_at&#39;), GSM1=c(1.0, 3.95, 4.05, 0.5), GSM2=rep(1.6, 4), GSM3=rep(2.5, 4), GSM4=rep(3.99, 4), GSM5=rep(3.0, 4), GSM6=rep(1.0, 4), GSM7=rep(0.5, 4)) expect_equal(pull(filter_15(test_tib)), c(&quot;2_s_at&quot;, &quot;3_s_at&quot;)) }) In order to test this function, we create a small sample tibble of expression data containing only seven samples and four IDs. Two of the rows do have more than 15% of their values exceeding log2(15), the other two do not. This test ensures that filter_15() selects the correct rows. Creating a small sample table like this can be very useful when testing your own code since you dont need to look at a large amount of data to see if its working correctly or not. 4. affy_to_hgnc() This is an important, but sometimes painful, part of using R. There is a great built-in package for connecting to Ensembl (a database of genomic information for many species) called biomaRt. We will use biomaRt to connect the affymetrix probe IDs to more recognizable HGNC gene IDs. The problem is that biomaRt depends on an external API (application program interface) to retrieve data, and this connection sometimes (oftentimes) doesnt work. While their may be more nuanced approaches to an unstable resource like this like automatically retrying failed connections, the best advice for the time being is to try running this function a few times if it doesnt work at first. The errors are clear when it comes to a failed connection, so know that when you get to this stage it likely isnt your codes fault. To build a biomaRt query, read the documentation in section 3 here. The biomart you should use is ENSEMBL_MART_ENSEMBL, the data set hsapiens_gene_ensembl, and you want to find the attributes c(\"affy_hg_u133_plus_2\", \"hgnc_symbol\"). The data you filter using filter_15() returns a list of affy_hg_u133_plus_2 probe IDs, and the gene names were interested in are stored in hgnc_symbol. This function should return a tibble, but biomaRts getBM() will only accept and return a data.frame. You can use dplyr::pull() to turn a tibble into a simple character vector, and dplyr::as_tibble() to go from a data frame to a tibble. Tests test_that(&quot;affy ids can be converted to HGNC names properly using affy_to_hgnc()&quot;, { # biomaRt super buggy so we can try to account for not connecting well response &lt;- try(affy_to_hgnc(tibble(&#39;1553551_s_at&#39;)), TRUE) if (grepl(&quot;Error&quot;, response[1])) { expect_warning(warning(&quot;Could not connect to ENSEMBL.&quot;)) } else { expect_equal(response$hgnc_symbol, c(&quot;MT-ND1&quot;, &quot;MT-TI&quot;, &quot;MT-TM&quot;, &quot;MT-ND2&quot;)) } }) As fun as it is to try to get biomaRt function to connect correctly, it is even more fun to test them. Since a failure to connect doesnt indicate an actual failure in our code we must use a try() block in order to capture if there is a connection error. Note that the try() function is a part of programming called error-handling which extends to many other languages. We often expect errors when running our programs (such as right now) but dont want to shut down our entire operation if its an error we can expect. Using try, except, and finally (the latter two not appearing here) we can account for *issues outside of our control and adapt our code to change the outcome. Using try except is not a replacement for writing code that doesnt generate errors. If you can avoid an error in the first place, that is far better than using error-handling. In this case we try() to use affy_to_hgnc() to connect to Ensembl and store the resulting error in response. We then check: if there is an Error then we throw a warning() to our testing output. This doesnt stop further testing from happening, but it ensures we know that something isnt quite right. If the response does not contain an error, we simply test that it returned the correct gene symbols for our random affy probe ID of choice. 5. reduce_data() We have one final step in manipulating our data before we plot it. We have our original data, the probe IDs and their associated HGNC symbols, and a list of good gene names and bad gene names. reduce_data takes these four inputs and returns a tibble that has reduced our expression data to only the genes of interest and has a column describing which set of genes it belongs to (good or bad). Changing the shape of the data is incredibly useful for ggplot, the tidyverse package we will use for plotting. While there is flexibility when using ggplot to plot data, having data in long format is typically ideal. Once again, there are multiple ways to reorganize data in this way. We used the base function match() to connect our probe IDs to with our HGNC IDs in name_ids. We then used tibble::add_column() to insert the new data in the correct location. Finally, we created two tibbles of good and bad genes using which() and the %in% modifier. Which evaluates true conditions across a range of data, so we can pass the list of genes we want and select the correct ones. For instance: library(tibble) tib &lt;- tibble(gene = c(&quot;gene1&quot;, &quot;gene2&quot;, &quot;gene3&quot;, &quot;gene4&quot;), affy = c(&quot;a_s_1&quot;, &quot;a_s_2&quot;, &quot;a_s_3&quot;, &quot;a_s_4&quot;)) which(tib$gene %in% c(&quot;gene2&quot;, &quot;gene3&quot;)) # returns the index of the TRUE rows ## [1] 2 3 tib$affy[which(tib$gene %in% c(&quot;gene2&quot;, &quot;gene3&quot;))] # use [] to get data back ## [1] &quot;a_s_2&quot; &quot;a_s_3&quot; Once again, there are many ways to reshape this data (some maybe more elegant than this!) and all we need is the data to be correctly shaped when it is returned. Tests test_that(&quot;reduce_data() is correctly changing the size and shape of the tibble&quot;, { t_tibble &lt;- tibble(probeids = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;, &quot;3_s_at&quot;), GSM1 = c(9.5, 7.6, 5.5), GSM2 = c(9.7, 7.2, 2.9), GSM3 = c(6.9, 4.3, 6.8)) names &lt;- tibble(affy_hg_u133_plus_2 = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;, &quot;3_s_at&quot;), hgnc_symbol = c(&quot;A-REAL-GENE&quot;, &quot;SONIC&quot;, &quot;UTWPU&quot;)) good &lt;- c(&quot;A-REAL-GENE&quot;) bad &lt;- c(&quot;SONIC&quot;) reduce_test &lt;- reduce_data(t_tibble, names, good, bad) result &lt;- tibble(probeids = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;), hgnc_symbol = c(&quot;A-REAL-GENE&quot;, &quot;SONIC&quot;), gene_set = c(&quot;good&quot;, &quot;bad&quot;), GSM1 = c(9.5, 7.6), GSM2 = c(9.7, 7.2), GSM3 = c(6.9, 4.3)) expect_equal(reduce_test, result) }) In order to test a function that changes a tibble, we need to use a tibble. We only create a test table that is three rows by four columns, but that is enough to get the gist of the function. We simply pass the four parameters to reduce_data() and we expect it to create a tibble like result. Ensure your output column names are the same here or your tests my fail. While not crucial to the success of your assignment, maintaining correct column names across multiple data transformations is an important skill. "],["assignment-3.html", "Assignment 3", " Assignment 3 Problem Statement High dimensional data is typically time and resource-intensive to analyze, interpret and visualize. Gene expression data (including microarray data) often consists of measurements for tens of thousands of known genes for every sample. Learning methods to inspect, reduce and display high dimensional data is necessary for many machine learning and bioinformatics problems. Learning Objectives Understand the basic principles of Principal Component Analysis (PCA) and hierarchical clustering / heatmaps Perform PCA in R and evaluate the basic outputs of PCA Generate basic clustered heatmaps Skill List R: scale(), transpose t(), prcomp(), heatmap(), ggplot2 Background on Microarrays A microarray consists of thousands of specifically designed single stranded DNA sequences affixed or bound to a solid surface (glass, nylon, etc.). Extracted RNA or DNA from samples of interest are labeled with fluorescent dyes, and hybridized with the bound probes by flowing across the surface. Non-hybridized molecules are washed away and b a laser will excite the attached dye to produce light to be detected by a scanner and converted to a digital image. Finally, image processing will transform each spot (bound probe) to a numerical measurement that can be used to infer expression levels for genes. Background on Principal Component Analysis Principal component analysis is an exploratory data analysis technique commonly used to reduce the dimensionality of data while trying to simultaneously minimize information loss. To understand the inner workings of PCA will require an in-depth review of linear algebra and is beyond the scope of this specific assignment. We will provide several links and references if you wish to do so on your own. At its core, PCA creates new uncorrelated linear combinations of the original variables that successively maximize variance. Thus, the first principle component (PC) represents the direction of the data that explains a maximal amount of variance. The second PC represents the direction that captures the second most variance and so on and so forth. As you can infer, if a small number of PCs capture a majority of variance contained within a dataset, they can be used as a lower dimensional representation of the data. Marisa et al. Gene Expression Classification of Colon Cancer into Molecular Subtypes: Characterization, Validation, and Prognostic Value. PLoS Medicine, May 2013. PMID: 23700391 The example intensity data was taken from the listed publication. In it, the authors proposed the use of gene expression profiling (through microarray technology) to generate a robust and reproducible classifier to identify subtypes of colorectal cancer samples. They identified six subtypes that they demonstrated to be significantly associated with distinct molecular pathways and clinical pathologies. We have provided you a sample expression matrix that was normalized and subjected to batch correction. We will use the example_intensity_data.csv as our data matrix going forward. Scaling data using R scale() Oftentimes, it is necessary to analyze or plot data that contains variables that differ by multiple orders of magnitude. In bioinformatics, this is an extremely common situation as different genes may be expressed at wildly different levels. For instance, in typical HTS experiments and as you will see later on in the course, you may detect genes with counts ranging from the single digits to those with counts in the tens of thousands. Heatmaps of gene expression data are commonly transformed by standard scaling in order to improve the visualization while retaining the underlying pattern of expression between samples. One such way to standardize data is the scale() function in R. Scale essentially takes a vector of values and determines the mean and standard deviation of the entire vector. Scale() will then subtract the mean from each element in the vector and divide each by the standard deviation. Z-scores thus correspond to the number of standard deviations by which a data point is above or below the mean of all the observations (i.e. a Z-score of 0 represents a value equal to the mean and a Z-score of 1 represents a value one standard deviation greater than the mean value). This has the effect of representing all of your data points on the same scale while preserving the pattern or profile of differences inherent between values across different observations. The transpose function t() is a function covered in the textbook for this course. As covered earlier, it converts a \\(m \\times n\\) matrix to \\(n \\times m\\). The built-in R function scale() operates on a column-wise basis. Transpose the matrix such that scaling occurs within genes rather than samples. You may also use the tidyverse pivot functions to perform this transpose operation if you prefer. Using what youve learned in the course and prior assignments, read in the example_intensity_data.csv to a proper format. Return the scaled matrix. #1: We want to both center and scale our matrix. Deliverables 1. The example_intensity_data.csv read in as a dataframe. Proportion of variance explained As mentioned earlier, principal components represent a successively maximal amount of variance in your dataset. It is often helpful to visualize the percentage of total variance explained by each principal component. Write a function to determine the proportion of variance explained by each principal component. Use the results generated by this function to create a bar chart displaying the variance explained by successive PCs. On this same plot, make a scatter and line plot showing the cumulative proportion of variance explained by each principal component. #1: The summary() function in R provides top-level information about model fitting and statistical objects. #2: You may access the standard deviation of the PCA results object by $sdev Deliverables 1. A vector with the variance explained for each PC. 2. A tibble with the labels for each PC, the variance explained and the cumulative variance explained. 3. A barchart of the proportion of individual variance explained by each principal component overlayed with a scatter plot of the cumulative sum of the variance explained for each successive principal component. Label all relevant axes and provide a descriptive caption for the figure. Plotting and visualization of PCA In typical HTS experiments, PCA is a common tool used to analyze the similarity between samples and determine if the experimental variable of interest (genotype, knockout, etc.) represents a large source of variance. One of the most common visualization is to plot the first two principal components (which may not always represent a majority of variance in your dataset) against each other. Typically, one will examine the pattern of clustering in this plot to see if samples are grouped together in any meaningful pattern according to important experimental variables. The value or score of each sample in terms of its principal components may be found in the pca results object accessed by $x. When plotting individual PCs against one another, remember that different principal components explain different amounts of variance and so it will often be the case that the scale of importance across the axes will be different. Deliverables 1. A scatterplot of PC1 vs PC2 from the pca_results$x values. Read in the metadata CSV and use it to annotate each sample with its corresponding assignment to the SixSubtypesClassification made in the original publication (samples belong to either c3 or c4). Label the points in the plot by color for their corresponding Six Subtypes Classification. Hierarchical Clustering and Heatmaps Now that weve discussed the basics of PCA, we will move on to discuss the use of hierarchical clustering and heatmaps. Heatmaps are a common plot used to quickly visualize the expression and pattern of many relevant observations across samples. For HTS data, they essentially depict the expression of genes (rows) for each sample (columns) as colors that denote the magnitude of expression to enable quick identification of patterns and changes between samples/experimental groups. Prior to constructing a heatmap, it is common to use some form of clustering algorithm to learn potential groups and patterns in the expression data. One such method is known as hierarchical clustering (specifically focusing on agglomerative clustering), a method which attempts to cluster data into hierarchies where single observations begin as separate points and are successively merged into larger clusters until all points have been grouped. Typically, agglomerative clustering will begin with a dissimilarity matrix that defines how close all observations are by calculating some distance metric for every pair of observations. There are multiple distance metrics that can be chosen, and one of the most commonly used is simple euclidean distance. A linkage function will then use these distance metrics to define how clusters are formed based on these distance values. For example, complete linkage will define clusters by the maximum value of all pairwise distances between two different clusters. We will be using the base R function heatmap() which by default uses euclidean distance and the hclust()`` function to produce a clustered heatmap. Theheatmap()` function will also scale your data row-wise to aid in visualization. To begin, we have provided you with a representative output of differentially expressed probes (differential_expression_results.csv) between the C3 and C4 subtypes. Filter this table to return probes with an adjusted p-value of &lt; .01. Use what youve learned to extract out the normalized intensity values for this subset of DE probes. Use the heatmap() function in R to create a heatmap of the normalized intensity values for these DE genes. #1: It is important to remember that commonly used color combinations in biology (red-green) are quite difficult to discern for those with color blindness. To simplify this design decision, we are going to use RColorBrewer, which both generates appropriate color palettes and has a number of built-in color blind friendly palettes. You may use the command display.brewer.all(colorblindFriendly=TRUE) to view these curated palettes. Deliverables 1. A list of the significant probes from the differential_expression_results.csv. 2. A matrix of the example_intensity_data.csv filtered to only contain probes contained within the list of significant probes. 3. A heatmap of the normalized intensity values for the differentially expressed probes (adjusted p-value &lt; .01). References https://www.genome.gov/about-genomics/fact-sheets/DNA-Microarray-Technology Govindarajan, R., Duraiyan, J., Kaliyappan, K. &amp; Palanisamy, M. Microarray and its applications. Journal of Pharmacy &amp; Bioallied Sciences 4, S310 (2012). Trevino, V., Falciani, F. &amp; Barrera-Saldaña, H. A. DNA microarrays: A powerful genomic tool for biomedical and clinical research. Molecular Medicine 13, 527541 (2007). Shlens, J. A Tutorial on Principal Component Analysis.(https://arxiv.org/abs/1404.1100) Lever, J., Krzywinski, M. &amp; Altman, N. Points of Significance: Principal component analysis. Nature Methods 14, 641642 (2017). Hastie, T., Hastie, T., Tibshirani, R., &amp; Friedman, J. H. (2001). The elements of statistical learning: Data mining, inference, and prediction. New York: Springer.https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf output: html_document: css: style.css "],["assignment-4.html", "Assignment 4", " Assignment 4 Problem Statement When dealing with gene counts in an mRNA-seq dataset, it is important to normalize the data before performing any analyses so you can make accurate comparisons of gene expression between samples. There are a number of different methods you can use to accomplish and the method you use will depend on the kinds of samples you have and the analyses you want to perform. This first part of this assignment will guide you through two commonly used methods: Counts Per Million and DESeq2 Normalization. Once you normalize your data you can do exploratory analysis and construct the appropriate graphs. As bioinformaticians, you will need to be able present your data in tidy reports with generated plots- sometimes between blocks of text. R Markdown is a good tool to accomplish this with, allowing you to display tables and plots alongside the bulk of your writing. Youve had some exposure to R Markdown in previous assignments, now you will start writing your own code to generate the visualizations for your report. It will be important to remember that you need to keep your code separate depending on its function: the function declarations and implementation- your code that will work behind-the-scenes to perform data manipulation and process inputs without user interaction on the back end- should stay in your main.R file while your calls to construct and display the tibbles and visuals created in the back end should be put in your report.Rmd file- despite the user not directly manipulating or editing the outputs you generate, they are still interacting with the display by viewing it and therefore the lines of code that call the functions to create these outputs belong in your front end. Learning Objectives Data normalization methods (CPM, DESeq2) Plotting data in Tidyverse Displaying results and compiling reports in R Markdown Application of Tidyverse functions Skill List DESeq normalization, referencing the linked Bioconductor vignette if needed Tibble manipulation Creating different types of graphs in ggplot2 Running PCA Instructions An empty project repo can be found here: https://github.com/BF591-R/bf591-assignment-4 You will be given main.R test_main.R report.Rmd verse_counts.tsv example_report.html Like previous assignments, the bulk of the work will be done in the skeleton file main.R, but unlike previous assignments the provided report.Rmd is also a skeleton file. Both documents will provide the information needed to complete the assignment. Please ensure that your inputs and outputs match the specifications listed- or at the very least can properly handle inputs as instructed and return the expected output since that is how your functions will be tested. To help you complete the assignment, you are being provided a sample report and a set of test functions that are similar to the ones we will use to grade. The sample report is provided in the appropriately named sample_report.html and the tests for you to use can be found in test_main.R. Please note that while the sample report does not display any code, it is okay and- preferred, really- for your report to display the code blocks like they have done in previous assignments. Tasks Implement the functions as described in main.R. [optional but highly recommended] Test your functions using `test_main.R Fill in report.Rmd as instructed. You will need the functions you implemented in main.R Knit report.Rmd and create your report.html Deliverables main.R with all of the functions completed report.Rmd filled in as instructed report.html knitted from your report.Rmd Function Details 1. read_data() Load a tsv located at a specified location Input (1): string path to file Output: (g x m) tibble Details: This function will be tested for handling various input strings and returning the proper output: dimensions, column names, if column gene is located in the first column, and output type tibble. Your test_main will include additional testing of output column types and lack of row names for your reference, to help catch errors early on. 2. filter_zero_var_genes() Filter out genes with zero variance Input (1): (g x m) tibble Output: (n x m) tibble Details: This function will be tested for handling an input (g x m) tibble and returning the proper output: column names, if column gene is located in the first column, names of genes returned, and output type tibble. Your test_main will include additional testing for row consistency- ensuring that the sample data still correlates with the gene names- for your reference and to help catch errors early on 3. timepoint_from_sample() Extract time point information from sample name Input (1): string (length 5) of sample name in format v[A-Z][a-z,1-9]_[1-9] (In other words: v[][]_[], where  is any capital modern English letter,  is any lower case modern English letter OR any Arabic number 0-9, and  is any Arabic number from 0-9) Output: string (length 2) of substring [A-Z][a-z,1-9] from sample name: Details: This function will be tested for handling various strings of length 5 in the form v[A-Z][a-z,1-9]_[1-9] and outputting the proper string, preserving letter case where letter case is provided. 4. sample_replicate() Grab sample replicate number from sample name Input (1): Input (1): string (length 5) of sample name in format v[A-Z][a-z,1-9]_[1-9] (In other words: v[][]_[], where  is any capital modern English letter,  is any lower case modern English letter OR any Arabic number 0-9, and  is any Arabic number from 0-9) Output: string (length 1 of substring [1-9] from sample name: v[A-Z][a-z,1-9]_[1-9] (In other words: [] from v[][]_[]) Details: This function will be tested for handling various strings of length 5 in the form v[A-Z][a-z,1-9]_[1-9] and outputting the proper character string. 5. meta_info_from_labels() Generate sample-level metadata from sample names and stores the data into a tibble. Will include columns named sample, timepoint, and replicate that store sample names, sample time points, and sample replicate, respectively. Input (1): Character vector of length _S_ of sample names with column names sample, timepoint, and replicate Output: a (_S_ x 3) tibble Details: This function will be tested for handling of a character vector of length _S_, where each element in the vector is a string with a length of 5 in the form of v[A-Z][a-z,1-9]_[1-9], and properly outputting a (_S_ x 3) tibble with columns named sample, timepoint, and replicate and rows that correspond with the input samples. Your test_main will include additional testing for the order of elements in column samples correspondence with the order of elements in the input vector for your reference. The column types of your output tibble will not be tested. 6. get_library_size() Calculate total read counts for each sample in a counts dataset. Input (1): a (n x m) tibble of raw read counts Output: tibble or named vector of read totals from each sample. Vectors must be length _S_, a tibble can be (1 x _S_) with sample names as columns names OR (_S_ x 2) where sample name is in the first column and library size is the second column Details: This function will be tested for the return of a tibble or named vector that have sample names which correspond with the appropriate library size. 7. normalize_by_cpm() Normalize raw counts data to counts per million using (counts) / (sample_library_size) * 10^6 Input (1): a (n x m) tibble of raw read counts Output: a (n x m) tibble with read count normalized to counts per million Details: This function will be tested to handle a (n x m) tibble. Its output will be tested for dimensions, column names, location of string and numeric column(s), performance of the desired equation on numeric columns, and that gene names still correspond to their rows. 8. deseq_normalize() Normalize raw counts data using DESeq2 Input (1): a (n x m) tibble of raw read counts Output: a (n x m) tibble of DESeq2 normalized counts data Details:This function will be tested to handle a (n x m) tibble. Its output will be tested for dimensions, column names, location of string and numeric column(s), performance of the desired equation on numeric columns, and that gene names still correspond to their rows. 9. plot_pca() Input (3): a (n x _S_) tibble of data, a (_S_ x 3) tibble of sample-level meta information, and a string Output: a ggplot scatter plot showing each sample, with PC1 on x-axis and PC2 on y-axis Details: The output of this will be tested for the appropriate test run and PCs used to plot. It may be visually inspected as part of your grade 10. plot_sample_distributions() Input (3): a (n x _S_) tibble of data, a boolean to determine whether to scale the y axis to log10 values, and a string Output: a ggplot boxplot that shows gene count distributions Details: This function will be tested on a (n x _S_) tibble. It will be tested for functionality of its inputs, handling of data, expected graph elements, and graph type. It may also be visually inspected as part of your grade 11. plot_variance_vs_mean() Input (3): a (n x _S_) tibble of data, a boolean to determine whether to scale the y axis to log10 values, and a string Output: a ggplot scatter plot where the x-axis is the rank of gene ordered by mean count over all samples, and the y-axis is the observed variance of the given gene. Each dot should have their transparency increased. The scatter plot should also be accompanied by a line representing the average mean and variance values Details: This function will be tested on a (n x _S_) tibble. It will be tested for functionality of its inputs, handling of data, expected graph elements, and graph type. It may also be visually inspected as part of your grade Hints Make sure you dont have any rownames on any of the tibbles your function(s) return. You should get a warning in your R console if you do. Sometimes Tydyverse is the best tool for the job, sometimes it isnt Some tasks are easier to accomplish with dataframes than tibbles. Not everyone will use these methods, but if you do you are welcome to use dataframes within a function as long as the inputs and outputs are the ones specified (returning a dataframe instead of a tibble may points). The Bioconductor vignette for DESeq2 is linked in part 3 of your report.Rmd. It can also be found here) In a previous version of this assignment, the output of filter_zero_var_genes() was transformed from (n x m) to (n x _s_) and called the counts_matrix. One of the TAs thought use of matrix would be confusing (since we want you to use datasets of type tibble) so it was re-worded, but this piece of trivia might be helpful for interpreting the Bioconductor vignette- or if you see any sort of matrix referenced in report.Rmd because it got overlooked during editing. There may be occasions where you will need to reshape your data from a wide format to a long format Symbols used in main.R~ g: initial number of Genes, m: initial number of columns expected when you import verse_counts.tsv, n: number of genes expected after you filter in part 1b, _S_: number of Samples "],["assignment-5.html", "Assignment 5", " Assignment 5 Problem Statement One of the main questions asked by high throughput sequencing is whether some variable or condition (e.g. treatment/disease status) induces a measurable, distinguishable change. In the context of mRNAseq, we are interested in whether we can determine if mRNA levels (what we actually measure) and genes (what we infer) are significantly up- or downregulated between conditions and if we can relate these changes back to a biological phenotype either directly or indirectly. Learning Objectives Understand the methodology and rationale for how DESeq2 normalizes data, models counts, and performs differential expression Perform differential expression analysis using DESeq2 and learn one method to analyze time course data Understand how to do basic inspection and evaluation of differential expression (DE) results Skill List DESeq2 Basic diagnostic plots of DE results High-quality figures and plots using R and ggplot DESeq2 Background DESeq2 is a well validated and highly cited method for differential expression analysis of HTS data. The original paper describing the methodology in more detail can be found here: Love, M.I., Huber, W. &amp; Anders, S. Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol 15, 550 (2014). https://doi.org/10.1186/s13059-014-0550-8 We will link back to the original DESeq2 vignette at the head of each applicable section for reference. We strongly encourage you to read the associated documentation as it succinctly summarizes the important concepts and methodology underlying each step of the process. It also provides example code snippets and answers to many common questions regarding the usage of DESeq2 can already be found in this documentation. Briefly, at its core, DESeq2 models read counts and tests for differential expression through negative binomial generalized linear models. DESeq2 normalizes count data by scaling by size factors calculated by the median-of-ratios (developed in the original publication for DESeq). For significance testing, DESeq2 performs a Wald test for model coefficients to generate p-values and these are adjusted for multiple testing correction through the Benjamini-Hochberg method. Generating a counts matrix DESeq2 takes as input a counts matrix where genes correspond to rows and samples to columns. For one of the more common use cases of differential expression analysis, mRNAseq experiments, we are interested in counts associated with genes; however, DESeq2 is broadly applicable to many kinds of high-throughout sequencing (HTS) count data. Returning to the analysis of mRNAseq specifically, there are three main approaches to map reads: Alignment to a reference genome by a splice-aware aligner and counting of reads falling into exonic regions using a reference annotation file Transcript-based assembly approaches (TopHat, Cufflinks) or transcript abundance quantification (Salmon, Kallisto) Reference free assembly followed by mapping and counting DESeq2 expects the values of the input matrix to be counts (integers) and for the purposes of this assignment, you have been provided with a count matrix of samples generated by the first method. Reads were aligned to the mouse genome (GENCODE GRCm39) with STAR and quantified to gene counts by Verse using the matching GTF file. Prefiltering Counts matrix DESeq2 Vignette - Pre-filtering Unless your experiment is quite large, filtering your counts matrix prior to DESeq2 is not strictly required. If your experiment incorporates hundreds of samples and a complicated model design with many interaction terms, you may wish to perform a combination of manual filtering and further optimization of DESeq2 settings. Otherwise, for most datasets, default settings in DESeq2 will run in a reasonably short amount of time on relatively modest hardware even without pre-filtering. There are many different strategies to filter count datasets and which you choose to employ should be informed by your objective, and your data. Some common filters include removing genes where the mean count across all samples is below some threshold or removing genes based on how many samples have a zero count. In general, extensive pre-filtering is not required and DESeq2 incorporates downstream methods to increase statistical power through independent filtering (described in methods). For this assignment, we will not be filtering the dataset prior to running DE analysis. Median-of-ratios normalization Original DESeq Publication As mentioned earlier, DESeq2 expects a non-transformed, non-normalized matrix of integer counts as input. DESeq2 performs its own normalization based on the underlying assumption that not all or a small amount of genes are truly differentially expressed. This is encapsulated in the median-of-ratios method which accounts for library size as well as RNA composition. Although DESeq2 performs this normalization in the background, it can help conceptually to understand the process by which it does so. First, a pseudo-reference sample is created and set equal to the geometric mean across all samples for a given gene. Then, ratios are calculated comparing each sample to this pseudo-reference on gene-wise basis. Next, on a sample-wise basis, the median value for all ratios is taken as that samples normalization factor and for a given sample, all counts are divided by this factor. You have learned how to extract these normalized counts by size factors in the last assignment. DESeq2 preparation DESeq2 Vignette - Preparation The DESeqDataSet is the object that holds the read counts and associated statistical measures generated through the DESeq2 algorithm. Although there are several ways to generate this object, we will focus on the DESeqDataSet method which directly takes a SummarizedExperiment object. OMeara et al. Transcriptional Reversion of Cardiac Myocyte Fate During Mammalian Cardiac Regeneration. Circ Res. Feb 2015. PMID: 25477501l The counts matrix provided was generated by data made available by the listed publication. We have only taken a subset of their samples, focusing specifically on the mRNAseq experiment during in vivo heart development from 0, 4, 7 days after birth as well as in adult mice. To start, we are going to consider the simplest use case for differential expression and subset our data to contain only the samples from postnatal day 0 and adult hearts. In addition to subsetting the counts matrix, we also need to construct the sample data information as a dataframe that lists the samplenames (columns in our counts matrix) as well as their associated information (in our case, timepoint). 1. Reading and subsetting the data from verse_counts.tsv and sample_metadata.csv We will use this as a demonstration of one of the simplest use cases for DESeq2, comparing two groups of samples based on a single experimental variable. Subset the samples to only include those from timepoints vP0 and vAd, which should correspond to a total of 4 samples (vP0_1, vP0_2, vAd_1, vAd_2). Store both the counts matrix and sample dataframe in a SummarizedExperiment object. You may ignore most of the columns in the metadata CSV as they were used in the generation of the data itself. Focus on the samplenames and timepoints columns. The columns (sample names) in your counts matrix and sample dataframe need to be in the same order You may need to convert your counts dataframe to a matrix The counts matrix is verse_counts.tsv and the sample metadata is sample_metadata.csv 2. Running DESeq2 Understanding Factor Levels DESeq2 Vignette - Factor Levels The factor level in DESeq2 determines which level represents the control group you want to compare against. By default, the reference level for factors will be chosen alphabetically. You can either manually change and set the reference factor level or specify a comparison of interest using DESeq2 Contrast() after performing the first steps of the DE analysis. Performing Differential Expression Analysis Here you may find the vignette for example usage of DESeq2. Please read the following section to instruct you on how to perform differential expression analysis in DESeq2: Running DESeq2 We will be running DESeq2 with a model design of ~timepoint with vP0 set as our reference level. This will test for differentially expressed genes in adult mouse heart ventricles vs. postnatal day 0. Return both the results as a dataframe and the dds object from running DESeq2 as a list. Assign the results dataframe and dds object to variables so you can use them later in your Rmd. 3. Annotating results to construct a labeled volcano plot Later in the analysis, we will construct a volcano plot. Typically, volcano plots will label genes by their significance as well as their direction of change. Convert the results dataframe to a tibble and add a column labeled volc_plot_status that you can easily input in ggplot2 to color points by the below criteria. The labels will encompass three groups: 1. Positive log fold change and significant at a given padj threshold, 2. Negative log fold change and significant at a given padj threshold, 3. Remaining non-significant genes Ensure your column is named exactly volc_plot_status or the test will not work without modification. Have the values for these labels be `UP, DOWN, NS respectively. 4. Diagnostic plot of the raw p-values for all genes Separate from differential expression analysis, it is always an important diagnostic to plot the unadjusted distribution of p-values obtained from an experiment. Built in to the definition of a p-value is the idea that under the null distribution and given all other assumptions made are true, p-values follow a uniform distribution. In general, you should typically observe a peak of values close to 0 and a roughly uniform distribution as you approach 1. The values closer to 0 will be a mix of situations where the alternative hypothesis is true as well as potential false positives. Significant deviations from this general pattern likely require close examination of your data and the significance test you are employing. Make a histogram of the raw p-values from all the genes discovered in the experiment. 5. Plotting the LogFoldChanges for differentially expressed genes It is often helpful visualize the log2FoldChanges for your differentially expressed genes to gain a sense for the distribution of up- and downregulated genes and gain a global view of how genes are changing. Subset your results to only include genes significant at a padj threshold of &lt; .10. Plot a histogram of the log2FoldChange values for these genes. The choice of FDR cutoff depends on cost The choice of a FDR cutoff or any p-value cutoff is always subjective and based on the objectives of your experiment and the cost of false discoveries. For example, if your initial sequencing experiment is exploratory and meant to generate hypotheses to be followed up in vitro or in vivo, it may be more appropriate to set a permissive cutoff (padj &lt; .20). P-value thresholds are set on an experiment by experiment basis and should not be adjusted simply to increase or decrease the number of DE genes retroactively. 6. Plotting the normalized counts of differentially expressed genes It can be helpful to visualize the normalized counts for a few differentially expressed genes as a quick diagnostic that your analysis is working as intended. Although you will likely not need to do this for every RNAseq analysis, it can provide confidence in your results and also help you remember the meaning of the directionality of your fold change values. Make a scatter / jitter plot of the DESeq2 normalized counts for the top ten significantly differentially expressed genes ranked by ascending padj If you are plotting all of the genes on the same plot, you may find it helpful to take the logarithm of the counts as different genes often have counts that differ by several orders of magnitude. You may find it helpful to add a slight horizontal jitter to the points to aid in visualization for cases where counts for each replicate are similar. You have already used DESeq2 to extract normalized counts. There are also directions available in the vignette. 7. Volcano Plot to visualize differential expression results A volcano plot is a common data plot that is intended to quickly display points of interest by plotting statistical significance against a magnitude of change. In the specific case of mRNAseq data, this entails plotting the -log(p-values / adjusted p-values) against the estimated log fold changes reported by most differential expression methods. Statistically significant genes with low p-values will appear higher on the y-axis of the plot and they will be separated by the magnitude and directionality of their change (i.e. Upregulated genes with larger fold change estimates will be further along to the right of the origin of the x-axis, and downregulated genes will be farther to the left of the origin on the x-axis). Typically, genes of interest that have additional biological relevance to the experiment will be directly annotated using domain knowledge. Make a volcano plot (log2FoldChange vs. -log10(padj)) for all the genes discovered in the experiment. Use the column generated in part 3 to color the genes with the appropriate label. 8. Running fgsea vignette We will be performing a GSEA on the results generated by running DESeq2 using the fgsea package. We will use log2FoldChange as our ranking metric and test against all the pathways in the C2 Canonical Pathways gene set collection provided by MSigDB and already downloaded here (c2.cp.v7.5.1.symbols.gmt). Read the section in the textbook or the fgsea documentation for the appropriate formats for the ranked gene list and the gene sets. You may read in the gene sets in the proper format using GSEABase or fgsea You will need to convert the mouse ensembl gene IDs to human HGNC symbols to match the genes provided in the C2 Canonical Pathways gene sets. If you are encountering issues, you may need to remove the version number of the annotation. As you have discovered, biomaRt can be difficult to work with. If you are encountering frequent issues, you may consider mapping your gene IDs and saving the results locally to avoid having to re-run the biomaRt query multiple times. You will inevitably have genes with duplicate names and/or genes with duplicate values for log2FoldChange. Choose a method to remove these values. 9. Plotting the top ten positive NES and top ten negative NES pathways Ensure that the pathway labels are visible You will probably get warnings about duplicate values or ranks, do your best to address these but dont get hung up on trying to find them all. References Dobin A, Davis CA, Schlesinger F, Drenkow J, Zaleski C, Jha S, Batut P, Chaisson M, Gingeras TR. STAR: ultrafast universal RNA-seq aligner. Bioinformatics. 2013 Jan 1;29(1):15-21. doi: 10.1093/bioinformatics/bts635. Epub 2012 Oct 25. PMID: 23104886; PMCID: PMC3530905. Zhu, Q., Fisher, S.A., Shallcross, J., Kim, J. (Preprint). VERSE: a versatile and efficient RNA-Seq read counting tool. bioRxiv 053306. doi:http://dx.doi.org/10.1101/053306 https://www.gencodegenes.org/mouse/ Anders, S. &amp; Huber, W. Differential expression analysis for sequence count data. Genome Biology 11, 112 (2010). Love, M. I., Huber, W. &amp; Anders, S. Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biology 15, 550 (2014). "],["class-outlines.html", "A Class Outlines ", " A Class Outlines "],["week-1.html", "A.1 Week 1", " A.1 Week 1 Course Intro &amp; Details Intro: Data in Biology Prelim: RStudio on SCC Prelim: The R Script Prelim: The Scripting Workflow Comm: RMarkdown &amp; knitr Prelim: Git + github R Prog: R Syntax Basics R Prog: Functions R Prog: Troubleshooting and Debugging Data Wrangle: The Tidyverse Data Wrangle: Tidyverse Basics Data Wrangle: Importing Data Bioinfo: CSV Files Data Wrangle: The tibble Data Wrangle: pipes Data Wrangle: Arranging Data Data Wrangle: Rearranging Data Assignment 1 "],["week-2.html", "A.2 Week 2", " A.2 Week 2 Assignment 1 Data Wrangle: Regular expressions Bioinfo: R in Biology Bioinfo: Types of Biological Data Bioinfo: Bioconductor Bioinfo: Gene Identifiers Bioinfo: Mapping Between Identifier Systems Bioinfo: Mapping Homologs Data Wrangle: Relational Data Data Viz: Grammar of Graphics Data Viz: Plotting One Dimension Data Viz: Visualizing Distributions R Prog: Unit Testing "],["week-3.html", "A.3 Week 3", " A.3 Week 3 Assignment 1 Review Assignment 2 Data Sci: Data Modeling Data Sci: A Worked Modeling Example Data Sci: Data Summarization Data Sci: Linear Models Data Sci: Flavors of Linear Models Bioinfo: Gene Expression Bioinfo: Gene Expression Data in Bioconductor Bioinfo: Microarrays Bioinfo: Microarray Gene Expression Data Bioinfo: Differential Expression: Microarrays (limma) "],["week-4.html", "A.4 Week 4", " A.4 Week 4 Assignment 3 Data Sci: Exploratory Data Analysis Data Sci: Principal Component Analysis Data Sci: Cluster Analysis Data Sci: Hierarchical Clustering Data Viz: Heatmaps Data Viz: Specifying Heatmap Colors Data Viz: Dendrograms Assignment 2 Review R Prog: Data structures R Prog: Factors R Prog: Iteration "],["week-5.html", "A.5 Week 5", " A.5 Week 5 Assignment 4 Bioinfo: High Throughput Sequencing Bioinfo: Count Data Bioinfo: RNASeq Bioinfo: RNASeq Gene Expression Data Bioinfo: Filtering Counts Bioinfo: Count Distributions Bioinfo: Count Normalization Bioinfo: Count Transformation Bioinfo: Differential Expression: RNASeq Assignment 3 Review Bioinfo: DESeq2/EdgeR Bioinfo: limma/voom Bioinfo: Gene Set Enrichment Analysis Bioinfo: Gene Sets Bioinfo: Over-representation Analysis Bioinfo: Rank-based Analysis Bioinfo: fgsea "],["week-6.html", "A.6 Week 6", " A.6 Week 6 Assignment 5 Data Sci: Statistical Distributions Data Sci: Random Variables Data Sci: Statistical Distribution Basics Data Sci: Distributions in R Data Sci: Discrete Distributions Data Sci: Continuous Distributions Data Sci: Empirical Distributions Assignment 4 Review Data Sci: Statistical Tests Data Sci: p-values Data Sci: [Multiple Hypothesis Testing] Data Sci: Statistical power R Prog: Coding Style and Conventions R Prog: [The styler package] "],["week-7.html", "A.7 Week 7", " A.7 Week 7 [Assignment 6] Data Viz: [Responsible Plotting] Data Viz: [Grammar of Graphics - Going Deeper] Data Viz: [Plotting Two or More Dimension] Data Viz: [Other Kinds of Plots] Data Viz: [Tips and Tricks] Assignment 5 Review Data Viz: [How To Use Heatmaps Responsibly] Data Viz: [Multiple Plots] Data Viz: [Facet wrapping] Data Viz: [Publication Ready Plots] "],["week-8.html", "A.8 Week 8", " A.8 Week 8 [Assignment 7] Rshiny: Rshiny [Assignment 6] Review "],["week-9.html", "A.9 Week 9", " A.9 Week 9 [Assignment 7] Checkin Bioinfo: [Single Cell Sequencing] Bioinfo: [Single Cell Clustering Methods] Bioinfo: [tSNE &amp; UMAP] Bioinfo: [Seurat] "],["week-10.html", "A.10 Week 10", " A.10 Week 10 Bioinfo: [Biological Pathways] Bioinfo: [Gene Regulatory Networks] Bioinfo: [Protein-Protein Networks] Bioinfo: [WGCNA] Data Sci: [Network Analysis] Data Viz: [Network visualization] "],["week-11.html", "A.11 Week 11", " A.11 Week 11 [Assignment 7] Review EngineeRing: [Toolification] EngineeRing: [Parallel Processing] EngineeRing: [Workflow Managers] "],["week-12.html", "A.12 Week 12", " A.12 Week 12 EngineeRing: [Object Orientated Programming in R] EngineeRing: [Building R Packages] "],["week-13.html", "A.13 Week 13", " A.13 Week 13 Comm: [Sharing and Documenting Project Code] Comm: [Reproducible Analysis in R] Comm: [Data Documentation and Publishing] "],["week-14---no-class-for-project-work.html", "A.14 Week 14 - no class for project work", " A.14 Week 14 - no class for project work "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
