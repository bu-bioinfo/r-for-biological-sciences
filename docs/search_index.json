[["index.html", "BF591 - R for Biological Sciences Syllabus Course Schedule Instructors Office Hours Course Values and Policies", " BF591 - R for Biological Sciences Syllabus Semester: Spring 2022 Location: EPC 209 and zoom Time: M/W 8:00AM - 9:45AM Contents: Course Schedule Instructors Office Hours Course Values and Policies This course introduces the R programming language through the lens of practitioners in the biological sciences, particularly biology and bioinformatics. Key concepts and patterns of the language are covered, including: RStudio Data wrangling with tidyverse Data visualization with ggplot Essential biological data shapes and formats Core bioconductor packages Basic data exploration, including elementary statistical modeling and summarization Elementary Data Science concepts Toolifying R scripts Communicating R code and results with RMarkdown Buidling R packages and unit testing strategies Building interactive tools with RShiny About 1/3 of the materials are inspired by the online textbook R for Data Science, while the rest has been developed by practicing bioinformaticians based on their experiences. Weekly programming assignments will help students apply these techniques to realistic problems involving analysis and visualization of biological data. Students will be introduced to a unit testing paradigm that will help them write correct code and deposit all their code into github for evaluation. Students will implement an end-to-end project that begins with one of a set of provided datasets, implements a set of data summarization and exploration operations on it, and allows interaction with an RShiny app. The course materials are aligned with BF528 Applications in Translational Bioinformatics and are intended to be taken in tandem, but the materials also stand alone as an independent class. Course Schedule Key: bold - class held over zoom struckout - class not held, reserved for project work Week Dates Topics Assignment Week 1 1/24 &amp; 1/26 Preliminaries Data in Biology R Programming Basics Data Wrangling &amp; tidyverse Basics Assignment 1 Week 2 1/31 &amp; 2/2 R in Biology Bio: Bioconductor Basics Data Viz: Grammar of Graphics Assignment 2 Week 3 2/7 &amp; 2/9 Data Sci: Data Modeling Bio: Gene Expression Bio: Differential Expression: Microarrays Data Sci: Clustering [Assignment 3] Week 4 2/14 &amp; 2/16 Data Sci: Distributions &amp; Tests R Programming: Structures &amp; Iteration Data Sci: Summarization &amp; Cleaning Bio: Gene Set Enrichment Analysis [Assignment 4] Week 5 2/22 &amp; 2/23 Bio: High Throughput Sequencing Bio: Differential Expression: RNASeq R Programming: Style &amp; Conventions [Assignment 5] Week 6 2/28 &amp; 3/2 Data Viz: Responsible Plotting [Assignment 6] Week 7 3/14 &amp; 3/16 Bio: Single Cell Sequencing R Programming: Pipelines &amp; Workflows [Assignment 7] Week 8 3/21 &amp; 3/23 RShiny [Assignment 8] Week 9 3/28 &amp; 3/30 Bio: Biological Networks Data Sci: Network Analysis Data Viz: Network Viz [Project] Week 10 4/4 &amp; 4/6 [Toolification] Week 11 4/11 &amp; 4/13 EngineeRing Week 12 4/20 Communicating with R Week 13 4/25 &amp; 4/27 No class - Project work Week 14 5/2 &amp; 5/4 No class - Project work Project due Instructors Primary instructor: Adam Labadorf (labadorf AT bu DOT edu) We have three staff members helping with the course, in no particular order (ok, I guess its alphabetical): Taylor Falk (BU BF MS Alumnus 21) is a Bioinformatics Developer working with the VA PTSD Brain Bank, developing infrastructure to support data generated out of our brains. He is championing the assignment strategy and will be available to help with issues. Mae Rose Gott (BU BF MS Alumna 21) is a Research Staff member working on a number of different projects across many different areas. She will be helping organize the course materials as we go forward. Vanessa Li (BU BF PhD Candidate) is a PhD candidate in Dr. Stefano Montis lab in Computational Biomedicine. She will primarily be helping with grading of your assignments. Joey Orofino (BU BF MS Alumnus 18) is a Research Scientist working on identifying small RNA based biomarkers of Parkinsons Disease Office Hours We hold the following office hours via Zoom: Joey - Mondays 2:30-3:30 Vanessa - Tuesdays 10-11AM Mae Rose - Wednesdays 8-9PM Taylor - Fridays 2-3PM The zoom link is pinned in the #announcements channel in slack. Course Values and Policies Everyone is welcome. Every background, race, color, creed, religion, ethnic origin, age, sex, sexual orientation, gender identity, nationality is welcome and celebrated in this course. Everyone deserves respect, patience, and kindness. Disrespectful language, discrimination, or harassment of any kind are not tolerated, and may result in removal from class or the University. This is not merely BU policy. The instructors deem these principles to be inviolable human rights. Students should feel safe reporting any and all instances of discrimination or harassment to the instructor, to any of the Bioinformatics Program leadership, or the BU Equal Opportunity Office. Everyone brings value. Each of us brings unique experiences, skills, and creativity to this course. Our diversity is our greatest asset. Collaboration is highly encouraged. All students are encouraged to work together and seek out any and all available resources when completing projects in all aspects of the course, including sharing coding ideas and strategies with each other as well as those found on the internet. Any and all available resources may be brought to bear. However, consistent with BU policy, the bulk of your code and your final reports should be written in your own words and represent your own work and understanding of the material. Copying/pasting large sections of code is not acceptable and will be investigated as cheating (we check). A safe space for dissent. For complex topics such as those covered in this class, there is seldom one correct answer, approach, or solution. Disagreement fosters innovation. All in the course, including students and TAs, are encouraged to express constructive criticism and alternative ideas on any aspect of the content. We are always learning. Our knowledge and understanding is always incomplete. Even experts are fallible. The bioinformatics field evolves rapidly, and Rome was not built in a day. Be kind to yourself and to others. You are always smarter and more knowledgable today than you were yesterday. "],["introduction.html", "1 Introduction 1.1 Who This Book Is For 1.2 A Note About Reinventing the Wheel 1.3 Sources and References", " 1 Introduction Since the publication of the first draft human genome in 2001, data has driven biological discovery at an exponential rate. Rapid technological innovations in data-generating biochemical instruments, computational resource availability, data storage, and analytical approaches including artificial intelligence, machine learning, and data science more generally have combined and synergized to enable advances in our understanding of biological systems by orders of magnitude. As the rate of development of these technologies has increased, so are practitioners of biological inquiry expected to keep up with the rapidly expanding set of knowledge, skills, and tools required to use them. Modern biological data analysis entails combining knowledge and skills from many domains, including not only biological concepts like molecular biology, genetics, genomics, and biochemistry, but also in computational and quantitative skills including statistics, mathematics, programming and software engineering, high performance and cloud computing, data visualization, and computer science. No one person can be expert in all of these areas, but modern software tools and packages made available by subject matter experts enable us to perform cutting edge analysis with a conceptual understanding of the topics. One such tool is the R programming language, a statistical programming language and environment specifically designed to run statistical analyses and visualize data. Today R is one of the two most popular programming languages in biological data analysis and bioinformatics (the other being python). A major innovation in the R language came with the introduction of the tidyverse, as set of open-source data manipulation and visualization packages, first developed by Hadley Wickham and now improved, supported, and maintained by his team of data scientists and software engineers and other individuals. The tidyverse is a collection of packages that specialize in different aspects of data manipulation with the goal of enabling powerful, consistent, and accurate data operations in the broad field of data science. While not changing the structure of the language per se, the tidyverse packages define a set of consistent programming conventions and patterns that are tailored to the types of manipulations required to make data tidy and, therefore, easier and more consistent to work with. The tidyverse therefore is something of its own language that is compatible with but distinct in convention from the base R language. This book and accompanying course focus on how to use R and its related package ecosystems to analyze, visualize, and communicate biological data analyses. As noted above, effective biological data analysis employs skills that span several knowledge domains. This book covers many of these topics in relatively shallow depth, but with the intent of presenting just enough in each to enable the learner to become proficient in most day-to-day biological analysis tasks. 1.1 Who This Book Is For This book was written for the practicing biologist wishing to learn how to use R to analyze biological data. A basic working knowledge of genetics, genomics, molecular biology, and biochemistry is assumed, but we endeavored to include enough pertinent background to understand the analysis concepts presented in the text. Basic knowledge of statistics is assumed, but again some background is provided as necessary to understand the analyses and concepts in the text. No further knowledge is assumed. 1.2 A Note About Reinventing the Wheel Many topics in this book are covered elsewhere in greater detail and depth. The content in each section is intended to stand alone, but may not provide a high level of detail that has been done better by others in online materials. These sections provide links to these other resources that provide more information, in case the instructions in this book are too terse or unclear. Wikipedia - Reinventing the Wheel 1.3 Sources and References The materials of this book were inspired and informed by a large number of sources, including books and freely available online materials. The authors would like to thank the creators and maintainers of these resources for their generosity in making their valuable contributions: R Materials * Hands-On Programming with R, by Garrett Grolemund * R for Data Science, by Hadley Wickam, Garrett Grolemund, et al * Advanced R, by Hadley Wickam * STAT 545 - Data wrangling, exploration, and analysis with R * What They Forgot to Teach You About R * Reproducible Analysis with R, by State of Alaskas Salmon and People Project, NCEAS * Data Science for Psychologists, by Hansjörg Neth Data visualization * How Charts Lie: Getting Smarter about Visual Information, by Alberto Cairo * The Functional Art - An Introduction to Information Graphics and Visualization, by Alberto Cairo * The Truthful Art - Data, Charts, and Maps for Communication, by Alberto Cairo "],["data-bio.html", "2 Data in Biology 2.1 A Brief History of Data in Molecular Biology 2.2 Biology as a Mature Data Science", " 2 Data in Biology 2.1 A Brief History of Data in Molecular Biology Molecular biology became a data science in 1953 when the structure of DNA was determined. Prior to this advance, biochemical assays of biological systems could make general statements about the characteristics and composition of biological macromolecules (e.g. there are two types of nucleic acids - those made of ribose (RNA) and deoxyribose (DNA)) and some quantitative statements about those compositions (e.g. there are roughly equal concentrations of purines - adenine, guanine - and pyrimidines - cytosine, thymine - in any single chromosome). However, once it was shown that each nucleic acid molecule had a specific (and eventually measurable) sequence, this opened the possibility of defining the genetic signature of every living thing on Earth which, in principle, would enable us to understand how life works from its most basic components. A tantalizing prospect, to say the least. It is perhaps a happy coincidence that our computational and data storage technologies began developing around the same time these molecular biology advances were being made. While mechanical computers had existed for more than a hundred years and arguably longer, the first modern computer, the Atanasoff-Berry Computer (ABC), was invented by John Vincent Atanasoff and Clifford Berry in 1942 at what is now Iowa State University. Over the following decades, the speed, sophistication, and reliability of computing machines increased exponentially, enabling ever larger and faster computations to be performed. The development of computational capabilities necessitated technologies that stored information that these machines could use, both for instructions to tell the computers what operations they should perform and data they should use to perform them. Until the 1950s, the most commonly available mechanical data storage technologies like writing, phonographic cylinders and disks (a.k.a. records), and punch cards were impractical or unsuitable to create and store the amount of data needed by these computers. A newer technology, magnetic storage, originally proposed in 1888 by Oberlin Smith quickly became the standard way digital computers read and stored information. With these technological advances, the second half of the 20th century saw rapid advances in our ability to determine and study the properties and function of biological molecular sequences, primarily DNA and RNA (although the first biological sequences scientists determined were proteins composed of amino acids using methods independently invented by Frederick Sanger and Pehr Edman). In 1970, Pauline Hogeweg and Ben Hesper defined the new discipline of bioinformatics as the study of informatic processes in biotic systems (in fact, the original term was proposed in Dutch, Hogeweg and Hespers native language). This early form of bioinformatics was a subfield of theoretical biology, studied by those who recognized that biological systems, much like our computer systems, can be viewed as information storage and processing systems themselves. This broad definition of bioinformatics began narrowing in practice to the study of genetic information as the amount of molecular sequence data we collected grew. By the early 1980s, biological sequence data stored on magnetic tape were being created and studied using new pattern recognition algorithms on computers, which were becoming more widely available at academic and research institutions. At the same time, the idea of determining the complete sequence of the human genome was born, leading to the inception of the The Human Genome Project and the present modern post genomic era. Biological Data Timeline - Setting the Stage 2.2 Biology as a Mature Data Science The completion of the first draft human genome ushered in a revolution in how we understand ourselves as humans, from our evolutionary history, our ancestry, our traits, and our health. It provided fundamentally new and empirical tools and approaches to human genetic and biomedical research, and the technologies and techniques that were developed in the completion of the draft sequence formed the foundation for genetic research in non-human systems as well. Biological Data Timeline - Human Genome Era While the focus of the human genome project was on determining the DNA sequence of the human genome, this sequence and the technologies used to ascertain it provide us with opportunities to learn many other properties of genomes and biological systems by analyzing the data with different approaches. For example, knowing the complete sequence of a genome also provides information on the number of genes it contains, how repetitive the sequence is, and when combined with genetic sequences of other individuals or organisms, how closely related genes or even organisms as a whole are. Thanks to the central dogma of molecular biology, the gene sequences also give us information about the intermediate RNA molecule and resultant proteins encoded by a genome, creating opportunities for new ideas, hypotheses, experiments, and even new data-generating assays and approaches. These advances are causing exponential growth of different types of biological data and its volume, necessitating ever more powerful and sophisticated computational resources and analytical methods with no signs of slowing. The biochemical instruments used to produce these data are continually improving the precision, accuracy, throughput, and cost of their output and operations. The Biologists Tools "],["preliminaries.html", "3 Preliminaries 3.1 The R Language 3.2 RStudio 3.3 The R Script 3.4 The Scripting Workflow 3.5 git + github", " 3 Preliminaries 3.1 The R Language R is a free programming language and environment where that language can be used. More specifically, R is a statistical programming language, designed for the express and exclusive purpose of conducting statistical analyses and visualizing data. Said differently, R is not a general purpose programming language unlike other languages such as python, Java, C, etc. As such, the languages real strengths are in the manipulation, analysis, and visualization of data and statistical procedures, though it is often used for other purposes (for example, web applications with RShiny, or writing books like this one with bookdown). You may download R for free from the Comprehensive R Archive Network. The effective biological analysis practitioner knows how to use multiple tools for their appropriate purposes. The most common programming languages in this field are python, R, and scripting languages like The Bourne Again Shell - bash. While it is beyond the scope of this book to cover which tools are best used where, R is appropriate wherever data analysis and visualization are needed. Any operations that do not involve these aspects (e.g. manipulating text files, programming web servers, etc) are likely more suitable for other languages and software. R Project Home Page Hands-On Programming with R - Installing R and RStudio Section on R from R for Data Science 3.2 RStudio This course assumes the learner is using the RStudio software platform for all analyses, unless otherwise noted. RStudio is a freely available and fully featured integrated development environment (IDE) for R, and has many convenient capabilities when learning R. RStudio may be downloaded and installed for free from the site above. All the examples and instructions in this book assume you have installed R are using RStudio. Be sure to turn off automatic environment saving in RStudio! Because this is so important, here it is again: By default, RStudio preserves your R environment when you shut it down and restores it when you start it again. This is very bad practice! The state of your R environment, which includes the values stored in variables, the R packages loaded, etc. from previously executed code is transient and may not reflect the results your code produces when run alone. Open the Tools &gt; Global Options menu and: Uncheck Restore .RData into workspace at startup Set Save workspace to .RData on exit: to Never Never save workspace The book R for Data Science has an excellent chapter on why this is a problem and how to change the RStudio setting to avoid it. Hands-On Programming with R - Installing R and RStudio RStudio Education - Beginners Guide ModernDrive R Series - Getting Started R for Data Science - What is real, and where does your analysis live? What They Forgot To Teach You About R - Always start R with a blank slate R for Data Science - RStudio 3.3 The R Script Before we cover the R language itself, we should talk about how you should run your code and where it should live. As mentioned, R is both a programming language and an environment where you can run code written in that language. The environment is a program (confusingly also called R) that allows you to interact with it and run simple lines of code one at a time. This environment is very useful for learning how the language works and troubleshooting, but it is not suitable for recording and running large, complex analyses that require many lines of code. Therefore, all important R code should be written and saved in a file before you run it! The code may not be correct, and the interactive R environment is helpful for debugging and troubleshooting, but as soon as the code works it should be saved to the file and rerun from there. With this in mind, the basic unit of an R analysis is the R script. An R script is a file that contains lines of R code that run sequentially as a unit to complete one or more tasks. Every R script file has a name, which you choose and should be descriptive but concise about what the script does; script.R, do_it.R, and a_script_that_implements_my_very_cool_but_complicated_analysis_and_plots.R are generally poor names for scripts, whereas analyze_gene_expression.R might be more suitable. In RStudio, you can create a new script file in the current directory using the File -&gt; New File -&gt; R Script menu item or the new R Script button at the top of the screen: New R Script Your RStudio configuration should now enable you to write R code into the (currently unsaved) file in the top left portion of the screen (labeled in the figure as File Editor). Basic RStudio Interface You are now nearly ready to start coding in R! How to name files Some useful and advanced tips on how to name files 3.4 The Scripting Workflow But hold on, were still not quite ready to start coding. As mentioned above, all important R code should be written and saved in a file before you run it! Your scripts will very quickly contain many lines of code that are meant to be run in sequential order. While developing your code it is very helpful to run each individual line separately, building up your script incrementally over time. To illustrate how to do this, we will begin with a simple R code that stores the result of an arithmetic expression to a new variable: # stores the result of 1+1 into a variable named &#39;a&#39; a &lt;- 1+1 The concepts in this line of code will be covered in greater depth later, but for now an intuitive understanding will suffice to explain the development workflow in RStudio. When developing, this is the suggested sequence of operations: Save your file (naming if necessary on the first save) with Ctrl-s on Windows or Cmd-s on Mac Execute the line or lines of code in your script you wish to evaluate using Ctrl-Enter on Windows or Cmd-Enter on Mac. By default only the line with the cursor is executed; you may click and drag with the mouse to select multiple lines to execute if needed. NB: you can press the up arrow key to recall previously run commands on the console. The executed code will be evaluated in the Console window, where you may inspect the result and modify the code if necessary. You may inspect the definitions of any variables you have declared in the Environment tab at the upper right. When you have verified that the code you executed does what you intend, ensure the code in the file you started from is updated appropriately. Go to step 1 The above steps are depicted in the following figure: RStudio workflow Over time, you will gain comfort with this workflow and become more flexible with how you use RStudio. If you followed the instructions above and prevented RStudio from saving your environment when you exit the program (which you should! Did I mention you should?!), none of the results of code you previously ran will be available upon starting a new RStudio session. Although this may seem inconvenient, this is an excellent opportunity to verify that your script in its current state does what you intend for it to do. It is extremely easy to ask R to do things you dont mean for it to do! Rerunning your scripts from the beginning in a new RStudio session is an excellent way to guard against this kind of error. This short page summarizes this very well, you should read it: What They Forgot To Teach You About R - Always start R with a blank slate R for Data Science - Workflow: scripts RStudio IDE cheatsheet (scroll down the page to find the cheatsheet entitled RStudio IDE cheatsheet) 3.5 git + github 3.5.1 Motivation Biological analysis entails writing code, which changes over time as you develop it, gain insight from your data, and identify new questions and hypotheses. A common pattern when developing scripts is to make copies of older code files to preserve them before making new changes to them. While it is a good idea to maintain a record of all the code you previously ran, over time this practice often leads to disorganized, cluttered, untidy analysis directories. For example, say you are working on a script named my_R_script.R and decide you want to add a new analysis that substantially changes the code. You might be tempted to make a copy of the current version of the code into a new file named my_R_script_v2.R that you then make changes to, leaving your original script intact and untouched going forward. You make your changes to your new script, produce some stunning and fascinating plots, present the analysis at a group meeting, only to discover later there was a critical bug in your code that made the plots misleading and requires substantial redevelopment. Bugs happen. There are two types of bugs: Syntax bugs: bugs due to incorrect language usage, which R will tell you about and can (usually) be easily identified and fixed Logic bugs: the code you write is syntactically correct, but does something other than what you intend Bugs are normal. The scenario described above, where you present results only to discover your code wasnt doing what you thought it was doing, is extremely common and it will happen to you. This is normal, and finding a bug in your code does not mean you are a bad programmer. Rather than edit your version 2 of your script directly, you decide it is sensible to copy the file to my_R_script_v2_BAD.R and edit the version 2 script to fix the bug. You are satisfied with your new version 2 script, and so make a new copy my_R_script_v2_final.R. Upon review of your analysis, you are asked to implement new changes to the script based on reviewer feedback. You make a new copy of your script to my_R_script_v2_final_revision.R and make the requested changes. Perhaps now your script is final, but in your directory you now have five different versions of your analysis: my_R_script.R my_R_script_v2.R my_R_script_v2_BAD.R my_R_script_v2_final.R my_R_script_v2_final_revision.R When you write your code, you may know which scripts are which, and if you follow good programming practice and carefully commented your code you or your successors may be able to sleuth what was done. However, as time passes, the intimate knowledge you thought you had about your code will be replaced by other more immediately important things; eventually you may not even understand or even recognize your own code, let alone someone else trying to understand it. Not an ideal situation in any case. A better solution involves recording changes to code over time in such a way that you can recover old code if needed, but dont clutter your analytical workspace with unneeded files. git provides an efficient solution to this problem. 3.5.2 git git is a free, open source version control software program. Version control software is used to track and record changes to code over time, potentially by many developers working on the same software project concurrently from different parts of the world. The base git software can be used on the command line, or with graphical user interface applications for popular operating systems. There are many excellent tutorials online (some linked below) that teach how to use git but the basic concepts are described below. The command line commands are listed, but the same operations apply in the graphical clients. A repository (or repo) is a collection of files in a directory that you have asked git to track (run git init in a new directory) Each file you wish to track must be explicitly added to the repo (run git add &lt;filename&gt; from within the git repo directory) When you modify a tracked file, git will notice those differences and show them to you with git status You may tell git to track the changes to the explicit files that changed (also run git add &lt;filename&gt; to record changes) A set of tracked changes is stored in the repo by making a commit. A commit takes a snapshot of all the tracked files in the repo at the time the commit is made (run git commit -c &lt;commit message&gt; with a concise commit message that describes what was done) Each commit has a date and time associated with it. The files in the repo can be reset to exactly the state they were in at any commit, thus preserving all previous versions of code. For the vast majority of use cases, the git init, git status, git add, and git commit operations are all you will need to use git effectively. Two more commands, git push and git pull are needed when sharing your code with others as described in the next section. Official git tutorial videos Official git book Git Immersion - a guided tour through git commands DataCamp - Git for data scientists 3.5.3 Git hosting platforms (GitHub) The basic git software only works on your local computer with local repositories. To share this code with others, and receive others contributions, a copy of the repo must be made available in a centralized location that everyone can access. One such place is github.com, which is a free web application that hosts git repos. bitbucket.org is another popular free git repo hosting service. These two services are practically the same, so we will focus on GitHub. There is no formal relationship between git and GitHub. git is an open source software project maintained by hundreds of developers around the world (and is hosted on GitHub). GitHub is an independently provided web service and application. The only connection between GitHub and git is that GitHub hosts git repos. As with git, there are many excellent tutorials on how to use GitHub, but the basic concepts are described below. First you must create an account on GitHub if you dont have one already Then, create a new repo on GitHub that you wish to contain your code The next step depends on whether you have an existing local repo or not: If you do not already have a local git repo: Follow the instructions on GitHub to clone your GitHub repo and create a local copy that is connected to the one on GitHub If you already have a local git repo: Follow the instructions on the GitHub to connect your local repo to the GitHub one (this is called adding a remote) Now, your local repo is connected to the same repo on GitHub, and the changes you make to your local files can be sent, or pushed to the repo on GitHub: Make changes to your local files, and git add and git commit them as above Update the remote repo on GitHub by pushing your local commits with git push Running git status will indicate whether your local repo is up to date with your remote GitHub repo When you are working on a team of contributors to a GitHub repo, your local files will become out of date as others push their changes. To ensure your local repo is up to date with the GitHub repo, you must pull your changes from GitHub with git pull. git was designed to automatically combine changes made to a code base by different developers whenever possible. However, if two people make changes to the same parts of the same file, git may not be able to resolve those changes on its own and the developers must communicate and decide what the code should be. These instances are called merge conflicts and can be challenging to resolve. Dealing with merge conflicts is beyond the scope of this book, but some resources are linked below for further reading. All the content and code for this book are stored and available on GitHub, as are the assignment code templates. Official GitHub Tutorial FreeCodeCamp - Git and GitHub For Beginners Official GitHub Tutorial on Merge Conflicts "],["prog-basics.html", "4 R Programming 4.1 Before you begin 4.2 Introduction 4.3 R Syntax Basics 4.4 Basic Types of Values 4.5 Data Structures 4.6 Logical Tests and Comparators 4.7 Functions 4.8 Iteration 4.9 Installing Packages 4.10 Saving and Loading R Data 4.11 Troubleshooting and Debugging 4.12 Coding Style and Conventions", " 4 R Programming 4.1 Before you begin If you have not done so already, be sure to follow the R Language, RStudio, and The R Script, and The Scripting Workflow sections before working through this chapter. The guidance in these sections will set you up for success! 4.2 Introduction As with other subjects covered so far, the basic syntax of R is covered very well in other free online materials. Some of those excellent resources are linked at the end of this section, but a brief overview of the concepts and syntax are covered here. The code examples below can be written into a script and evaluated as described above or entered on the R Console directly and run by pressing Enter. 4.3 R Syntax Basics At its core, R (like all programming languages) is basically a fancy calculator. The syntax of most basic arithmetic operations in R should be familiar to you: 1 + 2 # addition [1] 3 3 - 2 # subtraction [1] 1 4 * 2 # multiplication [1] 8 4 / 2 # division [1] 2 1.234 + 2.345 - 3.5*4.9 # numbers can have decimals [1] -13.571 1.234 + (2.345 - 3.5)*4.9 # expressions can contain parentheses [1] -4.4255 2**2 # exponentiation [1] 4 4**(1/2) # square root [1] 2 9**(1/3) # cube root [1] 3 The [1] lines above are the output given by R when the preceding expression is executed. Any portion of a line starting with a # is a comment and ignored by R. R also supports storing values into symbolic placeholders called variables, or objects. An expression like those above can be assigned into a variable with a name using the &lt;- operator: new_var &lt;- 1 + 2 Variables that have been assigned a value can be placed in subsequent expressions anywhere where their value is evaluated: new_var - 2 [1] 1 another_var &lt;- new_var * 4 The correct way to assign a value to a variable in R is with the &lt;- syntax, unlike many other programming languages which use =. However, although the = assignment syntax does work in R: new_var = 2 # works, but is not common convention! this is considered bad practice and may cause confusion later. You should always use the &lt;- syntax when assigning values to variables! In R, the period . does not have a special meaning like it does in many other languages like python, C, javascript, etc. Therefore, new.var is a valid variable name just like new_var, even though it may look strange to those familiar with these other languages. While including . in your R variable names is valid, the results that you will use in programs written in other languages that do have a meaning for this character. Therefore, it is good practice to avoid using . characters in your variable names to reduce the chances of conflicts later. Hands-On Programming with R R for Data Science - Workflow basics 4.4 Basic Types of Values The most common type of value in R is the number, e.g. 1.0 or 1e-5 for \\(10^{-5}\\). For most practical purposes, R does not distinguish between numbers with fractional parts (e.g. 1.123) and integers (e.g. 1); a number is a number. In addition to numbers, there are some other types of values that are special in R: logical or boolean values - TRUE or FALSE. Internally, R stores TRUE as the number 1 and FALSE as the number 0. Generally, R interprets non-zero numbers as TRUE and 0 as FALSE, but it is good practice to supply the tokens TRUE or FALSE when an argument expects a logical value. missing values - NA. NA is a special value that indicates a value is missing. missing vectors - NULL. Similar to NA, NULL indicates that a vector, rather than a value, is missing. Vectors will be described in the next section on data strutures. factors - Factors are a complex type used in statistical models and are covered in greater detail later infinity - Inf and -Inf. These values encode what R understands to be positive or negative infinity, or any number divided by 0. impossible values - NaN. This value corresponds to the mathematically impossible or undefined value of 0/0. character data - \"value\". R can store character data in the form of strings. Note R does not interpret string values by default, so \"1\" and 1 are distinct. dates and times - R has a basic type to store dates and times (together termed a datetime, which includes both components). Internally, R stores datetimes as the fractional number of days since January 1, 1970, using negative numbers for earlier dates. complex numbers - R can store complex numbers using the complex function. Unsurprisingly, R cannot perform computations on NA, NaN, or Inf values. Each of these values have an infectious quality to them, where if they are mixed in with other values, the result of the computation reverts to the first of these values encountered: # this how to create a vector of 4 values in R x &lt;- c(1,2,3,NA) mean(x) # compute the mean of values that includes NA [1] NA mean(x,na.rm=TRUE) # remove NA values prior to computing mean [1] 2 mean(c(1,2,3,NaN)) [1] NaN mean(c(NA,NaN,1)) [1] NA If your code produces values that are not numbers as you expect, this suggests there are one of these values in your input, and need to be handled explicitly. The difference between NA and NaN in R R for Data Science - Dates and date-times Dates and times in R Complex numbers in R 4.4.1 Factors . What is a factor in R? Why and how are they used? What is the difference between a character vector and a factor vector? Why and how do we manipulate factors (e.g. relevel)? 4.5 Data Structures 4.5.1 Vectors Data structures in R (and other languages) are ways of storing and organizing more than one value together. The most basic data structure in R is a one dimensional sequence of values called a vector: # the c() function creates a vector x &lt;- c(1,2,3) [1] 1 2 3 The vector in R has a special property that all values contained in the vector must have the same type, from the list described above. When constructing a vector, R will coerce values to the most general type if it encounters values of different types: c(1,2,&quot;3&quot;) [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; c(1,2,TRUE,FALSE) [1] 1 2 1 0 c(1,2,NA) # note missing values stay missing [1] 1 2 NA c(&quot;1&quot;,2,NA,NaN) # NA stays, NaN is cast to a character type [1] &quot;1&quot; &quot;2&quot; NA &quot;NaN&quot; In addition to having a single type, vectors also have a length, which is defined as the number of elements in the vector: x &lt;- c(1,2,3) length(x) [1] 3 Internally, R is much more efficient at operating on vectors than individual elements separately. With numeric vectors, you can perform arithmetic operations on vectors of compatible size just as easily as individual values: c(1,2) + c(3,4) [1] 4 6 c(1,2) * c(3,4) [1] 3 8 c(1,2) * c(3,4,5) # operating on vectors of different lengths raises warning, but still works [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length In the example above, we multiplied a vector of length 2 with a vector of length 3: c(1,2) * c(3,4,5) # operating on vectors of different lengths raises warning, but still works [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length Rather than raise an error and aborting, R merely emits a warning message about the vectors not having divisible lengths. So how did R decide the third value should be 5? Because R cycles through each vector and multiplies the values element-wise until the longest vector has had an operation performed on all its values: c(1,2) * c(3,4,5) # yields: 1*3 2*4 1*5 [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length c(1,2) * c(3,4,5,6) # yields: 1*3 2*4 1*5 2*6 [1] 3 8 5 12 R will sometimes work in ways you dont expect. Be careful to read warnings and check that your code does what you expect! 4.5.2 Matrices A matrix in R is simply the 2 dimensional version of a vector. That is, it is a rectangle of values that all have the same type, e.g. number, character, logical, etc. A matrix may be constructed using the vector notation described above and specifying the number of rows and columns the matrix should have, and Instead of having a length like a vector, it has \\(m \\times n\\) dimensions: # create a matrix with two rows and three columns containing integers A &lt;- matrix(c(1,2,3,4,5,6) nrow = 2, ncol = 3, byrow=1 ) A [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 dim(A) # the dim function prints out the dimensions of the matrix, rows first [1] 2 3 Because a matrix is 2 dimensional, it can be transposed from \\(m \\times n\\) to be \\(n \\times m\\) using the t() function: # A defined above as a 2 x 3 matrix t(A) [,1] [,2] [1,] 1 4 [2,] 2 5 [3,] 3 6 dim(t(A)) [1] 3 2 Hands-On Programming with R - Atomic Vectors R for Data Science - Vectors Advanced R - Vectors 4.5.3 Lists and data frames Vectors and matrices have the special property that all items must be of the same type, e.g. numbers. Lists and data frames are data structures that do not have this requirement. Similar to vectors, lists and data frames are both one dimensional sequences of values, but the values can be of mixed types. For instance, the first item of a list may be a vector of numbers, while the second is a vector of character strings. These are the most flexible data structures in R, and are among the most commonly used. Lists can be created using the list() function: my_list &lt;- list( c(1,2,3), c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) my_list [[1]] [1] 1 2 3 [[2]] [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; my_list[[1]] # access the first item of the list [1] 1 2 3 my_list[[2]] # access the second item of the list [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; The arguments passed to list() define the values and their order of the list. In the above example, the list has two elements: one vector of 3 numbers and one vector of 4 character strings. Note you can access individual items of the list using the [[N]] syntax, where N is the 1-based index of the element. Lists can also be defined and indexed by name: my_list &lt;- list( numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) my_list $numbers [1] 1 2 3 $categories [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; my_list$numbers # access the first item of the list [1] 1 2 3 my_list$categories # access the second item of the list [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; The elements of the list have been assigned the names numbers and categories when creating the list, though any valid R identifier names can be used. When elements are associated with names they can be accessed using the list$name syntax. Lists and data frames are the same underlying data structure, however differ in one important respect: the elements of a data frame must all have the same length, while the elements of a list do not. You may create a data frame with the data.frame() function: my_df &lt;- data.frame( # recall &#39;.&#39; has no special meaning in R numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) Error in data.frame(c(1, 2, 3), c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) : arguments imply differing number of rows: 3, 4 my_df &lt;- data.frame( numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) ) my_df numbers categories 1 1 A 2 2 B 3 3 C my_df$numbers [1] 1 2 3 my_df[1] # numeric indexing also works, and returns a subset data frame numbers 1 1 2 2 3 3 my_df[1]$numbers [1] 1 2 3 # this syntax is [&lt;row&gt;,&lt;column&gt;], and if either is omitted return all my_df[,1] # return all rows of the first column as a vector [1] 1 2 3 my_df$categories [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; Note the data frame is printed as a matrix with element names as columns and automatically numbered rows. You may access specific elements of a data frame in a number of ways: my_df$numbers[1] # extract the first value of the numbers column [1] 1 my_df[1,1] # same as above, recall the [&lt;row&gt;,&lt;column&gt;] syntax [1] 1 my_df$categories[3] # extract the third value of the categories column [1] &quot;C&quot; In the examples above, the operation of extracting out different parts of a vector, matrix, list, or data frame is called subsetting. R provides many different ways to subset a data structure and discussing all of them is beyond the scope of this book. However, mastering subsetting will help your code be more concise and correct. See the Read More link on Subsetting below: Advanced R - Subsetting Advanced R - Data Structures Advanced R - Subsetting 4.6 Logical Tests and Comparators As mentioned above, R recognizes logical values as a distinct type. R provides all the conventional infix logical operators: 1 == 1 # equality [1] TRUE 1 != 1 # inequality [1] FALSE 1 &lt; 2 # less than [1] TRUE 1 &gt; 2 # greater than [1] FALSE 1 &lt;= 2 # less than or equal to [1] TRUE 1 &gt;= 2 # greater than or equal to These operators also work on vectors, albeit with the same caveats about vector length as noted earlier: x &lt;- c(1,2,3) x == 2 [1] FALSE TRUE FALSE x &lt; 1 [1] FALSE FALSE FALSE x &lt; 3 [1] TRUE TRUE FALSE c(1,2) == c(1,3) [1] TRUE FALSE c(1,2) != c(1,3) [1] FALSE TRUE c(1,2) == c(1,2,3) [1] TRUE TRUE FALSE Warning message: In c(1, 2, 3) == c(1, 2) : longer object length is not a multiple of shorter object length R also provides many functions of the form is.X where X is some type or condition (recall that . is not a special character in R): is.numeric(1) # is the argument numeric? [1] TRUE is.character(1) # is the argument a string? [1] FALSE is.character(&quot;ABC&quot;) [1] TRUE is.numeric(c(1,2,3)) # recall a vector has exactly one type [1] TRUE is.numeric(c(1,2,&quot;3&quot;)) [1] FALSE is.na(c(1,2,NA)) [1] FALSE FALSE TRUE Quick-R - Operators 4.7 Functions Just as a variable is a symbolic representation of a value, a function is a symbolic representation of code. In other words, a function allows you to substitute a short name, e.g. mean, for a set of operations on a given input, e.g. the sum of a set of numbers divided by the number of numbers. R provides a very large number of functions for common operations in its default environment, and more functions are provided by packages you can install separately. Encapsulating many lines of code into a function is useful for (at least) five distinct reasons: Make your code more concise and readable Allow you to avoid writing the same code over and over (i.e. reuse it) Allow you to systematically test pieces of your code to make sure they do what you intend Allow you to share your code easily with others Program using a functional programming style (see note box below) At its core, R is a functional programming language. The details of what this means are outside the scope of this book, but as the name implies this refers to the language being structured around the use of functions. While this property has technical implications on the structure of the language, a more important consequence is the style of programming it entails. The functional programming style (or paradigm) has many advantages, including generally producing programs that are more concise, predictable, provably correct, and performant. provides a good starting point for learning about functional programming. In order to do anything useful, a function must generally be able to accept and execute on different inputs; e.g. the mean function wouldnt be very useful if it didnt accept a value! The terminology used in R and many other programming languages for this is the function must accept or allow you to pass it arguments. In R, functions accept arguments using the following *pattern: # a basic function signature function_name(arg1, arg2) # function accepts exactly 2 arguments Here, arg1 and arg2 are formal arguments or named arguments, indicating this function accepts two arguments. The name of the function (i.e. function_name) and the pattern of arguments it accepts is called the functions signature. Every function has at least one signature, and it is critical to understand it in order to use the function properly. In the above example, arg1 and arg2 are required arguments. This means that the function will not execute without exactly two arguments provided and will raise an error if you try otherwise: mean() # compute the arithmetic mean, but of what? Error in mean.default() : argument &quot;x&quot; is missing, with no default How do you know what arguments a function requires? All functions provided by base R and many other packages include detailed documentation that can be accessed directly through RStudio using either the ? or help(): RStudio - help and function signatures The second signature of the mean function introduces two new types of syntax: Default argument values - e.g. trim = 0. These are formal arguments that have a default value if not provided explicitly. Variable arguments - .... This means the mean function can accept arguments that are not explicitly listed in the signature. With these definitions, we can now understand the Arguments section of the help documentation: Arguments to the mean function In other words: x is the vector of values (more on this in the next section on data structures) we wish to compute the arithmetic mean for trim is a fraction (i.e. a number between 0 and 0.5) that instructs R to remove a portion of the largest and smallest values from x prior to computing the mean. na.rm is a logical value (i.e. either TRUE or FALSE) that instructs R to remove NA values from x before computing the mean. All function arguments can be specified by name, regardless of whether there is a default value or not. For instance, the following two mean calls are equivalent: # this generates 100 normally distributed samples with mean 0 and standard deviation 1 my_vals &lt;- rnorm(100,mean=0,sd=1) mean(my_vals) [1] -0.05826857 mean(x=my_vals) [1] -0.05826857 To borrow from the Zen of Python, Explicit is better than implicit. Being explicit about which variables are being passed as which arguments will almost always make your code easier to read and more likely to do what you intend. The ... argument catchall can be very dangerous. It allows you to provide arguments to a function that have no meaning, and R will not raise an error. Consider the following call to mean: # this generates 100 normally distributed samples with mean 0 and standard deviation 1 my_vals &lt;- rnorm(100,mean=0,sd=1) mean(x=my_vals,tirm=0.1) [1] -0.05826857 Did you spot the mistake? The trim argument name has been misspelled as tirm, but R did not report an error. Compare the value of mean without the typo: mean(x=my_vals,trim=0.1) [1]-0.02139839 The value we get is different, because R recognizes trim but not tirm and changes its behavior accordingly. Not all functions have the ... catchall in their signatures, but many do and so you must be diligent when supplying arguments to function calls! R for Data Science - Functions rdrr.io - Base R Function Reference This tutorial Advanced R - Functional Programming 4.7.1 DRY: Dont Repeat Yourself Sometimes you will find yourself writing the same code more than once to perform the same operation on different data. For example, one common data transformation is standardization or normalization which entails taking a series of numbers, subtracting the mean of all the numbers from them, and dividing each by the standard deviation of the numbers: # 100 normally distributed samples with mean 20 and standard deviation 10 my_vals &lt;- rnorm(100,mean=20,sd=10) my_vals_norm &lt;- (my_vals - mean(my_vals))/sd(my_vals) mean(my_vals_norm) [1] 0 sd(my_vals_norm) [1] 1 Later in your code, you may need to standardize a different set of values, so you decide to copy and paste your code from above and replace the variable name to reflect the new data: # new samples with mean 40 and standard deviation 5 my_other_vals &lt;- rnorm(100,mean=40,sd=5) my_other_vals_norm &lt;- (my_other_vals - mean(my_other_vals))/sd(my_vals) mean(my_other_vals_norm) [1] 0 sd(my_other_vals_norm) # this should be 1! [1] 0.52351 Notice the mistake? We forgot to change the variable name my_vals to my_other_vals in our pasted code, which produced an incorrect result. Good thing we checked! In general, if you are copying and pasting code from one part of your script to another, you are repeating yourself and have to do a lot of work to be sure you have modified your copy correctly. Copying and pasting code is tempting from an efficiency standpoint, but introduces may opportunities for (often undetected!) errors. Dont Repeat Yourself (DRY) is a principle of software development that emphasizes recognizing and avoiding writing the same code over and over by encapsulating code. In R, this is most easily done with functions. If you notice yourself copying and pasting code, or writing the same pattern of code more than once, this is an excellent opportunity to write your own function and avoid repeating yourself! 4.7.2 Writing your own functions R allows you to define your own function using the following syntax: function_name &lt;- function(arg1, arg2, ...) { # code that does something with arg1, arg2, etc return(some_result) } You define the name of your function, the number of arguments it accepts and their names, and the code within the function, which is also called the function body. Taking the example above, I would define a function named standardize that accepts a vector of numbers, subtracts the mean from all the values, and divides them by the standard deviation: standardize &lt;- function(x) { res &lt;- (x - mean(x))/sd(x) return(res) } my_vals &lt;- rnorm(100,mean=20,sd=10) my_vals_std &lt;- standardize(my_vals) mean(my_vals_std) [1] 0 sd(my_vals_std) [1] 1 my_other_vals &lt;- rnorm(100,mean=40,sd=5) my_other_vals_std &lt;- standardize(my_other_vals) mean(my_other_vals_std) [1] 0 sd(my_other_vals_std) [1] 1 Notice above we are assigning the value of the standardize function to new variables. In R and other languages, the result of a function is returned when the function is called; the value returned is called the return value. The return() function makes it clear what the function is returning. The return() function is not strictly necessary in R; the result of the last line of code in the body of a function is returned by default. However, to again to borrow from the Zen of Python, Explicit is better than implicit. Being explicit about what a function returns by using the return() function will make your code less error prone and easier to understand. 4.7.3 Scope In programming, there is a critically important concept called scope. Every variable and function you define when you program has a scope, which defines where in the rest of your code the variable can be accessed. In R, variables defined outside of a function have universal or top level scope, i.e. they can be accessed from anywhere in your script. However, variables defined inside functions can only be accessed from within that function. For example: x &lt;- 3 multiply_x_by_two &lt;- function() { y &lt;- x*2 # x is not defined as a parameter to the function, but is defined outside the function return(y) } x [1] 3 multiply_x_by_two() [1] 6 y Error: object &#39;y&#39; not found Notice that the variable x is accessible within the function multiply_x_by_two, but the variable y is not accessible outside that function. The reason that x is accessible within the function is that multiply_x_by_two inherits the scope where it is defined, which in this case is the top level scope of your script, which includes x. The scope of y is limited to the body of the function between the { } curly braces defining the function. Accessing variables within functions from outside the functions scope is very bad practice! Functions should be as self contained as possible, and any values they need should be passed as parameters. A better way to write the function above would be as follows: x &lt;- 3 multiply_by_two &lt;- function(x) { y &lt;- x*2 # x here is defined as whatever is passed to the function! y } x [1] 3 multiply_by_two(6) [1] 12 x # the value of x in the outer scope remains the same, because the function scope does not modify it [1] 3 Every variable and function you define is subject to the same scope rules above. Scope is a critical concept to understand when programming, and grasping how it works will make your code more predictable and less error prone. Scope in R 4.8 Iteration In programming, iteration refers to stepping sequentially through a set or collection of objects, be it a vector of numbers, the columns of a matrix, etc. In non-functional languages like python, C, etc. there are particular control structures that implement iteration, commonly called loops. If you have worked with these languages, you may be familiar with for and while loops, which are some of these iteration control structures. However, R was designed to execute iteration in a different way than these other languages, and provides two forms of iteration: vectorized operations, and functional programming with apply(). Note that R does have for and while loop support in the language. However, these loop structures often have poor performance, and should generally be avoided in favor of the functional style of iteration described below. How To Avoid For Loops in R If you really, really want to learn how to use for loops in R, read this, but dont say I didnt warn you when your code slows to a crawl for unknown reasons: R for Data Science - for loops 4.8.1 Vectorized operations The simplest form of iteration in R comes in vectorized computation. This sounds fancy, but it just means R intrinsically knows how to perform many operations on vectors and matrices as well as individual values. We have already seen examples of this above when performing arithmetic operations on vectors: x &lt;- c(1,2,3,4,5) x + 3 # add 3 to every element of vector x [1] 4 5 6 7 8 x * x # elementwise multiplication, 1*1 2*2 etc [1] 1 4 9 16 25 x_mat &lt;- matrix(c(1,2,3,4,5,6),nrow=2,ncol=3) x_mat + 3 # add 3 to every element of matrix x_mat [,1] [,2] [,3] [1,] 4 6 8 [2,] 5 7 9 # the * operator always means element-wise x_mat * x_mat [,1] [,2] [,3] [1,] 1 9 25 [2,] 4 16 36 In addition to simple arithmetic operations, R also has syntax for vector-vector, matrix-vector, and matrix-matrix operations, like matrix multiplication and dot products: # the %*% operator stands for matrix multiplication x_mat %*% c(1,2,3) # [ 2x3 ] * [ 3 ] [,1] [1,] 22 [2,] 28 x_mat %*% t(x_mat) # recall t() is the transpose function, making [ 2x3 ] * [ 3x2 ] [,1] [,2] [1,] 35 44 [2,] 44 56 These forms of implicit iteration are very powerful, and the R program has been highly optimized to perform these operations very quickly. If you can cast your iteration into a vector or matrix multiplication, it is a good idea to do so. For other more complex or custom iteration, we must first talk briefly about functional programming. 4.8.2 Functional programming R is a functional programming language at its core, which means it is designed around the use of functions. In the previous section, we saw that functions are defined and assigned to names just like variables. This means that functions can be passed to other functions just like variables! Consider the following example. Lets consider a general formulation of vector transformation: \\[ \\bar{\\mathbf{x}} = \\frac{\\mathbf{x} - t_r(\\mathbf{x})}{s(\\mathbf{x})} \\] Here, \\(\\mathbf{x}\\) is a vector of real numbers, and \\(\\bar{\\mathbf{x}}\\) is defined as a vector of the same length where each value has had some average or central value \\(t_r(\\mathbf{x})\\) subtracted from it, and is divided by a scaling factor \\(s(\\mathbf{x})\\) to control the range of resulting values. Both \\(t_r(\\mathbf{x})\\) and \\(s(\\mathbf{x})\\) are scalars (i.e. individual numbers) and dependent upon the values of \\(\\mathbf{x}\\). If \\(t_r\\) is arithmetic mean and \\(s\\) is standard deviation, we have defined the standardization transformation mentioned in earlier examples: x &lt;- rnorm(100, mean=20, sd=10) x_zscore &lt;- (x - mean(x))/sd(x) However, there are many different ways to define the central value of a set of numbers: arithmetic mean geometric mean median mode and many more Each of these central value methods accepts a vector of numbers, but their behaviors are different, and are appropriate in different situations. Likewise, there are many possible scaling strategies we might consider: standard deviation rescaling factor (e.g. set data range to be between -1 and 1) scaling to unit length (all values sum to 1) and others We may wish to explore these different methods without writing entirely new code for each combination when trying out different transformation techniques. In R and other functional languages, we can easily accomplish this by passing functions as arguments to other functions. Consider the following R function: # note R already has a built in function named &quot;transform&quot; my_transform &lt;- function(x, t_r, s) { return((x - t_r(x))/s(x)) } This should look familiar to the equation presented earlier, except now in code the arguments t_r and s are passed as arguments. If we wished to transform using a Z-score normalization, we could call my_transform as follows: x &lt;- rnorm(100,mean=20,sd=10) x_zscore &lt;- my_transform(x, mean, sd) mean(x_zscore) [1] 0 sd(x_zscore) [1] 1 In the my_transform function call, the second and third arguments are the names of the mean and sd functions, respectively. In the definition of my_transform we use the syntax t_r(x) and s(x) to indicate that these arguments should be treated as functions. Using this strategy, we could just as easily define a transformation using median and sum for t_r and s if we wished to: x &lt;- rnorm(100,mean=20,sd=10) x_transform &lt;- my_transform(x, median, sum) median(x_transform) [1] 0 sum(x_transform) # this quantity does not have an a priori known value (or meaning for that matter, it&#39;s just an example) [1] 0.013 We can also write our own functions and pass them to get the my_transform function to have desired behavior. The following scales the values of x to have a range of \\([0,1]\\): data_range &lt;- function(x) { return(max(x) - min(x)) } # my_transform computes: (x - min(x))/(max(x) - min(x)) x_rescaled &lt;- my_transform(x, min, data_range) min(x_rescaled) [1] 0 max(x_rescaled) [1] 1 The data_range function simply subtracts the minimum value of x from the maximum value and returns the result. This feature of passing functions as arguments to other functions is a fundamental property of functional programming languages. Now we are ready to finally talk about how iteration is performed in R. Advanced R - Functional Programming Functional Programming Tutorial 4.8.3 apply() and friends When working with lists and matrices in R, there are often times when you want to perform a computation on every row or every column separately. A common example of this in data science mentioned above is feature standardization. Earlier we wrote a Z-score transformation that accepts a vector, subtracts the mean from each element, and divides the result by the standard deviation of the data. This ensures the data has a mean and standard deviation of 0 and 1, respectively. However, this function only operates on a single vector of numbers. Large datasets have many features, each of which may be individual vectors, that we desire to perform this same Z-score transformation on separately. In other words, we have one function that we wish to execute on either every row or every column of a matrix and return the result. This is a form of iteration that can be implemented in a functional style using the apply function. This is the signature of the apply function, from the RStudio help(apply) page: apply(X, MARGIN, FUN, ..., simplify = TRUE) Here, X is a matrix (i.e. a rectangle of numbers) that we wish to perform a computation on for either each row or each column. MARGIN indicates whether the matrix should be traversed by rows (MARGIN=1) or columns (MARGIN=2). FUN is the name of a function that accepts a vector and returns either a vector or a scalar value that we wish to execute on either the rows or columns. apply() then executes FUN on each row or column of X and returns the result. For example: zscore &lt;- function(x) { return((x-mean(x))/sd(x)) } # construct a matrix of 50 rows by 100 columns with samples drawn from a normal distribution x_mat &lt;- matrix( rnorm(100*50, mean=20, sd=5), nrow=50, ncol=100 ) # z-transform the rows of x_mat, so that each column has mean,sd of 0,1 x_mat_zscore &lt;- apply(x_mat, 2, zscore) # we can check that all the columns of x_mat_zscore have mean close to zero with apply too x_mat_zscore_means &lt;- apply(x_mat_zscore, 2, mean) # note: due to machine precision errors, these results will not be exactly zero, but are very close # note: the all() function returns true if all of its arguments are TRUE all(x_mat_zscore_means&lt;1e-15) [1] TRUE The same approach can be used when X is a list or data frame rather than a matrix using the lapply() function (hint: the l in lapply stands for list). Here is the function signature for lapply: lapply(X, FUN, ...) Recall that lists and data frames can be thought of as vectors where each element can be its own vector. Therefore, there is only one axis along which to iterate on the elements and there is not MARGIN argument as in apply. This function returns a new list of the same dimension as the original list with elements returned by FUN: x &lt;- list( feature1=rnorm(100,mean=20,sd=10), feature2=rnorm(100,mean=50,sd=5) ) x_zscore &lt;- lapply(x, zscore) # check that the means are close to zero x_zscore_means &lt;- lapply(x_zscore, mean) all(x_zscore_means &lt; 1e-15) [1] TRUE This functional programming pattern might be counter intuitive at first, but it is well worth your while to learn. R for Data Science - Iteration 4.9 Installing Packages Advanced functionality in R is provided through packages written and supported by R community members. With the exception of bioconductor packages, all R packages are hosted on the Comprehensive R Archive Network (CRAN) web site. At the time of writing, there are more than 18,000 packages hosted on CRAN that you can install. To install a package from CRAN, use the install.packages function in the R console: # install one package install.packages(&quot;tidyverse&quot;) # install multiple packages install.packages(c(&quot;readr&quot;,&quot;dplyr&quot;)) As mentioned above, many packages used in biological data analysis are not hosted on CRAN, but in Bioconductor. The Bioconductor projects mission is to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assays. Practically, this means Bioconductor packages are subject to stricter standards for documentation, coding conventions and structure, and standards compliance compared with the relatively more lax CRAN package submission process. 4.10 Saving and Loading R Data While it is always a good idea to save results in tabular form in CSV Files, sometimes it is convenient to save complicated R objects and data structures like lists to a file that can be read back into R easily. This can be done with the saveRDS() and readRDS() functions: a_complicated_list &lt;- list( categories = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), data_matrix = matrix(c(1,2,3,4,5,6),nrows=2,ncols=3), nested_list = list( a = c(1,2,3), b = c(4,5,6) ) ) saveRDS(a_complicated_list, &quot;a_complicated_list.rda&quot;) # later, possibly in a different script a_complicated_list &lt;- readRDS(&quot;a_complicated_list.rda&quot;) These functions are very convenient for saving results of complicated analyses and reading them back in later, especially if those analyses were time consuming. R also has the functions save() and load(). These functions are similar to saveRDS and readRDS, except they do not allow loading individual objects into new variables. Instead, save accepts one or more variable names that are in the global namespace and saves them all to a file: var_a &lt;- &quot;a string&quot; var_b &lt;- 1.234 save(var_a, var_b, file=&quot;saved_variables.rda&quot;) Then later when the file saved_variables.rda is load()ed, the all the variables saved into the file are loaded into the namespace with their saved values: # assume var_a and var_b are not defined yet load(&quot;saved_variables.rda&quot;) var_a [1] &quot;a string&quot; var_b [1] 1.234 This requires the programmer to remember the names of the variables that were saved into the file. Also, if there are variables that already exist in the current environment that are also saved in the file, those variable values will be overridden possibly without the programmers knowledge or intent. There is no way to change the variable names of variable saved in this way. For these reasons, saveRDS and loadRDS are generally safer to use as you can be more explicit about what you are saving and loading. saveRDS() and readRDS() R manual readRDS() and saveRDS() official documentation 4.11 Troubleshooting and Debugging Bugs in code are normal. You are not a bad programmer if your code has bugs (thank goodness!). However, some bugs can be very difficult to fix, and some are even difficult to find. You will spend a substantial amount of time debugging your code in R, especially as you are learning the language and its many quirks. You will encounter R error and warning messages routinely during development, and not all of them are straightforward to understand. It is important that you learn how to seek the answers to the problems R reports on your own; your colleagues (and instructors!) will thank you for it. There is no standard approach to debugging, but here we borrow ideas from Hadley Wickams excellent section on debugging in his Advanced R book: Google! - copy and paste the error into google and see what comes back. Especially when starting out, the errors you receive have been encountered countless times by others before you, and solutions/explanations of them are already out there. If you arent already familiar with Stack Overflow, you will be very soon. Make it repeatable - When you encounter an error, dont change anything in your code and try again to make sure you get the same error again. This may require you to isolate the code with the error in a different setting to make it more easy to run. If you do, this means the error is repeatable, or replicable, and you can now try modifying the code in question to see if and how the error changes. Find out where the bug is* - Most bugs involve multiple lines of code, only a subset of which contains the actual error. Sometimes the exact line where the error occurs is obvious, but other times the error is a consequence of a mistake assumption made earlier in the code. Fix it and test it - When you have identified the specific issue causing the bug, modify the code so it produces the correct result and then rigorously test your fix to make sure it is correct. Sometimes making one change to code causes side effects elsewhere in your code in ways that are difficult to predict. Ideally, you have already written unit tests that explicitly test parts of your code, but if not you will need to use other means of convincing yourself that your fix worked. This debugging process will become second nature as you work more in R. Practically speaking, the most basic debugging method is to run code that isnt working the way it should, print out intermediate results to inspect the state of your variables, and make adjustments accordingly. In RStudio, the Environment Inspector in the top right of the interface makes inspecting the current values of your variables very easy. You can also easily execute lines of code from your script in the interpreter at the bottom right using Cntl-Enter and test out modifications there. Sometimes you will be working with highly nested data structures like lists of lists. These objects can be difficult to inspect due to their size. The str() function, which stands for structure, will pretty print an object with its values and its structure: nested_lists &lt;- list( a=list( item1=c(1,2,3), item2=c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) ), b=list( var1=1:10, var2=100:110, var3=1000:1010 ), c=c(10,9,8,7,6,5) ) nested_lists $a $a$item1 [1] 1 2 3 $a$item2 [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; $b $b$var1 [1] 1 2 3 4 5 6 7 8 9 10 $b$var2 [1] 100 101 102 103 104 105 106 107 108 109 110 $b$var3 [1] 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 str(nested_lists) List of 3 $ a:List of 2 ..$ item1: num [1:3] 1 2 3 ..$ item2: chr [1:3] &quot;A&quot; &quot;B&quot; &quot;C&quot; $ b:List of 3 ..$ var1: int [1:10] 1 2 3 4 5 6 7 8 9 10 ..$ var2: int [1:11] 100 101 102 103 104 105 106 107 108 109 ... ..$ var3: int [1:11] 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 ... $ c: num [1:6] 10 9 8 7 6 5 The output str is more concise and descriptive than simply printing out the object. RStudio has many more debugging tools you can use. Check out the section on debugging in Hadley Wickams Advanced R book in the Read More box for description of these tools. Hands-On Programming with R - Debugging R Code Advanced R - Debugging 4.12 Coding Style and Conventions Some very common worries among new programmers is: Is my code terrible? How do I write good code? There is no gold standard for what makes code good, but there are some questions you can ask of your code as a guide: 4.12.1 Is my code correct? Does it produce the desired output? This is pretty obviously important in principle, but it can be difficult to be sure that your code is correct. This is especially difficult if your codebase is large and complicated as it tends to become over time. While simple trial and error is an effective first approach, a more reliable albeit time- and thought-intensive strategy is to write explicit tests for your code and run them regularly. 4.12.2 Does my code follow the DRY principle? Dont Repeat Yourself (DRY) is a powerful and helpful strategy to make your code more reliable. This typically involves identifying common patterns in your code and moving them to functions or objects. 4.12.3 Did I choose concise but descriptive variable and function names? Variable and function names should be descriptive when necessary and not too long. Try to put yourself in the shoes of someone who is reading your code for the first time and see if you can figure out what it does. Better yet, offer to buy a friend a coffee in return for them looking at it! 4.12.4 Did I use indentation and naming conventions consistently throughout my code? Consistently formatted code is much easier to read (and possibly understand) than inconsistent code. Consider the following code example: calcVal &lt;- function(x, a, arg=2) { return(sum(x*a)**2)} calc_val_2 &lt;- function(x, a, b, arg) { res &lt;- sum(b+a*x)**arg return(res)} This code is inconsistent in several ways: naming conventions - calcVal is camel case, calc_val_2 is snake case new lines and whitespace - calcVal is all on one line, calc_val_2 is on multiple lines unhelpful indentation - calc_val_2 has a function body that is not indented, and the close curly brace is appended to the last line of the body unhelpful function and argument names - the function names describe very little about what the functions do, and the argument names x, a, etc are not very descriptive about what they represent unused function arguments - the arg argument in calcVal isnt used anywhere in the function the two functions appear to do something very similar and could be made simpler using a default argument A more consistent version of this code might look like: exponent_product &lt;- function(x, a, offset=0, arg=2) { return(sum(offset+a*x)**arg) } This code is much cleaner, more consistent, and easier to read. 4.12.5 Did I write comments, especially when what the code does is not obvious? Sometimes what a piece of code does is obvious from looking at it: x &lt;- x + 1 Clearly this line of code takes the value of x, whatever it is, and adds 1 to it. However, it may not be obvious why a piece of code does what it does. In these cases, it may be very helpful to record your thinking about a line of code as a comment: # add 1 as a pseudocount x &lt;- x + 1 Then when you or someone else reads the code, it will be obvious what you were thinking when you wrote it. In your career, you will encounter situations where you need to figure out what you were thinking when you wrote a piece of code. Endeavor to make future you proud of current you! 4.12.6 How easy would it be for someone else to understand my code? If someone else who has never seen my code before is asked to run and understand it, how easy would it be for them to do so? 4.12.7 Is my code easy to maintain/change? This is related to the previous question, but is distinct in that understanding what code does is just the first step in being able to make desired changes to it. 4.12.8 The styler package . "],["data-wrangling.html", "5 Data Wrangling 5.1 The Tidyverse 5.2 Tidyverse Basics 5.3 Importing Data 5.4 The tibble 5.5 Tidy Data 5.6 pipes 5.7 Arranging Data 5.8 Grouping Data 5.9 Rearranging Data 5.10 Relational Data", " 5 Data Wrangling 5.1 The Tidyverse The tidyverse is an opinionated collection of R packages designed for data science. The packages are all designed to work together with a unified approach that helps code look consistent and neat. In the opinion of this author, the tidyverse practically changes the R language from a principally statistical programming language into an efficient and expressive data science language. While it is still important to understand the language fundamentals presented in our chapter on the R programming language, the tidyverse uses a distinct set of coding conventions that lets it achieve greater expressiveness, conciseness, and correctness relative to the base R language. As a data science language, R+tidyverse (referred to as simply tidyverse in this book) is strongly focused on operations related to loading, manipulating, visualizing, summarizing, and analyzing data sets from many domains. While this is a major strength of tidyverse and its community, it means that many educational materials are written for this general use case, and not for those practicing biological data analysis. While the general data manipulation operations are often the same between biological data analysis and these general case examples, biological analysis practitioners must nonetheless translate concepts from these general cases to the common data analysis tasks they must perform. Some analytical patterns are more common in biological data analysis than others, so these materials focus on that subset of operations in this book to aid the learning in applying the concepts to their problems as directly as possible. R for Data Science - Data Wrangling Introduction ModernDive - Data Wrangling 5.2 Tidyverse Basics Since tidyverse is a set of packages that work together, you will often want to load multiple packages at the same time. The tidyverse authors recognize this, and defined a set of reasonable packages to load at once when loading the metapackage (i.e. a package that contains multiple packages): library(tidyverse) -- Attaching packages --------------------------------------------- tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.6 v dplyr 1.0.7 v tidyr 1.1.4 v stringr 1.4.0 v readr 2.1.1 v forcats 0.5.1 -- Conflicts ------------------------------------------------ tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() The packages in the Attaching packages section are those loaded by default, and each of these packages adds a unique set of functions to your R environment. We will mention functions from many of these packages as we go through this chapter, but for now here is a table of these packages and briefly what they do: Package Description ggplot2 Plotting using the grammar of graphics tibble Simple and sane data frames tidyr Operations for making data tidy readr Read rectangular text data into tidyverse purrr Functional programming tools for tidyverse dplyr A Grammar of Data Manipulation stringr Makes working with strings in R easier forcats Operations for using categorical variables Notice the dplyr::filter() syntax in the Conflicts section. filter is defined as a function in both the dplyr package as well as the base R stats package. The stats package is loaded by default when you run R, and thus the filter function is defined (specifically, it performs linear filtering on time series data). However, when dplyr is loaded, it also has a filter function which overrides the definition from the stats package. This is why the tidyverse package reports this as a conflict when loaded: -- Conflicts ------------------------------------------------ tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() This is tidyverse telling you that the filter function has been redefined and you should make sure you are aware of that. However, if you did want to use the original stats defined filter function, you may still access it using the :: namespace operator. This operator lets you look inside a loaded package, for example dplyr, and access a function within the namespace of that package: library(dplyr) # the filter() function definition is now from the dplyr package, and not from the stats package # the following two lines of code execute the same function filter(df) dplyr::filter(df) # to access the stats definition of the filter function, use the namespace operator stats::filter(df) Most functions defined by a package can be accessed using the :: operator, and it is often a good idea to do so, to ensure you are calling the right function. 5.3 Importing Data The first operation we typically must perform when analyzing data is reading our data from a file into R. The readr package in the default tidyverse packages contains the following similar functions that import data from delimited text files: Function Brief description/use read_csv Delimiter: , - Decimal separator: . read_csv2 Delimiter: ; - Decimal separator: , read_tsv Delimiter: &lt;tab&gt; - Decimal separator: . read_delim Delimiter: set by user - Decimal separator: . Some CSV files can be very large and may be compressed to save space. There are many different file compression algorithms, but the most common in data science and biology are gzip and bzip. All the readr file reading functions can work with compressed files directly, so you do not need to decompress them first. Each of these functions returns a special data frame called a tibble, which is explained in the next section. Note that readr also has functions for writing delimited files. These functions behave similarly to the read_X functions but instead of creating a tibble from a file, they create a file from a tibble. You will frequently need to export the results of your analysis to share with collaborators and also as part of larger workflows that use tools other than R. R for Data Science - Data Import readr - read_delim reference readr - write_delim reference 5.4 The tibble Data in tidyverse is organized primarily in a special data frame object called a tibble. The tibble() function is defined in the tibble package of the tidyverse: library(tibble) # or library(tidyverse) tbl &lt;- tibble( x = rnorm(100, mean=20, sd=10), y = rnorm(100, mean=50, sd=5) ) tbl # A tibble: 100 x 2 x y &lt;dbl&gt; &lt;dbl&gt; 1 16.5 54.6 2 14.4 54.3 3 7.87 53.7 4 8.06 50.8 5 37.2 57.1 6 16.5 51.9 7 15.8 50.1 8 40.3 44.3 9 12.0 49.8 10 23.8 50.1 # ... with 90 more rows A tibble stores rectangular data, i.e. a grid of data elements with where every column has the same number of rows. You can access individual columns in the same way as with base R data frames: tbl$x [1] 29.572549 12.015877 15.235536 23.071761 32.254703 48.048651 21.905756 [8] 15.511768 34.872685 21.352433 12.515230 23.608096 6.778630 12.342237 ... tbl[1,&quot;x&quot;] # access the first element of x # A tibble: 1 x 1 x &lt;dbl&gt; 1 29.6 tbl$x[1] [1] 29.57255 tibbles (and regular data frames) typically have names for their columns. In the above example, the column names are x and y, accessed using the colnames function: colnames(tbl) [1] &quot;x&quot; &quot;y&quot; Column names may be changed using this same function: colnames(tbl) &lt;- c(&quot;a&quot;,&quot;b&quot;) tbl # A tibble: 100 x 2 a b &lt;dbl&gt; &lt;dbl&gt; 1 16.5 54.6 2 14.4 54.3 3 7.87 53.7 4 8.06 50.8 5 37.2 57.1 6 16.5 51.9 7 15.8 50.1 8 40.3 44.3 9 12.0 49.8 10 23.8 50.1 # ... with 90 more rows As we will see again later, we can also use dplyr::rename to rename columns as well: dplyr::rename(tbl, a = x, b = y ) tibbles and dataframes also have row names as well as column names: rownames(tbl) [1] &quot;1&quot; &quot;2&quot; &quot;3&quot;... However, the tibble support for row names is only included for compatibility with base R data frames and should generally be avoided. The reason is that row names are basically a character column that has different semantics than every other column, and the authors of tidyverse believe row names are better stored as a normal column. tibble - working with row names The tibble package provides a convenient way to construct simple tibbles manually with the tribble() function, which stands for transposed tibble: gene_stats &lt;- tribble( ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p, &quot;apoe&quot;, 12.509293, 0.1032, 34.239521, 1.3e-5, &quot;hoxd1&quot;, 4.399211, 0.6323, 16.332318, 0.0421, &quot;snca&quot;, 45.748431, 4.2e-9, 0.757188, 0.9146, ) gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 ## 2 hoxd1 4.40 0.632 16.3 0.0421 ## 3 snca 45.7 0.0000000042 0.757 0.915 This made-up dataset includes statistics and p-values from two different statistical tests (again, made up) for three human genes. We will use this example below in the Arranging Data section. tibble documentation R for Data Science - tibbles 5.5 Tidy Data The tidyverse packages are designed to operate with so-called tidy data. From the tidy data section of the R for Data Science book, the following rules make data tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Here, a variable is a quantity or property that every observation in our dataset has, and each observation is a separate instance of those variable (e.g. a different sample, subject, etc). In our gene_stats example tibble above, the columns referring to different test statistics are the variables, and each gene in each row is an observation, and each cell has a value; we can therefore say that the dataset is tidy: gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 ## 2 hoxd1 4.40 0.632 16.3 0.0421 ## 3 snca 45.7 0.0000000042 0.757 0.915 Each row being an observation is somewhat abstract in this case; we could say that we observed the same test for all the genes in the tibble. Depending on what dataset we are working with, we sometimes have to be flexible in our conceptualization of what constitutes variables and observations. The R for Data Science book depicts these rules in the following illustration: Tidy data - from R for Data Science These rules are pretty generic, and in general a dataset might require some work to manipulate it into tidy form. Fortunately for those of us working in biology and bioinformatics, very many of our datasets are already provided in a format that is very close to being tidy, or the tools we use to process biological data do so for us. For this reason, details about the tidying operations that might be needed for data in the general case are left for reading in the tidy data section of R for Data Science book. There is one very big and common exception to the claim above that biological data is usually already tidy that. Briefly, from the illustration above, tidy data has observations as rows and variables as columns. However, the datasets that we often use, e.g. gene expression matrices, are organized to have variables (e.g. genes) as rows and observations (e.g. samples) as columns. This means some operations to summarize variables across observations, which are very common to compute, are not easily done with the tidyverse functions like mutate(). We describe how to work around this difference in the section Biological data is NOT Tidy!. Tidy data - R for Data Science 5.6 pipes One of the key tidyverse programming patterns is chaining manipulations of tibbles together using the %&gt;% operator. We very often want to perform serial operations on a data frame, for example read in a file, rename one of the columns, subset the rows based on some criteria, and compute summary statistics on the result. We might implement such operations using a variable and assignment: # data_file.csv has two columns: bad_cOlumn_name and numeric_column data &lt;- readr::read_csv(&quot;data_file.csv&quot;) data &lt;- dplyr::rename(data, &quot;better_column_name&quot;=bad_cOlumn_name) data &lt;- dplyr::filter(data, better_column_name %in% c(&quot;condA&quot;,&quot;condB&quot;)) data_grouped &lt;- dplyr::group_by(data, better_column_name) summarized &lt;- dplyr::summarize(data_grouped, mean(numeric_column)) The repeated use of data and the intermediate data_grouped variable may be unnecessary if youre only interested in the summarized result. The code is also not very straightforward to read. Using the %&gt;% operator, we can write the same sequence of operations in a much more concise manner: data &lt;- readr::read_csv(&quot;data_file.csv&quot;) %&gt;% dplyr::rename(&quot;better_column_name&quot;=bad_cOlumn_name) %&gt;% dplyr::filter(better_column_name %in% c(&quot;condA&quot;,&quot;condB&quot;)) %&gt;% dplyr::group_by(better_column_name) %&gt;% dplyr::summarize(mean(numeric_column)) Note that the function calls in the piped example do not have the data variable passed in explicitly. This is because the %&gt;% operator passes the result of the function immediately preceding it as the first argument to the next function automatically. This convention allows us to focus on writing only the important parts of the code that perform the logic of our analysis, and avoid unnecessary and potentially distracting additional characters that make the code less readable. R for Data Science - Pipes %&gt;% operator documentation in the magrittr package 5.7 Arranging Data After we have loaded our data from a file into a tibble, we often need to manipulate it in various ways to make the values amenable to our desired analysis. Such manipulations might include renaming poorly named columns, filtering out certain records, deriving new columns using the values in others, changing the order of rows etc. These operations may collectively be termed arranging the data and many are provided in the *dplyr package. We will cover some of the most common data arranging functions here, but there are many more in the dplyr package worth knowing. In the examples below, we will make use of the following made-up tibble that contains fake statistics and p-values for three human genes: gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 ## 2 hoxd1 4.40 0.632 16.3 0.0421 ## 3 snca 45.7 0.0000000042 0.757 0.915 The gene_stats tibble above is a simple example of a very common type of data we work with in biology; namely instead of raw data, we work with statistics that have been computed using raw data that help us interpret the results. While these statistics may not be data per se, we can still use all the functions and strategies in the tidyverse to work with them. The tidyverse is a very big place. RStudio created many helpful cheatsheets to aid in looking up how do to certain operations. The cheatsheet on dplyr has lots of useful information on how to use the many functions in the package. 5.7.1 dplyr::mutate() - Create new columns using other columns Many biological analysis procedures perform some kind of statistical test on a collection of features (e.g. genes) and produce p-values that indicate how surprising each feature is according to the test. The p-values in our tibble are nominal, i.e. they have not been adjusted for multiple hypotheses. Briefly, when we run multiple tests like we are on each of our three genes, there is a chance that some of the tests will have a small p-value simply by chance. Multiple testing correction procedures adjust nominal p-values to account for this possibility in a number of different ways, but the most common procedure in biological analysis is the Benjamini-Hochberg or False Discovery Rate (FDR) procedure. In R, the p.adjust function can perform several of these procedures, including FDR: dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;) ) ## # A tibble: 3 x 6 ## gene test1_stat test1_p test2_stat test2_p test1_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 0.155 ## 2 hoxd1 4.40 0.632 16.3 0.0421 0.632 ## 3 snca 45.7 0.0000000042 0.757 0.915 0.0000000126 Notice how the adjusted p-values are larger than the nominal ones; this is the effect of the multiple testing procedure. Since we have two sets of p-values, we must compute the FDR on each of them, which we can do in the same call to mutate(): gene_stats_mutated &lt;- dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;) ) gene_stats_mutated ## # A tibble: 3 x 7 ## gene test1_stat test1_p test2_stat test2_p test1_padj test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 0.155 0.000039 ## 2 hoxd1 4.40 0.632 16.3 0.0421 0.632 0.0632 ## 3 snca 45.7 0.0000000042 0.757 0.915 0.0000000126 0.915 Another common operation is to create new columns derived from the values in multiple other columns. We (or our wetlab colleagues) might decide it is convenient to have a new column with TRUE or FALSE based on whether the gene was significant in either test. Such a column would make it easy to filter genes down to just ones that might be interesting in tools like Excel. We can create new columns from multiple columns just as easily using the mutate() function: dplyr::mutate(gene_stats_mutated, signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05) ) ## # A tibble: 3 x 9 ## gene test1_stat test1_p test2_stat test2_p test1_padj test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 0.155 0.000039 ## 2 hoxd1 4.40 0.632 16.3 0.0421 0.632 0.0632 ## 3 snca 45.7 0.0000000042 0.757 0.915 0.0000000126 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; Recall that the | and &amp; operators execute or and and logic, respectively. The above example required the creation of a new variable gene_stats_mutated because the columns test1_padj and test2_padj need to be in the tibble before computing the new fields. However, in mutate(), columns created first in the function call are available to later columns. In the following example, note that test1_padj is created first and then used to create the signif columns: dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), # test1_padj created test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), #test1_padj used signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05) ) ## # A tibble: 3 x 9 ## gene test1_stat test1_p test2_stat test2_p test1_padj test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apoe 12.5 0.103 34.2 0.000013 0.155 0.000039 ## 2 hoxd1 4.40 0.632 16.3 0.0421 0.632 0.0632 ## 3 snca 45.7 0.0000000042 0.757 0.915 0.0000000126 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; The alternative would be to split this into two mutate() commands, the first creating the adjusted p-value columns and the second creating the significance columns. dplyr recognizes how common it is to build new variables off of other new variables in a mutate() command, and therefore provides this convenient behavior. mutate() can also be used to modify columns in place. The official convention for human gene symbols is that they are upper case, but for some reason our tibble contains lower case gene symbols. We can correct this using mutate() but first we should talk about the stringr package which makes working with strings much easier than with base R functions. R for Data Science - Add new variables with mutate() dplyr mutate() reference 5.7.2 stringr - Working with character values Base R does not have very convenient functions for working with character strings (or just strings). This is due to its original intent a statistical programming language, where string manipulation is not (in principle) a common operation. However, in practice, we must frequently manipulate strings while loading, cleaning, and analyzing datasets. The stringr package aims to make working with strings as easy as possible. The package includes many useful functions for operating on strings, including searching for patterns, mutating strings, lexicographical sorting, combining multiple strings together (i.e. concatenation), and performing complex search/replace operations. There are far too many useful functions to cover here and you should become comfortable reading the stringr documentation and the very helpful stringr cheatsheet. In the previous section, we noted that the gene symbols in our tibble were lower case while official gene symbols are in upper case. We can use the stringr function stringr::str_to_upper() with the dplyr::mutate() function to perform this adjustment easily: dplyr::mutate(gene_stats, gene=stringr::str_to_upper(gene) ) ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ## 2 HOXD1 4.40 0.632 16.3 0.0421 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 Now are gene symbols have the appropriate case, and our wetlab colleagues wont complain about it. :) 5.7.2.1 Regular expressions Many of the string operations in the stringr package use regular expression syntax. A regular expression is a sequence of characters that describes patterns in text. Regular expressions are written in a sort of mini programming language where certain characters have special meaning that help in defining search patterns that identifies the location of sequences of characters in text that follow a particular pattern specified by the regular expression. This is similar to the Find functionality in many word processors, but is more powerful due to the flexibility of the patterns that can be found. A simple example will be helpful. Lets say we have a tibble containing the result of a (made-up) statistical test for all the genes in a genome: de_genes ## # A tibble: 39,535 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MT-TF 5799 0.000910 0.00941 ## 2 MT-RNR1 153 0.0272 0.0342 ## 3 MT-TV 115 0.0228 0.0301 ## 4 MT-RNR2 495 0.00318 0.0123 ## 5 MT-TL1 20201 0.000377 0.00841 ## 6 MT-ND1 160 0.0179 0.0258 ## 7 MT-TI 3511 0.00247 0.0115 ## 8 MT-TQ 772 0.00376 0.0129 ## 9 MT-TM 301 0.00325 0.0124 ## 10 MT-ND2 12 0.107 0.111 ## # ... with 39,525 more rows Now lets say were interested in examining the results for the BRCA family of genes, BRCA1 and BRCA2. We can use filter() on the data frame to look for them individually: de_genes %&gt;% filter(hgnc_symbol == &quot;BRCA1&quot; | hgnc_symbol == &quot;BRCA2&quot;) ## # A tibble: 2 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BRCA1 41 0.0321 0.0386 ## 2 BRCA2 447 0.0140 0.0223 This isnt so bad, but we can do the same thing with stringr::str_detect() , which returns TRUE if the provided pattern matches the input and FALSE otherwise, a regular expression, and the [dplyr::filter() function], which is described in greater detail in a later section: dplyr::filter(de_genes, str_detect(hgnc_symbol,&quot;^BRCA[12]$&quot;)) ## # A tibble: 2 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BRCA1 41 0.0321 0.0386 ## 2 BRCA2 447 0.0140 0.0223 The argument \"^BRCA[12]$\" is a regular expression that searches for the following: Search for genes that start with the characters BRCA - the ^ at the beginning of the pattern stands for the start of the string For genes that start with BRCA, then look for genes where the next character is either 1 or 2 with [12] - the characters between the [] are searched for explicitly, and any character encountered that is not listed between them results in a non-match For genes that start with BRCA followed with either a 1 or a 2, match successfully if the number is at the end of the name - the $ at the end of the pattern stands for the end of the string We can use these principles to find genes with more complex naming conventions. The Homeobox (HOX) genes encode DNA binding proteins that regulate gene expression of genes involved in morphgenesis and cell differentiation in vertebrates. In humans, HOX genes are organized into 4 clusters of paralogs that were the result of three DNA duplication events in the distant evolutionary past(Abbasi 2015), where each cluster encodes a subset of 13 distinct HOX genes placed next to each other. Each of these clusters has been assigned a letter identifier A-D (e.g. HOXA, HOXB, HOXC, and HOXD) and each paralogous gene within each cluster is assigned the same number (e.g. HOXA4, HOXB4, HOXC4, and HOXD4 are paralogs). There are 13 HOX genes in total, though not all genes remain in all clusters (e.g. HOXA1, HOXB1, and HOXD1 exist but HOXC1 was lost over time). The following figure depicts the human HOX gene clusters: Human HOX gene clusters - Veraksa, A.; Del Campo, M.; McGinnis, W. Developmental Patterning Genes and their Conserved Functions: From Model Organisms to Humans. Mol. Genet. Metab. 2000, 69 (2), 85100. Lets say we want to extract out all the HOX genes from our gene statistics. We can write a regular expression that matches the pattern described above: dplyr::filter(de_genes, str_detect(hgnc_symbol,&quot;^HOX[A-D][0-9]+$&quot;)) %&gt;% dplyr::arrange(hgnc_symbol) ## # A tibble: 39 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HOXA1 8734 0.000858 0.00934 ## 2 HOXA10 4149 0.00152 0.0102 ## 3 HOXA11 567 0.0101 0.0188 ## 4 HOXA13 411 0.0105 0.0191 ## 5 HOXA2 554 0.00600 0.0151 ## 6 HOXA3 18 0.0919 0.0959 ## 7 HOXA4 475 0.0113 0.0199 ## 8 HOXA5 434 0.0127 0.0211 ## 9 HOXA6 3983 0.00252 0.0115 ## 10 HOXA7 897 0.00961 0.0183 ## # ... with 29 more rows In this query we used two new regular expression features: within the [] we specified a range of characters A-D and 0-9 which will match any of the characters between A and D (i.e. A, B, C, or D) and 0 and 9 respectively the + character means match one or more of the preceding expression, which in our case is the [0-9]. This allows us to match genes with only a single number (e.g. HOXA1) as well as double digit numbers (e.g. HOXA10). Since we know the cluster identifier part of the HOX gene names (i.e. the [A-D] part) is exactly one character long, we could alternatively write the regular expression as follows, using the special . character: dplyr::filter(de_genes, str_detect(hgnc_symbol,&quot;^HOX.[0-9]+$&quot;)) %&gt;% dplyr::arrange(hgnc_symbol) ## # A tibble: 39 x 4 ## hgnc_symbol mean p padj ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HOXA1 8734 0.000858 0.00934 ## 2 HOXA10 4149 0.00152 0.0102 ## 3 HOXA11 567 0.0101 0.0188 ## 4 HOXA13 411 0.0105 0.0191 ## 5 HOXA2 554 0.00600 0.0151 ## 6 HOXA3 18 0.0919 0.0959 ## 7 HOXA4 475 0.0113 0.0199 ## 8 HOXA5 434 0.0127 0.0211 ## 9 HOXA6 3983 0.00252 0.0115 ## 10 HOXA7 897 0.00961 0.0183 ## # ... with 29 more rows Here, the . character is interpreted by the regex to match any single character, regardless of what it is, between the HOX part and the number part. This also requires that there exist exactly one character between the two parts; a gene symbol HOX1 would not be matched, because the 1 would match to the ., but no number remains to match to the [0-9]+ part. Sometimes you want to search text for characters that are considered special in the regular expression language. For example, if you had a list of filenames: filenames &lt;- tribble( ~name, &quot;annotation.csv&quot;, &quot;file1.txt&quot;, &quot;file2.txt&quot;, &quot;results.csv&quot; ) and wanted to limit to just those with the .txt extension, you need to match using a literal . character: filter(filenames, stringr::str_detect(name,&quot;[.]txt$&quot;)) ## # A tibble: 2 x 1 ## name ## &lt;chr&gt; ## 1 file1.txt ## 2 file2.txt Inside a [], characters do not have their usual regular expression meaning, and therefore [.] will match a literal . character. Instead of using the [] syntax, you may also escape these literal characters using two back slashes: filter(filenames, stringr::str_detect(name,&quot;\\\\.txt$&quot;)) ## # A tibble: 2 x 1 ## name ## &lt;chr&gt; ## 1 file1.txt ## 2 file2.txt Regular expressions are very powerful, and can do much more than what is described here. See the regular expression tutorial linked in the readmore box to learn more details. R for Data Science - Strings stringr documentation stringr cheatsheet RegexOne - regular expression tutorial 5.7.3 dplyr::select() - Subset Columns by Name Our mutate() operations above created a number of new columns in our tibble, but we did not specify where in the tibble the new columns should go. Lets consider the mutated tibble we created with all four new columns: dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05), gene=stringr::str_to_upper(gene) ) ## # A tibble: 3 x 9 ## gene test1_stat test1_p test2_stat test2_p test1_padj test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 0.155 0.000039 ## 2 HOXD1 4.40 0.632 16.3 0.0421 0.632 0.0632 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 0.0000000126 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; From a readability standpoint, it might be helpful if all the columns that are about each test were grouped together, rather than having to look at the end of the tibble to find them. The dplyr::select() function allows you to pick specific columns out of a larger tibble in whatever order you choose: stats &lt;- dplyr::select(gene_stats, test1_stat, test2_stat) stats ## # A tibble: 3 x 2 ## test1_stat test2_stat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 34.2 ## 2 4.40 16.3 ## 3 45.7 0.757 Here we have explicitly selected the statistics columns. dplyr also has helper functions that allow for more flexible selection of columns. For example, if all of the columns we wished to select ended with _stat, we could use the ends_with() helper function: stats &lt;- dplyr::select(gene_stats, ends_with(&quot;_stat&quot;)) stats ## # A tibble: 3 x 2 ## test1_stat test2_stat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 34.2 ## 2 4.40 16.3 ## 3 45.7 0.757 If you so desire, select() allows for the renaming of selected columns: stats &lt;- dplyr::select(gene_stats, t=test1_stat, chisq=test2_stat ) stats ## # A tibble: 3 x 2 ## t chisq ## &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 34.2 ## 2 4.40 16.3 ## 3 45.7 0.757 If we knew that these test statistics actually corresponded to some kind of t-test and a \\(\\chi\\)-squared test, naming the columns of the tibble appropriately may help others (and possibly you) understand your code better. We can use the dplyr::select() function to obtain our desired column order: dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05), gene=stringr::str_to_upper(gene) ) %&gt;% dplyr::select( gene, test1_stat, test1_p, test1_padj, test2_stat, test2_p, test2_padj, signif_either, signif_both ) ## # A tibble: 3 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 0.155 34.2 0.000013 0.000039 ## 2 HOXD1 4.40 0.632 0.632 16.3 0.0421 0.0632 ## 3 SNCA 45.7 0.0000000042 0.0000000126 0.757 0.915 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; Now the order of our columns is clear and convenient. It is not necessary to list the columns for each test statistic on the same line, but the author thinks this makes the code easier to read and understand. R for Data Science - Select columns with select() dplyr reference select helper functions 5.7.4 dplyr::filter() - Pick rows out of a data set Often, the first step in interpreting an analysis is to identify the features that are significant at some adjusted p-value threshold. First we will save our mutated tibble to another variable, to aid in demonstration: gene_stats_mutated &lt;- dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05), gene=stringr::str_to_upper(gene) ) %&gt;% dplyr::select( gene, test1_stat, test1_p, test1_padj, test2_stat, test2_p, test2_padj, signif_either, signif_both ) Now we can use the dplyr::filter() function to select rows based on whether they are significant in either test this with our above example. dplyr::filter(gene_stats_mutated, test1_padj &lt; 0.05) ## # A tibble: 1 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SNCA 45.7 0.0000000042 0.0000000126 0.757 0.915 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; dplyr::filter(gene_stats_mutated, test2_padj &lt; 0.05) ## # A tibble: 1 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 0.155 34.2 0.000013 0.000039 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; Here we are filtering the result so that only genes with nominal p-value less than 0.05 remain. Note we filter on the two tests separately, but we can also combine these tests using logical operators to achieve different results: # | means &quot;logical or&quot;, meaning the row is retained if either condition is true dplyr::filter(gene_stats_mutated, test1_padj &lt; 0.05 | test2_padj &lt; 0.05) ## # A tibble: 2 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 0.155 34.2 0.000013 0.000039 ## 2 SNCA 45.7 0.0000000042 0.0000000126 0.757 0.915 0.915 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; Only APOE and SCNA are significant in at least one of the tests. # &amp; means &quot;logical and&quot;, meaning the row is retained only if both conditions are true dplyr::filter(gene_stats_mutated, test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05) ## # A tibble: 0 x 9 ## # ... with 9 variables: gene &lt;chr&gt;, test1_stat &lt;dbl&gt;, test1_p &lt;dbl&gt;, ## # test1_padj &lt;dbl&gt;, test2_stat &lt;dbl&gt;, test2_p &lt;dbl&gt;, test2_padj &lt;dbl&gt;, ## # signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; It looks like we dont have any genes that are significant by both tests. Filtering results like this is one of the most common operations we do on the results of biological analyses. R for Data Science - Filter rows with filter() dplyr - filter() reference 5.7.5 dplyr::arrange() - Order rows based on their values Another common operation when working with biological analysis results is ordering them by some meaningful value. Like above, p-values are often used to prioritize results by simply sorting them in ascending order. The arrange() function is how to perform this sorting in tidyverse: stats_sorted_by_test1_p &lt;- dplyr::arrange(gene_stats, test1_p) stats_sorted_by_test1_p ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 snca 45.7 0.0000000042 0.757 0.915 ## 2 apoe 12.5 0.103 34.2 0.000013 ## 3 hoxd1 4.40 0.632 16.3 0.0421 Note we are sorting by nominal p-value here, not adjusted p-value. In general, sorting by nominal or adjusted p-value results in the same order of results. The only exception is when, due to the way the FDR procedure works, some adjusted p-values will be identical, making the relative order of those tests with the same FDR meaningless. In contrast, it is very rare that nominal p-values will be identical, and since they induce the same ordering of results, when sorting analysis results there are advantages to using nominal p-value, rather than adjusted p-value. In general, the larger the magnitude of the statistic, the smaller the p-value (for two-tailed tests), so if we so desired we could induce a similar ranking by arranging the data by the statistic in descending order: # desc() is a helper function that causes the results to be sorted in descending # order for the given column dplyr::arrange(gene_stats, desc(abs(test1_stat))) ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 snca 45.7 0.0000000042 0.757 0.915 ## 2 apoe 12.5 0.103 34.2 0.000013 ## 3 hoxd1 4.40 0.632 16.3 0.0421 Here we first apply the base R abs() function to compute the absolute value of the test 1 statistic and then specify that we want to sort largest first. Note although we dont have any negative values in our dataset, we should not assume that in general, so it is safer for us to be complete and add the absolute value call in case later we decide to copy and paste this code into another analysis. Thats pretty much all there is to arrange(). R for Data Science - Arrange rows witharrange() dplyr arrange() reference 5.7.6 Putting it all together In the previous sections, we performed the following operations: Created new columns by computing false discovery rate on the nominal p-values using the dplyr::mutate() and p.adjust functions Created new columns that indicate the patterns of significance for each gene using dplyr::mutate() Mutated the gene symbol case using stringr::str_to_upper and dplyr::mutate() Reordered the columns to group related variables with select() Filtered genes based on whether they have an adjusted p-value less than 0.05 for either and both statistical tests using dplyr::filter() Sorted the results by p-value using dplyr::arrange() For the sake of illustration, these steps were presented separately, but together they represent a single unit of data processing and thus might profitably be done in the same R command using %&gt;%: gene_stats &lt;- dplyr::mutate(gene_stats, test1_padj=p.adjust(test1_p,method=&quot;fdr&quot;), test2_padj=p.adjust(test2_p,method=&quot;fdr&quot;), signif_either=(test1_padj &lt; 0.05 | test2_padj &lt; 0.05), signif_both=(test1_padj &lt; 0.05 &amp; test2_padj &lt; 0.05), gene=stringr::str_to_upper(gene) ) %&gt;% dplyr::select( gene, test1_stat, test1_p, test1_padj, test2_stat, test2_p, test2_padj, signif_either, signif_both ) %&gt;% dplyr::filter( test1_padj &lt; 0.05 | test2_padj &lt; 0.05 ) %&gt;% dplyr::arrange( test1_p ) gene_stats ## # A tibble: 2 x 9 ## gene test1_stat test1_p test1_padj test2_stat test2_p test2_padj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SNCA 45.7 0.0000000042 0.0000000126 0.757 0.915 0.915 ## 2 APOE 12.5 0.103 0.155 34.2 0.000013 0.000039 ## # ... with 2 more variables: signif_either &lt;lgl&gt;, signif_both &lt;lgl&gt; This complete pipeline now contains all of our manipulations and our mutated tibble can be passed on to downstream analysis or collaborators. 5.8 Grouping Data Sometimes we are interested in summarizing subsets of our data defined by some grouping variable. Consider the following made-up sample metadata for a set of individuals in an Alzheimers disease (AD) study: metadata &lt;- tribble( ~ID, ~condition, ~age_at_death, ~Braak_stage, ~APOE_genotype, &quot;A01&quot;, &quot;AD&quot;, 78, 5, &quot;e4/e4&quot;, &quot;A02&quot;, &quot;AD&quot;, 81, 6, &quot;e3/e4&quot;, &quot;A03&quot;, &quot;AD&quot;, 90, 5, &quot;e4/e4&quot;, &quot;A04&quot;, &quot;Control&quot;, 80, 1, &quot;e3/e4&quot;, &quot;A05&quot;, &quot;Control&quot;, 79, 0, &quot;e3/e3&quot;, &quot;A06&quot;, &quot;Control&quot;, 81, 0, &quot;e2/e3&quot; ) This is a typical setup for metadata in these types of experiments. There is a sample ID column which uniquely identifies each subject, a condition variable indicating which group each subject is in, and clinical information like age at death, Braak stage (a measure of Alzheimers disease pathology in the brain), and diploid APOE genotype (e2 is associated with reduced risk of AD, e3 is baseline, and e4 confers increased risk). An important experimental design consideration is to match sample attributes between groups as well as possible to avoid confounding our comparison of interest. In this case, age at death is one such variable that we wish to match between groups. Although these values look pretty well matched between AD and Control groups, it would be better to check explicitly. We can do this using dplyr::group_by() to group the rows together based on condition and dplyr::summarize() to compute the mean age at death for each group: dplyr::group_by(metadata, condition ) %&gt;% dplyr::summarize(mean_age_at_death = mean(age_at_death)) ## # A tibble: 2 x 2 ## condition mean_age_at_death ## &lt;chr&gt; &lt;dbl&gt; ## 1 AD 83 ## 2 Control 80 The dplyr::group_by() accepts a tibble and a column name for a column that contains a categorical variable (i.e. a variable with discrete values like AD and Control) and separates the rows in the tibble into groups according to the distinct values of the column. The dplyr::summarize() function accepts the grouped tibble and creates a new tibble with contents defined as a function of values for columns in for each group. From the example above, we see that the mean age at death is indeed different between the two groups, but not by much. We can go one step further and compute the standard deviation age range to further investigate: dplyr::group_by(metadata, condition ) %&gt;% dplyr::summarize( mean_age_at_death = mean(age_at_death), sd_age_at_death = sd(age_at_death), lower_age = mean_age_at_death-sd_age_at_death, upper_age = mean_age_at_death+sd_age_at_death, ) ## # A tibble: 2 x 5 ## condition mean_age_at_death sd_age_at_death lower_age upper_age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 83 6.24 76.8 89.2 ## 2 Control 80 1 79 81 Note the use of summarized variables defined first being used in variables defined later. Now we can see that the age ranges defined by +/- one standard deviation clearly overlap, which gives us more confidence that our average age at death for AD and Control are not significantly different. We used +/- one standard deviation to define the likely mean age range above and below the arithmetic mean for simplicity in the example above. The proper way to assess whether these distributions are significantly different is to use an appropriate statistical test like a t-test. Like other functions in dplyr, dplyr::summarize() has some helper functions that give it additional functionality. One useful helper function is n(), which is defined as the number of records within each group. We will add one more column to our summarized sample metadata from above that reports the number of subjects within each condition: dplyr::group_by(metadata, condition ) %&gt;% dplyr::summarize( num_subjects = n(), mean_age_at_death = mean(age_at_death), sd_age_at_death = sd(age_at_death), lower_age = mean_age_at_death-sd_age_at_death, upper_age = mean_age_at_death+sd_age_at_death ) ## # A tibble: 2 x 6 ## condition num_subjects mean_age_at_death sd_age_at_death lower_age upper_age ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 3 83 6.24 76.8 89.2 ## 2 Control 3 80 1 79 81 We now have a column with the number of subjects in each condition. Hadley Wickham is from New Zealand, which uses British, rather than American, English. Therefore, in many places, both spellings are supported in the tidyverse; e.g. both summarise() and summarize() are supported. R for Data Science - Grouped summaries with summarise() dplyr summarise() reference 5.9 Rearranging Data Sometimes the shape and format of our data is not the most convenient for performing certain operations on it, even if it is tidy. Lets say we are considering the range of statistics that were computed for all of our genes in the gene_stats tibble, and wanted to compute the average statistic over all genes for both tests. Recall our tibble has separate columns for each test: gene_stats &lt;- tribble( ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p, &quot;APOE&quot;, 12.509293, 0.1032, 34.239521, 1.3e-5, &quot;HOXD1&quot;, 4.399211, 0.6323, 16.332318, 0.0421, &quot;SNCA&quot;, 45.748431, 4.2e-9, 0.757188, 0.9146, ) gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ## 2 HOXD1 4.40 0.632 16.3 0.0421 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 For convenience, we desire our output to be in table form, with one row per test and the statistics for each test as columns. We could do this manually like so: tribble( ~test_name, ~min, ~mean, ~max, &quot;test1_stat&quot;, min(gene_stats$test1_stat), mean(gene_stats$test1_stat), max(gene_stats$test1_stat), &quot;test2_stat&quot;, min(gene_stats$test2_stat), mean(gene_stats$test2_stat), max(gene_stats$test2_stat), ) ## # A tibble: 2 x 4 ## test_name min mean max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 test1_stat 4.40 20.9 45.7 ## 2 test2_stat 0.757 17.1 34.2 This gets the job done, but is clearly very ugly, error prone, and would require significant work if we later added more statistics columns. Instead of typing out the values we desire manually, we can pivot our tibble using the tidyr::pivot_longer() function, so that the column values are placed in a new column and the corresponding values are placed in yet another column. This process is illustrated in the following figure: Pivot longer moves columns and values to two new columns In the figure, the original tibble has genes along rows and samples as columns. When the sample columns are pivoted, the value of each column name is placed in a new column named Sample and repeated for as many rows there are in the tibble. A second new column Value is populated with the corresponding values that were in each column. The gene associated with each value is preserved and repeated vertically until all the table columns and values have been pivoted. This process of pivoting transforms the tibble into so called long form. Returning to our gene_stats example, we can apply some operations to the tibble to easily perform the summarization we did above in a much more expressive manner: gene_stats %&gt;% tidyr::pivot_longer(ends_with(&quot;_stat&quot;), names_to=&quot;test&quot;, values_to=&quot;stat&quot;) %&gt;% dplyr::group_by(test) %&gt;% dplyr::summarize(min = min(stat), mean = mean(stat), max = max(stat)) ## # A tibble: 2 x 4 ## test min mean max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 test1_stat 4.40 20.9 45.7 ## 2 test2_stat 0.757 17.1 34.2 You may verify that the numbers are identical in this pivoted tibble as the one we manually created earlier. This pivoting method will produce the desired output regardless of the number of tests we include the table, so long as the column names end in \"_test\". The inverse of pivot_longer() is pivot_wider(). If you have variables gathered in single columns like that produced by pivot_longer() you can reverse the process with this function to create a tibble with those variables as columns. Pivoting - R for Data Science pivot_longer() reference 5.10 Relational Data As mentioned in our section on types of biological data, we often need to combine different sources of data together to aid in interpretation of our results. Below we redefine the tibble of gene statistics from above to have properly capitalized gene symbols: gene_stats &lt;- tribble( ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p, &quot;APOE&quot;, 12.509293, 0.1032, 34.239521, 1.3e-5, &quot;HOXD1&quot;, 4.399211, 0.6323, 16.332318, 0.0421, &quot;SNCA&quot;, 45.748431, 4.2e-9, 0.757188, 0.9146, ) gene_stats ## # A tibble: 3 x 5 ## gene test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ## 2 HOXD1 4.40 0.632 16.3 0.0421 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 Our gene identifiers in this data frame are gene symbols which, while convenient for our human brains to remember, can change over time and have many aliases (e.g. the APOE gene has also been called AD2, LDLCQ5, APO-E, and ApoE4 as listed on its genecard). This can make writing code that refers to a specific gene difficult, since there are so many possible names to look for. Fortunately, there are alternative gene identifier systems that do a better job of maintaining stable, consistent gene identifiers, one of the most popular being Ensembl. Ensembl gene IDs always take the form ENSGNNNNNNNNNNN where Ns are digits. These IDs are much more stable and predictable than gene symbols, and are preferable when working with genes in code. We wish to add Ensembl IDs for the genes in our gene_stats result to the tibble as a new column. Now suppose we have obtained another file with cross referenced gene identifiers like the following: gene_map &lt;- tribble( ~symbol, ~ENSGID, ~gene_name, &quot;APOE&quot;, &quot;ENSG00000130203&quot;, &quot;apolipoprotein E&quot;, &quot;BRCA1&quot;, &quot;ENSG00000012048&quot;, &quot;BRCA1 DNA repair associated&quot;, &quot;HOXD1&quot;, &quot;ENSG00000128645&quot;, &quot;homeobox D1&quot;, &quot;SNCA&quot;, &quot;ENSG00000145335&quot;, &quot;synuclein alpha&quot;, ) gene_map ## # A tibble: 4 x 3 ## symbol ENSGID gene_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE ENSG00000130203 apolipoprotein E ## 2 BRCA1 ENSG00000012048 BRCA1 DNA repair associated ## 3 HOXD1 ENSG00000128645 homeobox D1 ## 4 SNCA ENSG00000145335 synuclein alpha Imagine that this file contains mappings for all ~60,000 genes in the human genome. While it might be simple to look up our three genes in this file and annotate manually, it is easier to ask dplyr to do it for us. We can do this using the dplyr::left_join() function which accepts two data frames and the names of columns in each that share common values: dplyr::left_join( x=gene_stats, y=gene_map, by=c(&quot;gene&quot; = &quot;symbol&quot;) ) ## # A tibble: 3 x 7 ## gene test1_stat test1_p test2_stat test2_p ENSGID gene_name ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipoprot~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox D1 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein a~ Notice that the additional columns in gene_map that were not involved in the join (i.e. ENSG and gene_name) are appended to the gene_stats tibble. If we wanted to use pipes, we could implement the same join above as follows: gene_stats %&gt;% dplyr::left_join( gene_map, by=c(&quot;gene&quot; = &quot;symbol&quot;) ) ## # A tibble: 3 x 7 ## gene test1_stat test1_p test2_stat test2_p ENSGID gene_name ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipoprot~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox D1 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein a~ Under the hood, the dplyr::left_join() function took the values in gene_stats$gene and looked for the corresponding row in gene_map with the same value in in the symbol column. It then appends all the additional columns of gene_map for the matching rows and returns the result. We have added the identifier mapping we desired with only a few lines of code. And whats more, this code will work no matter how many genes we had in gene_stats as long as gene_map contains a mapping value for the values in gene_stats$gene. But what happens if we have genes in gene_stats that dont exist in our mapping? In the above example, we use a left join because we want to include all the rows in gene_stats regardless of whether a mapping exists in gene_map. In this case this was fine because all of our genes in gene_stats had a corresponding row in the mapping. However, notice that in the mapping there is an additional gene, BRCA1 that is not in our gene statistics tibble. If we reverse the order of the join, observe what happens: gene_map %&gt;% dplyr::left_join( gene_stats, by=c(&quot;symbol&quot; = &quot;gene&quot;) ) ## # A tibble: 4 x 7 ## symbol ENSGID gene_name test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE ENSG00000130203 apolipoprotein~ 12.5 1.03e-1 34.2 1.3 e-5 ## 2 BRCA1 ENSG00000012048 BRCA1 DNA repa~ NA NA NA NA ## 3 HOXD1 ENSG00000128645 homeobox D1 4.40 6.32e-1 16.3 4.21e-2 ## 4 SNCA ENSG00000145335 synuclein alpha 45.7 4.2 e-9 0.757 9.15e-1 Now the order of the rows is the same as in gene_map, and the columns for our missing gene BRCA1 are filled with NA. This is the left join at work, where the record from gene_map is included regardless of whether a corresponding value was found in gene_stats. There are additional types of joins besides left joins. Right joins are simply the opposite of left joins: gene_stats %&gt;% dplyr::right_join( gene_map, by=c(&quot;gene&quot; = &quot;symbol&quot;) ) ## # A tibble: 4 x 7 ## gene test1_stat test1_p test2_stat test2_p ENSGID gene_name ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipopr~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox ~ ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein~ ## 4 BRCA1 NA NA NA NA ENSG00000012048 BRCA1 DNA~ The result is the same as the left join on gene_map except the order of the resulting columns is different. Inner joins return results for rows that have a match between the two tibbles. Recall our left join on gene_map included BRCA1 even though it was not found in gene_stats. An inner join will not include this row, because no match in gene_stats was found: gene_map %&gt;% dplyr::inner_join( gene_stats, by=c(&quot;symbol&quot; = &quot;gene&quot;) ) ## # A tibble: 3 x 7 ## symbol ENSGID gene_name test1_stat test1_p test2_stat test2_p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 APOE ENSG00000130203 apolipoprotein E 12.5 1.03e-1 34.2 1.3 e-5 ## 2 HOXD1 ENSG00000128645 homeobox D1 4.40 6.32e-1 16.3 4.21e-2 ## 3 SNCA ENSG00000145335 synuclein alpha 45.7 4.2 e-9 0.757 9.15e-1 One last type of join is the dplyr::full_join() (also sometimes called an outer join). As you may expect, a full join will return all rows from both tibbles whether a match in the other table was found or not. 5.10.1 Dealing with multiple matches In the example above, there was a one-to-one relationship between the gene symbols in both tibbles. This made the joined tibble tidy. However, when a one-to-many relationship exists, i.e. one gene symbol in one tibble has multiple rows in the other, this can lead to what appears to be duplicate rows in the joined result. Due to the relative instability of gene symbols, it is very common to have multiple Ensembl genes associated with a single gene symbol. The following takes a gene mapping of Ensembl IDs to gene symbols and identifies cases where multiple Ensembl IDs map to a single gene symbol: readr::read_tsv(&quot;mart_export.tsv&quot;) %&gt;% dplyr::filter( `HGNC symbol` != &quot;NA&quot; &amp; # many unstudied genes have Ensembl IDs but no official symbol `HGNC symbol` %in% `HGNC symbol`[duplicated(`HGNC symbol`)]) %&gt;% dplyr::arrange(`HGNC symbol`) ## Rows: 68349 Columns: 3 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;\\t&quot; ## chr (3): Gene stable ID, HGNC symbol, Gene name ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 7,268 x 3 ## `Gene stable ID` `HGNC symbol` `Gene name` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ENSG00000261846 AADACL2 arylacetamide deacetylase like 2 ## 2 ENSG00000197953 AADACL2 arylacetamide deacetylase like 2 ## 3 ENSG00000262466 AADACL2-AS1 &lt;NA&gt; ## 4 ENSG00000242908 AADACL2-AS1 &lt;NA&gt; ## 5 ENSG00000276072 AATF apoptosis antagonizing transcription factor ## 6 ENSG00000275700 AATF apoptosis antagonizing transcription factor ## 7 ENSG00000281173 ABCB10P1 &lt;NA&gt; ## 8 ENSG00000280461 ABCB10P1 &lt;NA&gt; ## 9 ENSG00000274099 ABCB10P1 &lt;NA&gt; ## 10 ENSG00000282479 ABCB10P3 &lt;NA&gt; ## # ... with 7,258 more rows There are over 7,000 Ensembl IDs that map to the same gene symbol as another Ensembl ID. That is more than 10% of all Ensembl IDs. So now lets create a new gene_stats tibble with one of these gene symbols and join with the map to see what happens: gene_map &lt;- readr::read_tsv(&quot;mart_export.tsv&quot;) ## Rows: 68349 Columns: 3 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;\\t&quot; ## chr (3): Gene stable ID, HGNC symbol, Gene name ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. gene_stats &lt;- tribble( ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p, &quot;APOE&quot;, 12.509293, 0.1032, 34.239521, 1.3e-5, &quot;HOXD1&quot;, 4.399211, 0.6323, 16.332318, 0.0421, &quot;SNCA&quot;, 45.748431, 4.2e-9, 0.757188, 0.9146, &quot;DEAF1&quot;, 0.000000, 1.0, 0, 1.0 ) %&gt;% left_join(gene_map, by=c(&quot;gene&quot; = &quot;HGNC symbol&quot;)) gene_stats ## # A tibble: 5 x 7 ## gene test1_stat test1_p test2_stat test2_p `Gene stable ID` `Gene name` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipopro~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox D1 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein ~ ## 4 DEAF1 0 1 0 1 ENSG00000282712 DEAF1 tran~ ## 5 DEAF1 0 1 0 1 ENSG00000177030 DEAF1 tran~ Notice how there are two rows for the DEAF1 gene that have identical values except for the Stable Gene ID column. This is a very common problem when mapping gene symbols to other gene identifiers and there is no general solution to picking the best mapping, short of manually inspecting all of the duplicates and choosing which one is the most appropriate yourself (which obviously is a huge amount of work). However, we do desire to remove the duplicated rows. In this case, since all the values besides the Ensembl ID are the same, effectively it doesnt matter which duplicate rows we eliminate. We can do this using the duplicated() function, which returns TRUE for all but the first row of a set of duplicated values: filter(gene_stats, !duplicated(gene)) ## # A tibble: 4 x 7 ## gene test1_stat test1_p test2_stat test2_p `Gene stable ID` `Gene name` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 APOE 12.5 0.103 34.2 0.000013 ENSG00000130203 apolipopro~ ## 2 HOXD1 4.40 0.632 16.3 0.0421 ENSG00000128645 homeobox D1 ## 3 SNCA 45.7 0.0000000042 0.757 0.915 ENSG00000145335 synuclein ~ ## 4 DEAF1 0 1 0 1 ENSG00000282712 DEAF1 tran~ However, in general, you must be careful about identifying these types of one-to-many mapping issues and also about how you mitigate them. R for Data Science - Relational Data dplyr - mutate joins References "],["data-science.html", "6 Data Science 6.1 Data Modeling 6.2 Statistical Distributions &amp; Tests . 6.3 Clustering . 6.4 Network Analysis .", " 6 Data Science Data science is an enormous and rapidly growing field that incorporates elements of statistics, computer science, software engineering, high performance and cloud computing, and big data management, as well as syntheses with knowledge from other social and physical science fields to model and predict phenomena captured by data collected from the real world. Many books have and will be written on this subject, and this one does not pretend to even attempt to give this area an adequate treatment. Like the rest of this book, the topics covered here are opinionated and presented through the lens of a biological analyst practitioner with enough high level conceptual details and a few general principles to hopefully be useful in that context. 6.1 Data Modeling The goal of data modeling is to describe a dataset using a relatively small number of mathematical relationships. Said differently, a model uses some parts of a dataset to try to accurately predict other parts of the dataset in a way that is useful to us. Models are human inventions; they reflect our beliefs about the way the universe works. The successful model identifies patterns within a dataset that are the result of causal relationships in the universe that led to the phenomena that were measured while accounting for noise in the data. However, the model itself does not identify or even accurately reflect those causal effects. The model merely summarizes patterns and we as scientists are left to interpret those patterns and design follow up experiments to investigate the nature of those causal relationships using our prior knowledge of the world. There are several principles to keep in mind when modeling data: Data are never wrong. All data are collected using processes and devices designed and implemented by humans, who always have biases and make assumptions. All data measure something about the universe, and so are true in some sense of the word. If what we intended to measure and what we actually measured were not the same thing, that is due to our errors in collection or interpretation, not due to the data being wrong. If we approach our dataset with a particular set of hypotheses and the data dont support those hypotheses, it is our beliefs of the world and our understanding of the dataset that are wrong, not the data itself. Not all data are useful. Just because data isnt wrong, it doesnt mean it is useful. There may have been systematic errors in the collection of the data that makes interpreting them difficult. Data collected for one purpose may not be useful for any other purposes. And sometimes, a dataset collected for a particular purpose may simply not have the information needed to answer our questions; if what we measure has no relationship to what we wish to predict, the data itself is not useful - though the knowledge that what we measured has no detectable effect on the thing we wish to predict may be very useful! All models are wrong, but some are useful. George Box, the renowned British statistician, famously asserted this in a 1976 paper to the Journal of the American Statistical Association. (Box 1976). By this he meant that every model we create is a simplification of the system we are seeking to model, which is by definition not identical to that system. To perfectly model a system, our model would need to be precisely the system we are modeling, which is no longer a model but the system itself. Fortunately, even though we know our models are always wrong to some degree, they may nonetheless be useful because they are not too wrong. Some models may indeed be too wrong, though. Data do not contain causal information. Correlation does not mean causation. Data are measurements of the results of a process in the universe that we wish to understand; the data are possibly reflective of that process, but do not contain any information about the process itself. We cannot infer causal relationships from a dataset alone. We must construct possible causal models using our knowledge of the world first, then apply our data to our model and other alternative models to compare their relative plausibility. All data have noise. The usefulness of a model to describe a dataset is related to the relative strength of the patterns and noise in the dataset when viewed through the lens of the model; conceptually, the so-called signal to noise ratio of the data. The fundamental concern of statistics is quantifying uncertainty (i.e. noise), and separating it from real signal, though different statistical approaches (e.g. frequentist vs Bayesian) reason about uncertainty in different ways. Modeling begins (or should begin) with posing one or more scientific models of the process or phenomenon we wish to understand. The scientific model is conceptual; it reflects our belief of the universe and proposes a causal explanation for the phenomenon. We then decide how to map that scientific model onto a statistical model, which is a mechanical procedure that quantifies how well our scientific model explains a dataset. The scientific model and statistical model are related but independent choices we make. There may be many valid statistical models that represent a given scientific model. However, sometimes in practice we lack sufficient knowledge about the process to propose scientific models first, requiring data exploration and summarization first to suggest reasonable starting points. This section pertains primarily to models specified explicitly by humans. There is another class of models, namely those created by certain machine learning algorithms like neural networks and deep learning, that discover models from data. These models are fundamentally different than those designed by human minds, in that they are often accurate and therefore useful, but it can be very difficult if not impossible to understand how they work. While these are important types of models that fall under the umbrella of data science, we limit the content of this chapter to human designed statistical models. 6.1.1 A Worked Modeling Example As an example, lets consider a scenario where we wish to assess whether any of three genes can help us distinguish between patients who have Parkinsons Disease and those who dont by measuring the relative activity of those genes in blood samples. We have the following made-up dataset: gene_exp ## # A tibble: 200 x 5 ## sample_name `Disease Status` `Gene 1` `Gene 2` `Gene 3` ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 P1 Parkinson&#39;s 257. 636. 504. ## 2 P2 Parkinson&#39;s 241. 556. 484. ## 3 P3 Parkinson&#39;s 252. 426. 476. ## 4 P4 Parkinson&#39;s 271. 405. 458. ## 5 P5 Parkinson&#39;s 248. 482. 520. ## 6 P6 Parkinson&#39;s 275. 521. 460. ## 7 P7 Parkinson&#39;s 264. 415. 568. ## 8 P8 Parkinson&#39;s 276. 450. 495. ## 9 P9 Parkinson&#39;s 274. 490. 496. ## 10 P10 Parkinson&#39;s 251. 584. 573. ## # ... with 190 more rows Our imaginary dataset has 100 Parkinsons and 100 control subjects. For each of our samples, we have a sample ID, Disease Status of Parkinson's or Control, and numeric measurements of each of three genes. Below are violin plots of our (made-up) data set for these three genes: By inspection, it appears that Gene 1 has no relationship with disease; we may safely eliminate this gene from further consideration. Gene 2 appears to have a different profile depending on disease status, where control individuals have a higher average expression and a lower variance. Unfortunately, despite this qualitative difference, this gene may not be useful for telling whether someone has disease or not - the ranges completely overlap. Gene 3 appears to discriminate between disease and control. There is some overlap in the two expression distributions, but above a certain expression value these data suggest a high degree of predictive accuracy may be obtained with this gene. Measuring this gene may therefore be useful, if the results from this dataset generalize to all people with Parkinsons Disease. So far, we have not done any modeling, but instead relied on plotting and our eyes. A more quantitative question might be: how much higher is Gene 3 expression in Parkinsons Disease than control? Another way of posing this question is: if I know a patient has Parkinsons Disease, what Gene 3 expression value do I expect them to have? Written this way, we have turned our question into a prediction problem: if we only had information that a patient had Parkinsons Disease, what is the predicted expression value of their Gene 3? Another way to pose this prediction question is in the opposite (and arguably more useful) direction: if all we knew about a person was their Gene 3 gene expression, how likely is it that the person has Parkinsons Disease? If this gene expression is predictive enough of a persons disease status, it may be a viable biomarker of disease and thus might be useful in a clinical setting, for example when identifying presymptomatic individuals or assessing the efficacy of a pharmacological treatment. Although it may seems obvious, before beginning to model a dataset, we must start by posing the scientific question as concisely as possible, as we have done above. These questions will help us identify which modeling techniques are appropriate and help us ensure we interpret our results correctly. We will use this example dataset throughout this chapter to illustrate some key concepts. 6.1.2 Data Summarization Broadly speaking, data summarization is the process of finding a lower-dimensional representation of a larger dataset. There are many ways to summarize a set of data; each approach will emphasize different aspects of the dataset, and have varying degrees of accuracy. Consider the gene expression of Gene 1 for all individuals in our example above, plotted as a distribution with a histogram: ggplot(gene_exp, aes(x=`Gene 1`)) + geom_histogram(bins=30,fill=&quot;#a93c13&quot;) 6.1.2.1 Point Estimates The data are concentrated around the value 250, and become less common for larger and smaller values. Since the extents to the left and right of the middle of the distribution appear to be equally distant, perhaps the arithmetic mean is a good way to identify the middle of the distribution: ggplot(gene_exp, aes(x=`Gene 1`)) + geom_histogram(bins=30,fill=&quot;#a93c13&quot;) + geom_vline(xintercept=mean(gene_exp$`Gene 1`)) By eye, the mean does seem to correspond well to the value that is among the most frequent, and successfully captures an important aspect of the data: its central tendency. Summaries that compute a single number are called point estimates. Point estimates collapse the data into one singular point, one value. The arithmetic mean is just one measure of central tendency, computed by taking the sum of all the values and dividing by the number of values. The mean may be a good point estimate of the central tendency of a dataset, but it is sensitive to outlier samples. Consider the following examples: library(patchwork) well_behaved_data &lt;- tibble(data = rnorm(1000)) data_w_outliers &lt;- tibble(data = c(rnorm(800), rep(5, 200))) # oops I add some outliers :^) g_no_outlier &lt;- ggplot(well_behaved_data, aes(x = data)) + geom_histogram(fill = &quot;#56CBF9&quot;, color = &quot;grey&quot;, bins = 30) + geom_vline(xintercept = mean(well_behaved_data$data)) + ggtitle(&quot;Mean example, no outliers&quot;) g_outlier &lt;- ggplot(data_w_outliers, aes(x = data)) + geom_histogram(fill = &quot;#7FBEEB&quot;, color = &quot;grey&quot;, bins = 30) + geom_vline(xintercept = mean(data_w_outliers$data)) + ggtitle(&quot;Mean example, big outliers&quot;) g_no_outlier | g_outlier The median is another measure of central tendency, which is found by identifying the value that divides the samples into equal halves when sorted from smallest to largest. The median is more robust in the presence of outliers. g_no_outlier &lt;- ggplot(well_behaved_data, aes(x = data)) + geom_histogram(fill = &quot;#AFBED1&quot;, color = &quot;grey&quot;, bins = 30) + geom_vline(xintercept = median(well_behaved_data$data)) + ggtitle(&quot;Median example&quot;) g_outlier &lt;- ggplot(data_w_outliers, aes(x = data)) + geom_histogram(fill = &quot;#7FBEEB&quot;, color = &quot;grey&quot;, bins = 30) + geom_vline(xintercept = median(data_w_outliers$data)) + ggtitle(&quot;Median example, big outliers&quot;) g_no_outlier | g_outlier 6.1.2.2 Dispersion Central tendencies are important aspects of the data but dont describe what the data do for values outside this point estimate of central tendency; in other words, we have not expressed the spread, or dispersion of the data. We decide that perhaps computing the standard deviation of the data may characterize the spread well, since it appears to be symmetric around the mean. We can layer this information on the plot as well to inspect it: g1_mean &lt;- mean(gene_exp$`Gene 1`) g1_sd &lt;- sd(gene_exp$`Gene 1`) ggplot(gene_exp, aes(x=`Gene 1`)) + geom_histogram(bins=30,fill=&quot;#a93c13&quot;) + geom_vline(xintercept=g1_mean) + geom_segment(x=g1_mean-g1_sd, y=10, xend=g1_mean+g1_sd, yend=10) The width of the horizontal line is proportional to the mean +/- one standard deviation around the mean, and has been placed arbitrarily on the y axis at y = 10 to show how this range covers the data in the histogram. The +/- 1 standard deviation around the mean visually describes the spread of the data reasonably well. Measures the spread of the data, typically around its perceived center (a mean). Often related to the distribution of the data. Standard deviation: A measure of how close values are to the mean. Bigger standard deviations mean data is more spread out. data &lt;- tibble(data = c(rnorm(1000, sd=1.75))) ggplot(data, aes(x = data)) + geom_histogram(fill = &quot;#EAC5D8&quot;, color = &quot;white&quot;, bins = 30) + geom_vline(xintercept = c(-2, -1, 0, 1, 2) * sd(data$data)) + xlim(c(-6, 6)) + ggtitle(&quot;Standard deviations aplenty&quot;, paste(&quot;SD:&quot;, sd(data$data))) Variance: Similar to SD (its the square of SD), variance measures how far a random value is from the mean. data &lt;- tibble(data = c(rnorm(1000, sd=0.5))) ggplot(data, aes(x = data)) + geom_histogram(fill = &quot;#DBD8F0&quot;, color = &quot;white&quot;, bins = 30) + geom_vline(xintercept = mean(data$data)) + xlim(c(-6, 6)) + ggtitle(&quot;Same mean as SD plot, but different variance&quot;, paste(&quot;Variance:&quot;, sd(data$data))) 6.1.2.3 Distributions With these two pieces of knowledge - the mean accurately describes the center of the data and the standard deviation describes the spread - we now recognize that these data may be normally distributed, and therefore we can potentially describe the dataset mathematically. We decide to visually inspect this possibility by layering a normal distribution on top of our data using stat_function: g1_mean &lt;- mean(gene_exp$`Gene 1`) g1_sd &lt;- sd(gene_exp$`Gene 1`) ggplot(gene_exp, aes(x=`Gene 1`)) + geom_histogram( aes(y=after_stat(density)), bins=30, fill=&quot;#a93c13&quot; ) + stat_function(fun=dnorm, args = list(mean=g1_mean, sd=g1_sd), size=2) Note the histogram bars are scaled with aes(y=[after_stat](https://ggplot2.tidyverse.org/reference/aes_eval.html)(density)) to the density of values in each bin to make all the bar heights sum to 1 so that the y scale matches that of a normal distribution. We have now created our first model: we chose to express the dataset as a normal distribution parameterized by the mean and standard deviation and standard deviation of the data. Using the values of 254 and 11 as our mean and standard deviation, respectively, we can express our model mathematically as follows: \\[ Gene\\;1 \\sim \\mathcal{N}(254, 11) \\] Here the \\(\\sim\\) symbol means distributed as and the \\(\\mathcal{N}(\\mu,\\sigma)\\) represents a normal distribution with mean of \\(\\mu\\) and standard deviation of \\(\\sigma\\). This is mathematical formulation means the same thing as saying we are modeling Gene 1 expression as a normal distribution with mean of 254 and standard deviation of 11. Without any additional information about a new sample, we would expect the expression of that gene to be 254, although it may vary from this value. The normal distribution is the most common distribution observed in nature, but it is hardly the only one. We could have proposed other distributions to instead summarize our data: g_norm &lt;- ggplot(tibble(data = rnorm(5000)), aes(x = data)) + geom_histogram(fill = &quot;#D0FCB3&quot;, bins = 50, color = &quot;gray&quot;) + ggtitle(&quot;Normal distribution&quot;, &quot;rnorm(n = 1000)&quot;) g_unif &lt;- ggplot(tibble(data = runif(5000)), aes(x = data)) + geom_histogram(fill = &quot;#271F30&quot;, bins = 50, color = &quot;white&quot;) + ggtitle(&quot;Uniform distribution&quot;, &quot;runif(n = 1000)&quot;) g_logistic &lt;- ggplot(tibble(data = rlogis(5000)), aes(x = data)) + geom_histogram(fill = &quot;#9BC59D&quot;, bins = 50, color = &quot;black&quot;) + ggtitle(&quot;Logistic distribution&quot;, &quot;rlogis(n = 1000)&quot;) g_exp &lt;- ggplot(tibble(data = rexp(5000, rate = 1)), aes(x = data)) + geom_histogram(fill = &quot;#6C5A49&quot;, bins = 50, color = &quot;white&quot;) + ggtitle(&quot;Exponential distribution&quot;, &quot;rexp(n = 1000, rate = 1)&quot;) (g_norm | g_unif) / (g_logistic | g_exp) In addition to the normal distribution, we have also plotted samples drawn from a continuous uniform distribution between 0 and 1, a logistic distribution which is similar to the normal distribution but has heavier tails, and an exponential distribution. There are many more distributions than these, and many of them were discovered to arise in nature and encode different types of processes and relationships. A few notes on our data modeling example before we move on: Our model choice was totally subjective. We looked at the data and decided that a normal distribution was a reasonable choice. There were many other choices we could have made, and all of them would be equally valid, though they may not all describe the data equally well. We cant know if this is the correct model for the data. By eye, it appears to be a reasonably accurate summary. However, there is no such thing as a correct model; some models are simply better at describing the data than others. Recall that all models are wrong, and some models are useful. Our model may be useful, but it is definitely wrong to some extent. We dont know how well our model describes the data yet. So far weve only used our eyes to choose our model which might be a good starting point considering our data are so simple, but we have not yet quantified how well our model describes the data, or compared it to alternative models to see which is better. This will be discussed briefly in a later section. 6.1.3 Linear Models Our choice of a normal distribution to model our Gene 1 gene expression was only descriptive; it was a low-dimensional summary of our dataset. However, it was not very informative; it doesnt tell us anything useful about Gene 1 expression with respect to our scientific question of distinguishing between Parkinsons Disease and Control individuals. To do that, we will need to find a model that can make predictions that we may find useful if we receive new data. To do that, we will introduce a new type of model: the linear model. A linear model is any statistical model that relates one outcome variable as a linear combination (i.e. sum) of one or more explanatory variables. This may be expressed mathematically as follows: \\[ Y_i = \\beta_0 + \\beta_1 \\phi_1 ( X_{i1} ) + \\beta_2 \\phi_2 ( X_{i2} ) + \\ldots + \\beta_p \\phi_p ( X_{ip} ) + \\epsilon_i \\] Above, \\(Y_i\\) is some outcome or response variable we wish to model, \\(X_{ij}\\) is our explanatory or predictor variable \\(j\\) for observatoin \\(i\\), and \\(\\beta_j\\) are coefficients estimated to minimize the difference between the predicted outcome \\(\\hat{Y_i}\\) and the observed \\(Y_i\\) over all observations. \\(\\phi_j\\) is a possibly non-linear transformation of the explanatory variable \\(X_ij\\); note these functions may be non-linear so long as the predicted outcome is modeled as a linear combination of the transformed variables. The rest of this section is dedicated to a worked example of a linear model for gene expression data. Let us begin with a beeswarm plot plot of Gene 3: library(ggbeeswarm) ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) + geom_beeswarm() The expression values within each disease status look like they might be normally distributed just like Gene 1, so lets summarize each group with the arithmetic mean and standard deviation as before, and instead plot both distributions as histograms: exp_summ &lt;- pivot_longer( gene_exp, c(`Gene 3`) ) %&gt;% group_by(`Disease Status`) %&gt;% summarize(mean=mean(value),sd=sd(value)) pd_mean &lt;- filter(exp_summ, `Disease Status` == &quot;Parkinson&#39;s&quot;)$mean c_mean &lt;- filter(exp_summ, `Disease Status` == &quot;Control&quot;)$mean ggplot(gene_exp, aes(x=`Gene 3`, fill=`Disease Status`)) + geom_histogram(bins=20, alpha=0.6,position=&quot;identity&quot;) + annotate(&quot;segment&quot;, x=c_mean, xend=pd_mean, y=20, yend=20, arrow=arrow(ends=&quot;both&quot;, angle=90)) + annotate(&quot;text&quot;, x=mean(c(c_mean,pd_mean)), y=21, hjust=0.5, label=&quot;How different?&quot;) We can make a point estimate of this difference by simply subtracting the means: pd_mean - c_mean ## [1] 164.0942 In other words, this point estimate suggests that on average Parkinsons patients have 164.1 greater expression than Controls. We can plot this relationship relatively simply: ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) + geom_beeswarm() + annotate(&quot;segment&quot;, x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) However, this point estimate tells us nothing about how confident we are about the difference. We can do this by using an linear regression by modeling Gene 3 as a function of disease status: fit &lt;- lm(`Gene 3` ~ `Disease Status`, data=gene_exp) fit ## ## Call: ## lm(formula = `Gene 3` ~ `Disease Status`, data = gene_exp) ## ## Coefficients: ## (Intercept) `Disease Status`Parkinson&#39;s ## 334.6 164.1 The coefficient associated with having the disease status of Parkinsons disease is almost exactly equal to our difference in means. We also note that the coefficient labeled (Intercept) is nearly equal to the mean of our control samples (334.6). Under the hood, this simple linear model did the same calculation we did by subtracting the means of each group but estimated the means using all the data at once, instead of point estimates. Another advantage of using lm() over the point estimate method is the model can estimate how confident the model was that the difference in mean between Parkinsons and controls subjects. Lets print some more information about the model than before: summary(fit) ## ## Call: ## lm(formula = `Gene 3` ~ `Disease Status`, data = gene_exp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -124.303 -25.758 -2.434 30.518 119.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 334.578 4.414 75.80 &lt;2e-16 *** ## `Disease Status`Parkinson&#39;s 164.094 6.243 26.29 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 44.14 on 198 degrees of freedom ## Multiple R-squared: 0.7773, Adjusted R-squared: 0.7761 ## F-statistic: 691 on 1 and 198 DF, p-value: &lt; 2.2e-16 We again see our coefficient estimates for the intercept (i.e. mean control expression) and our increase in Parkinsons, but also a number of other terms, in particular Pr(&gt;|t|) and Multiple R-squared. The former is the p-value associated with each of the coefficient estimates, both of which are very small, indicating to us that the model was very confident of the estimated values. The latter, multiple R-squared or \\(R^2\\), describes how much of the variance in the data was explained by the model it found as a fraction between 0 and 1. This model explains 77.7% of the variance of the data, which is substantial. The \\(R^2\\) value also has an associated p-value, which is also very small. Overall, these statistics suggest this model fits the data very well. We can plot the results of a linear model for each of our genes relatively easily: pd_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Parkinson&#39;s&quot;)$`Gene 1`) c_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Control&quot;)$`Gene 1`) g1 &lt;- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 1`, color=`Disease Status`)) + geom_beeswarm() + annotate(&quot;segment&quot;, x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) + theme(legend.position=&quot;none&quot;) pd_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Parkinson&#39;s&quot;)$`Gene 2`) c_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Control&quot;)$`Gene 2`) g2 &lt;- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 2`, color=`Disease Status`)) + geom_beeswarm() + annotate(&quot;segment&quot;, x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) + theme(legend.position=&quot;none&quot;) pd_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Parkinson&#39;s&quot;)$`Gene 3`) c_mean &lt;- mean(filter(gene_exp,`Disease Status`==&quot;Control&quot;)$`Gene 3`) g3 &lt;- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) + geom_beeswarm() + annotate(&quot;segment&quot;, x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) + theme(legend.position=&quot;none&quot;) g1 | g2 | g3 We can also compute the corresponding linear model fits, and confirm that the coefficients agree with the directions observed in the plot, as well as that all the associations are significant FDR &lt; 0.05: fit1 &lt;- lm(`Gene 1` ~ `Disease Status`, data=gene_exp) fit2 &lt;- lm(`Gene 2` ~ `Disease Status`, data=gene_exp) fit3 &lt;- lm(`Gene 3` ~ `Disease Status`, data=gene_exp) gene_stats &lt;- bind_rows( c(&quot;Gene 1&quot;,coefficients(fit1),summary(fit1)$coefficients[2,4]), c(&quot;Gene 2&quot;,coefficients(fit2),summary(fit2)$coefficients[2,4]), c(&quot;Gene 3&quot;,coefficients(fit3),summary(fit3)$coefficients[2,4]) ) colnames(gene_stats) &lt;- c(&quot;Gene&quot;,&quot;Intercept&quot;,&quot;Parkinson&#39;s&quot;,&quot;pvalue&quot;) gene_stats$padj &lt;- p.adjust(gene_stats$pvalue,method=&quot;fdr&quot;) gene_stats ## # A tibble: 3 x 5 ## Gene Intercept `Parkinson&#39;s` pvalue padj ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Gene 1 250.410735540151 6.95982194659045 3.92131170913765e-06 3.92e- 6 ## 2 Gene 2 597.763814010886 -93.6291659250254 2.37368730897756e-13 3.56e-13 ## 3 Gene 3 334.57788193953 164.094204856583 1.72774902843408e-66 5.18e-66 We have just performed our first elementary differential expression analysis using a linear model. Specifically, we examined each of our genes for a statistical relationship with having Parkinsons disease. The mechanics of this analysis are beyond the scope of this book, but later when we consider differential expression analysis packages in chapter 7 this pattern should be familiar after this example. If you are familiar with logistic regression, you might have wondered why we didnt model disease status, which is a binary variable, as a function of gene expression like Disease Status ~ Gene 3. There are several reasons for this, complete separation principally among them. In logistic regression, complete separation occurs when all the predictor values (e.g. gene expression) for one outcome group are greater or smaller than the other; i.e. there is no overlap in the values between groups. This causes logistic regression to fail to converge, leaving these genes with no statistics even though these genes are potentially the most interesting! There are methods (e.g. Firths Logistic Regression) that overcome this problem, but methods that model gene expression as a function of other outcome variables were developed first and remain the most popular. Modeling Basics - R for Data Science Least squares regression 6.1.4 Flavors of Linear Models The linear model implemented above is termed linear regression due to the way it models the relationship between the predictor variables and the outcome. Specifically, linear regression makes some strong assumptions about that relationship that may not always hold for all datasets. To address this limitation, a more flexible class of linear models called generalized linear models that allow these assumptions to be relaxed by changing the mathematical relationship between the predictors and outcome using a link function, and/or by mathematically transforming the predictors themselves. Some of the more common generalized linear models are listed below: Logistic regression - models a binary outcome (e.g. disease vs control) as a linear combination of predictor variables by using the logistic function as the link function. Multinomial regression - models a multinomial (i.e. categorical variable with more than two categories) using a multinomial logit function link function Poisson regression - models an outcome variable that are count data as a linear combination of predictors using the logarithm as the link function; this model is commonly used when modeling certain types of high throughput sequencing data Negative binomial regression - also models an outcome variable that are count data but relaxes some of the assumptions of Poisson regression, namely the mean-variance equality constraint; negative binomial regression is commonly used to model gene expression estimated from RNASeq data. Generalized linear models are very flexible, and there are many other types of these models used in biology and data science. It is important to be aware of the characteristics of the outcome you wish to model and choose the modeling method that is most suitable. Generalized linear models 6.1.5 Assessing Model Accuracy . 6.2 Statistical Distributions &amp; Tests . 6.2.1 Statistical Distributions By definition, a distribution is a function that shows the possible values for a variable and how often they occur. Generally, they are divided into two categories: discrete distributions and continuous distributions. 6.2.1.1 Discrete distributions 6.2.1.2 Bernoulli random trail and more One of the examples for discrete random variable distribution is the Bernoulli function. A Bernoulli trail has only 2 outcomes with probability \\(p\\) and \\((1-p)\\). Consider flipping a fair coin, and the random variable X can take value 0 or 1 indicating you get a head or a tail. If its a fair coin, we would expect the Pr{ X = 0 } = Pr{ X = 1 } = 0.5. Or, if we throw a die and we record X = 1 when we get a six, and X = 0 otherwise, then Pr{ X = 0 } = 5/6 and Pr{ X = 1 } = 1/6. Now consider a slightly complicated situation: what if we are throwing the die \\(n\\) times and we would like to analyze the total number of six, say \\(x\\), we get during those \\(n\\) throws? Now, this leads us to the binomial distribution. If its a fair die, we would say our proportion parameter p = 1/6, which means the probability we are getting a six is 1/6 for each throw. \\[\\begin{equation} f(x) = \\frac {n!} {x!(n-x)!}p^x (1-p) ^{(n-x)} \\end{equation}\\] The geometric random variable, similar to the binomial, is also from a sequence of random Bernoulli trials with a constant probability parameter p.  But this time, we define the random variable X as the number of consecutive failures before the first success. In this case, the probability of x consecutive failures followed by success on trial x+1 is: \\[\\begin{equation} f(x) = p * (1-p)^x \\end{equation}\\] The negative binomial distribution goes one step forward. This time, we are still performing a sequence of independent Bernoulli random trials with a constant probability of success equal to p. But now we would like to record the random variable Z to be the total number of failures before we finally get to the \\(r^{th}\\) success. In other words, when we get to the \\(r^{th}\\) success, we had x+r Bernoulli random trails, in which x times failed and r times succeeded. \\[\\begin{equation} f(x) = \\frac {x+r-1} {r-1} p^r {(1-p)}^x \\end{equation}\\] 6.2.1.3 Poisson The Poisson distribution is used to express the probability of a given number of events occurring in a fixed interval of time or space, and these events occur with a known constant mean rate and independently of the time since the last event. But no one understands this definition. The formula for Poisson distribution is: \\[\\begin{equation} f(k; \\lambda) = Pr(X=k) = \\frac {\\lambda^k e^{-\\lambda}} {k!} \\end{equation}\\] lambda is the expected value of the random variable X k is the number of occurrences e is Eulers number (e=2.71828) okay, the formula makes it even more confusing. Imagine you are working at a mail reception center, and your responsibility is to receive incoming letters. Assume the number of incoming letters is not affected by the day of the week or season of the year. You are expected to get 20 letters on average in a day. But, the actual number of letters you receive each day will not be perfectly 20. You recorded the number of letters you receive each day in a month (30 days). In the following plot, each dot represents a day. The x-axis is calender day, and y-axis is the number of letters you receive on that day. Although on average you are receiving 20 letters each day, the actual number of letters each day vary a lot. set.seed(2) my_letter &lt;- rpois(n = 30, lambda = 20) plot(my_letter, main = &quot;Letters received each day&quot;, xlab = &quot;day of the month&quot;, ylab = &quot;number of letters&quot;, pch = 19, col = &quot;royalblue&quot; ) abline(a = 20, b = 0, lwd = 2, lty = 3, col = &quot;salmon&quot;) Now, lets plot the density plot of our data. The x-axis is the number of letters on a single day, and the y-axis is the probability. plot(density(my_letter), lwd = 2, col = &quot;royalblue&quot;, main = &quot;Probability of number of letters each day&quot;, xlab = &quot;number of letters&quot; ) Since we only have 30 data points, it doesnt look like a good curve. But, after we worked at the mail reception for 5000 days, it becomes much closer to the theoretical Poisson distribution with lambda = 20. set.seed(3) plot(density(rpois(n = 5000, lambda = 20)), lwd = 2, col = &quot;royalblue&quot;, main = &quot;Probability of number of letters each day&quot;, xlab = &quot;number of letters&quot; ) Here is the theoretical Poisson distribution with lambda = 20. plot(dpois(c(1:40), lambda = 20), lwd = 2, type = &quot;l&quot;, col = &quot;royalblue&quot;, ylab = &quot;probability&quot;, main = &quot;Poisson lambda=20&quot; ) If we want to know whats the probability to receive, for example, 18 letters, we can use dpois() function. dpois(x = 18, lambda = 20) ## [1] 0.08439355 If we want to know the probability of receiving 18 or less letters, use ppois() function. ppois(q = 18, lambda = 20, lower.tail = T) ## [1] 0.3814219 It is the cumulative area colored in the following plot: plot(dpois(c(1:40), lambda = 20), lwd = 2, type = &quot;l&quot;, col = &quot;royalblue&quot;, ylab = &quot;probability&quot;, main = &quot;Poisson lambda=20&quot; ) polygon( x = c(1:18, 18), y = c(dpois(c(1:18), lambda = 20), 0), border = &quot;royalblue&quot;, col = &quot;lightblue1&quot; ) segments(x0 = 18, y0 = 0, y1 = 0.08439355, lwd = 2, lty = 2, col = &quot;salmon&quot;) qpois() is like the reverse of ppois(). qpois(p = 0.3814219, lambda = 20) ## [1] 18 Lets review the definition of Poisson distribution. - these events occur in a fixed interval of time or space, which is a day; - these events occur with a known constant mean rate, which is 20 letters; - independently of the time since the last event, which means the number of letters you receive today is independent of the letter you receive tomorrow. more work today dont guarantee less work tomorrow, just like in real life Now lets re-visit the formula. \\[\\begin{equation} f(k; \\lambda) = Pr(X=k) = \\frac {\\lambda^k e^{-\\lambda}} {k!} \\end{equation}\\] lambda is the expected value of X, which is 20 letters in this example. k is the number of occurrences, which is the number of letters you get on a specific day. e is Eulers number (e=2.71828) 6.2.1.4 Continuous distributions There are a number of different continuous distributions. Here we will focus on the Normal distribution (Gaussian distribution). A lot of variables in real life are normally distributed, common examples include peoples height, blood pressure, and students exam score. This is what a normal distribution with mean = 0 and standard deviation = 1 looks like: set.seed(2) norm &lt;- rnorm(n = 50000, mean = 0, sd = 1) plot(density(norm), main = &quot;A Normal distribution&quot;, xlab = &quot;x&quot;, lwd = 2, col = &quot;royalblue&quot; ) Similar as the Poisson distribution above, there are several functions to work with normal distribution, including rnorm(), dnorm(), pnorm(), and qnorm(). rnorm() is used to draw random data points from a normal distribution with a given mean and standard deviation. set.seed(2) rnorm( n = 6, # number of data points to draw mean = 0, # mean sd = 1 ) # standard deviation ## [1] -0.89691455 0.18484918 1.58784533 -1.13037567 -0.08025176 0.13242028 dnorm() is the density at a given quantile. For instance, in the normal distribution (mean=0, sd=1) above, the probability density at 0.5 is roughly 0.35. dnorm(x = 0.5, mean = 0, sd = 1) ## [1] 0.3520653 plot(density(norm), main = &quot;A Normal distribution&quot;, xlab = &quot;x&quot;, lwd = 2, col = &quot;royalblue&quot; ) segments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 3, col = &quot;salmon&quot;) segments(x0 = -5, y0 = 0.3520653, x1 = 0.5, lwd = 2, lty = 3, col = &quot;salmon&quot;) points(x = 0.5, y = 0.3520653, cex = 1.5, lwd = 2, col = &quot;red&quot;) text(x = 1.1, y = 0.36, labels = &quot;0.3521&quot;, col = &quot;red2&quot;) pnorm() gives the distribution function. Or, you can think of it as the cumulative of the left side of the density function until a given value, which is the area colored in light blue in the following plot. pnorm(q = 0.5, mean = 0, sd = 1) ## [1] 0.6914625 plot(density(norm), main = &quot;A Normal distribution&quot;, xlab = &quot;x&quot;, lwd = 2, col = &quot;royalblue&quot; ) polygon( x = c(density(norm)$x[density(norm)$x &lt;= 0.5], 0.5), y = c(density(norm)$y[density(norm)$x &lt;= 0.5], 0), border = &quot;royalblue&quot;, col = &quot;lightblue1&quot; ) segments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 2, col = &quot;salmon&quot;) qnorm() gives the quantile function. You can think of it as the reverse of pnorm(). For instance: pnorm(q = 0.5, mean = 0, sd = 1) ## [1] 0.6914625 qnorm(p = 0.6914625, mean = 0, sd = 1) ## [1] 0.5000001 Similarly, other distributions such as chi-square distribution, they all have the same set of functions: rchisq(), dchisq(), pchisq() and qchisq() etc. 6.2.2 Statistical Tests . What is a statistical test, and when is it appropriate to run one? How do we know which tests are appropriate in a given situation? What is the difference between a parametric and non-parametric test? What are some common tests (e.g. t-test, Chi-sq, hypergeometric, etc), and how do we run them in R? 6.2.3 p-values . What is a p-value? What is the relationship between a p-value and a distribution? What does a p-value look like when plotted with a distribution? How do we interpret p-values? How do we compute a p-value using a distribution and a statistic using R? (Should we mention critical values?) 6.2.4 Multiple Hypothesis Testing . What is multiply hypothesis testing adjustment? Why is it important? What are some common adjustment methods (Bonferroni (FWER) and Benjamini-Hochberg (FDR))? How do we interpret adjusted p-values (depends on the adjustment method)? How do we compute adjusted p-values in R? 6.2.5 Statistical power . Conceptually, what is statistical power, and what does it mean? Why is it important? What is the relationship between a dataset, an analysis, and power? What aspects of an analysis influence the statistical power of a test? (I dont think its reasonable to ask students to perform power calculations, just to be aware that power is a thing and generally what it is). 6.3 Clustering . What is clustering? What is the goal of clustering? How does clustering relate to data modeling (e.g. clustering is usually hypothesis-free)? How is clustering useful (i.e. what can we do with different clusters)? What are some common clustering methods, and how do we implement and visualize the results in R? 6.4 Network Analysis . What is a network? What is network analysis? What are some example network analysis applications in biology and bioinformatics (there is a full ksection on [Biological Networks] in the bio chapter, these are just illustrative examples)? How do we represent networks in R, and how do we analyze them? ([Network visualization] is covered in the data viz chapter, might be helpful to write these two sections together) References "],["data-visualization.html", "7 Data Visualization 7.1 Grammar of Graphics 7.2 Plotting One Dimension 7.3 Visualizing Distributions 7.4 Plotting Two or More Dimensions . 7.5 Other Kind of Plots . 7.6 Tips and Tricks . 7.7 Multiple Plots . 7.8 Responsible Plotting . 7.9 Publication Ready Plots . 7.10 Network visualization .", " 7 Data Visualization Data visualization is a core component of both exploring data and communicating results to others. The goal of data visualization is to present data in a graphical way that shows the reader patterns that would not otherwise be visible. Despite its ubiquity and importance, effective data visualization is challenging, and while many tools and approaches exist there is no gold standard to follow in any meaningful sense. Rather, an effective visualization has the following properties: Depicts accurate data Depicts data accurately Shows enough, but not too much, of the data for the viewer to gain insight Is self contained - no additional information (except a caption) is required to understand the contents of the figure Beyond these, a great visualization has some additional properties: Exposes patterns in the data not easily observable by other methods Invites the viewer to ask more questions about the data 7.1 Grammar of Graphics The grammar of graphics is a system of rules that describes how data and graphical aesthetics (e.g. color, size, shape, etc) are combined to form graphics and plots. First popularized in the book The Grammar of Graphics by Leland Wilkinson and co-authors in 1999, this grammar is a major contribution to the structural theory of statistical graphics. In 2005, Hadley Wickam wrote an implementation of the grammar of graphics in R called ggplot2 (gg stands for grammar of graphics). Under the grammar of graphics, every plot is the combination of three types of information: data, geometry, and aesthetics. Data is the data we wish to plot. Geometry is the type of geometry we wish to use to depict the data (e.g. circles, squares, lines, etc). Aesthetics connect the data to the geometry and defines how the data controls the way the selected geometry looks. A simple example will help to explain. Consider the following made up sample metadata tibble for a study of subjects who died with Alzheimers Disease (AD) and neuropathologically normal controls: ad_metadata ## # A tibble: 20 x 8 ## ID age_at_death condition tau abeta iba1 gfap braak_stage ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A1 73 AD 96548 176324 157501 78139 4 ## 2 A2 82 AD 95251 0 147637 79348 4 ## 3 A3 82 AD 40287 45365 17655 127131 2 ## 4 A4 81 AD 82833 181513 225558 122615 4 ## 5 A5 84 AD 73807 69332 47247 46968 3 ## 6 A6 81 AD 69108 139420 57814 24885 3 ## 7 A7 91 AD 138135 209045 101545 32704 6 ## 8 A8 88 AD 63485 104154 80123 31620 3 ## 9 A9 74 AD 59485 148125 27375 50549 3 ## 10 A10 69 AD 48357 27260 47024 78507 2 ## 11 C1 80 Control 62684 93739 131595 124075 3 ## 12 C2 77 Control 63598 69838 7189 35597 3 ## 13 C3 72 Control 52951 19444 54673 17221 2 ## 14 C4 75 Control 12415 7451 16481 38510 0 ## 15 C5 77 Control 62805 0 40419 108479 3 ## 16 C6 77 Control 1615 3921 2439 35430 0 ## 17 C7 73 Control 62233 77113 81887 96024 3 ## 18 C8 77 Control 59123 94715 72350 76833 3 ## 19 C9 80 Control 21705 3046 26768 113093 1 ## 20 C10 73 Control 15781 16592 10271 100858 1 For context, tau protein and amyloid beta peptides from the amyloid precursor protein aggregate into neurofibrillary tangles and A-beta plaques, respectively, the brains of people with AD. Generally, the amount of both of these pathologies is associated with more severe disease. Braak stage is a neuropathological assessment of the amount of pathology in a brain that is associated with the severity of disease, where 0 indicates absence of pathology and 6 with widespread involvement in multiple brain regions. Aggregation of tau is also a consequence of normal aging, so must accompany neurological symptoms such as dementia to indicate an AD diagnosis post mortem. Note we have control samples as well as AD. Tauopathy: tau protein accumulates in the cell bodies of affected neurons - Wikipedia The histology measures tau, abeta, iba1, and gfap have been quantified using digital microscopy, where brain sections are stained with immunohistochemistry to identify the location and degree of pathology; the measures in the table are the number of pixels of a 400 x 400 pixel image of a piece of brain tissue that fluoresce when stained with the corresponding antibody. Tau and A-beta antibodies are specialized to the types of aggregated proteins mentioned above and provide a quantification of the level of overall AD pathology. Ionized calcium binding adaptor molecule 1 (IBA1) is a marker of activated microglia, the resident macrophages of the brain, which is an indication of neuroinflammation. Glial fibrillary acidic protein (GFAP) is a marker for activated astrocytes, specialized cells that derive from the neuron lineage, are critical for maintaining the blood brain barrier, and are also involved in the neuroinflammatory response. Lets say we wished to visualize the relationship between age at death and the amount of tau pathology. A scatter plot where each marker is a subject with \\(x\\) and \\(y\\) position corresponding to age_at_death and tau respectively. The following R code creates such a plot with ggplot2: ggplot(data=ad_metadata, mapping = aes(x = age_at_death, y=tau)) + geom_point() All ggplot2 plots begin with the ggplot() function call, which is passed a tibble with the data to be plotted. We then define the aesthetics are defined by mapping the x coordinate to the age_at_death column and the y coordinate to the tau column with aes(x = age_at_death, y=tau). Finally, the geometry as point with geom_point(), meaning marks will be made at pairs of x,y coordinates. The plot shows what we expect given our knowledge of the relationship between age and amount of tau; the two look to be positively correlated. However, we are not capturing the whole story: we know that there are both AD and Control subjects in this dataset. How does condition relate to this relationship we see? We can layer on an additional aesthetic of color to add this information to the plot: ggplot(data=ad_metadata, mapping = aes(x = age_at_death, y=tau, color=condition)) + geom_point() This looks a little clearer, showing that Control subjects generally have both an earlier age at death and a lower amount of tau pathology. This might be a problem, however, since if the age distributions of AD and Control groups are different that might pose a problem with confounding. We should investigate this. Instead of plotting age at death and tau against each other, we will examine the distributions of each of these variables for AD and Control samples separately. We will use the violin geometry with geom_violin() to look at the distributions of age_at_death: ggplot(data=ad_metadata, mapping = aes(x=condition, y=age_at_death)) + geom_violin() We can see immediately that there are big differences between the age distributions of the two groups. This is not ideal, but perhaps we can adjust for these effects in downstream analyses. Wed like to look at the tau distributions as well, but it would be nice to have these two plots side by side in the same plot. To do that, we will use another library called patchwork, which allows independent ggplot2 plots to be arranged together with a simple expressive syntax: library(patchwork) age_boxplot &lt;- ggplot(data=ad_metadata, mapping = aes(x=condition, y=age_at_death)) + geom_boxplot() tau_boxplot &lt;- ggplot(data=ad_metadata, mapping=aes(x=condition, y=tau)) + geom_boxplot() age_boxplot | tau_boxplot # this puts the plots side by side This confirms our suspicion, and also reveals a serious problem with our samples: we have strong confounding of tau and age at death between AD and Control samples. This means that if we look for differences between AD and Control, we wont know if the difference is due to the amount of tau pathology or due to age of the subjects. With this sample set, we simply cannot confidently answer that question. Just a few simple plots alerted us to this problem; hopefully more expensive datasets have not already been generated for these samples, so that hopefully different subjects are available that could avoid this confounding. This has been a biological data analysis oriented tutorial on plotting meant to illustrate the principles of the grammar of graphics. Namely, every plot has data, geometry, and aesthetics that can be independently controlled to produce many types of plots. Many of these plots have names, like scatter plots and boxplots, but as you compose different types of geometries and aesthetics together you may find yourself generating plots that arent so easily named. The next sections of this chapter are a kind of cook book of different kinds plots you can generate with data of different shapes. It is not intended to be comprehensive, but a helpful guide when you are trying to decide how to visualize your own datasets. If you want to go directly to the comprehensive documentation of the many types of ggplot2 plots, peruse the R Graph Gallery site. ggplot2 - Elegant Graphics for Data Analysis, by Hadley Wickam R for Data Science - Data Vizualization R Graph Gallery patchwork package 7.2 Plotting One Dimension The simplest plots involve plotting a single vector of numbers, or several such vectors (e.g. for different samples). Each value in the vector typically corresponds to a category or fixed value, for example the tau column from the example above has pairs of (ID, tau value). The order of these numbers can be changed, but the vector remains one dimensional or 1-D. 7.2.1 Bar chart Bar charts map length (i.e. the height or width of a box) to the scalar value of a number. The difference in visual length can help the viewer notice consistent patterns in groups of bars, depending on how they are arranged: ggplot(ad_metadata, mapping = aes(x=ID,y=tau)) + geom_bar(stat=&quot;identity&quot;) Note the stat=\"identity\" argument is required because by default geom_bar counts the number of values for each value of x, which in our case is only ever one. This plot is not particularly helpful, so lets change the fill color of the bars based on condition: ggplot(ad_metadata, mapping = aes(x=ID,y=tau,fill=condition)) + geom_bar(stat=&quot;identity&quot;) Slightly better, but maybe we can see even more clearly if we sort our tibble by tau first. Sorting elements in these 1-D charts is somewhat complicated, and is explained in the [Reordering 1-D Data Elements] section below. Bar charts can also plot negative numbers. In the following example, we center the tau measurements by subtracting the mean from each value before plotting: mutate(ad_metadata, tau_centered=(tau - mean(tau))) %&gt;% ggplot(mapping = aes(x=ID, y=tau_centered, fill=condition)) + geom_bar(stat=&quot;identity&quot;) 7.2.2 Lollipop plots Similar to bar charts, so-called lollipop plots replace the bar with a line segment and a circle. The length of the line segment is proportional to the magnitude of the number, and the point marks the length of the segment as a height on the y or length on the x axis, depending on orientation. ggplot(ad_metadata) + geom_point(mapping=aes(x=ID, y=tau)) + geom_segment(mapping=aes(x=ID, xend=ID, y=0, yend=tau)) Note that aes() mappings can be made on the ggplot() object or on each individual geometry function call, to specify different mappings based on geometry. 7.2.3 Stacked Area charts Stacked area charts can visualize multiple 1D data that share a common categorical axis. The charts consist of one line per variable with vertices that correspond to x and y values similar to a bar or lollipop plots. Each variable is plotted using the previous one as a baseline, so that the height of the data points for each category is proportional to their sum. The space between the lines for each variable and the previous one are filled with a color. The following plot visualizes the amount of marker stain for each of the four genes for each individaul: pivot_longer( ad_metadata, c(tau,abeta,iba1,gfap), names_to=&#39;Marker&#39;, values_to=&#39;Intensity&#39; ) %&gt;% ggplot(aes(x=ID,y=Intensity,group=Marker,fill=Marker)) + geom_area() We notice that subject A4 has the highest overall level of marker intensity, followed by A1, A7, etc. The control samples overall have less intensity across all markers. Certain samples, A2 and C5, have little to no abeta aggregation, and C6 has little to no tau. Stacked area plots require three pieces of data: x - a numeric or categorical axis for vertical alignment y - a numeric axis to draw vertical proportions group - a categorical variable that indicates which (x,y) pairs correspond to the same line In the example above, we needed to pivot our tibble so that the different markers and their values were placed into columns Marker and Intensity, respectively. Data for stacked bar charts will usually need to be in this long format, as described in Rearranging Data. Sometimes it is more helpful to view the relative proportion of values in each category rather than the actual values. The result is called a proportional stacked area plots. While not a distinct plot type, we can create one by preprocessing our data by dividing each value by the column sum: pivot_longer( ad_metadata, c(tau,abeta,iba1,gfap), names_to=&#39;Marker&#39;, values_to=&#39;Intensity&#39; ) %&gt;% group_by(ID) %&gt;% # we want to divide each subjects intensity values by the sum of all four markers mutate( `Relative Intensity`=Intensity/sum(Intensity) ) %&gt;% ungroup() %&gt;% # ungroup restores the tibble to its original number of rows after the transformation ggplot(aes(x=ID,y=`Relative Intensity`,group=Marker,fill=Marker)) + geom_area() Now the values for each subject have been normalized to each sum to 1. In this way, we might note that the relative proportion of abeta seems to be greater in AD samples than Controls, but that may not be true of tau. These observations may inspire us to ask these questions more rigorously than we have done so far by inspection. 7.2.4 Parallel Coordinate plots 7.3 Visualizing Distributions The distribution is one of the most important properties of a set of numbers. A distribution describes the general shape of the numbers, i.e. what is the relative frequency of the values, or ranges of values, within the data. Understanding the distribution of a data set is critical when choosing methods to apply, since many methods are only appropriate when data is distributed in certain ways, e.g. linear regression assumes the response variable is normally distributed, otherwise the result of the model cannot be interpreted properly. Often, we dont know how our data are distributed when we obtain it and so we must examine the distribution empirically. The visualizations in this section are all used for the purpose of depicting the distribution of a set of numbers. 7.3.1 Histogram The most common way to plot the distribution of a 1-D set of data is the histogram. The histogram divides up the range of a dataset from minimum to maximum into bins usually of the same width and tabulates the number of values that fall within each bin. Below is a histogram of our age_at_death measurement for all samples: ggplot(ad_metadata) + geom_histogram(mapping=aes(x=age_at_death)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that the histogram does not look very complete, because there are only 20 values in our data. We can mitigate this somewhat by increasing the number of bins the data range is divided into: ggplot(ad_metadata) + geom_histogram(mapping=aes(x=age_at_death),bins=10) This is a little bit better, but there are still some bins (76-79, 84-87) that have no values. Compare this to the following synthetic dataset of 1000 normally distributed values: tibble( x=rnorm(1000) ) %&gt;% ggplot() + geom_histogram(aes(x=x)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. For distributions with a small number of samples, histograms might not be the best visualization. We will continue with synthetic normally distributed dataset for the remaining examples. ggplot allows you to easily plot multiple distributions on the same plot: tibble( x=c(rnorm(1000),rnorm(1000,mean=4)), type=c(rep(&#39;A&#39;,1000),rep(&#39;B&#39;,1000)) ) %&gt;% ggplot(aes(x=x,fill=type)) + geom_histogram(bins=30, alpha=0.6, position=&quot;identity&quot;) The alpha=0.6, position=\"identity\" arguments makes the bars partially transparent so you can see the overlap more clearly. ggplot2 geom_histogram reference R Graph Gallery - histograms 7.3.2 Density Another way to describe a distribution is with a density plot. Instead of binning the values into intervals and drawing bars with height proportional to the number of values in each bin, a density plot draws a smoothly interpolated line that approximates the distribution instead. A key difference between a histogram and a density plot is the density plot is always normalized so the integral under the curve is approximately 1, whereas a histogram may be either counts or, if the counts in each bin are divided by the total number of data points, a proportion. Compare the histogram and density plots of the age_at_death variable from our example tibble: library(patchwork) hist_g &lt;- ggplot(ad_metadata) + geom_histogram(mapping=aes(x=age_at_death),bins=30) density_g &lt;- ggplot(ad_metadata) + geom_density(mapping=aes(x=age_at_death),fill=&quot;#c9a13daa&quot;) hist_g | density_g Notice the overall shape of the two distributions is similar, with the highest values in both around age 77. The density plot is a smoother representation of a histogram, but its accuracy is still highly sensitive to the number of measurements used to construct it. Compare the histogram and density plots of two sets of 1000 normally distributed samples with different means: library(patchwork) normal_samples &lt;- tibble( x=c(rnorm(1000),rnorm(1000,mean=4)), type=c(rep(&#39;A&#39;,1000),rep(&#39;B&#39;,1000)) ) hist_g &lt;- ggplot(normal_samples) + geom_histogram( mapping=aes(x=x,fill=type), alpha=0.6, position=&quot;identity&quot;, bins=30 ) density_g &lt;- ggplot(normal_samples) + geom_density( mapping=aes(x=x,fill=type), alpha=0.6, position=&quot;identity&quot; ) hist_g | density_g Again the two types of plots depict similar distributions, although they are different enough to possibly suggest different interpretations. In general, density plots might be preferable over histograms if the data are noisy or sparse in that they produce cleaner plots, but potentially at the expense of accuracy when the number of samples is low. ggplot2 geom_density reference R Graph Gallery - density plots 7.3.3 Boxplot Box plots, or box and whisker plots, are extremely common when used to describe distributions. Below is a boxplot of age at death divided by condition: ggplot(ad_metadata) + geom_boxplot(mapping=aes(x=condition,y=age_at_death)) Boxplots are drawn assuming the data are unimodal (i.e. shaped like a hill, possibly slanted to one side or the other), where the extents of the box represent the 1st and 3rd quartile of the data, the central line is the median, the whiskers are drawn as 1.5 times the value outside the 1st and 3rd quartiles. Sometimes individual values more extreme than the whiskers are drawn individually to identify them as outliers. Boxplot anatomy. IQR stands for inner quartile range, the distance between the 1st and 3rd quartile - Wikipedia However, boxplots have some significant shortcomings. Primarily, the rectangle of the inner quartile range does not describe the actual distribution of the samples within it. Although the median can give a sense of skewness, if the data are not unimodal this may be misleading. Consider the following distributions plotted as boxplots or as violin plots (described in the next section): library(patchwork) normal_samples &lt;- tibble( x=c(rnorm(1000),rnorm(1000,4),rnorm(1000,2,3)), type=c(rep(&#39;A&#39;,2000),rep(&#39;B&#39;,1000)) ) g &lt;- ggplot(normal_samples, aes(x=type,y=x,fill=type)) boxplot_g &lt;- g + geom_boxplot() violin_g &lt;- g + geom_violin() boxplot_g | violin_g The two distributions look almost identical in the boxplot figure; however they are dramatically different when visualized using a method like a violin plot where the contours of the entire distribution are depicted. Unless you are certain that your data are unimodal, one of the other distribution visualization methods in this section will likely more accurately depict your data than a boxplot. ggplot2 geom_boxplot reference R Graph Gallery - boxplots 7.3.4 Violin plot As seen in the last section, a violin plot is another way to depict a distribution by producing a shape where the width is proportional to the value along the x or y axis, depending on orientation. The violin shape is similar in principle to a histogram or a density plot, in that it describes the contour of all the data in the distribution, not just the quantiles and extents, as in a box plot. Below is a violin plot of the tau measures from our example tibble: ggplot(ad_metadata) + geom_violin(aes(x=condition,y=tau,fill=condition)) The violin plot is both more and less descriptive than a boxplot; it does depict the entire distribution of the data, but also doesnt include features like median by default. ggplot2 geom_violin reference R Graph Gallery - violin plots 7.3.5 Beeswarm plot The beeswarm plot is similar to a violin plot, but instead of plotting the contours of the data, it plots the data itself as points like in a scatter plot. The individual values of the distribution are organized vertically and spaced such that the points dont overlap. In this plot, the distribution of age at death is plotted for each kind of sample and the markers are colored by the amount of tau: library(ggbeeswarm) ggplot(ad_metadata) + geom_beeswarm(aes(x=condition,y=age_at_death,color=condition),cex=2,size=2) We may not have noticed before that our AD samples have a big gap in ages between 74 and 81; since the beeswarm plot displays all the data, we can see it easily here. Beeswarm plots are typically only useful when the number of values is within a range; not too many and not too few. The example above is close to having too few values per group for this plot to be useful, but consider the following with too many samples: normal_samples &lt;- tibble( x=c(rnorm(1000),rnorm(1000,4),rnorm(1000,2,3)), type=c(rep(&#39;A&#39;,2000),rep(&#39;B&#39;,1000)) ) ggplot(normal_samples, aes(x=type,y=x,color=type)) + geom_beeswarm() This plot likely has too many samples to be the right choice (its also ugly), but it does give an idea of the distribution of the data. In the previous examples the markers for each group also determined the color of the group. This makes the chart a bit easier to read and more pleasing to the eye, but is technically redundant. You can use however profitably however to color markers by some other value that might be of interest. Consider this final example where markers are colored by another randomly generated variable: normal_samples &lt;- tibble( x=c(rnorm(100),rnorm(100,4),rnorm(100,2,3)), type=c(rep(&#39;A&#39;,200),rep(&#39;B&#39;,100)), category=sample(c(&#39;healthy&#39;,&#39;disease&#39;),300,replace=TRUE) ) ggplot(normal_samples, aes(x=type,y=x,color=category)) + geom_beeswarm() We are now effectively visualizing three dimensions which may provide insight into the data. beeswarm package reference 7.3.6 Ridgeline If you have many non-trivial distributions that you would like the user to compare, a good option is a ridgeline chart. The ridgeline plot is simply multiple density plots drawn for different variables within the same plot. Like the beeswarm plot, ridgeline plots are provided by another package outside ggplot2. library(ggridges) tibble( x=c(rnorm(100),rnorm(100,4),rnorm(100,2,3)), type=c(rep(&#39;A&#39;,200),rep(&#39;B&#39;,100)), ) %&gt;% ggplot(aes(y=type,x=x,fill=type)) + geom_density_ridges() ## Picking joint bandwidth of 0.825 Many distributions may be plotted: tibble( x=rnorm(10000,mean=runif(10,1,10),sd=runif(2,1,4)), type=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;,&quot;I&quot;,&quot;J&quot;),1000) ) %&gt;% ggplot(aes(y=type,x=x,fill=type)) + geom_density_ridges(alpha=0.6,position=&quot;identity&quot;) ## Picking joint bandwidth of 0.438 R Graph Gallery - ridgeline plots ggridges package vignette ggridges package index on CRAN 7.4 Plotting Two or More Dimensions . 7.4.1 Scatter plot . 7.4.2 Line plot . ggplot(ad_metadata,mapping=aes(x=ID, y=tau)) + geom_line() ## geom_path: Each group consists of only one observation. Do you need to adjust ## the group aesthetic? 7.4.3 Bubble plot . 7.4.4 Heatmaps . Heatmaps visualize values associated with a grid of points \\((x,y,z)\\), where \\(x\\) and \\(y\\) are coordinates and \\(z\\) is a continuous value. Typically \\(x\\) and \\(y\\) are evenly spaced between minima and maxima on each axis, but this is not necessarily the case, where unequal spacing is sometimes used. 7.5 Other Kind of Plots . 7.5.1 Chord . 7.5.2 Sankey . 7.5.3 Dendrogram . 7.6 Tips and Tricks . 7.6.1 Reordering 1-D Data Elements . How do we enforce an order on elements in a plot? Why might we want to do that? ad_metadata %&gt;% arrange(tau) %&gt;% ggplot(mapping = aes(x=ID,y=tau,fill=condition)) + geom_bar(stat=&quot;identity&quot;) But wait, our bars are in the same order as in the previous plot. Sorting our data prior to plotting did not have the desired effect. The reason is that although the ID column is a character string in our original tibble, ggplot converts it into a factor prior to plotting. Recall that a factor is a numeric type composed of sequential integers, each of which having a character label associated with it. 7.6.2 Confidence Intervals . What are some ways to depict uncertainty in a plot? 7.6.3 Annotations . How do we add text annotations to plots? Manually vs data-driven. 7.7 Multiple Plots . How do we add multiple datasets to plots? 7.7.1 Secondary Axes . How and when do we add series with different scales to the same plot? 7.7.2 Facet wrapping . What is facet wrapping and in which contexts is it useful (i.e. group_by)? 7.7.3 Multipanel Figures . How do we add multiple, unrelated subplots to the same larger plot? 7.8 Responsible Plotting . Good plots empower us to ask good questions. - Alberto Cairo, How Charts Lie 7.9 Publication Ready Plots . ggpubr - publication ready plotting with ggplot 7.10 Network visualization . How do we visualize networks? "],["biology-bioinformatics.html", "8 Biology &amp; Bioinformatics 8.1 R in Biology 8.2 CSV Files 8.3 Types of Biological Data 8.4 Common Biological Data Matrices 8.5 Bioconductor 8.6 Gene Identifiers 8.7 Gene Expression 8.8 Genomic Intervals . 8.9 Over-representation Analysis 8.10 Biological Networks . 8.11 Single Cell Sequencing Analysis .", " 8 Biology &amp; Bioinformatics 8.1 R in Biology R became popular in biological data analysis in the early to mid 2000s, when microarray technology came into widespread use enabling researchers to look for statistical differences in gene expression for thousands of genes across large numbers of samples. As a result of this popularity, a community of biological researchers and data analysts created a collection of software packages called Bioconductor, which made a vast array of cutting edge statistical and bioinformatic methodologies widely available. Due to its ease of use, widespread community support, rich ecosystem of biological data analysis packages, and to it being free, R remains one of the primary languages of biological data analysis. The languages early focus on statistical analysis, and later transition to data science in general, makes it very useful and accessible to perform the kinds of analyses the new data science of biology required. It is also a bridge between biologists without a computational background and statisticians and bioinformaticians, who invent new methods and implement them as R packages that are easily accessible by all. The language and the package ecosystems and communities it supports continue to be a major source of biological discovery and innovation. As a data science, biology benefits from the main strengths of R and the tidyverse when combined with the powerful analytical techniques available in Bioconductor packages, namely to manipulate, visualize, analyze, and communicate biological data. 8.2 CSV Files The most common, convenient, and flexible data file format in biology and bioinformatics is the character-delimited or character-separated text file. These files are plain text files (i.e. not the native file format of any specific program, like Excel) that generally contain rectangles of data. When formatted correctly, you can think of these files as containing a grid or matrix of data values with some number of columns, each of which has the same number of values. Each line of these files has some number of data values separated by a consistent character, most commonly the comma which are called comma-separated value, or CSV, files and filenames typically end with the extension .csv. Note that other characters, especially the character, may be used to create valid files in this format, and all the same general principles apply. This is an example of a simple CSV file: id,somevalue,category,genes 1,2.123,A,APOE 4,5.123,B,&quot;HOXA1,HOXB1&quot; 7,8.123,,SNCA Some properties and principles of CSV files: The first line often but not always contains the column names of each column Each value is delimited by the same character, in this case , Values can be any value, including numbers and characters When a value contains the delimiting character (e.g. HOXA1,HOXB1 contains a ,), the value is wrapped in double quotes Values can be missing, indicated by sequential delimiters (i.e. ,, or one , at the end of the line, if the last column value is missing) There is no delimiter at the end of the lines To be well-formatted every line must have the same number of delimited values These same properties and principles apply to all character-separated files, regardless of the specific delimiter used. If a file follows these principles, they can be loaded very easily into R or any other data analysis setting. 8.3 Types of Biological Data In very general terms, there are five types of data used in biological data analysis: raw/primary data, processed data, analysis results, metadata, and annotation data. Raw/primary data. These data are the primary observations made by whatever instruments/techniques we are using for our experiment. These may include high-throughput sequencing data, mass/charge ratio data from mass spectrometry, 16S rRNA sequencing data from metagenomic studies, SNPs from genotyping assays, etc. These data can be very large and are often not efficiently processed using R. Instead, specialized tools built outside of R are used to first process the primary data into a form that is amenable to analysis. Processed data. Processed data is the result of any analysis or transformation of primary data into an intermediate, more interpretable form. For example, when performing gene expression analysis with RNASeq, short reads that form the primary data are typically aligned against a genome and then counted against annotated genes, resulting in a count for each gene in the genome. Typically, these counts are then combined for many samples into a single matrix and subject to downstream analyses like differential expression. Analysis results. Analysis results arent data per se, but are the results of analysis of primary data or processed results. For example, in gene expression studies, a first analysis is often differential expression, where each gene in the genome is tested for expression associated with some outcome of interest across many samples. Each gene then has a number of statistics and related values, like log2 fold change, nominal and adjusted p-value, etc. These forms of data are usually what we use to form interpretations of our datasets and therefore we must manipulate them in much the same way as any other dataset. Metadata. In most experimental settings, multiple samples have been processed and had the same primary data type collected. These samples also have information associated with them, which is here termed metadata, or data about data. In our gene expression of post mortem brain experiments mentioned above, the information about the individuals themselves, including age at death, whether the person had a disease, the measurements of tissue quality, etc. is the metadata. The primary and processed data and metadata are usually stored in different files, where the metadata (or sample information or sample data, etc) will have one column indicating the unique identifier (ID) of each sample. The processed data will typically have columns named for each of the sample IDs. Annotation data. A tremendous amount of knowledge has been generated about biological entities, e.g. genes, especially since the publication of the human genome. Annotation data is publicly available information about the features we measure in our experiments. This may come in the form of the coordinates in a genome where genes exist, any known functions of those genes, the domains found in proteins and their relative sequence, gene identifier cross references across different gene naming systems (e.g. symbol vs Ensembl ID), single nucleotide polymorphism genomic locations and associations with traits or diseases, etc. This is information that we use to aid in interpretation of our experimental data but generally do not generate ourselves. Annotation data comes in many forms, some of which are in CSV format. The figure below contains a simple diagram of how these different types of data work together for a typical biological data analysis. Information flow in biological data analysis 8.4 Common Biological Data Matrices Typically the first data set you will work with in R is processed data as described in the previous section. This data has been transformed from primary data in some way such that it (usually) forms a numeric matrix with features as rows and samples as columns. The first column of these files usually contains a feature identifier, e.g. gene identifier, genomic locus, probe set ID, etc and the remaining columns have numerical values, one per sample. The first row is usually column names for all the columns in the file. Below is an example of one of these files from a microarray gene expression dataset loaded into R: intensities &lt;- read_csv(&quot;example_intensity_data.csv&quot;) intensities # A tibble: 54,675 x 36 probe GSM972389 GSM972390 GSM972396 GSM972401 GSM972409 GSM972412 GSM972413 GSM972422 GSM972429 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1007_s_at 9.54 10.2 9.72 9.68 9.35 9.89 9.70 9.67 9.87 2 1053_at 7.62 7.92 7.17 7.24 8.20 6.87 6.62 7.23 7.45 3 117_at 5.50 5.56 5.06 7.44 5.19 5.72 5.87 6.15 5.46 4 121_at 7.27 7.96 7.42 7.34 7.49 7.76 7.44 7.66 8.02 5 1255_g_at 2.79 3.10 2.78 2.91 3.02 2.73 2.78 3.56 2.83 6 1294_at 7.51 7.28 7.00 7.18 7.38 6.98 6.90 7.54 7.66 7 1316_at 3.89 4.36 4.24 3.94 4.20 4.34 4.06 4.24 4.11 8 1320_at 4.65 4.91 4.70 4.78 5.06 4.71 4.55 4.58 5.10 9 1405_i_at 8.03 7.47 5.42 7.21 9.48 6.79 6.57 8.50 6.36 10 1431_at 3.09 3.78 3.33 3.12 3.21 3.27 3.37 3.84 3.32 # ... with 54,665 more rows, and 26 more variables: GSM972433 &lt;dbl&gt;, GSM972438 &lt;dbl&gt;, GSM972440 &lt;dbl&gt;, # GSM972443 &lt;dbl&gt;, GSM972444 &lt;dbl&gt;, GSM972446 &lt;dbl&gt;, GSM972453 &lt;dbl&gt;, GSM972457 &lt;dbl&gt;, # GSM972459 &lt;dbl&gt;, GSM972463 &lt;dbl&gt;, GSM972467 &lt;dbl&gt;, GSM972472 &lt;dbl&gt;, GSM972473 &lt;dbl&gt;, # GSM972475 &lt;dbl&gt;, GSM972476 &lt;dbl&gt;, GSM972477 &lt;dbl&gt;, GSM972479 &lt;dbl&gt;, GSM972480 &lt;dbl&gt;, # GSM972487 &lt;dbl&gt;, GSM972488 &lt;dbl&gt;, GSM972489 &lt;dbl&gt;, GSM972496 &lt;dbl&gt;, GSM972502 &lt;dbl&gt;, # GSM972510 &lt;dbl&gt;, GSM972512 &lt;dbl&gt;, GSM972521 &lt;dbl&gt; The file has 54,676 rows, consisting of one header row which R loads in as the column names, and the remaining are probe sets, one per row. There are 36 columns, where the first contains the probe set ID (e.g. 1007_s_at) and the remaining 35 columns correspond to samples. 8.4.1 Biological data is NOT Tidy! As mentioned in the tidy data section, the tidyverse packages assume data to be in so-called tidy format, with variables as columns and observations as rows. Unfortunately, certain forms of biological data are typically available in the opposite orientation - variables are in rows and observations are in columns. This is primarily true in feature data matrices, e.g. gene expression counts matrices, where the number of variables (e.g. genes) is much larger than the number of samples, which tend to be small very small compared with the number of features. This format is convenient for humans to interact with using, e.g. spreadsheet programs like Microsoft Excel, but can unfortunately make performing certain operations on them challenging in tidyverse. For example, consider the microarray expression dataset in the previous section. Each of the 54,676 rows is a probeset, and each of the 35 numeric columns is a sample. This is a very large number of probesets to consider, especially if we plan to conduct a statistical test on each, which would impose a substantial multiple hypothesis testing burden on our results. We may therefore wish to eliminate probesets that have very low variance from the overall dataset, since these probesets are not likely to have a detectable statistical difference in our tests. However, computing the variance for each probeset is a computation across all columns, not on columns themselves, and this is not what tidyverse is designed to do well. Said differently, R and tidyverse do not operate by default on the rows of a data frame, tibble, or matrix. Both base R and tidyverse are optimized to perform computations on columns, not rows. The reasons for this are buried in the details of how the R program itself was written to organize data internally and are beyond the scope of this book. The consequence of this design choice is that, while we can perform operations on the rows rather than the columns of a data structure, our code may perform very poorly (i.e. take a very long time to run). When working with these datasets, we have a couple options to deal with this issue: Pivot into long format. As described in the Rearranging Data section, we can rearrange our tibble to be more amenable to certain computations. In our earlier example, we wish to group all of our measurements by probeset and compute the variance of each, then possibly filter out probesets based on low variance. We can therefore combine pivot_longer(), group_by(), summarize(), and finally left_join() to perform this operation. Exactly how to do this is left as an exercise in Assignment 1. Compute row-wise statistics using apply(). As described in Iteration, R is a functional programming language and implements iteration in a functional style using the apply() function. The apply() function accepts a MARGIN argument of 1 or 2 if the provided function is to be applied to the columns or rows, respectively. This method can be used to compute a summary statistic on each row of a tibble and the result saved back into the tibble using the column set operator: intensity_variance &lt;- apply(intensities, 2, var) intensities$variance &lt;- intensity_variance 8.5 Bioconductor Bioconductor is an organized collection of strictly biological analysis methods packages for R. These packages are hosted and maintained outside of CRAN because the maintainers enforce a more rigorous set of coding quality, testing, and documentation standards than is required by normal R packages. These stricter requirements arose from a recognition that software packages are generally only as usable as their documentation allows them to be, and also that many if not most of the users of these packages are not statisticians or experienced computer programmers. Rather, they are people like us: biological analysis practitioners who may or may not have substantial coding experience but must analyze our data nonetheless. The excellent documentation and community support of the bioconductor ecosystem is a primary reason why R is such a popular language in biological analysis. Bioconductor is divided into roughly two sets of packages: core maintainer packages and user contributed packages. The core maintainer packages are among the most critical, because they define a set of common objects and classes (e.g. the ExpressionSet class in the Biobase package) that all Bioconductor packages know how to work with. This common base provides consistency among all Bioconductor packages thereby streamlining the learning process. User contributed and maintained packages provide specialized functionality for specific types of analysis. Bioconductor itself must be installed prior to installing other Bioconductor packages. To [install bioconductor], you can run the following: if (!require(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(version = &quot;3.14&quot;) Bioconductor undergoes regular releases indicated by a version, e.g. version = \"3.14\" at the time of writing. Every bioconductor package also has a version, and each of those versions may or may not be compatible with a specific version of Bioconductor. To make matters worse, Bioconductor versions are only compatible with certain versions of R. Bioconductor version 3.14 requires R version 4.1.0, and will not work with older R versions. These versions can cause major version compatibility issues when you are forced to use older versions of R, as may sometimes be the case on managed compute clusters. There is not a good general solution for ensuring all your packages work together, but the general best rule of thumb is to use the most current version of R and all packages at the time when you write your code. The BiocManager package is the only Bioconductor package installable using install.packages(). After installing the BiocManager package, you may then install bioconductor packages: # installs the affy bioconductor package for microarray analysis BiocManager::install(&quot;affy&quot;) One key aspect of Bioconductor packages is consistent and helpful documentation; every package page on the Bioconductor.org site lists a block of code that will install the package, e.g. for the affy package. In addition, Biconductor provides three types of documentation: Workflow tutorials on how to perform specific analysis use cases Package vignettes for every package, which provide a worked example of how to use the package that can be adapted by the user to their specific case Detailed, consistently formatted reference documentation that gives precise information on functionality and use of each package The thorough and useful documentation of packages in Bioconductor is one of the reasons why the package ecosystem, and R, is so widely used in biology and bioinformatics. The base Bioconductor packages define convenient data structures for storing and analyzing many types of data. Recall earlier in the Types of Biological Data section, we described five types of biological data: primary, processed, analysis, metadata, and annotation data. Bioconductor provides a convenient class for storing many of these different data types together in one place, specifically the SummarizedExperiment class in the package of the same name package. An illustration of what a SummarizedExperiment object stores is below, from the Bioconductor maintainers paper in Nature: SummarizedExperiment Schematic - Huber, et al. 2015. Orchestrating High-Throughput Genomic Analysis with Bioconductor. Nature Methods 12 (2): 11521. As you can see from the figure, this class stores processed data (assays), metadata (colData and exptData), and annotation data (rowData). The SummarizedExperiment class is used ubiquitously throughout the Bioconductor package ecosystem, as are other core classes some of which we will cover later in this chapter. 8.6 Gene Identifiers 8.6.1 Gene Identifier Systems Since the first genetic sequence for a gene was determined in 1965 - an alanine tRNA in yeast(Holley et al. 1965) - scientists have been trying to give them names. In 1979, formal guidelines for the nomenclature used for human genes was proposed(Shows et al. 1979) along with the founding of the Human Gene Nomenclature Committee (Bruford et al. 2020), so that researchers would have a common vocabulary to refer to genes in a consistent way. In 1989, this committee was placed under the auspices of the Human Genome Organisation (HUGO), becoming the HUGO Gene Nomenclature Committee (HGNC). The HGNC has been the official body providing guidelines and gene naming authority since, and as such HGNC gene names are often called gene symbols. As per Bruford et al. (2020), the official human gene symbol guidelines are as follows: Each gene is assigned a unique symbol, HGNC ID and descriptive name. Symbols contain only uppercase Latin letters and Arabic numerals. Symbols should not be the same as commonly used abbreviations Nomenclature should not contain reference to any species or G for gene. Nomenclature should not be offensive or pejorative. Gene symbols are the most human-readable system for naming genes. The gene symbols BRCA1, APOE, and ACE2 may be familiar to you as they are involved in common diseases, namely breast cancer, Alzheimers Disease risk, and SARS-COV-2, respectively. Typically, the gene symbol is an acronym that roughly represents a label of what the gene is or does (or was originally found to be or do, as many genes are subsequently discovered to be involved in entirely separate processes as well), e.g. APOE represents the gene apolipoprotein E. This convention is convenient for humans when reading and identifying genes. However, standardized though the symbol conventions may be, they are not always easy for computers to work with, and ambiguities can cause mapping problems. Other gene identifier systems developed along with other gene information databases. In 2000, the Ensembl genome browser was launched by the Wellcome Trust, a charitable foundation based in the United Kingdom, with the goal of providing automated annotation of the human genome. The Ensembl Project, which supports the genome browser, recognized even before the publication of the human genome that manual annotation and curation of genes is a slow and labor intensive process that would not provide researchers around the world timely access to information. The project therefore created a set of automated tools and pipelines to collect, process, and publish rapid and consistent annotation of genes in the human genome. Since its initial release, Ensembl now supports over 50,000 different genomes. Ensembl assigns a automatic, systematic ID called the Ensembl Gene ID to every gene in its database. Human Ensembl Gene IDs take the form ENSG plus an 11 digit number, optionally followed by a period delimited version number. For example, at the time of writing the BRCA1 gene has an Ensembl Gene ID of ENSG00000012048.23. The stable ID portion (i.e. ENSGNNNNNNNNNNN) will remain associated with the gene forever (unless the gene annotation changes dramatically in which case it is retired). The .23 is the version number of the gene annotation, meaning this gene has been updated 22 times (plus its initial version) since addition to the database. The additional version information is very important for reproducibility of biological analysis, since conclusions drawn by results of these analyses are usually based on the most current information about a gene which are continually updated over time. Ensembl maintains annotations for many different organisms, and the gene identifiers for each genome contain codes that indicate which organism the gene is for. Here are the codes for genes in several different species: Gene ID Prefix Organism Example ENSG Homo sapiens ENSG00000139618 ENSMUSG Mus musculus (mouse) ENSMUSG00000017167 ENSDARG Danio rerio (zebrafish) ENSDARG00000024771 ENSMMUG Macaca mulata (Macaque) ENSMMUG00000020312 ENSVPAG Vicugna pacos (Alpaca) ENSVPAG00000001990 Ensembl Gene IDs and gene symbols are the most commonly used gene identifiers. The GENCODE project which provides consistent and stable genome annotation releases for human and mouse genomes, uses these two types of identifiers with its official annotations. Ensembl provides stable identifiers for transcripts as well as genes. Transcript identifiers correspond to distinct patterns of exons/introns that have been identified for a gene, and each gene has one or more distinct transcripts. Ensembl Transcript IDs have the form ENSTNNNNNNNNNNN.NN similar to Gene IDs. Ensembl is not the only gene identifier system besides gene symbol. Several other databases have devised and maintain their own identifiers, most notably [Entrez Gene IDs](UCSC Gene IDs used by the NCBI Gene database which assigns a unique integer to each gene in each organism, and the Online Mendelian Inheritance in Man (OMIM) database, which has identifiers that look like OMIM:NNNNN, where each OMIM ID refers to a unique gene or human phenotype. However, the primary identifiers used in modern human biological research remain Ensembl IDs and official HGNC gene symbols. 8.6.2 Mapping Between Identifier Systems A very common operation in biological analysis is to map between gene identifier systems, e.g. you are given Ensembl Gene ID and want to map to the more human-recognizable gene symbols. Ensembl provides a service called BioMart that allows you to download annotation information for genes, transcripts, and other information maintained in their databases. It also provides limited access to external sources of information on their genes, including cross references to HGNC gene symbols and some other gene identifier systems. The Ensembl website has helpful documentation on how to use BioMart to download annotation data using the web interface. In addition to downloading annotations as a CSV file from the web interface, Ensembl also provides the biomaRt Bioconductor package to allow programmatic access to the same information directly from R. There is much more information in the Ensembl databases than gene annotation data that can be accessed via BioMart, but we will provide a brief example of how to extract gene information here: # this assumes biomaRt is already installed through bioconductor library(biomaRt) # the human biomaRt database is named &quot;hsapiens_gene_ensembl&quot; ensembl &lt;- useEnsembl(biomart=&quot;ensembl&quot;, dataset=&quot;hsapiens_gene_ensembl&quot;) # listAttributes() returns a data frame, turn into a tibble to help with formatting as_tibble(listAttributes(ensembl)) # A tibble: 3,143 x 3 name description page &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 ensembl_gene_id Gene stable ID feature_page 2 ensembl_gene_id_version Gene stable ID version feature_page 3 ensembl_transcript_id Transcript stable ID feature_page 4 ensembl_transcript_id_version Transcript stable ID version feature_page 5 ensembl_peptide_id Protein stable ID feature_page 6 ensembl_peptide_id_version Protein stable ID version feature_page 7 ensembl_exon_id Exon stable ID feature_page 8 description Gene description feature_page 9 chromosome_name Chromosome/scaffold name feature_page 10 start_position Gene start (bp) feature_page # ... with 3,133 more rows The name column provides the programmatic name associated with the attribute that can be used to retrieve that annotation information using the getBM() function: # create a tibble with ensembl gene ID, HGNC gene symbol, and gene description gene_map &lt;- as_tibble( getBM( attributes=c(&quot;ensembl_gene_id&quot;, &quot;hgnc_gene_symbol&quot;, &quot;description&quot;), mart=ensembl ) ) gene_map # A tibble: 68,012 x 3 ensembl_gene_id hgnc_symbol description &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 ENSG00000210049 MT-TF mitochondrially encoded tRNA-Phe (UUU/C) [Source:HGNC Symbol;Acc:HGNC:748~ 2 ENSG00000211459 MT-RNR1 mitochondrially encoded 12S rRNA [Source:HGNC Symbol;Acc:HGNC:7470] 3 ENSG00000210077 MT-TV mitochondrially encoded tRNA-Val (GUN) [Source:HGNC Symbol;Acc:HGNC:7500] 4 ENSG00000210082 MT-RNR2 mitochondrially encoded 16S rRNA [Source:HGNC Symbol;Acc:HGNC:7471] 5 ENSG00000209082 MT-TL1 mitochondrially encoded tRNA-Leu (UUA/G) 1 [Source:HGNC Symbol;Acc:HGNC:7~ 6 ENSG00000198888 MT-ND1 mitochondrially encoded NADH:ubiquinone oxidoreductase core subunit 1 [So~ 7 ENSG00000210100 MT-TI mitochondrially encoded tRNA-Ile (AUU/C) [Source:HGNC Symbol;Acc:HGNC:748~ 8 ENSG00000210107 MT-TQ mitochondrially encoded tRNA-Gln (CAA/G) [Source:HGNC Symbol;Acc:HGNC:749~ 9 ENSG00000210112 MT-TM mitochondrially encoded tRNA-Met (AUA/G) [Source:HGNC Symbol;Acc:HGNC:749~ 10 ENSG00000198763 MT-ND2 mitochondrially encoded NADH:ubiquinone oxidoreductase core subunit 2 [So~ # ... with 68,002 more rows With this Ensembl Gene ID to HGNC symbol mapping in hand, you can combine the tibble above with other tibbles containing gene information by joining them together. BioMart/biomaRt is not the only ways to map different gene identifiers. The Bioconductor package AnnotateDbi also provides this functionality in a flexible format independent of the Ensembl databases. This package includes not only gene identifier mapping and information, but also identifiers from technology platforms, e.g. probe set IDs from microarrays, that can help when working with these types of data. The package also allows comprehensive and flexible homolog mapping. As with all Bioconductor packages, the AnnotationDbi documentation is well written and comprehensive, though knowledge of relational databases is helpful in understanding how the packages work. Ensembl BioMart portal Ensembl BioMart web portal documentation biomaRt Bioconductor documentation 8.6.3 Mapping Homologs Sometimes it is necessary to link datasets from different organisms together by orthology. For example, in an experiment performed in mice, we might be interested in comparing gene expression patterns observed in the mouse samples to a publicly available human dataset. In these contexts, we must link gene identifiers from one organism to their corresponding homologs in the other. BioMart enables us to extract these linked identifiers by explicitly connecting different biomaRt databases with the getLDS() function: human_db &lt;- useEnsembl(&quot;ensembl&quot;, dataset = &quot;hsapiens_gene_ensembl&quot;) mouse_db &lt;- useEnsembl(&quot;ensembl&quot;, dataset = &quot;mmusculus_gene_ensembl&quot;) orth_map &lt;- as_tibble( getLDS(attributes = c(&quot;ensembl_gene_id&quot;, &quot;hgnc_symbol&quot;), mart = human_db, attributesL = c(&quot;ensembl_gene_id&quot;, &quot;mgi_symbol&quot;), martL = mouse_db ) ) orth_map # A tibble: 26,390 x 4 Gene.stable.ID HGNC.symbol Gene.stable.ID.1 MGI.symbol &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 ENSG00000198695 MT-ND6 ENSMUSG00000064368 &quot;mt-Nd6&quot; 2 ENSG00000212907 MT-ND4L ENSMUSG00000065947 &quot;mt-Nd4l&quot; 3 ENSG00000279169 PRAMEF13 ENSMUSG00000094303 &quot;&quot; 4 ENSG00000279169 PRAMEF13 ENSMUSG00000094722 &quot;&quot; 5 ENSG00000279169 PRAMEF13 ENSMUSG00000095666 &quot;&quot; 6 ENSG00000279169 PRAMEF13 ENSMUSG00000094741 &quot;&quot; 7 ENSG00000279169 PRAMEF13 ENSMUSG00000094836 &quot;&quot; 8 ENSG00000279169 PRAMEF13 ENSMUSG00000074720 &quot;&quot; 9 ENSG00000279169 PRAMEF13 ENSMUSG00000096236 &quot;&quot; 10 ENSG00000198763 MT-ND2 ENSMUSG00000064345 &quot;mt-Nd2&quot; # ... with 26,380 more rows The mgi_symbol field refers to the gene symbol assigned in the Mouse Genome Informatics database maintained by The Jackson Laboratory. 8.7 Gene Expression Gene expression is the process by which information from a gene is used in the synthesis of functional gene products that affect a phenotype. While gene expression studies are often focused on protein coding genes, there are many other functional molecules, e.g. transfer RNAs, lncRNAs, etc, that are produced by this process. The gene expression process has many steps and intermediate products, as depicted in the following figure: The Gene Expression Process - An overview of the flow of information from DNA to protein in a eukaryote The specific parts of the genome that code for genes are copied, or transcribed into RNA molecules called transcripts. In lower organisms like bacteria, these RNA molecules are passed on directly to ribosomes, which translate them into proteins. In most higher organisms like eukaryotes, these initial transcripts, called pre-messenger RNA (pre-mRNA) transcripts are further processed such that certain parts of the transcript, called introns, are spliced out and the flanking sequences, called exons, are concatenated together. After this splicing process is complete, the pre-mRNA transcripts become mature messenger RNA (mRNA) transcripts which go on to be exported from the nucleus and loaded into ribosomes in the cytoplasm to undergo translation into proteins. In gene expression studies, the relative abundance, or number of copies, of RNA or mRNA transcripts in a sample is measured. The measurements are non-negative numbers that are proportional to the relative abundance of a transcript with respect to some reference, either another gene, or in the case of high throughput assays like microarrays or high throughput sequencing, relative to measurements of all distinct transcripts examined in the experiment. Conceptually, the larger the magnitude of a transcript abundance measurement, the more copies of that transcript were in the original sample. There are many ways to measure the abundance of RNA transcripts; the following are some of the common methods at the time of writing: Light absorbance - RNA absorbs ultraviolet light with wavelength 260 nm, which can be used to determine RNA concentration in a sample using specialized equipment Northern blot - measures relative abundance of an RNA with a specific sequence quantitative polymerase chain reaction (qPCR) - measures relative abundance of an RNA with a specific sequence using PCR amplification Oligonucleotide and microarrays - measures relative abundance of thousands of genes simultaneously using known DNA probe sequences and fluorescently tagged RNA molecules High throughput RNA sequencing (RNASeq) - measures thousands to millions of RNA fragments simultaneously in proportion to their relative abundance While any of the measurement methods above may be analyzed in R, the high throughput methods (i.e. microarray, high throughput sequencing) are the primary concern of this chapter. These methods generate measurements for thousands of transcripts or genes simultaneously, requiring the power and flexibility of programmatic analysis to process in a practical amount of time. The remainder of this chapter is devoted to understanding the specific technologies, data types, and analytical strategies involved in working with these data. Gene expression measurements are almost always inherently relative, either due to limitations of the measurement methods (e.g. microarrays, described below) or because measuring all molecules in a sample will almost always be prohibitively expensive, have diminishing returns, and it is very difficult if not impossible to determine if all the molecules have been measured. This means we cannot in general associate the numbers associated with the measurements with an absolute molecular copy number. An important implication of the inherent relativity of these measurements is: absence of evidence is not evidence of absence. In other words, if a transcript has an abundance measurement of zero, this does not necessarily imply that the gene is not expressed. It may be that the gene is indeed expressed, but the copy number is sufficiently small that it was not detected by the assay. 8.7.1 Gene Expression Data in Bioconductor The SummarizedExperiment container is the standard way to load and work with gene expression data in Bioconductor. This container requires the following information: assays - one or more measurement assays (e.g. gene expression) in the form of a feature by sample matrix colData - metadata associated with the samples (i.e. columns) of the assays rowData - metadata associated with the features (i.e. rows) of the assays exptData - additional metadata about the experiment itself, like protocol, project name, etc The figure below illustrates how the SummarizedExperiment container is structured and how the different data elements are accessed: SummarizedExperiment Schematic - Huber, et al. 2015. Orchestrating High-Throughput Genomic Analysis with Bioconductor. Nature Methods 12 (2): 11521. Many Bioconductor packages for specific types of data, e.g. limma create these SummarizedExperiment objects for you, but you may also create your own by assembling each of these data into data frames manually: # microarray expression dataset intensities intensities &lt;- readr::read_delim(&quot;example_intensity_data.csv&quot;,delim=&quot; &quot;) # the first column of intensities tibble is the probesetId, extract to pass as rowData rowData &lt;- intensities[&quot;probeset_id&quot;] # remove probeset IDs from assay and turn into a base R dataframe so that we can assign rownames, since tibbles don&#39;t support row names intensities &lt;- as.data.frame( select(intensities, -probeset_id) ) rownames(intensities) &lt;- rowData$probeset_id # these column data are made up, but you would have a sample metadata file to use colData &lt;- tibble( sample_name=colnames(intensities), condition=sample(c(&quot;case&quot;,&quot;control&quot;),ncol(intensities),replace=TRUE) ) se &lt;- SummarizedExperiment( assays=list(intensities=intensities), colData=colData, rowData=rowData ) se ## class: SummarizedExperiment ## dim: 54675 35 ## metadata(0): ## assays(1): intensities ## rownames(54675): 1007_s_at 1053_at ... AFFX-TrpnX-5_at AFFX-TrpnX-M_at ## rowData names(1): probeset_id ## colnames(35): GSM972389 GSM972390 ... GSM972512 GSM972521 ## colData names(2): sample_name condition Detailed documentation of how to create and use the SummarizedExperiment is available in the SummarizedExperiment vignette. SummarizedExperiment is the successor to the older ExpressionSet container. Both are still used by Bioconductor packages, but SummarizedExperiment is more modern and flexible, so it is suggested for use whenever possible. 8.7.2 Microarrays A Microarray Device - Thermo Fisher Scientific Microarrays are devices that measure the relative abundance of thousands of distinct DNA sequences simultaneously. Short (~25 nucleotide) single-stranded DNA molecules called probes are deposited on a small glass slide in a grid of spots, where each spot contains many copies of a probe with identical sequence. The probe sequences are selected a priori based on the purpose of the array. For example, gene expression microarrays have probes that correspond to the coding regions of the genome for a species of interest, while genotyping microarrays use sequences with known variants found in a population of genomes, most often the human population. Microarrays of the same type (e.g. human gene expression) all have the same set of probes. The choice of DNA sequence probes therefore determines what the microarray measures and how to interpret the data. The design of a microarray is illustrated in the following figure. Illustration of Microarray Design A microarray device generates data by applying a specially prepared DNA sample to it; the sample usually corresponds to a single biological specimen, e.g. an individual patient. The preparation method for the sample depends on what is being measured: When measuring DNA directly, e.g. genetic variants, the DNA itself is biochemically extracted When measuring gene expression via RNA abundance, RNA is first extracted and then reverse transcribed to complementary DNA (cDNA) In either case, the molecules that will be applied to the microarray are DNA molecules. After extraction and preparation, the DNA molecules are then randomly cut up into shorter molecules (i.e. sheared) and each molecule has a molecular tag biochemically ligated to it that will emit fluorescence when excited by a specific wavelength of light. This tagged DNA sample is then washed over the microarray chip, where DNA molecules that share sequence complementarity with the probes on the array pair together. After this treatment, the microarray is washed to remove DNA molecules that did not have a match on the array, leaving only those molecules with a sequence match that remain bound to probes. The microarray chip is then loaded into a scanning device, where a laser with a specific wavelength of light is then shone onto the array, causing the spots with tagged DNA molecules associated with probes to fluoresce, and other spots remain dark. A high resolution image is taken of the fluorescent array, and the image is analyzed to map the intensity of the light on each spot to a numeric value proportional to its intensity. The reason for this is that, since each spot has many individual probe molecules contained within it, the more copies of the corresponding DNA molecule were in the sample, the more light the spot emits. In this way, the relative abundance of all probes on the array are measured simultaneously. The process of generating microarray data from a sample is illustrated in the following figure. Illustration of Microarray Data Generation Process After the microarray has been scanned, the relative copy number of the DNA in the sample matching the probes on the microarray are expressed as the intensity of fluorescence of each probe. The raw intensity data from the scan has been processed and analyzed by the scanner software to account for technical biases and artefacts of the scanning instrument and data generation process. The data from a single scan is processed and stored in a file in CEL format, a proprietary data format that stores the raw probe intensity data that can be loaded for downstream analysis. On a high level, there are three steps involved in analyzing gene expression microarray data: Summarization of probes to probesets. Each gene is represented by multiple probes with different sequences. Summarization is a statistical procedure that computes a single value for each probeset from its corresponding probes. Normalization. This includes removing background signal from individual arrays as well as adjusting probeset intensities so that they are comparable across multiple sample arrays. Quality control. Compare all normalized samples within a sample set to identify, mitigate, or eliminate potential outlier samples. Analysis. Using the quality controlled expression data, Implement statistical analysis to answer research questions. The full details of microarray analysis are beyond the scope of this book. However in the following sections we cover some of the basic entry points to performing these steps in R and Bioconductor. 8.7.3 Microarray Data in Bioconductor The CEL data files from a set of microarrays can be loaded into R for analysis using the affy Bioconductor package. This package provides all the functions necessary for loading the data and performing key preprocessing operations. Typically, two or more samples were processed in this way, resulting in a set of CEL files that should be processed together. These CEL files will typically be all stored in the same directory, and may be loaded using the affy::ReadAffy function: # read all CEL files in a single directory affy_batch &lt;- affy::ReadAffy(celfile.path=&quot;directory/of/CELfiles&quot;) # or individual files in different directories cel_filenames &lt;- c( list.files( path=&quot;CELfile_dir1&quot;, full.names=TRUE, pattern=&quot;.CEL$&quot; ), list.files( path=&quot;CELfile_dir2&quot;, full.names=TRUE, pattern=&quot;.CEL$&quot; ) ) affy_batch &lt;- affy::ReadAffy(filenames=cel_filenames) The affy_batch variable is a AffyBatch container, which stores information on the probe definitions based on the type of microarray, probe-level intensity for each sample, and other information about the experiment contained within the CEL files. The affy package provides functions to perform probe summarization and normalization. The most popular method to accomplish this at the time of writing is called Robust Multi-array Average or RMA (Irizarry et al. 2003), which performs summarization and normalization of multiple arrays simultaneously. Below, the RMA algorithm is used to normalize an example dataset provided by the affydata Bioconductor package. Certain Bioconductor packages provide example datasets for use with companion analysis packages. For example, the affydata package provides the Dilution dataset, which was generated using two concentrations of cDNA from human liver tissue and a central nervous system cell line. To load a data package into R, first run library(&lt;data package&gt;) and then data(&lt;dataset name&gt;). library(affy) library(affydata) data(Dilution) # normalize the Dilution microarray expression values # note Dilution is an AffyBatch object eset_rma &lt;- affy::rma(Dilution,verbose=FALSE) # plot distribution of non-normalized probes # note rma normalization takes the log2 of the expression values, # so we must do so on the raw data to compare raw_intensity &lt;- as_tibble(exprs(Dilution)) %&gt;% mutate(probeset_id=rownames(exprs(Dilution))) %&gt;% pivot_longer(-probeset_id, names_to=&quot;Sample&quot;, values_to = &quot;Intensity&quot;) %&gt;% mutate( `log2 Intensity`=log2(Intensity), Category=&quot;Before Normalization&quot; ) # plot distribution of normalized probes rma_intensity &lt;- as_tibble(exprs(eset_rma)) %&gt;% mutate(probesetid=featureNames(eset_rma)) %&gt;% pivot_longer(-probesetid, names_to=&quot;Sample&quot;, values_to = &quot;log2 Intensity&quot;) %&gt;% mutate(Category=&quot;After RMA Normalization&quot;) dplyr::bind_rows(raw_intensity, rma_intensity) %&gt;% ggplot(aes(x=Sample, y=`log2 Intensity`)) + geom_boxplot() + facet_wrap(vars(Category)) Above, we first apply RMA to the Dilution microarray dataset using the affy::rma() function. Then we plot the log 2 intensities of the probes in each array, first using the raw intensities and then after RMA normalization. The effect of normalization is clearly visible, where the quartiles of the probe intensity distributions have been adjusted to be consistent across arrays and the dynamic range of probe values has been controlled. affy Bioconductor package affydata Bioconductor package Microarray probes and probe sets Irizarry et al. - RMA Algorithm Details 8.7.4 Differential Expression: Microarrays (limma) . limma, which is short for linear models of microarrays, is one of the top most downloaded Bioconductor packages. limma is utilized for analyzing microarray gene expression data, with a focus on analyses using linear models to integrate all of the data from an experiment. limma was developed for microarray analysis prior to the development of sequencing based gene expression methods (i.e. [RNASeq]) but has since added functionality to analyze other types of gene expression data. limma excels at analyzing these types of data as it can support arbitrarily complex experimental designs while maintaining strong statistical power. An experiment with a large number of conditions or predictors can still be analyzed even with small sample sizes. his brief example is from the limma User Guide chapter 15, and covers loading and processing data from an RNA-seq experiment. We go into more depth while working with limma in assignment 6. Without going into too much detail, the design variable is how we inform limma of our experimental conditions, and where limma draws from to construct its linear models correctly. This design is relatively simple, just four samples belonging to two different conditions (the swirls here refer to the swirl of zebra fish, you can just see them as a phenotypic difference) design &lt;- data.frame(swirl = c(&quot;swirl.1&quot;, &quot;swirl.2&quot;, &quot;swirl.3&quot;, &quot;swirl.4&quot;), condition = c(1, -1, 1, -1)) dge &lt;- DGEList(counts=counts) keep &lt;- filterByExpr(dge, design) dge &lt;- dge[keep,,keep.lib.sizes=FALSE] DGEList() is a function from edgeR, of which limma borrows some loading and filtering functions. This experiment filters by expression level, and uses square bracket notation ([]) to reduce the number of rows. Finally, the expression data is transformed into CPM, counts per million, and a linear model is applied to the data with lmFit(). topTable() is used to view the most differentially expressed data. # limma trend logCPM &lt;- cpm(dge, log=TRUE, prior.count=3) fit &lt;- lmFit(logCPM, design) fit &lt;- eBayes(fit, trend=TRUE) topTable(fit, coef=ncol(design)) What is limma? One of many packages utilized for analyzing microarray gene expression data, with a focus on linear models Can compare RNA targets in an arbitrarily complex way Designed for complex experiments with a variety of condtions What types of analysis can limma perform? applicable to data from any quantitative gene expression technology including microoarrays, RNA-seq and quantitative PCR How is limma different from normal linear models? The hallmark of the limma approach is the use of linear models to analyse entire experiments as an integrated whole rather than making piece-meal comparisons between pairs of treatments. How do we run a microarray analysis with limma? We do an in-depth analysis of multiple microarray analysis packages in assignment 6, but for now Homepage BioConductor Publication 8.7.5 High Throughput Sequencing . What is high throughput sequencing (HTS)? What does HTS data look like? What does HTS measure, and what kinds of questions can we answer with it? How is HTS data analyzed? (How) do we analyze it in R? 8.7.6 Count Data . What is count data? Where does it come from? What are the statistical properties of counts and how do those properties influence our choice of analysis methods ( i.e. Poisson/negative binomial distribution, regularized log transformation to normality, etc)? 8.7.7 RNASeq . What is RNASeq? What does RNASeq measure (steady state RNA copy number)? 8.7.8 Differential Expression: RNASeq . How do we analyze RNASeq data (i.e. differential expression) in R? What are DESeq2/EdgeR, why are they different than other differential expression methods? How do we interpret DE results? 8.8 Genomic Intervals . What are genomic intervals? What do they mean and how do we use them? How do we load intervals into R and use them in our analyses? 8.8.1 Gene Set Enrichment Analysis With the constant evolution of high-throughput sequencing (HTS) technologies, the size and dimensionality of data generated has been ever increasing. The question of interest has shifted from how do we generate the data to how do we make meaningful biological interpretations on a genome-wide level. The simplest and most common output of HTS experiments is a list of interesting genes. In the specific case of differential gene expression analysis, it is possible and quite common to obtain hundreds or even thousands of differentially expressed genes in a single experiment that may be directly or indirectly related to the phenotype of interest. While it can be helpful and fruitful to research these genes individually, this form of personal inspection is limited by ones domain knowledge and by the size of the results. On a biological level, it is complicated by the fact that most processes are often coordinated by the actions of many genes/gene products working in concert. Cellular signaling pathways, phenotypic differences or responses to various stimuli are typically associated with changes in the expression pattern of many genes that share common biological functions or regulation. Gene set enrichment analysis is an umbrella term for methods designed to analyze expression data and capture changes in higher-level biological processes and pathways by organizing genes into biologically relevant groups or gene sets. We will discuss the background of two common gene set enrichment analyses (over-representation analysis and rank-based Gene Set Enrichment Analysis), their advantages and disadvantages, and walk-through an example of how they can each be implemented in R. Before this, we will briefly touch upon the definition of a gene set and describe how to construct or obtain gene sets. 8.8.2 Gene Sets Gene sets are curated lists of genes that are grouped together by some logical association or pre-existing domain knowledge. Their primary use is to facilitate the biological interpretation of expression data by capturing high-level interactions between biologically relevant groups of genes. These sets are highly flexible and may be constructed based on any a priori knowledge or classification. For example, one could define a gene set that includes all genes that are members of the same biochemical pathway or a gene set that consists of all genes that are induced upon treatment with a particular pharmacological agent. While it is perfectly valid to construct new gene sets, there are many existing and curated collections of gene sets that are maintained and contributed to by communities of scientists throughout the world. Below, we will highlight some of the major collections that are commonly used and cited in published work: KEGG Pathways The Kyoto Encyclopedia of Genes and Genomes (KEGG) is a repository for biological pathway information that the authors describe as a means to computerize functional interpretations as part of the pathway reconstruction process based on the hierarchically structured knowledge about the genomic, chemical, and network spaces. The KEGG Pathways database consists of maps displaying the functional and regulatory relationships of genes within various metabolic and cell signaling pathways. The KEGG offers a web service that allows for the extraction of the genes and other information belonging to specific pathways. GO Annotations The Gene Ontology provides a network representation of biological systems from the molecular-level to the organismal level by defining a graph based representation of connected terms using a controlled vocabulary. At a high level, GO annotations consist of three broad ontologies termed molecular function, cellular component, and biological process. The molecular function is the specific activity a gene product performs on a molecular level (i.e. a protein kinase would be annotated with protein kinase activity). The biological process defines the higher-level programs and pathways thats accomplished by the activities of the gene product (i.e. if we take our previous example of a protein kinase, its biological process might be assigned as signal transduction). The cellular component refers to the localization of the gene product within the cell (i.e. the cellular component of a protein kinase might be cytosol). A gene product can be annotated with zero or more terms from each ontology, and these annotations are based on multiple levels of evidence from published work. The GO annotations may be accessed or downloaded directly from their own website, and there exist a number of web services that use GO annotations in the background (DAVID, enrichR, etc.) Molecular Signatures Database The Molecular Signatures Database (MSigDB) is a collection of gene sets curated, maintained and provided by the Broad Institute and UC San Diego. Although intended and designed for specific use with the Gene Set Enrichment Analysis Methodology, they are freely available and only require proper attribution for other uses. The MSigDB consists of 9 major collections of gene sets: H (hallmark gene sets), C1 (positional gene sets), C2 (curated gene sets), c3 (regulatory target gene sets), c4 (computational gene sets), c5 (ontology gene sets), c6 (oncogenic signature gene sets), c7 (immunologic signature gene sets), c8 (cell type signature gene sets). They are available in formats ready for use in the GSEA methodology and other formats that are easily imported into various settings for custom use. The gene sets as well as the GSEA methodology are available directly from their website. There are a number of R-specific packages that have been developed for working directly with these gene sets such as GSEABase or fgsea, which we will discuss later. 8.8.3 Working with gene sets in R We will walk through two quick examples of how to read in an example collection of gene sets. We will utilize the Hallmarks gene set collection, downloaded directly from the MSigDB website, which consists of 50 gene sets representing well-defined biological processes and generated by aggregating together many pre-existing gene sets. The MSigDB provides these gene set collections in the GMT format. These files are tab-delimited and each row in the format represents one gene set with the first column being the name of the gene set, and the second column a short description. The remaining columns each represent a gene and unequal lengths of columns per row is allowed. As is, the GMT format is designed to work specifically with the GSEA methodology developed and provided by the Broad Institute. However, we will also show two ways to manually parse these gene collections for exploration and further use in R. The first way to parse these gene sets would be to use various tidyverse functions that you should be familiar with already to construct a tibble. Essentially, we read the file, rename the first two columns for convenience, and use a combination of pivot_longer() and group_by() to quickly access genes by pathway. Below, we have the results of these operations and used them to display a summary of the number of genes in all the pathways contained with the hallmark pathways gene collection. read_delim(&#39;h.all.v7.5.1.symbols.gmt&#39;, delim=&#39;\\t&#39;, col_names=FALSE) %&gt;% dplyr::rename(Pathway = X1, Desc=X2) %&gt;% dplyr::select(-Desc) %&gt;% pivot_longer(!Pathway, values_to=&#39;genes&#39;, values_drop_na=TRUE, names_to = NULL) %&gt;% group_by(Pathway) %&gt;% summarise(n=n()) ## # A tibble: 50 x 2 ## Pathway n ## &lt;chr&gt; &lt;int&gt; ## 1 HALLMARK_ADIPOGENESIS 200 ## 2 HALLMARK_ALLOGRAFT_REJECTION 200 ## 3 HALLMARK_ANDROGEN_RESPONSE 100 ## 4 HALLMARK_ANGIOGENESIS 36 ## 5 HALLMARK_APICAL_JUNCTION 200 ## 6 HALLMARK_APICAL_SURFACE 44 ## 7 HALLMARK_APOPTOSIS 161 ## 8 HALLMARK_BILE_ACID_METABOLISM 112 ## 9 HALLMARK_CHOLESTEROL_HOMEOSTASIS 74 ## 10 HALLMARK_COAGULATION 138 ## # ... with 40 more rows We can see that there are 50 total pathways in the hallmarks gene collection that contain varying numbers of genes. We discarded the description column and if we were to save the results after using pivot_longer() instead of piping them to group_by and summarise(), we would have a tibble in the long format with each row representing a single pathway and single gene. However, there do exist various packages that have been developed to specifically handle gene set data in R such as the previously mentioned GSEABase. This is a collection of functions and class-based objects that facilitate working with gene sets. The foundation of the GSEABase package is the GeneSet and GeneSetCollection classes which store gene sets and metadata or a collection of GeneSet objects, respectively. We will use the GSEABase package to read in the collection of gene sets we previously downloaded. library(&#39;GSEABase&#39;) hallmarks_gmt &lt;- getGmt(con=&#39;h.all.v7.5.1.symbols.gmt&#39;) hallmarks_gmt ## GeneSetCollection ## names: HALLMARK_TNFA_SIGNALING_VIA_NFKB, HALLMARK_HYPOXIA, ..., HALLMARK_PANCREAS_BETA_CELLS (50 total) ## unique identifiers: JUNB, CXCL2, ..., SRP14 (4383 total) ## types in collection: ## geneIdType: NullIdentifier (1 total) ## collectionType: NullCollection (1 total) If we simply access the hallmarks_gmt variable, we can see that it is a GeneSetCollection object containing 50 total gene sets which encompass 4383 unique identifiers or HGNC symbols. Although this package supports a range of functions, we will focus on the basics. For a more thorough description of the classes and methods, please read their extended documentation available here. The geneIds method will return a list with each pathway as a named vector of associated gene ids: head(geneIds(hallmarks_gmt), 2) ## $HALLMARK_TNFA_SIGNALING_VIA_NFKB ## [1] &quot;JUNB&quot; &quot;CXCL2&quot; &quot;ATF3&quot; &quot;NFKBIA&quot; &quot;TNFAIP3&quot; &quot;PTGS2&quot; ## [7] &quot;CXCL1&quot; &quot;IER3&quot; &quot;CD83&quot; &quot;CCL20&quot; &quot;CXCL3&quot; &quot;MAFF&quot; ## [13] &quot;NFKB2&quot; &quot;TNFAIP2&quot; &quot;HBEGF&quot; &quot;KLF6&quot; &quot;BIRC3&quot; &quot;PLAUR&quot; ## [19] &quot;ZFP36&quot; &quot;ICAM1&quot; &quot;JUN&quot; &quot;EGR3&quot; &quot;IL1B&quot; &quot;BCL2A1&quot; ## [25] &quot;PPP1R15A&quot; &quot;ZC3H12A&quot; &quot;SOD2&quot; &quot;NR4A2&quot; &quot;IL1A&quot; &quot;RELB&quot; ## [31] &quot;TRAF1&quot; &quot;BTG2&quot; &quot;DUSP1&quot; &quot;MAP3K8&quot; &quot;ETS2&quot; &quot;F3&quot; ## [37] &quot;SDC4&quot; &quot;EGR1&quot; &quot;IL6&quot; &quot;TNF&quot; &quot;KDM6B&quot; &quot;NFKB1&quot; ## [43] &quot;LIF&quot; &quot;PTX3&quot; &quot;FOSL1&quot; &quot;NR4A1&quot; &quot;JAG1&quot; &quot;CCL4&quot; ## [49] &quot;GCH1&quot; &quot;CCL2&quot; &quot;RCAN1&quot; &quot;DUSP2&quot; &quot;EHD1&quot; &quot;IER2&quot; ## [55] &quot;REL&quot; &quot;CFLAR&quot; &quot;RIPK2&quot; &quot;NFKBIE&quot; &quot;NR4A3&quot; &quot;PHLDA1&quot; ## [61] &quot;IER5&quot; &quot;TNFSF9&quot; &quot;GEM&quot; &quot;GADD45A&quot; &quot;CXCL10&quot; &quot;PLK2&quot; ## [67] &quot;BHLHE40&quot; &quot;EGR2&quot; &quot;SOCS3&quot; &quot;SLC2A6&quot; &quot;PTGER4&quot; &quot;DUSP5&quot; ## [73] &quot;SERPINB2&quot; &quot;NFIL3&quot; &quot;SERPINE1&quot; &quot;TRIB1&quot; &quot;TIPARP&quot; &quot;RELA&quot; ## [79] &quot;BIRC2&quot; &quot;CXCL6&quot; &quot;LITAF&quot; &quot;TNFAIP6&quot; &quot;CD44&quot; &quot;INHBA&quot; ## [85] &quot;PLAU&quot; &quot;MYC&quot; &quot;TNFRSF9&quot; &quot;SGK1&quot; &quot;TNIP1&quot; &quot;NAMPT&quot; ## [91] &quot;FOSL2&quot; &quot;PNRC1&quot; &quot;ID2&quot; &quot;CD69&quot; &quot;IL7R&quot; &quot;EFNA1&quot; ## [97] &quot;PHLDA2&quot; &quot;PFKFB3&quot; &quot;CCL5&quot; &quot;YRDC&quot; &quot;IFNGR2&quot; &quot;SQSTM1&quot; ## [103] &quot;BTG3&quot; &quot;GADD45B&quot; &quot;KYNU&quot; &quot;G0S2&quot; &quot;BTG1&quot; &quot;MCL1&quot; ## [109] &quot;VEGFA&quot; &quot;MAP2K3&quot; &quot;CDKN1A&quot; &quot;CCN1&quot; &quot;TANK&quot; &quot;IFIT2&quot; ## [115] &quot;IL18&quot; &quot;TUBB2A&quot; &quot;IRF1&quot; &quot;FOS&quot; &quot;OLR1&quot; &quot;RHOB&quot; ## [121] &quot;AREG&quot; &quot;NINJ1&quot; &quot;ZBTB10&quot; &quot;PLPP3&quot; &quot;KLF4&quot; &quot;CXCL11&quot; ## [127] &quot;SAT1&quot; &quot;CSF1&quot; &quot;GPR183&quot; &quot;PMEPA1&quot; &quot;PTPRE&quot; &quot;TLR2&quot; ## [133] &quot;ACKR3&quot; &quot;KLF10&quot; &quot;MARCKS&quot; &quot;LAMB3&quot; &quot;CEBPB&quot; &quot;TRIP10&quot; ## [139] &quot;F2RL1&quot; &quot;KLF9&quot; &quot;LDLR&quot; &quot;TGIF1&quot; &quot;RNF19B&quot; &quot;DRAM1&quot; ## [145] &quot;B4GALT1&quot; &quot;DNAJB4&quot; &quot;CSF2&quot; &quot;PDE4B&quot; &quot;SNN&quot; &quot;PLEK&quot; ## [151] &quot;STAT5A&quot; &quot;DENND5A&quot; &quot;CCND1&quot; &quot;DDX58&quot; &quot;SPHK1&quot; &quot;CD80&quot; ## [157] &quot;TNFAIP8&quot; &quot;CCNL1&quot; &quot;FUT4&quot; &quot;CCRL2&quot; &quot;SPSB1&quot; &quot;TSC22D1&quot; ## [163] &quot;B4GALT5&quot; &quot;SIK1&quot; &quot;CLCF1&quot; &quot;NFE2L2&quot; &quot;FOSB&quot; &quot;PER1&quot; ## [169] &quot;NFAT5&quot; &quot;ATP2B1&quot; &quot;IL12B&quot; &quot;IL6ST&quot; &quot;SLC16A6&quot; &quot;ABCA1&quot; ## [175] &quot;HES1&quot; &quot;BCL6&quot; &quot;IRS2&quot; &quot;SLC2A3&quot; &quot;CEBPD&quot; &quot;IL23A&quot; ## [181] &quot;SMAD3&quot; &quot;TAP1&quot; &quot;MSC&quot; &quot;IFIH1&quot; &quot;IL15RA&quot; &quot;TNIP2&quot; ## [187] &quot;BCL3&quot; &quot;PANX1&quot; &quot;FJX1&quot; &quot;EDN1&quot; &quot;EIF1&quot; &quot;BMP2&quot; ## [193] &quot;DUSP4&quot; &quot;PDLIM5&quot; &quot;ICOSLG&quot; &quot;GFPT2&quot; &quot;KLF2&quot; &quot;TNC&quot; ## [199] &quot;SERPINB8&quot; &quot;MXD1&quot; ## ## $HALLMARK_HYPOXIA ## [1] &quot;PGK1&quot; &quot;PDK1&quot; &quot;GBE1&quot; &quot;PFKL&quot; &quot;ALDOA&quot; &quot;ENO2&quot; ## [7] &quot;PGM1&quot; &quot;NDRG1&quot; &quot;HK2&quot; &quot;ALDOC&quot; &quot;GPI&quot; &quot;MXI1&quot; ## [13] &quot;SLC2A1&quot; &quot;P4HA1&quot; &quot;ADM&quot; &quot;P4HA2&quot; &quot;ENO1&quot; &quot;PFKP&quot; ## [19] &quot;AK4&quot; &quot;FAM162A&quot; &quot;PFKFB3&quot; &quot;VEGFA&quot; &quot;BNIP3L&quot; &quot;TPI1&quot; ## [25] &quot;ERO1A&quot; &quot;KDM3A&quot; &quot;CCNG2&quot; &quot;LDHA&quot; &quot;GYS1&quot; &quot;GAPDH&quot; ## [31] &quot;BHLHE40&quot; &quot;ANGPTL4&quot; &quot;JUN&quot; &quot;SERPINE1&quot; &quot;LOX&quot; &quot;GCK&quot; ## [37] &quot;PPFIA4&quot; &quot;MAFF&quot; &quot;DDIT4&quot; &quot;SLC2A3&quot; &quot;IGFBP3&quot; &quot;NFIL3&quot; ## [43] &quot;FOS&quot; &quot;RBPJ&quot; &quot;HK1&quot; &quot;CITED2&quot; &quot;ISG20&quot; &quot;GALK1&quot; ## [49] &quot;WSB1&quot; &quot;PYGM&quot; &quot;STC1&quot; &quot;ZNF292&quot; &quot;BTG1&quot; &quot;PLIN2&quot; ## [55] &quot;CSRP2&quot; &quot;VLDLR&quot; &quot;JMJD6&quot; &quot;EXT1&quot; &quot;F3&quot; &quot;PDK3&quot; ## [61] &quot;ANKZF1&quot; &quot;UGP2&quot; &quot;ALDOB&quot; &quot;STC2&quot; &quot;ERRFI1&quot; &quot;ENO3&quot; ## [67] &quot;PNRC1&quot; &quot;HMOX1&quot; &quot;PGF&quot; &quot;GAPDHS&quot; &quot;CHST2&quot; &quot;TMEM45A&quot; ## [73] &quot;BCAN&quot; &quot;ATF3&quot; &quot;CAV1&quot; &quot;AMPD3&quot; &quot;GPC3&quot; &quot;NDST1&quot; ## [79] &quot;IRS2&quot; &quot;SAP30&quot; &quot;GAA&quot; &quot;SDC4&quot; &quot;STBD1&quot; &quot;IER3&quot; ## [85] &quot;PKLR&quot; &quot;IGFBP1&quot; &quot;PLAUR&quot; &quot;CAVIN3&quot; &quot;CCN5&quot; &quot;LARGE1&quot; ## [91] &quot;NOCT&quot; &quot;S100A4&quot; &quot;RRAGD&quot; &quot;ZFP36&quot; &quot;EGFR&quot; &quot;EDN2&quot; ## [97] &quot;IDS&quot; &quot;CDKN1A&quot; &quot;RORA&quot; &quot;DUSP1&quot; &quot;MIF&quot; &quot;PPP1R3C&quot; ## [103] &quot;DPYSL4&quot; &quot;KDELR3&quot; &quot;DTNA&quot; &quot;ADORA2B&quot; &quot;HS3ST1&quot; &quot;CAVIN1&quot; ## [109] &quot;NR3C1&quot; &quot;KLF6&quot; &quot;GPC4&quot; &quot;CCN1&quot; &quot;TNFAIP3&quot; &quot;CA12&quot; ## [115] &quot;HEXA&quot; &quot;BGN&quot; &quot;PPP1R15A&quot; &quot;PGM2&quot; &quot;PIM1&quot; &quot;PRDX5&quot; ## [121] &quot;NAGK&quot; &quot;CDKN1B&quot; &quot;BRS3&quot; &quot;TKTL1&quot; &quot;MT1E&quot; &quot;ATP7A&quot; ## [127] &quot;MT2A&quot; &quot;SDC3&quot; &quot;TIPARP&quot; &quot;PKP1&quot; &quot;ANXA2&quot; &quot;PGAM2&quot; ## [133] &quot;DDIT3&quot; &quot;PRKCA&quot; &quot;SLC37A4&quot; &quot;CXCR4&quot; &quot;EFNA3&quot; &quot;CP&quot; ## [139] &quot;KLF7&quot; &quot;CCN2&quot; &quot;CHST3&quot; &quot;TPD52&quot; &quot;LXN&quot; &quot;B4GALNT2&quot; ## [145] &quot;PPARGC1A&quot; &quot;BCL2&quot; &quot;GCNT2&quot; &quot;HAS1&quot; &quot;KLHL24&quot; &quot;SCARB1&quot; ## [151] &quot;SLC25A1&quot; &quot;SDC2&quot; &quot;CASP6&quot; &quot;VHL&quot; &quot;FOXO3&quot; &quot;PDGFB&quot; ## [157] &quot;B3GALT6&quot; &quot;SLC2A5&quot; &quot;SRPX&quot; &quot;EFNA1&quot; &quot;GLRX&quot; &quot;ACKR3&quot; ## [163] &quot;PAM&quot; &quot;TGFBI&quot; &quot;DCN&quot; &quot;SIAH2&quot; &quot;PLAC8&quot; &quot;FBP1&quot; ## [169] &quot;TPST2&quot; &quot;PHKG1&quot; &quot;MYH9&quot; &quot;CDKN1C&quot; &quot;GRHPR&quot; &quot;PCK1&quot; ## [175] &quot;INHA&quot; &quot;HSPA5&quot; &quot;NDST2&quot; &quot;NEDD4L&quot; &quot;TPBG&quot; &quot;XPNPEP1&quot; ## [181] &quot;IL6&quot; &quot;SLC6A6&quot; &quot;MAP3K1&quot; &quot;LDHC&quot; &quot;AKAP12&quot; &quot;TES&quot; ## [187] &quot;KIF5A&quot; &quot;LALBA&quot; &quot;COL5A1&quot; &quot;GPC1&quot; &quot;HDLBP&quot; &quot;ILVBL&quot; ## [193] &quot;NCAN&quot; &quot;TGM2&quot; &quot;ETS1&quot; &quot;HOXB9&quot; &quot;SELENBP1&quot; &quot;FOSL2&quot; ## [199] &quot;SULT2B1&quot; &quot;TGFB3&quot; The names method will return all of the gene set names contained within a specific collection: head(names(hallmarks_gmt),2) ## [1] &quot;HALLMARK_TNFA_SIGNALING_VIA_NFKB&quot; &quot;HALLMARK_HYPOXIA&quot; We can access a specific gene set contained within this collection by referring to its name and using the following notation: hallmarks_gmt[[&#39;HALLMARK_ANGIOGENESIS&#39;]] ## setName: HALLMARK_ANGIOGENESIS ## geneIds: VCAN, POSTN, ..., CXCL6 (total: 36) ## geneIdType: Null ## collectionType: Null ## details: use &#39;details(object)&#39; Simply accessing the object will provide a high-level summary of the information contained within. To access a specific value of this GeneSet object, we would call one of the slots (a core concept in object-oriented programming). In our particular case, we could extract the gene names contained assigned to this GeneSet by calling the geneIds slot as shown below to return a vector of the gene names. We can see the first five below and also the length of the returned vector by using the base R length() function: head(hallmarks_gmt[[&#39;HALLMARK_ANGIOGENESIS&#39;]]@geneIds) ## [1] &quot;VCAN&quot; &quot;POSTN&quot; &quot;FSTL1&quot; &quot;LRPAP1&quot; &quot;STC1&quot; &quot;LPL&quot; length(hallmarks_gmt[[&#39;HALLMARK_ANGIOGENESIS&#39;]]@geneIds) ## [1] 36 GSEABase includes a number of built-in functions for reading gene sets in from various sources and performing common operations such as set intersections, set differences, and ID conversions. We will demonstrate the usage of some of these in the next section covering Over-representation analysis. 8.9 Over-representation Analysis One of the most common ways to utilize gene sets to evaluate gene expression results is to perform Over-representation Analysis (ORA). Let us assume that we have obtained a list of differentially expressed genes from an experiment. We are curious to know if within this list of differentially expressed genes, do we see an over-representation or enrichment of genes belonging to gene sets of interest? In more general terms, the goal of ORA is to determine how likely it is that there is a non-random association between a gene being differentially expressed and having membership in a chosen (and ideally relevant) gene set. In R, we can do a simple ORA by utilizing a Fishers exact test and a contingency table. For a purely hypothetical example, let us assume that we have performed differential gene expression analysis between two different cell lines. We obtain a list of 10,000 total genes (our background) discovered in the experiment and find that at our chosen statistical threshold, 1,000 of these are differentially expressed. To keep things simple, we will perform a single ORA test against the Hallmarks Angiogenesis gene set using a sample list of 1000 differentially expressed genes selected from data generated by Marisa et al. 2013. The Hallmarks Angiogenesis gene set consists of 36 genes and we find that 13 of these are also present in our list of differentially expressed genes. Please note that we are removing this list of genes from its original meaning and context found in the publication and simply using it to demonstrate the basic steps occurring during ORA. All of these numbers and lists were arbitrarily chosen and this experimental setup is purely hypothetical. Another issue to note is that typically the background list should represent the entire pool of genes from which any differentially expressed genes could have been selected. For expression experiments, it is typical to choose all of the detected genes (regardless of significance) as the background. The number of genes in the organisms genome could potentially also be an appropriate background Also, it is important to keep in mind that in reality, ORA is nearly always performed on a larger scale against a variety of different gene sets. This allows for the unbiased discovery of potentially novel and unexpected enrichment in other biological areas of interest. It also necessitates the need for multiple-testing correction, which we have discussed in multiple hypothesis testing. To begin, we would want to prepare a contingency table which describes the various overlaps between our sets of interest. For a 2x2 contingency table, these four values will be: Genes present in our list of differentially expressed genes and present in our gene set Genes present in our list of differentially expressed genes and not present in our gene set Genes not present in our list of differentially expressed genes and present in our gene set Genes not present in our list of differentially expressed genes and not present in our gene set To demonstrate what this would look like, we have manually constructed a contingency table with labels and totals added below. If you look at the margins of the table and recall the previously given values above, you can reconstruct the logic used to generate each of the values in all the cells. Differentially Expressed Not Differentially Expressed Total In Gene Set 13 23 36 Not in Gene Set 987 8977 9964 Total 1000 9000 10000 For the purposes of this example, we are reading in our differentially expressed genes from an external file, but this vector could be generated in any number of ways depending upon where and how your results are stored. Following good coding practices, we will write a small function that takes this list of DE genes and a GeneSet object to programmatically generate a contingency table: There are many ways to construct a contingency table. This is just one way that was chosen to make calculations of the values contained within the table transparent and easy to understand. #load and read our list of DE genes contained within a newline delimited txt file de_genes &lt;- scan(&#39;example_de_list.txt&#39;, what=character(), sep=&#39;\\n&#39;) #define a function that takes a list of DE genes, and a specific GeneSet from a GeneSetCollection to generate a contingency table #using set operations in GSEABase make_contingency &lt;- function(de_list, GeneSetobj) { #make our de list into a simple GeneSet object using GSEABase de_geneset &lt;- GeneSet(de_list, setName=&#39;1000 DE genes&#39;) #If we had the full results, we could determine this value without manually setting it background_len &lt;- 10000 #Calculate the values inside the contingency table using set operations de_in &lt;- length((de_geneset &amp; GeneSetobj)@geneIds) de_notin &lt;- length(setdiff(de_geneset, GeneSetobj)@geneIds) set_notde &lt;- length(setdiff(GeneSetobj, de_geneset)@geneIds) notin_notde &lt;- background_len - (de_in + de_notin + set_notde) #return a matrix of the contingency values return(matrix(c(de_in, de_notin, set_notde, notin_notde), nrow=2)) } contingency_table &lt;- make_contingency(de_genes, hallmarks_gmt[[&#39;HALLMARK_ANGIOGENESIS&#39;]]) contingency_table ## [,1] [,2] ## [1,] 13 23 ## [2,] 987 8977 We perform the Fishers Exact test using the built-in R function fisher.test() and view the summarized output by simply calling the variable where we stored the test results: fisher_results &lt;- fisher.test(contingency_table, alternative=&#39;greater&#39;) fisher_results ## ## Fisher&#39;s Exact Test for Count Data ## ## data: contingency_table ## p-value = 2.382e-05 ## alternative hypothesis: true odds ratio is greater than 1 ## 95 percent confidence interval: ## 2.685749 Inf ## sample estimates: ## odds ratio ## 5.139308 Specific values of the results can be accessed by the $notation (i.e. fisher_results$p.value). The full list of returned values may be found in the R documentation for fisher.test() Back to our hypothetical example and focusing on the p-value returned of r fisher_results$p.value, we can interpret this as the probability of randomly obtaining results as or more extreme than what we observed assuming the null hypothesis that there is no association between differential expression and gene set membership is true. Based on these results and if this p-value was below our pre-determined statistical threshold, we could make the conclusion that there is an enrichment or over-representation of our differentially expressed genes from this experiment in the Hallmark Angiogenesis gene set. Relating this back to the experiment, we might hypothesize that the differences between our two cell lines might be driving gene expression changes that result in alterations in genes involved in angiogenesis. This might motivate potential further in vitro experiments on these cell lines, including migration and proliferation assays, that could reveal if this enrichment of angiogenesis genes is reflected at a phenotypic or functional level. ORA is a quick and useful way to generate further hypotheses to investigate specific mechanisms of action or regulation. For example, after identifying a gene set as being enriched or over-represented, one could further test the specific genes in the set by examining their directionality of change or asking if dependent pathways/networks are also perturbed. One major limitation of ORA is that it relies on the choice of arbitrary statistical thresholds to define interesting or differentially expressed genes. To reiterate again, p-value thresholds hold no inherent biological meaning and are subjectively determined. Changing the p-value threshold may result in dramatic differences in the outcome of ORA. Additionally, expression datasets may measure tens of thousands of genes in a single experiment, and filtering by a p-value threshold may discard potentially useful information. ORA (though often modified with slightly different statistical methodologies) is implemented in a number of different R packages such as topGO or various web services including DAVID and enrichR 8.9.1 Rank-based Analysis . What is GSEA and how does it work? How is it different than over-representation analysis, both as a method and with respect to the input it accepts? How do we implement GSEA in R (use the fgsea package)? 8.10 Biological Networks . What are biological networks? What do they represent (networks encode relationships between entities)? Why are biological networks useful and what can we learn from them? 8.10.1 Biological Pathways . What is a biological pathway? Why are they important, and how are they useful? 8.10.2 Gene Regulatory Networks . What are gene regulatory networks? Why are they important? How do we identify them (data driven correlation, wetlab experiments, others?)? How are they useful? 8.10.3 Protein-Protein Interaction Network . What are protein-protein interaction (PPI) networks? What information do they represent (direct association, functional association, etc)? Where does PPI information come from (datasets, databases, etc)? What are some ways we can use PPI information when interpreting other biological data (like differential expression? not sure)? 8.10.4 WGCNA . What is WGCNA? What problem does WGCNA attempt to solve, and how is it different than other classes of analysis methods (e.g. differential expression)? What data are appropriate for use in WGCNA? How do we run an interpret a WGCNA analysis in R? 8.11 Single Cell Sequencing Analysis . 8.11.1 Single Cell Sequencing . What is the goal of single cell sequencing? How is it generated, and what does it measure? What questions can we ask of single cell data that we cannot ask of other types of sequencing data? How is single cell data loaded into R? 8.11.2 Single Cell Analysis . What is the general analytical workflow for single cell analysis? Why do we perform each step along the analysis path? What are key parameters that we must choose that influence the results? How do we perform single cell analysis in R (Seurat)? 8.11.3 Single Cell Clustering . 8.11.4 Dimensionality Reduction &amp; Projection . PCA, tSNE, UMAP 8.11.5 Single Cell Data Visualization . 8.11.6 Single Cell Marker Analysis . References "],["engineering.html", "9 EngineeRing 9.1 Unit Testing 9.2 Toolification . 9.3 Pipelines &amp; Workflows . 9.4 Parallel Processing . 9.5 R Packages .", " 9 EngineeRing 9.1 Unit Testing Writing code that does what you mean for it to do is often harder than it might seem, especially in R. Also, as your code grows in size and complexity, and you use good programming practice like writing functions, changing one part of your code may have unexpected effects on other parts that you didnt change. Unless you are using a programming language that has support for proof-based correctness guarantees, it may be impossible to determine if your code is always correct. As you might imagine, so-called total correctness is very difficult to attain, and often requires more time to implement than is practical (unless youre programming something where correctness is very important, e.g. for a self-driving car). However, there is a collection of approaches that can give us reasonable assurances that our code does what we mean for it to do. These approaches are called software testing frameworks that explicitly test our code for correctness. There are many different testing frameworks, but they all employ the general principle that we test our codes correctness by passing it inputs for which we we know what the output should be. For example, consider the following function that sums two numbers: add &lt;- function(x,y) { return(x+y) } We can test this function using a known set of inputs and explicitly comparing the result with the expected output: result &lt;- add(1,2) result == 3 [1] TRUE Our test instance in this case is input x=1,y=2 and the expected output is 3. By comparing the result of this input with the expected output, we have showed that at least for these specific inputs the function behaves as intended. The testing terminology used in this case is the test passed. If the result had been anything other than 3, the test would have failed. The above is an example of a test, but it is an informal test; it is not yet integrated into a framework since we have to manually inspect the result as passing or failing. In a testing framework, you as the developer of your code also write tests for your code and runs those tests frequently as your code evolves to make sure it continues to behave as you expect over time. The R package testthat provides such a testing framework that tries to make testing as fun as possible, so that you get a visceral satisfaction from writing tests. Its true that writing tests for your own code may feel tedious and very not fun, but the tradeoff is that tested code is more likely to be correct, saving you from potentially embarrassing (or worse) errors! Writing tests using testthat is very easy, using the example test written above (remember to install the package using install.packages(\"testthat\") first). library(testthat) test_that(&quot;add() correctly adds things&quot;, { expect_equal(add(1,2),3) expect_equal(add(5,6),11) } ) Test passed Test passed! How satisfying! The test_that function accepts two arguments: a concise, human readable description of the test one or more tests enclosed by {} written using expect_X functions from the testthat package In the example above, we are explicitly testing that the result of add(1,2) is equal to 3 and add(5,6) is equal to 11; specifically, we called expect_equal, which accepts two arguments that it uses to test equality. We have written two explicit test cases (i.e. 1+2 == 3 and 5+6 == 11) under the same test heading. If we had a mistake in our test such that the expected output was wrong, testthat would inform us not only of the failure, but more details about what happened compared to what we asked it to expect: test_that(&quot;add() correctly adds things&quot;, { expect_equal(add(1,2),3) expect_equal(add(5,6),10) } ) -- Failure (Line 3): add() correctly adds things ------------------------------- add(5, 6) not equal to 10. 1/1 mismatches [1] 11 - 10 == 1 Error: Test failed In this case, our test case was incorrect, but this would be very helpful information to have if we had correctly specified input and expected output and the test failed! It means we did something wrong, but now we are aware of it and can fix it. The general testing strategy usually involves writing an R script that only contains tests like the example above and not analysis code; the tests in your test script call the functions you have written in your other scripts to check for their correctness exactly like above. Then, whenever you make substantial changes to your analysis code, you can simply run your test script to check whether everything went ok. Of course, as you add more functions to your analysis script you need to add new tests for that code. If we had put our test above in a script file called test_functions.R we could run them on our analysis code like the following: add &lt;- function(x,y) { return(x+y) } testthat::test_file(&quot;test_functions.R&quot;) == Testing test_functions.R ======================================================= [ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ] Done! The ultimate testing strategy is called test driven development where you write your tests before developing any analysis code, even for functions that dont exist yet. Imagine we decide we need a new function that multiplies two numbers together and havent written it yet. testtthat can handle the situation where you call a function that isnt defined yet: test_that(&quot;mul() correctly multiplies things&quot;,{ expect_equal(mul(1,2),2) }) -- Error (Line 1): new function ------------------------------------------------ Error in `mul(1, 2)`: could not find function &quot;mul&quot; Backtrace: 1. testthat::expect_equal(mul(1, 2), 2) 2. testthat::quasi_label(enquo(object), label, arg = &quot;object&quot;) 3. rlang::eval_bare(expr, quo_get_env(quo)) Error: Test failed In this case, the test failed because mul() is not defined yet, but you have already done the hard part of writing the test! Now all you have to do is write the mul() function and keep working on it until the tests pass. Writing tests first and analysis code later is a great way to be thoughtful about how your code is structured, along with the usual benefit of testing that it means your code is more likely to be correct. Testing - R packages testthat Reference 9.2 Toolification . 9.3 Pipelines &amp; Workflows . 9.4 Parallel Processing . 9.5 R Packages . R packages by Hadley Wickam "],["rshiny.html", "10 RShiny 10.1 Overview 10.2 Learning Objectives 10.3 Skill List", " 10 RShiny 10.1 Overview 10.2 Learning Objectives 10.3 Skill List "],["communicating-with-r.html", "11 Communicating with R 11.1 RMarkdown &amp; knitr 11.2 bookdown", " 11 Communicating with R No matter what your job is, you will at some point need to communicate the findings of your analysis with others. This will usually entail a combination of different forms of communication, including text, tables, and visualizations like plots. Depending on your audience, communicating may also include your code which describes very precisely (and ideally, reproducibly) what you did in your analysis. This chapter describes a few tools and techniques you can use to create static reports that help you communicate in all these ways. 11.1 RMarkdown &amp; knitr A markup language is a special kind of programming language used to annotate and decorate plain text with information about its intended formatting and structure. The syntax of a markup language is intended to be easy to read and write by humans and also machine readable, so that it may be processed by formatting programs into different formats, e.g. the same markup text might be converted into HTML or PDF. markdown is one such markup language. The markup is simple, and provides basic formatting syntax by default. The following contains some examples of markdown syntax: You can *emphasize* text, or **really emphasize it**. Lists are pretty easy to read as well: * item 1 * item 2 * item 3 If you need an enumerated list you can do that too: 1. item 1 2. item 2 3. item 3 You can easily include links to web sites like [Google](http://google.com) and images: ![an image of a master of the universe](https://upload.wikimedia.org/wikipedia/commons/b/bc/Juvenile_Ragdoll.jpg){width=50%} The markdown from above might be formatted as follows: You can emphasize text, or really emphasize it. Lists are pretty easy to read as well: item 1 item 2 item 3 If you need an enumerated list you can do that too: item 1 item 2 item 3 You can easily include links to web sites like Google and images: an image of a master of the universe Refer to the markdown documentation for a complete listing of supported markdown syntax. Some other markup languages you might recognize: ReStructured Text - the markup language used in most python documentation LaTeX - a markup language designed to help format mathematical expression HTML - the web markup language (HyperText Markup Language) wikitext - the markup language used by Wikipedia As the name suggests, RMarkdown is an extension of markdown that works in R. The most important extension is the ability to include code blocks in R in between markdown formatted text that can be executed. This enables writing executable reports that update their results whenever the document is rerun interspersed with explanatory text and other descriptive elements. For this reason, RMarkdown files are sometimes called RMarkdown notebooks, because they can be used to record both narrative text and results, similar to a traditional lab notebook. RMarkdown files typically end with .Rmd. RStudio has full RMarkdown integration to make writing RMarkdown notebooks very easy. Below is a screenshot of an RMarkdown document loaded in RStudio: RMarkdown notebook example The grey lines starting with ```{r} define a special code block that contains R code that should be executed and the output placed after the block. When run, RStudio will show you the output of the notebook within its interface: RMarkdown notebook example after running Notice how there is now a plot in the rendered document on the right. The code within the block was run, the plot generated, and inserted into the report. In this case, the notebook created an HTML document that RStudio knows how to display, but could also be opened in a standard web browser. These blocks are not standard R syntax, but are instead understood and processed by the knitr R package. As the name suggests, the process of generating a report from an RMarkdown document is called knitting; you would ask RStudio to knit your RMarkdown into a report. RMarkdown documents can be knitted into many different formats, including HTML, PDF, Microsoft Word, and even some slide presentation formats. When exporting to HTML, the report may include interactive elements including plots, collapsible sections, and code blocks, and maps. RMarkdown documents can also be written to accept parameters. This means you can write a single RMarkdown document that can be run on different inputs. Reports for standardized analytical pipelines, as are often implemented by high throughput sequencing cores, can thus be generated trivially for new datasets as they are generated without writing any additional code. RMarkdown - Getting Started RMarkdown package documentation and guide Literate programming 11.2 bookdown Another R package that processed RMarkdown documents is bookdown. As the name suggests, this package is designed to write books using RMarkdown. This book you are reading was written in RMarkdown and generated using bookdown! You can look at all the source code on GitHub. "],["contribution-guide.html", "12 Contribution Guide 12.1 Custom blocks", " 12 Contribution Guide This page contains some conventions and tips for writing material for this book. 12.1 Custom blocks There are a number of custom blocks that can be used for highlighting salient information throughout the text. The code inside the corresponding block below can be used to insert different standout boxes: ::: {.box .tldr} This is a tl;dr ::: ::: {.box .note} This is a note ::: ::: {.box .hint} This is a hint ::: ::: {.box .important} This is an important ::: ::: {.box .warning} This is a warning ::: "],["assignment-format.html", "Assignment Format", " Assignment Format GitHub repo templates here "],["starting-an-assignment.html", "Starting an Assignment Starting your own git repository and cloning it", " Starting an Assignment :^) good luck When starting a project there are a small number of steps to get things set up just right in order to have a smooth and seamless experience. While it is absolutely possible to complete these projects on your own machine and your own installation of R and RStudio, it will be easier and faster to do your work on BUs SCC. The experience will be more straightforward and basically identical to doing it on your own. Starting your own git repository and cloning it In our development of these assignments we used git and github.com to manage our updates and allow for easy sharing. We can now pass these repositories (repos) along to you using GitHub classroom so you can follow the steps within. Contents Each assignment will have these following files within, roughly:  reference_report.html  main.R  README.md  report.Rmd  test_main.R reference_report.html - This is the completed report of the assignment. You will be endeavoring to replicate this while completing the assignment, with some room for creative differences in elements like plotting. main.R - This is the main R script in the assignment, where you will be doing a lot of your programming. This script contains function definitions and descriptions, but with functions that are empty. Complete the functions as described in the script and ensure they pass the tests. README.md - Every repository should have a README. Usually it contains useful information about installation, usage, and licensing. In this case, it will have helpful links back to assignment instructions. report.Rmd - Another empty file, this markdown script will source() your main.R script, loading the functions into its environment. There will be empty code blocks throughout this document, use the functions you developed in main.R to replicate the figures or match the captions seen in finalized_report.pdf. test_main.R - This is a completed testing script produced by us to help you quickly determine how will your code is running. This file sources your main.R file, meaning it runs it to completion and stores any functions and variables in the active environment. It then runs tests to ensure your functions are working in a predictable way. Tests are incredibly valuable tools, and potentially one of the strongest in a programmers toolkit. Most assignments beyond assignment 1 have a complementary guide hosted on the website page. These go into more background about how functions and tests work so use them if youre feeling stuck or would like more context. https://bu-bioinfo.github.io/r_for_biological_sciences/assignment-format.html GitHub tutorial Find your assignment link Clone your repository, you will see a new repo in the form: myusername/assignment-X Inside this repo, click the big g r e e n button and copy the HTTP link to your repository. Login to https://scc-ondemand.bu.edu/. Start an interactive R session (Interactive Apps &gt; RStudio Server), use the default settings but increase the hours to like 12 or something. Or if you like high pressure situations set it to like 20 minutes. Once your session is loaded and you are login in, select the New Project button below the Edit button. With the prompt open, select Version Control &gt; git, and paste your repos url into the first prompt. Feel free to rename the project, but also like dont? Thats confusing. You can change where this folder is setup on SCC, but I find ~/Documents or ~/Documents/BF591 to be a good home for organizing my various projects. You will need to login to complete this process, enter your GitHub username and password when prompted. R will download all of the repository contents, and add an .RProj file to the directory. This RProject file is useful for switching between R assignments in RStudio. This context menu is in the top right of the RStudio interface: Committing and Pushing with R (and without) With the RProject set up, you now have a directory on SCC that has cloned your remote repository in GitHub. At this moment all of the files are the same, since you have made no progress on your assignment (depressing, I know). In a burst of productive energy, you will start to make changes to main.R and write your beautiful, buggy functions. Once youre done for a little bit, what next? You could leave it on SCC and it would probably be pretty safe, but what if you want to share this code easily? Or if a cataclysm strikes Western Massachusetts? Your changes are gone and life will have lost all meaning. To prevent this cataclysm, you must add, commit, and push your code back to GitHub. Using RStudio Save the files you want to commit, and select the Git tab in the top right viewer (by default). Youll notice the files you changed are listed here, along with varying symbols. A question mark ? means the file is untracked, an M means a tracked file has been modified. A D means a tracked file has been deleted. If I select a file I want to add back to GitHub, I can tick the checkbox and it will switch to the left column. This is called staging a file. Once Ive staged all the files I want to add, I can click on the Commit button. This brings up another window, which will again list the files you are staging and actually show you the differences youre making (this is called the diff, based on the GNU utility of the same name). The most important part is the Commit box. A commit is a series of file updates made together, typically to solve a task. It is paramount that you include commit messages, so you can track what it is youre doing with your work! Just a sentence is usually enough to describe what the changes in this commit are doing. Once youve pressed the commit button, all thats left to do is to push your commit. This sends the commit to GitHub where it can safely live forever, duplicated across the world and safe from most cataclysms. In the same vein, the pull button is useful if your GitHub has been updated more recently than your current SCC directory. This can happen if youre editing these files from another computer or on GitHub.com directly. Using the command line That was kind of a lot of clicking and funny little menus, and my doctor says I am at high risk for a repetitive stress injury, so cant I just use my keyboard instead? Yes, of course. Well use the terminal, which is also built-in to RStudio but really just emulates the standard bash terminal that you would use to interact with SCC normally. Click on the terminal tab at the bottom of your layout (default). Marvel at the wonderful monospace font and rest your hands upon the keyboard. To see how the directory is doing, we can type git status to see what files have been changed. Wow! Looks like I have a lot of stuff to deal with. Whats nice is the git command offers a lot of helpful tips for getting me on my way. I want to add these files, and I really dont care how many I had so I am going to use git add *, which basically adds everything that can be found in the current directory. If I only wanted one or two things, I could specify git add file1.txt file2.txt and leave the other files alone. Once I have added (or removed git rm file1.txt) the files I want to commit, I simply use git commit -m \"Added a lot more content to tutorials! to create the commit and add my very important message. Messages are not mandatory but if you slack on being descriptive you will regret it when you come back later to figure out what in Gods name you were doing that day. Finally, with my commit done and files added, I simply say git push to send my wonderful code back to GitHub, and I can sit back and marvel at my handiwork. Put simply, the three commands git add *, git commit -m \"message\", and git push will save the changes to your code and keep your repository up to date. Easier than clicking around RStudio but both ways are super valid! "],["assignment-1.html", "Assignment 1 Problem Statement Learning Objectives and Skill List Instructions Hints", " Assignment 1 Problem Statement This assignment will focus on basic functions of R with an emphasis on tidyverse implementations. tidyverse is a collection of packages, pioneered by Hadley Wickham and RStudio, that looks to standardize procedures, functionality, and syntax in R. To gain familiarity with R, we will be working with a microarray dataset that contains gene probe expression data for various samples collected from cancer patients. In bioinformatics, it is common to have multiple datasets for your different modes of data (i.e. microarray expression data is kept separate from clinical data detailing the samples). You will get an opportunity to work with both of these datasets, and be required to cross reference between the two. Learning Objectives and Skill List Install various packages needed for analysis Load Data Gain familiarity with common tidyverse operations such as groupby(), mutate, and summarize. Create a small plot to display results Utilize R Markdown to create an attractive format for sharing data. Instructions Our main focus for this assignment are installing packages, manipulating data, and summarizing important statistics across both samples and features (genes). An empty project repo can be found here: https://github.com/bu-bioinfo/bf591-assignment-1 The project is laid out as such: main.R report.Rmd reference_report.html Each step of the assignment is explained in the R markdown file, report.Rmd. There you will find a list of tasks to explicity implement functions in your empty main.R script. The main.R script contains skeletons of each function youll need to implement, explaining what each function should do, the parameters it expects to receive, and what type of output is expected to be returned. A reference report, reference_report.html is also provided. Assuming you successfully implement all the functions in main.R, your generated report should look identical to the information displayed in reference_report.html. In this way, you can use reference_report.html as a guide to determine if you are correctly implementing your functions. Here is the suggested workflow for developing and checking your code in this assignment: main.R contains function definitions, including signature descriptions, for a number of functions, but the bodies of those functions are currently blank report.Rmd has code chunks that call functions defined in main.R - you do not need to write anything in the Rmd file (but you may) Your task is to read the function descriptions and the text in the Rmarkdown document and fill in the function bodies to produce the desired behavior in main.R You can test your work by executing individual code chunks in report.Rmd and comparing your output to the example compiled report in the repo In the workflow, you will go back and forth between developing code in main.R and running code chunks in report.Rmd When you have developed function bodies for all the functions and executed all the code chunks in the report successfully, you should be able to knit the entire report Hints When developing the period_to_underscore() function, you might find the stringr::str_replace_all() function helpful. The pattern argument to these functions is interpreted as a regular expression or regex for short. A regular expression is a sequence of characters (i.e. a string) written in a language that describes patterns in text, similar to Find and Replace operations in word processing software, but is more powerful and flexible in the kinds of patterns it can detect. Some characters have special meaning in regular expressions, one of which is the . character. In order to identify the literal period character like we are trying to do, we must instruct the regular expression to do so by either escaping the character with \\\\. or place it in a range with [.]. Either of these two methods will work to replace a literal . with _. See the section on Regular expressions for more information. "],["assignment-2.html", "Assignment 2 Problem Statement Learning Objectives Skill List Instructions Function Details", " Assignment 2 Problem Statement Arranging the structure of our data inputs is vital to using R, as is creating the correct environment for analysis by installing packages. It is also important to share data and results easily using R markdown. Learning Objectives Install various packages needed for analysis Load data, filter that data, and retrieve HGNC ids for the data Create a small plot to display results Utilize R Markdown to create an attractive format for sharing data. Skill List How to utilize R markdown to create a report of data analysis Installing and loading packages in R Utilizing Bioconductor to equate affy ids to HGNC ids (gene names) Instructions Our main focus for this assignment are installing packages, manipulating data, and plotting our manipulated data. We will be borrowing from the same CSV of expression data from BF528s Project 1, available here. An empty project repo can be found here: https://github.com/bu-bioinfo/bf591-assignment-2 The project is laid out as such: main.R test_main.R report.Rmd A skeleton of the functions you need to complete is in main.R. Tests have been pre-written to test your code and help you ensure it is running correctly, these are in test_main.R. Finally, we are also introducing the concept of R Markdown, which for this assignment is report.Rmd. The document itself goes into greater detail, but you will: Complete the functions in main.R and use testthat:test_file('test_main.R') to ensure they work correctly. Read the R Markdown file and complete the section called Assignment. To do this, you can source('main.R') to bring over the functions you wrote in step one. Finally, annotate the functions you wrote and Knit the R Markdown report, complete with your additional comments and code execution. This page will go into detail on how the functions and their associated tests should work. Function Details 1. Bioconductor While many useful R packages can be loaded through CRAN using the install.packages() syntax, a lot of specifically bioinformatics packages are exclusively released on Bioconductor. For this assignment we only need the package called biomaRt. R programmers fancy themselves very clever, so Rs show up a lot. Naturally, we want to load our packages at the beginning of our script so all of the code we write beneath can access it as it runs. However, if a user or ourselves already has this package installed we dont want to waste their time installing it again. The function require() can help us avoid unnecessary installation time and will help us develop faster. The Bioconductor link above has an example of this method. This section is untested. 2. load_expression() Perhaps the most integral part of using the many data wrangling abilities of R is actually entering your data into the R environment. While there are many ways to do this in R, we ultimately want this data to be in a tibble, which means the current form of the CSV will make R very angry if you attempt to load it in. This is because tibbles dont support row names very well, and the first column of our data doesnt have a name. Try to use the load_expression() data to load data from a filename parameter and return a tibble of that information. I called my firs column probeids. Tests The tests for this function are: test_that(&quot;loading csv correctly&quot;, { result_tib &lt;- load_expression(&quot;/project/bf528/project_1/data/example_intensity_data.csv&quot;) expect_equal(dim(result_tib), c(54675, 36)) expect_true(is_tibble(result_tib)) }) This test uses the load_expression() you write to store the returned tibble in result_tib. While this test is using the same file for data input as you are, this may not always be the case. The test then compares the dimensions of that result, and expects 54,675 rows and 36 columns. These are the dimensions of the input CSV. It also checks to confirm it is a tibble object (because tibbles are better than dataframes). 3. filter_15() In order to filter the numerous rows we have for this data, we introduce a function that filters the probe IDs in the tibble our data is stored in. We want to capture probes that have a suitably high level of expression, so we are setting log2(15) as the cutoff for an expression level. We will keep a row if 15% of the values in that row exceed log2(15) (about 3.9). Since we may want to examine the probe IDs we find, the function simple returns the values of the probe IDs (column 1) instead of returning the entire tibble. This function presents an important concept in R: using built-ins to speed up our code. Built-ins are functions and packages that are optimized to process data in a certain way. Since were looking at each row of a table, we could simply use a for loop to iterate one row at a time. This is slow, though, and for this function might take 5-10 seconds to run (a long time for a program like this!). Instead, you could use a function like apply() or lapply() to filter every row at once. This solution takes mere moments. Tests library(tibble) test_that(&quot;the correct rows are filtered out in filter_15()&quot;, { test_tib &lt;- tibble(probeids=c(&#39;1_s_at&#39;, &#39;2_s_at&#39;, &#39;3_s_at&#39;, &#39;4_s_at&#39;), GSM1=c(1.0, 3.95, 4.05, 0.5), GSM2=rep(1.6, 4), GSM3=rep(2.5, 4), GSM4=rep(3.99, 4), GSM5=rep(3.0, 4), GSM6=rep(1.0, 4), GSM7=rep(0.5, 4)) expect_equal(pull(filter_15(test_tib)), c(&quot;2_s_at&quot;, &quot;3_s_at&quot;)) }) In order to test this function, we create a small sample tibble of expression data containing only seven samples and four IDs. Two of the rows do have more than 15% of their values exceeding log2(15), the other two do not. This test ensures that filter_15() selects the correct rows. Creating a small sample table like this can be very useful when testing your own code since you dont need to look at a large amount of data to see if its working correctly or not. 4. affy_to_hgnc() This is an important, but sometimes painful, part of using R. There is a great built-in package for connecting to Ensembl (a database of genomic information for many species) called biomaRt. We will use biomaRt to connect the affymetrix probe IDs to more recognizable HGNC gene IDs. The problem is that biomaRt depends on an external API (application program interface) to retrieve data, and this connection sometimes (oftentimes) doesnt work. While their may be more nuanced approaches to an unstable resource like this like automatically retrying failed connections, the best advice for the time being is to try running this function a few times if it doesnt work at first. The errors are clear when it comes to a failed connection, so know that when you get to this stage it likely isnt your codes fault. To build a biomaRt query, read the documentation in section 3 here. The biomart you should use is ENSEMBL_MART_ENSEMBL, the data set hsapiens_gene_ensembl, and you want to find the attributes c(\"affy_hg_u133_plus_2\", \"hgnc_symbol\"). The data you filter using filter_15() returns a list of affy_hg_u133_plus_2 probe IDs, and the gene names were interested in are stored in hgnc_symbol. This function should return a tibble, but biomaRts getBM() will only accept and return a data.frame. You can use dplyr::pull() to turn a tibble into a simple character vector, and dplyr::as_tibble() to go from a data frame to a tibble. Tests test_that(&quot;affy ids can be converted to HGNC names properly using affy_to_hgnc()&quot;, { # biomaRt super buggy so we can try to account for not connecting well response &lt;- try(affy_to_hgnc(tibble(&#39;1553551_s_at&#39;)), TRUE) if (grepl(&quot;Error&quot;, response[1])) { expect_warning(warning(&quot;Could not connect to ENSEMBL.&quot;)) } else { expect_equal(response$hgnc_symbol, c(&quot;MT-ND1&quot;, &quot;MT-TI&quot;, &quot;MT-TM&quot;, &quot;MT-ND2&quot;)) } }) As fun as it is to try to get biomaRt function to connect correctly, it is even more fun to test them. Since a failure to connect doesnt indicate an actual failure in our code we must use a try() block in order to capture if there is a connection error. Note that the try() function is a part of programming called error-handling which extends to many other languages. We often expect errors when running our programs (such as right now) but dont want to shut down our entire operation if its an error we can expect. Using try, except, and finally (the latter two not appearing here) we can account for *issues outside of our control and adapt our code to change the outcome. Using try except is not a replacement for writing code that doesnt generate errors. If you can avoid an error in the first place, that is far better than using error-handling. In this case we try() to use affy_to_hgnc() to connect to Ensembl and store the resulting error in response. We then check: if there is an Error then we throw a warning() to our testing output. This doesnt stop further testing from happening, but it ensures we know that something isnt quite right. If the response does not contain an error, we simply test that it returned the correct gene symbols for our random affy probe ID of choice. 5. reduce_data() We have one final step in manipulating our data before we plot it. We have our original data, the probe IDs and their associated HGNC symbols, and a list of good gene names and bad gene names. reduce_data takes these four inputs and returns a tibble that has reduced our expression data to only the genes of interest and has a column describing which set of genes it belongs to (good or bad). Changing the shape of the data is incredibly useful for ggplot, the tidyverse package we will use for plotting. While there is flexibility when using ggplot to plot data, having data in long format is typically ideal. Once again, there are multiple ways to reorganize data in this way. We used the base function match() to connect our probe IDs to with our HGNC IDs in name_ids. We then used tibble::add_column() to insert the new data in the correct location. Finally, we created two tibbles of good and bad genes using which() and the %in% modifier. Which evaluates true conditions across a range of data, so we can pass the list of genes we want and select the correct ones. For instance: library(tibble) tib &lt;- tibble(gene = c(&quot;gene1&quot;, &quot;gene2&quot;, &quot;gene3&quot;, &quot;gene4&quot;), affy = c(&quot;a_s_1&quot;, &quot;a_s_2&quot;, &quot;a_s_3&quot;, &quot;a_s_4&quot;)) which(tib$gene %in% c(&quot;gene2&quot;, &quot;gene3&quot;)) # returns the index of the TRUE rows ## [1] 2 3 tib$affy[which(tib$gene %in% c(&quot;gene2&quot;, &quot;gene3&quot;))] # use [] to get data back ## [1] &quot;a_s_2&quot; &quot;a_s_3&quot; Once again, there are many ways to reshape this data (some maybe more elegant than this!) and all we need is the data to be correctly shaped when it is returned. Tests test_that(&quot;reduce_data() is correctly changing the size and shape of the tibble&quot;, { t_tibble &lt;- tibble(probeids = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;, &quot;3_s_at&quot;), GSM1 = c(9.5, 7.6, 5.5), GSM2 = c(9.7, 7.2, 2.9), GSM3 = c(6.9, 4.3, 6.8)) names &lt;- tibble(affy_hg_u133_plus_2 = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;, &quot;3_s_at&quot;), hgnc_symbol = c(&quot;A-REAL-GENE&quot;, &quot;SONIC&quot;, &quot;UTWPU&quot;)) good &lt;- c(&quot;A-REAL-GENE&quot;) bad &lt;- c(&quot;SONIC&quot;) reduce_test &lt;- reduce_data(t_tibble, names, good, bad) result &lt;- tibble(probeids = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;), hgnc_symbol = c(&quot;A-REAL-GENE&quot;, &quot;SONIC&quot;), gene_set = c(&quot;good&quot;, &quot;bad&quot;), GSM1 = c(9.5, 7.6), GSM2 = c(9.7, 7.2), GSM3 = c(6.9, 4.3)) expect_equal(reduce_test, result) }) In order to test a function that changes a tibble, we need to use a tibble. We only create a test table that is three rows by four columns, but that is enough to get the gist of the function. We simply pass the four parameters to reduce_data() and we expect it to create a tibble like result. Ensure your output column names are the same here or your tests my fail. While not crucial to the success of your assignment, maintaining correct column names across multiple data transformations is an important skill. "],["class-outlines.html", "A Class Outlines A.1 Week 1 A.2 Week 2 A.3 Week 3 A.4 Week 4 A.5 Week 5 A.6 Week 6 A.7 Week 7 A.8 Week 8 A.9 Week 9 A.10 Week 10 A.11 Week 11 A.12 Week 12 A.13 Week 13 - no class for project work A.14 Week 14 - no class for project work", " A Class Outlines A.1 Week 1 Course Intro &amp; Details Intro: Data in Biology Prelim: RStudio on SCC Prelim: The R Script Prelim: The Scripting Workflow Comm: RMarkdown &amp; knitr Prelim: Git + github R Prog: R Syntax Basics R Prog: Functions R Prog: Troubleshooting and Debugging Data Wrangle: The Tidyverse Data Wrangle: Tidyverse Basics Data Wrangle: Importing Data Bioinfo: CSV Files Data Wrangle: The tibble Data Wrangle: pipes Data Wrangle: Arranging Data Data Wrangle: Rearranging Data Assignment 1 A.2 Week 2 Assignment 1 Data Wrangle: Regular expressions Bioinfo: R in Biology Bioinfo: Types of Biological Data Bioinfo: Bioconductor Bioinfo: Gene Identifiers Bioinfo: Mapping Between Identifier Systems Bioinfo: Mapping Homologs Data Wrangle: Relational Data Data Viz: Grammar of Graphics Data Viz: Plotting One Dimension Data Viz: Visualizing Distributions R Prog: Unit Testing A.3 Week 3 Assignment 2 Assignment 1 Review Data Sci: Data Modeling Data Sci: A Worked Modeling Example Data Sci: Data Summarization Data Sci: Linear Models Data Sci: Flavors of Linear Models Bioinfo: Gene Expression Bioinfo: Gene Expression Data in Bioconductor Bioinfo: Microarrays Bioinfo: Microarray Data in Bioconductor Bioinfo: [Differential Expression: Microarrays (limma)] Data Sci: [Clustering] Data Viz: [Heatmaps] Data Viz: [Dendrograms] A.4 Week 4 Data Sci: [Statistical Distributions &amp; Tests] Data Sci: Statistical Distributions Data Sci: [Statistical Tests] Data Sci: [p-values] Data Sci: [Multiple Hypothesis Testing] Data Sci: [Statistical power] R Prog: Data structures R Prog: [Factors] R Prog: Iteration Bioinfo: [Genomic Intervals] Bioinfo: Gene Set Enrichment Analysis Bioinfo: Gene Sets Bioinfo: Over-representation Analysis Bioinfo: [Rank-based Analysis] Bioinfo: [fgsea] A.5 Week 5 Bioinfo: [High Throughput Sequencing] Bioinfo: [RNASeq] Bioinfo: [Count Data] Bioinfo: [Differential Expression: RNASeq] Bioinfo: [DESeq2/EdgeR] Bioinfo: [limma/voom] R Prog: Coding Style and Conventions R Prog: [The styler package] A.6 Week 6 Data Viz: [Responsible Plotting] Data Viz: [Grammar of Graphics - Going Deeper] Data Viz: [Plotting Two or More Dimension] Data Viz: [Other Kinds of Plots] Data Viz: [Tips and Tricks] Data Viz: [Multiple Plots] Data Viz: [Facet wrapping] Data Viz: [Publication Ready Plots] A.7 Week 7 Bioinfo: [Single Cell Sequencing] Bioinfo: [Single Cell Clustering Methods] Bioinfo: [tSNE &amp; UMAP] Bioinfo: [Seurat] EngineeRing: [Workflow Managers] A.8 Week 8 Rshiny: Rshiny A.9 Week 9 Bioinfo: [Biological Pathways] Bioinfo: [Gene Regulatory Networks] Bioinfo: [Protein-Protein Networks] Bioinfo: [WGCNA] Data Sci: [Network Analysis] Data Viz: [Network visualization] A.10 Week 10 EngineeRing: [Toolification] EngineeRing: [Parallel Processing] A.11 Week 11 EngineeRing: [Object Orientated Programming in R] EngineeRing: [Building R Packages] A.12 Week 12 Comm: [Sharing and Documenting Project Code] Comm: [Reproducible Analysis in R] Comm: [Data Documentation and Publishing] A.13 Week 13 - no class for project work A.14 Week 14 - no class for project work "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
