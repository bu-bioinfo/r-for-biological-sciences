[["index.html", "BF591 - R for Biological Sciences Syllabus Course Schedule Course Values and Policies", " BF591 - R for Biological Sciences Syllabus Semester: Spring 2022 Location: TBD Time: M/W 8:00AM - 9:45AM Contents: Course Schedule [Instructors] Course Values and Policies This course introduces the R programming language through the lens of practitioners in the biological sciences, particularly biology and bioinformatics. Key concepts and patterns of the language are covered, including: RStudio Data wrangling with tidyverse Data visualization with ggplot Essential biological data shapes and formats Core bioconductor packages Basic data exploration, including elementary statistical modeling and summarization Elementary Data Science concepts Toolifying R scripts Communicating R code and results with RMarkdown Buidling R packages and unit testing strategies Building interactive tools with RShiny About 1/3 of the materials are inspired by the online textbook R for Data Science, while the rest has been developed by practicing bioinformaticians based on their experiences. Weekly programming assignments will help students apply these techniques to realistic problems involving analysis and visualization of biological data. Students will be introduced to a unit testing paradigm that will help them write correct code and deposit all their code into github for evaluation. Students will implement an end-to-end project that begins with one of a set of provided datasets, implements a set of data summarization and exploration operations on it, and allows interaction with an RShiny app. The course materials are aligned with BF528 Applications in Translational Bioinformatics and are intended to be taken in tandem, but the materials also stand alone as an independent class. Course Schedule NB: Subject to change Week Dates Topics Assignment 1 1/24 &amp; 1/26 Preliminaries R Programming: Basics Data Wrangling: Basics Bio: R in Biology [Assignment 1] 2 1/31 &amp; 2/2 Data Wrangling: Arranging Data Viz: Grammar of Graphics Bio: Bioconductor Basics Assignment 2 3 2/7 &amp; 2/9 Data Sci: Data Modeling Bio: Gene Expression Bio: Differential Expression: Microarrays Data Sci: Clustering [Assignment 3] 3 2/14 &amp; 2/16 Data Sci: Distributions &amp; Tests R Programming: Structures &amp; Iteration Data Sci: Summarization &amp; Cleaning Bio: Gene Set Enrichment Analysis [Assignment 4] 5 2/22 &amp; 2/23 Bio: High Throughput Sequencing Bio: Differential Expression: RNASeq Bio: Gene Set Enrichment Analysis R Programming: Style &amp; Conventions [Assignment 5] 6 2/28 &amp; 3/2 Data Viz: Responsible Plotting Assignment 6 7 3/14 &amp; 3/16 Bio: Single Cell Sequencing R Programming: Pipelines &amp; Workflows [Assignment 7] 8 3/21 &amp; 3/23 RShiny [Assignment 8] 9 3/28 &amp; 3/30 Bio: Biological Networks Data Sci: Network Analysis Data Viz: Network Viz [Project] 10 4/4 &amp; 4/6 Toolification 11 4/11 &amp; 4/13 EngineeRing 12 4/20 Communicating with R 13 4/25 &amp; 4/27 No class - Project work 14 5/2 &amp; 5/4 No class - Project work Project due NB: struckout - class not held, reserved for project work Course Values and Policies Everyone is welcome. Every background, race, color, creed, religion, ethnic origin, age, sex, sexual orientation, gender identity, nationality is welcome and celebrated in this course. Everyone deserves respect, patience, and kindness. Disrespectful language, discrimination, or harassment of any kind are not tolerated, and may result in removal from class or the University. This is not merely BU policy. The instructors deem these principles to be inviolable human rights. Students should feel safe reporting any and all instances of discrimination or harassment to the instructor, to any of the Bioinformatics Program leadership, or the BU Equal Opportunity Office. Everyone brings value. Each of us brings unique experiences, skills, and creativity to this course. Our diversity is our greatest asset. Collaboration is highly encouraged. All students are encouraged to work together and seek out any and all available resources when completing projects in all aspects of the course, including sharing coding ideas and strategies with each other as well as those found on the internet. Any and all available resources may be brought to bear. However, consistent with BU policy, the bulk of your code and your final reports should be written in your own words and represent your own work and understanding of the material. Copying/pasting large sections of code is not acceptable and will be investigated as cheating (we check). A safe space for dissent. For complex topics such as those covered in this class, there is seldom one correct answer, approach, or solution. Disagreement fosters innovation. All in the course, including students and TAs, are encouraged to express constructive criticism and alternative ideas on any aspect of the content. We are always learning. Our knowledge and understanding is always incomplete. Even experts are fallible. The bioinformatics field evolves rapidly, and Rome was not built in a day. Be kind to yourself and to others. You are always smarter and more knowledgable today than you were yesterday. "],["introduction-overview.html", "1 Introduction &amp; Overview 1.1 Who This Book Is For 1.2 A Note About Reinventing the Wheel 1.3 Sources and References", " 1 Introduction &amp; Overview Since the publication of the first draft human genome in 2001, data has driven biological discovery at an exponential rate. Rapid technological innovations in data-generating biochemical instruments, computational resource availability, data storage, and analytical approaches including artificial intelligence, machine learning, and data science more generally have combined and synergized to enable advances in our understanding of biological systems by orders of magnitude. As the rate of development of these technologies has increased, so are practitioners of biological inquiry expected to keep up with the rapidly expanding set of knowledge, skills, and tools required to use them. Modern biological data analysis entails combining knowledge and skills from many domains, including not only biological concepts like molecular biology, genetics, genomics, and biochemistry, but also in computational and quantitative skills including statistics, mathematics, programming and software engineering, high performance and cloud computing, data visualization, and computer science. No one person can be expert in all of these areas, but modern software tools and packages made available by subject matter experts enable us to perform cutting edge analysis with a conceptual understanding of the topics. One such tool is the R programming language, a statistical programming language and environment specifically designed to run statistical analyses and visualize data. R became popular in biological data analysis in the early to mid 2000s, when microarray technology came into widespread use enabling researchers to look for statistical differences in gene expression for thousands of genes across large numbers of samples. As a result of this popularity, a community of biological researchers and data analysts created a collection of software packages called Bioconductor, which made a vast array of cutting edge statistical and bioinformatic methodologies widely available. Today R is one of the two most popular programming languages in biological data analysis and bioinformatics (the other being python). A major innovation in the R language came with the introduction of the tidyverse, as set of open-source data manipulation and visualization packages, first developed by Hadley Wickham and now improved, supported, and maintained by his team of data scientists and software engineers and other individuals. The tidyverse is a collection of packages that specialize in different aspects of data manipulation with the goal of enabling powerful, consistent, and accurate data operations in the broad field of data science. While not changing the structure of the language per se, the tidyverse packages define a set of consistent programming conventions and patterns that are tailored to the types of manipulations required to make data tidy and, therefore, easier and more consistent to work with. The tidyverse therefore is something of its own language that is compatible with but distinct in convention from the base R language. This book and accompanying course focus on how to use R and its related package ecosystems to analyze, visualize, and communicate biological data analyses. As noted above, effective biological data analysis employs skills that span several knowledge domains. This book covers many of these topics in relatively shallow depth, but with the intent of presenting just enough in each to enable the learner to become proficient in most day-to-day biological analysis tasks. 1.1 Who This Book Is For This book was written for the practicing biologist wishing to learn how to use R to analyze biological data. A basic working knowledge of basic genetics, genomics, molecular biology, and biochemistry is assumed, but we endeavored to include enough pertinent background to understand the analysis concepts presented in the text. Basic knowledge of statistics is assumed, but again some background is provided as necessary to understand the analyses and concepts in the text. No further knowledge is assumed. 1.2 A Note About Reinventing the Wheel Many topics in this book are covered elsewhere in greater detail and depth. The content in each section is intended to stand alone, but may not provide a high level of detail that has been done better by others in online materials. These sections provide links to these other resources that provide more information, in case the instructions in this book are too terse or unclear. Wikipedia - Reinventing the Wheel 1.3 Sources and References The materials of this book were inspired and informed by a large number of sources, including books and freely available online materials. The authors would like to thank the generosity of the creators and maintainers of these resources for making their valuable contributions: Hands-On Programming with R, by Garrett Grolemund R for Data Science, by Hadley Wickam, Garrett Grolemund, et al STAT 545 - Data wrangling, exploration, and analysis with R What They Forgot to Teach You About R Reproducible Analysis with R, by State of Alaskas Salmon and People Project, NCEAS How Charts Lie: Getting Sparter about Visual Information, by Alberto Cairo "],["preliminaries.html", "2 Preliminaries 2.1 The R Language 2.2 RStudio 2.3 Before you begin 2.4 The R Script 2.5 The Scripting Workflow 2.6 git + github", " 2 Preliminaries 2.1 The R Language R is a free programming language and environment where that language can be used. More specifically, R is a statistical programming language, designed for the express and exclusive purpose of conducting statistical analyses and visualizing data. Said differently, R is not a general purpose programming language unlike other languages such as python, Java, C, etc. As such, the languages real strengths are in the manipulation, analysis, and visualization of data and statistical procedures, though it is often used for other purposes (for example, web applications with RShiny, or writing books like this one with bookdown). You may download R for free from the Comprehensive R Archive Network. The effective biological analysis practitioner knows how to use multiple tools for their appropriate purposes. The most common programming languages in this field are python, R, and scripting languages like The Bourne Again Shell - bash. While it is beyond the scope of this book to cover which tools are best used where, R is appropriate wherever data analysis and visualization are needed. Any operations that do not involve these aspects (e.g. manipulating text files, programming web servers, etc) are likely more suitable for other languages and software. R Project Home Page Hands-On Programming with R - Installing R and RStudio Section on R from R for Data Science 2.2 RStudio This course assumes the learner is using the RStudio software platform for all analyses, unless otherwise noted. RStudio is a freely available and fully featured integrated development environment (IDE) for R, and has many convenient capabilities when learning R. RStudio may be downloaded and installed for free from the site above. By default, RStudio preserves your R environment when you shut it down and restores it when you start it again. This is very bad practice! The state of your R environment, which includes the values stored in variables, the R packages loaded, etc. from previously executed code is transient and may not reflect the results your code produces when run alone. Open the Tools &gt; Global Options menu and: 1. Uncheck Restore .RData into workspace at startup 2. Set Save workspace to .RData on exit: to Never Never save workspace The book R for Data Science has an excellent chapter on why this is a problem and how to change the RStudio setting to avoid it. Hands-On Programming with R - Installing R and RStudio RStudio Education - Beginners Guide ModernDrive R Series - Getting Started R for Data Science - What is real, and where does your analysis live? What They Forgot To Teach You About R - Always start R with a blank slate R for Data Science - RStudio 2.3 Before you begin All the examples and instructions in this chapter assume you have installed R are using RStudio. Be sure to turn off automatic environment saving in RStudio! Because this is so important, here it is again: By default, RStudio preserves your R environment when you shut it down and restores it when you start it again. This is very bad practice! The state of your R environment, which includes the values stored in variables, the R packages loaded, etc. from previously executed code is transient and may not reflect the results your code produces when run alone. Open the Tools &gt; Global Options menu and: Uncheck Restore .RData into workspace at startup Set Save workspace to .RData on exit: to Never Never save workspace The book R for Data Science has an excellent chapter on why this is a problem and how to change the RStudio setting to avoid it. What They Forgot To Teach You About R - Always start R with a blank slate 2.4 The R Script Before we cover the R language itself, we should talk about how you should run your code and where it should live. As mentioned, R is both a programming language and an environment where you can run code written in that language. The environment is a program (confusingly also called R) that allows you to interact with it and run simple lines of code one at a time. This environment is very useful for learning how the language works and troubleshooting, but it is not suitable for recording and running large, complex analyses that require many lines of code. Therefore, all important R code should be written and saved in a file before you run it! The code may not be correct, and the interactive R environment is helpful for debugging and troubleshooting, but as soon as the code works it should be saved to the file and rerun from there. With this in mind, the basic unit of an R analysis is the R script. An R script is a file that contains lines of R code that run sequentially as a unit to complete one or more tasks. Every R script file has a name, which you choose and should be descriptive but concise about what the script does; script.R, do_it.R, and a_script_that_implements_my_very_cool_but_complicated_analysis_and_plots.R are generally poor names for scripts, whereas analyze_gene_expression.R might be more suitable. In RStudio, you can create a new script file in the current directory using the File -&gt; New File -&gt; R Script menu item or the new R Script button at the top of the screen: New R Script Your RStudio configuration should now enable you to write R code into the (currently unsaved) file in the top left portion of the screen (labeled in the figure as File Editor). Basic RStudio Interface You are now nearly ready to start coding in R! How to name files Some useful and advanced tips on how to name files 2.5 The Scripting Workflow But hold on, were still not quite ready to start coding. As mentioned above, all important R code should be written and saved in a file before you run it! Your scripts will very quickly contain many lines of code that are meant to be run in sequential order. While developing your code it is very helpful to run each individual line separately, building up your script incrementally over time. To illustrate how to do this, we will begin with a simple R code that stores the result of an arithmetic expression to a new variable: # stores the result of 1+1 into a variable named &#39;a&#39; a &lt;- 1+1 The concepts in this line of code will be covered in greater depth later, but for now an intuitive understanding will suffice to explain the development workflow in RStudio. When developing, this is the suggested sequence of operations: Save your file (naming if necessary on the first save) with Ctrl-s on Windows or Cmd-s on Mac Execute the line or lines of code in your script you wish to evaluate using Ctrl-Enter on Windows or Cmd-Enter on Mac. By default only the line with the cursor is executed; you may click and drag with the mouse to select multiple lines to execute if needed. NB: you can press the up arrow key to recall previously run commands on the console. The executed code will be evaluated in the Console window, where you may inspect the result and modify the code if necessary. You may inspect the definitions of any variables you have declared in the Environment tab at the upper right. When you have verified that the code you executed does what you intend, ensure the code in the file you started from is updated appropriately. Go to step 1 The above steps are depicted in the following figure: RStudio workflow Over time, you will gain comfort with this workflow and become more flexible with how you use RStudio. If you followed the instructions above and prevented RStudio from saving your environment when you exit the program (which you should! Did I mention you should?!), none of the results of code you previously ran will be available upon starting a new RStudio session. Although this may seem inconvenient, this is an excellent opportunity to verify that your script in its current state does what you intend for it to do. It is extremely easy to ask R to do things you dont mean for it to do! Rerunning your scripts from the beginning in a new RStudio session is an excellent way to guard against this kind of error. This short page summarizes this very well, you should read it: What They Forgot To Teach You About R - Always start R with a blank slate R for Data Science - Workflow: scripts RStudio IDE cheatsheet (scroll down the page to find the cheatsheet entitled RStudio IDE cheatsheet) 2.6 git + github 2.6.1 Motivation Biological analysis entails writing code, which changes over time as you develop it, gain insight from your data, and identify new questions and hypotheses. A common pattern when developing scripts is to make copies of older code files to preserve them before making new changes to them. While it is a good idea to maintain a record of all the code you previously ran, over time this practice often leads to disorganized, cluttered, untidy analysis directories. For example, say you are working on a script named my_R_script.R and decide you want to add a new analysis that substantially changes the code. You might be tempted to make a copy of the current version of the code into a new file named my_R_script_v2.R that you then make changes to, leaving your original script intact and untouched going forward. You make your changes to your new script, produce some stunning and fascinating plots, present the analysis at a group meeting, only to discover later there was a critical bug in your code that made the plots misleading and requires substantial redevelopment. Bugs happen. There are two types of bugs: Syntax bugs: bugs due to incorrect language usage, which R will tell you about and can (usually) be easily identified and fixed Logic bugs: the code you write is syntactically correct, but does something other than what you intend Bugs are normal. The scenario described above, where you present results only to discover your code wasnt doing what you thought it was doing, is extremely common and it will happen to you. This is normal, and finding a bug in your code does not mean you are a bad programmer. Rather than edit your version 2 of your script directly, you decide it is sensible to copy the file to my_R_script_v2_BAD.R and edit the version 2 script to fix the bug. You are satisfied with your new version 2 script, and so make a new copy my_R_script_v2_final.R. Upon review of your analysis, you are asked to implement new changes to the script based on reviewer feedback. You make a new copy of your script to my_R_script_v2_final_revision.R and make the requested changes. Perhaps now your script is final, but in your directory you now have five different versions of your analysis: my_R_script.R my_R_script_v2.R my_R_script_v2_BAD.R my_R_script_v2_final.R my_R_script_v2_final_revision.R When you write your code, you may know which scripts are which, and if you follow good programming practice and carefully commented your code you or your successors may be able to sleuth what was done. However, as time passes, the intimate knowledge you thought you had about your code will be replaced by other more immediately important things; eventually you may not even understand or even recognize your own code, let alone someone else trying to understand it. Not an ideal situation in any case. A better solution involves recording changes to code over time in such a way that you can recover old code if needed, but dont clutter your analytical workspace with unneeded files. git provides an efficient solution to this problem. 2.6.2 git git is a free, open source version control software program. Version control software is used to track and record changes to code over time, potentially by many developers working on the same software project concurrently from different parts of the world. The base git software can be used on the command line, or with graphical user interface applications for popular operating systems. There are many excellent tutorials online (some linked below) that teach how to use git but the basic concepts are described below. The command line commands are listed, but the same operations apply in the graphical clients. A repository (or repo) is a collection of files in a directory that you have asked git to track (run git init in a new directory) Each file you wish to track must be explicitly added to the repo (run git add &lt;filename&gt; from within the git repo directory) When you modify a tracked file, git will notice those differences and show them to you with git status You may tell git to track the changes to the explicit files that changed (also run git add &lt;filename&gt; to record changes) A set of tracked changes is stored in the repo by making a commit. A commit takes a snapshot of all the tracked files in the repo at the time the commit is made (run git commit -c &lt;commit message&gt; with a concise commit message that describes what was done) Each commit has a date and time associated with it. The files in the repo can be reset to exactly the state they were in at any commit, thus preserving all previous versions of code. For the vast majority of use cases, the git init, git status, git add, and git commit operations are all you will need to use git effectively. Two more commands, git push and git pull are needed when sharing your code with others as described in the next section. Official git tutorial videos Official git book Git Immersion - a guided tour through git commands DataCamp - Git for data scientists 2.6.3 Git hosting platforms (GitHub) The basic git software only works on your local computer with local repositories. To share this code with others, and receive others contributions, a copy of the repo must be made available in a centralized location that everyone can access. One such place is github.com, which is a free web application that hosts git repos. bitbucket.org is another popular free git repo hosting service. These two services are practically the same, so we will focus on GitHub. There is no formal relationship between git and GitHub. git is an open source software project maintained by hundreds of developers around the world (and is hosted on GitHub). GitHub is an independently provided web service and application. The only connection between GitHub and git is that GitHub hosts git repos. As with git, there are many excellent tutorials on how to use GitHub, but the basic concepts are described below. First you must create an account on GitHub if you dont have one already Then, create a new repo on GitHub that you wish to contain your code The next step depends on whether you have an existing local repo or not: If you do not already have a local git repo: Follow the instructions on GitHub to clone your GitHub repo and create a local copy that is connected to the one on GitHub If you already have a local git repo: Follow the instructions on the GitHub to connect your local repo to the GitHub one (this is called adding a remote) Now, your local repo is connected to the same repo on GitHub, and the changes you make to your local files can be sent, or pushed to the repo on GitHub: Make changes to your local files, and git add and git commit them as above Update the remote repo on GitHub by pushing your local commits with git push Running git status will indicate whether your local repo is up to date with your remote GitHub repo When you are working on a team of contributors to a GitHub repo, your local files will become out of date as others push their changes. To ensure your local repo is up to date with the GitHub repo, you must pull your changes from GitHub with git pull. git was designed to automatically combine changes made to a code base by different developers whenever possible. However, if two people make changes to the same parts of the same file, git may not be able to resolve those changes on its own and the developers must communicate and decide what the code should be. These instances are called merge conflicts and can be challenging to resolve. Dealing with merge conflicts is beyond the scope of this book, but some resources are linked below for further reading. All the content and code for this book are stored and available on GitHub, as are the assignment code templates. Official GitHub Tutorial FreeCodeCamp - Git and GitHub For Beginners Official GitHub Tutorial on Merge Conflicts "],["prog-basics.html", "3 R Programming 3.1 R Syntax Basics 3.2 Basic Types of Values 3.3 Data Structures 3.4 Logical Tests and Comparators 3.5 Functions 3.6 Iteration 3.7 Troubleshooting/Debugging Strategies 3.8 Coding Style and Conventions", " 3 R Programming As with other subjects covered so far, the basic syntax of R is covered very well in other free online materials. Some of those excellent resources are linked at the end of this section, but a brief overview of the syntax is covered here. The code examples below can be written into a script and evaluated as described above or entered on the R Console directly and run by pressing Enter. 3.1 R Syntax Basics At its core, R (like all programming languages) is basically a fancy calculator. The syntax of most basic arithmetic operations in R should be familiar to you: 1 + 2 # addition [1] 3 3 - 2 # subtraction [1] 1 4 * 2 # multiplication [1] 8 4 / 2 # division [1] 2 1.234 + 2.345 - 3.5*4.9 # numbers can have decimals [1] -13.571 1.234 + (2.345 - 3.5)*4.9 # expressions can contain parentheses [1] -4.4255 2**2 # exponentiation [1] 4 4**(1/2) # square root [1] 2 9**(1/3) # cube root [1] 3 The [1] lines above are the output given by R when the preceding expression is executed. Any portion of a line starting with a # is a comment and ignored by R. R also supports storing values into symbolic placeholders called variables, or objects. An expression like those above can be assigned into a variable with a name using the &lt;- operator: new_var &lt;- 1 + 2 Variables that have been assigned a value can be placed in subsequent expressions anywhere where their value is evaluated: new_var - 2 [1] 1 another_var &lt;- new_var * 4 The correct way to assign a value to a variable in R is with the &lt;- syntax, unlike many other programming languages which use =. However, although the = assignment syntax does work in R: new_var = 2 # works, but is not common convention! this is considered bad practice and may cause confusion later. You should always use the &lt;- syntax when assigning values to variables! In R, the period . does not have a special meaning like it does in many other languages like python, C, javascript, etc. Therefore, new.var is a valid variable name just like new_var, even though it may look strange to those familiar with these other languages. While including . in your R variable names is valid, the results that you will use in programs written in other languages that do have a meaning for this character. Therefore, it is good practice to avoid using . characters in your variable names to reduce the chances of conflicts later. Hands-On Programming with R R for Data Science - Workflow basics 3.2 Basic Types of Values The most common type of value in R is the number, e.g. 1.0 or 1e-5 for \\(10^{-5}\\). For most practical purposes, R does not distinguish between numbers with fractional parts (e.g. 1.123) and integers (e.g. 1); a number is a number. In addition to numbers, there are some other types of values that are special in R: logical or boolean values - TRUE or FALSE. Internally, R stores TRUE as the number 1 and FALSE as the number 0. Generally, R interprets non-zero numbers as TRUE and 0 as FALSE, but it is good practice to supply the tokens TRUE or FALSE when an argument expects a logical value. missing values - NA. NA is a special value that indicates a value is missing. missing vectors - NULL. Similar to NA, NULL indicates that a vector, rather than a value, is missing. Vectors will be described in the next section on data strutures. factors - Factors are a complex type used in statistical models and are covered in greater detail later infinity - Inf and -Inf. These values encode what R understands to be positive or negative infinity, or any number divided by 0. impossible values - NaN. This value corresponds to the mathematically impossible or undefined value of 0/0. character data - \"value\". R can store character data in the form of strings. Note R does not interpret string values by default, so \"1\" and 1 are distinct. dates and times - R has a basic type to store dates and times (together termed a datetime, which includes both components). Internally, R stores datetimes as the fractional number of days since January 1, 1970, using negative numbers for earlier dates. complex numbers - R can store complex numbers using the complex function. Unsurprisingly, R cannot perform computations on NA, NaN, or Inf values. Each of these values have an infectious quality to them, where if they are mixed in with other values, the result of the computation reverts to the first of these values encountered: # this how to create a vector of 4 values in R x &lt;- c(1,2,3,NA) mean(x) # compute the mean of values that includes NA [1] NA mean(x,na.rm=TRUE) # remove NA values prior to computing mean [1] 2 mean(c(1,2,3,NaN)) [1] NaN mean(c(NA,NaN,1)) [1] NA If your code produces values that are not numbers as you expect, this suggests there are one of these values in your input, and need to be handled explicitly. The difference between NA and NaN in R R for Data Science - Dates and date-times Dates and times in R Complex numbers in R 3.3 Data Structures 3.3.1 Vectors Data structures in R (and other languages) are ways of storing and organizing more than one value together. The most basic data structure in R is a one dimensional sequence of values called a vector: # the c() function creates a vector x &lt;- c(1,2,3) [1] 1 2 3 The vector in R has a special property that all values contained in the vector must have the same type, from the list described above. When constructing a vector, R will coerce values to the most general type if it encounters values of different types: c(1,2,&quot;3&quot;) [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; c(1,2,TRUE,FALSE) [1] 1 2 1 0 c(1,2,NA) # note missing values stay missing [1] 1 2 NA c(&quot;1&quot;,2,NA,NaN) # NA stays, NaN is cast to a character type [1] &quot;1&quot; &quot;2&quot; NA &quot;NaN&quot; In addition to having a single type, vectors also have a length, which is defined as the number of elements in the vector: x &lt;- c(1,2,3) length(x) [1] 3 Internally, R is much more efficient at operating on vectors than individual elements separately. With numeric vectors, you can perform arithmetic operations on vectors of compatible size just as easily as individual values: c(1,2) + c(3,4) [1] 4 6 c(1,2) * c(3,4) [1] 3 8 c(1,2) * c(3,4,5) # operating on vectors of different lengths raises warning, but still works [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length In the example above, we multiplied a vector of length 2 with a vector of length 3: c(1,2) * c(3,4,5) # operating on vectors of different lengths raises warning, but still works [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length Rather than raise an error and aborting, R merely emits a warning message about the vectors not having divisible lengths. So how did R decide the third value should be 5? Because R cycles through each vector and multiplies the values element-wise until the longest vector has had an operation performed on all its values: c(1,2) * c(3,4,5) # yields: 1*3 2*4 1*5 [1] 3 8 5 Warning message: In c(1, 2) * c(3, 4, 5) : longer object length is not a multiple of shorter object length c(1,2) * c(3,4,5,6) # yields: 1*3 2*4 1*5 2*6 [1] 3 8 5 12 R will sometimes work in ways you dont expect. Be careful to read warnings and check that your code does what you expect! 3.3.2 Matrices A matrix in R is simply the 2 dimensional version of a vector. That is, it is a rectangle of values that all have the same type, e.g. number, character, logical, etc. A matrix may be constructed using the vector notation described above and specifying the number of rows and columns the matrix should have, and Instead of having a length like a vector, it has \\(m \\times n\\) dimensions: # create a matrix with two rows and three columns containing integers A &lt;- matrix(c(1,2,3,4,5,6) nrow = 2, ncol = 3, byrow=1 ) A [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 dim(A) # the dim function prints out the dimensions of the matrix, rows first [1] 2 3 Because a matrix is 2 dimensional, it can be transposed from \\(m \\times n\\) to be \\(n \\times m\\) using the t() function: # A defined above as a 2 x 3 matrix t(A) [,1] [,2] [1,] 1 4 [2,] 2 5 [3,] 3 6 dim(t(A)) [1] 3 2 Hands-On Programming with R - Atomic Vectors R for Data Science - Vectors Advanced R - Vectors 3.3.3 Lists and data frames Vectors and matrices have the special property that all items must be of the same type, e.g. numbers. Lists and data frames are data structures that do not have this requirement. Similar to vectors, lists and data frames are both one dimensional sequences of values, but the values can be of mixed types. For instance, the first item of a list may be a vector of numbers, while the second is a vector of character strings. These are the most flexible data structures in R, and are among the most commonly used. Lists can be created using the list() function: my_list &lt;- list( c(1,2,3), c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) my_list [[1]] [1] 1 2 3 [[2]] [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; my_list[[1]] # access the first item of the list [1] 1 2 3 my_list[[2]] # access the second item of the list [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; The arguments passed to list() define the values and their order of the list. In the above example, the list has two elements: one vector of 3 numbers and one vector of 4 character strings. Note you can access individual items of the list using the [[N]] syntax, where N is the 1-based index of the element. Lists can also be defined and indexed by name: my_list &lt;- list( numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) my_list $numbers [1] 1 2 3 $categories [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; my_list$numbers # access the first item of the list [1] 1 2 3 my_list$categories # access the second item of the list [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; The elements of the list have been assigned the names numbers and categories when creating the list, though any valid R identifier names can be used. When elements are associated with names they can be accessed using the list$name syntax. Lists and data frames are the same underlying data structure, however differ in one important respect: the elements of a data frame must all have the same length, while the elements of a list do not. You may create a data frame with the data.frame() function: my_df &lt;- data.frame( # recall &#39;.&#39; has no special meaning in R numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) ) Error in data.frame(c(1, 2, 3), c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) : arguments imply differing number of rows: 3, 4 my_df &lt;- data.frame( numbers=c(1,2,3), categories=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) ) my_df numbers categories 1 1 A 2 2 B 3 3 C my_df$numbers [1] 1 2 3 my_df[1] # numeric indexing also works, and returns a subset data frame numbers 1 1 2 2 3 3 my_df[1]$numbers [1] 1 2 3 # this syntax is [&lt;row&gt;,&lt;column&gt;], and if either is omitted return all my_df[,1] # return all rows of the first column as a vector [1] 1 2 3 my_df$categories [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; Note the data frame is printed as a matrix with element names as columns and automatically numbered rows. You may access specific elements of a data frame in a number of ways: my_df$numbers[1] # extract the first value of the numbers column [1] 1 my_df[1,1] # same as above, recall the [&lt;row&gt;,&lt;column&gt;] syntax [1] 1 my_df$categories[3] # extract the third value of the categories column [1] &quot;C&quot; In the examples above, the operation of extracting out different parts of a vector, matrix, list, or data frame is called subsetting. R provides many different ways to subset a data structure and discussing all of them is beyond the scope of this book. However, mastering subsetting will help your code be more concise and correct. See the Read More link on Subsetting below: Advanced R - Subsetting Advanced R - Data Structures Advanced R - Subsetting 3.4 Logical Tests and Comparators As mentioned above, R recognizes logical values as a distinct type. R provides all the conventional infix logical operators: 1 == 1 # equality [1] TRUE 1 != 1 # inequality [1] FALSE 1 &lt; 2 # less than [1] TRUE 1 &gt; 2 # greater than [1] FALSE 1 &lt;= 2 # less than or equal to [1] TRUE 1 &gt;= 2 # greater than or equal to These operators also work on vectors, albeit with the same caveats about vector length as noted earlier: x &lt;- c(1,2,3) x == 2 [1] FALSE TRUE FALSE x &lt; 1 [1] FALSE FALSE FALSE x &lt; 3 [1] TRUE TRUE FALSE c(1,2) == c(1,3) [1] TRUE FALSE c(1,2) != c(1,3) [1] FALSE TRUE c(1,2) == c(1,2,3) [1] TRUE TRUE FALSE Warning message: In c(1, 2, 3) == c(1, 2) : longer object length is not a multiple of shorter object length R also provides many functions of the form is.X where X is some type or condition (recall that . is not a special character in R): is.numeric(1) # is the argument numeric? [1] TRUE is.character(1) # is the argument a string? [1] FALSE is.character(&quot;ABC&quot;) [1] TRUE is.numeric(c(1,2,3)) # recall a vector has exactly one type [1] TRUE is.numeric(c(1,2,&quot;3&quot;)) [1] FALSE is.na(c(1,2,NA)) [1] FALSE FALSE TRUE Quick-R - Operators 3.5 Functions Just as a variable is a symbolic representation of a value, a function is a symbolic representation of code. In other words, a function allows you to substitute a short name, e.g. mean, for a set of operations on a given input, e.g. the sum of a set of numbers divided by the number of numbers. R provides a very large number of functions for common operations in its default environment, and more functions are provided by packages you can install separately. Encapsulating many lines of code into a function is useful for (at least) five distinct reasons: Make your code more concise and readable Allow you to avoid writing the same code over and over (i.e. reuse it) Allow you to systematically test pieces of your code to make sure they do what you intend Allow you to share your code easily with others Program using a functional programming style (see note box below) At its core, R is a functional programming language. The details of what this means are outside the scope of this book, but as the name implies this refers to the language being structured around the use of functions. While this property has technical implications on the structure of the language, a more important consequence is the style of programming it entails. The functional programming style (or paradigm) has many advantages, including generally producing programs that are more concise, predictable, provably correct, and performant. provides a good starting point for learning about functional programming. In order to do anything useful, a function must generally be able to accept and execute on different inputs; e.g. the mean function wouldnt be very useful if it didnt accept a value! The terminology used in R and many other programming languages for this is the function must accept or allow you to pass it arguments. In R, functions accept arguments using the following *pattern: # a basic function signature function_name(arg1, arg2) # function accepts exactly 2 arguments Here, arg1 and arg2 are formal arguments or named arguments, indicating this function accepts two arguments. The name of the function (i.e. function_name) and the pattern of arguments it accepts is called the functions signature. Every function has at least one signature, and it is critical to understand it in order to use the function properly. In the above example, arg1 and arg2 are required arguments. This means that the function will not execute without exactly two arguments provided and will raise an error if you try otherwise: mean() # compute the arithmetic mean, but of what? Error in mean.default() : argument &quot;x&quot; is missing, with no default How do you know what arguments a function requires? All functions provided by base R and many other packages include detailed documentation that can be accessed directly through RStudio using either the ? or help(): RStudio - help and function signatures The second signature of the mean function introduces two new types of syntax: Default argument values - e.g. trim = 0. These are formal arguments that have a default value if not provided explicitly. Variable arguments - .... This means the mean function can accept arguments that are not explicitly listed in the signature. With these definitions, we can now understand the Arguments section of the help documentation: Arguments to the mean function In other words: x is the vector of values (more on this in the next section on data structures) we wish to compute the arithmetic mean for trim is a fraction (i.e. a number between 0 and 0.5) that instructs R to remove a portion of the largest and smallest values from x prior to computing the mean. na.rm is a logical value (i.e. either TRUE or FALSE) that instructs R to remove NA values from x before computing the mean. All function arguments can be specified by name, regardless of whether there is a default value or not. For instance, the following two mean calls are equivalent: # this generates 100 normally distributed samples with mean 0 and standard deviation 1 my_vals &lt;- rnorm(100,mean=0,sd=1) mean(my_vals) [1] -0.05826857 mean(x=my_vals) [1] -0.05826857 To borrow from the Zen of Python, Explicit is better than implicit. Being explicit about which variables are being passed as which arguments will almost always make your code easier to read and more likely to do what you intend. The ... argument catchall can be very dangerous. It allows you to provide arguments to a function that have no meaning, and R will not raise an error. Consider the following call to mean: # this generates 100 normally distributed samples with mean 0 and standard deviation 1 my_vals &lt;- rnorm(100,mean=0,sd=1) mean(x=my_vals,tirm=0.1) [1] -0.05826857 Did you spot the mistake? The trim argument name has been misspelled as tirm, but R did not report an error. Compare the value of mean without the typo: mean(x=my_vals,trim=0.1) [1]-0.02139839 The value we get is different, because R recognizes trim but not tirm and changes its behavior accordingly. Not all functions have the ... catchall in their signatures, but many do and so you must be diligent when supplying arguments to function calls! R for Data Science - Functions rdrr.io - Base R Function Reference This tutorial Advanced R - Functional Programming 3.5.1 DRY: Dont Repeat Yourself Sometimes you will find yourself writing the same code more than once to perform the same operation on different data. For example, one common data transformation is standardization or normalization which entails taking a series of numbers, subtracting the mean of all the numbers from them, and dividing each by the standard deviation of the numbers: # 100 normally distributed samples with mean 20 and standard deviation 10 my_vals &lt;- rnorm(100,mean=20,sd=10) my_vals_norm &lt;- (my_vals - mean(my_vals))/sd(my_vals) mean(my_vals_norm) [1] 0 sd(my_vals_norm) [1] 1 Later in your code, you may need to standardize a different set of values, so you decide to copy and paste your code from above and replace the variable name to reflect the new data: # new samples with mean 40 and standard deviation 5 my_other_vals &lt;- rnorm(100,mean=40,sd=5) my_other_vals_norm &lt;- (my_other_vals - mean(my_other_vals))/sd(my_vals) mean(my_other_vals_norm) [1] 0 sd(my_other_vals_norm) # this should be 1! [1] 0.52351 Notice the mistake? We forgot to change the variable name my_vals to my_other_vals in our pasted code, which produced an incorrect result. Good thing we checked! In general, if you are copying and pasting code from one part of your script to another, you are repeating yourself and have to do a lot of work to be sure you have modified your copy correctly. Copying and pasting code is tempting from an efficiency standpoint, but introduces may opportunities for (often undetected!) errors. Dont Repeat Yourself (DRY) is a principle of software development that emphasizes recognizing and avoiding writing the same code over and over by encapsulating code. In R, this is most easily done with functions. If you notice yourself copying and pasting code, or writing the same pattern of code more than once, this is an excellent opportunity to write your own function and avoid repeating yourself! 3.5.2 Writing your own functions R allows you to define your own function using the following syntax: function_name &lt;- function(arg1, arg2, ...) { # code that does something with arg1, arg2, etc return(some_result) } You define the name of your function, the number of arguments it accepts and their names, and the code within the function, which is also called the function body. Taking the example above, I would define a function named standardize that accepts a vector of numbers, subtracts the mean from all the values, and divides them by the standard deviation: standardize &lt;- function(x) { res &lt;- (x - mean(x))/sd(x) return(res) } my_vals &lt;- rnorm(100,mean=20,sd=10) my_vals_std &lt;- standardize(my_vals) mean(my_vals_std) [1] 0 sd(my_vals_std) [1] 1 my_other_vals &lt;- rnorm(100,mean=40,sd=5) my_other_vals_std &lt;- standardize(my_other_vals) mean(my_other_vals_std) [1] 0 sd(my_other_vals_std) [1] 1 Notice above we are assigning the value of the standardize function to new variables. In R and other languages, the result of a function is returned when the function is called; the value returned is called the return value. The return() function makes it clear what the function is returning. The return() function is not strictly necessary in R; the result of the last line of code in the body of a function is returned by default. However, to again to borrow from the Zen of Python, Explicit is better than implicit. Being explicit about what a function returns by using the return() function will make your code less error prone and easier to understand. 3.5.3 Scope In programming, there is a critically important concept called scope. Every variable and function you define when you program has a scope, which defines where in the rest of your code the variable can be accessed. In R, variables defined outside of a function have universal or top level scope, i.e. they can be accessed from anywhere in your script. However, variables defined inside functions can only be accessed from within that function. For example: x &lt;- 3 multiply_x_by_two &lt;- function() { y &lt;- x*2 # x is not defined as a parameter to the function, but is defined outside the function return(y) } x [1] 3 multiply_x_by_two() [1] 6 y Error: object &#39;y&#39; not found Notice that the variable x is accessible within the function multiply_x_by_two, but the variable y is not accessible outside that function. The reason that x is accessible within the function is that multiply_x_by_two inherits the scope where it is defined, which in this case is the top level scope of your script, which includes x. The scope of y is limited to the body of the function between the { } curly braces defining the function. Accessing variables within functions from outside the functions scope is very bad practice! Functions should be as self contained as possible, and any values they need should be passed as parameters. A better way to write the function above would be as follows: x &lt;- 3 multiply_by_two &lt;- function(x) { y &lt;- x*2 # x here is defined as whatever is passed to the function! y } x [1] 3 multiply_by_two(6) [1] 12 x # the value of x in the outer scope remains the same, because the function scope does not modify it [1] 3 Every variable and function you define is subject to the same scope rules above. Scope is a critical concept to understand when programming, and grasping how it works will make your code more predictable and less error prone. Scope in R 3.6 Iteration In programming, iteration refers to stepping sequentially through a set or collection of objects, be it a vector of numbers, the columns of a matrix, etc. In non-functional languages like python, C, etc. there are particular control structures that implement iteration, commonly called loops. If you have worked with these languages, you may be familiar with for and while loops, which are some of these iteration control structures. However, R was designed to execute iteration in a different way than these other languages, and provides two forms of iteration: vectorized operations, and functional programming with apply(). Note that R does have for and while loop support in the language. However, these loop structures often have poor performance, and should generally be avoided in favor of the functional style of iteration described below. How To Avoid For Loops in R If you really, really want to learn how to use for loops in R, read this, but dont say I didnt warn you when your code slows to a crawl for unknown reasons: R for Data Science - for loops 3.6.1 Vectorized operations The simplest form of iteration in R comes in vectorized computation. This sounds fancy, but it just means R intrinsically knows how to perform many operations on vectors and matrices as well as individual values. We have already seen examples of this above when performing arithmetic operations on vectors: x &lt;- c(1,2,3,4,5) x + 3 # add 3 to every element of vector x [1] 4 5 6 7 8 x * x # elementwise multiplication, 1*1 2*2 etc [1] 1 4 9 16 25 x_mat &lt;- matrix(c(1,2,3,4,5,6),nrow=2,ncol=3) x_mat + 3 # add 3 to every element of matrix x_mat [,1] [,2] [,3] [1,] 4 6 8 [2,] 5 7 9 # the * operator always means element-wise x_mat * x_mat [,1] [,2] [,3] [1,] 1 9 25 [2,] 4 16 36 In addition to simple arithmetic operations, R also has syntax for vector-vector, matrix-vector, and matrix-matrix operations, like matrix multiplication and dot products: # the %*% operator stands for matrix multiplication x_mat %*% c(1,2,3) # [ 2x3 ] * [ 3 ] [,1] [1,] 22 [2,] 28 x_mat %*% t(x_mat) # recall t() is the transpose function, making [ 2x3 ] * [ 3x2 ] [,1] [,2] [1,] 35 44 [2,] 44 56 These forms of implicit iteration are very powerful, and the R program has been highly optimized to perform these operations very quickly. If you can cast your iteration into a vector or matrix multiplication, it is a good idea to do so. For other more complex or custom iteration, we must first talk briefly about functional programming. 3.6.2 Functional programming R is a functional programming language at its core, which means it is designed around the use of functions. In the previous section, we saw that functions are defined and assigned to names just like variables. This means that functions can be passed to other functions just like variables! Consider the following example. Lets consider a general formulation of vector transformation: \\[ \\bar{\\mathbf{x}} = \\frac{\\mathbf{x} - t_r(\\mathbf{x})}{s(\\mathbf{x})} \\] Here, \\(\\mathbf{x}\\) is a vector of real numbers, and \\(\\bar{\\mathbf{x}}\\) is defined as a vector of the same length where each value has had some average or central value \\(t_r(\\mathbf{x})\\) subtracted from it, and is divided by a scaling factor \\(s(\\mathbf{x})\\) to control the range of resulting values. Both \\(t_r(\\mathbf{x})\\) and \\(s(\\mathbf{x})\\) are scalars (i.e. individual numbers) and dependent upon the values of \\(\\mathbf{x}\\). If \\(t_r\\) is arithmetic mean and \\(s\\) is standard deviation, we have defined the standardization transformation mentioned in earlier examples: x &lt;- rnorm(100, mean=20, sd=10) x_zscore &lt;- (x - mean(x))/sd(x) However, there are many different ways to define the central value of a set of numbers: arithmetic mean geometric mean median mode and many more Each of these central value methods accepts a vector of numbers, but their behaviors are different, and are appropriate in different situations. Likewise, there are many possible scaling strategies we might consider: standard deviation rescaling factor (e.g. set data range to be between -1 and 1) scaling to unit length (all values sum to 1) and others We may wish to explore these different methods without writing entirely new code for each combination when trying out different transformation techniques. In R and other functional languages, we can easily accomplish this by passing functions as arguments to other functions. Consider the following R function: # note R already has a built in function named &quot;transform&quot; my_transform &lt;- function(x, t_r, s) { return((x - t_r(x))/s(x)) } This should look familiar to the equation presented earlier, except now in code the arguments t_r and s are passed as arguments. If we wished to transform using a Z-score normalization, we could call my_transform as follows: x &lt;- rnorm(100,mean=20,sd=10) x_zscore &lt;- my_transform(x, mean, sd) mean(x_zscore) [1] 0 sd(x_zscore) [1] 1 In the my_transform function call, the second and third arguments are the names of the mean and sd functions, respectively. In the definition of my_transform we use the syntax t_r(x) and s(x) to indicate that these arguments should be treated as functions. Using this strategy, we could just as easily define a transformation using median and sum for t_r and s if we wished to: x &lt;- rnorm(100,mean=20,sd=10) x_transform &lt;- my_transform(x, median, sum) median(x_transform) [1] 0 sum(x_transform) # this quantity does not have an a priori known value (or meaning for that matter, it&#39;s just an example) [1] 0.013 We can also write our own functions and pass them to get the my_transform function to have desired behavior. The following scales the values of x to have a range of \\([0,1]\\): data_range &lt;- function(x) { return(max(x) - min(x)) } # my_transform computes: (x - min(x))/(max(x) - min(x)) x_rescaled &lt;- my_transform(x, min, data_range) min(x_rescaled) [1] 0 max(x_rescaled) [1] 1 The data_range function simply subtracts the minimum value of x from the maximum value and returns the result. This feature of passing functions as arguments to other functions is a fundamental property of functional programming languages. Now we are ready to finally talk about how iteration is performed in R. Advanced R - Functional Programming Functional Programming Tutorial 3.6.3 apply() and friends When working with lists and matrices in R, there are often times when you want to perform a computation on every row or every column separately. A common example of this in data science mentioned above is feature standardization. Earlier we wrote a Z-score transformation that accepts a vector, subtracts the mean from each element, and divides the result by the standard deviation of the data. This ensures the data has a mean and standard deviation of 0 and 1, respectively. However, this function only operates on a single vector of numbers. Large datasets have many features, each of which may be individual vectors, that we desire to perform this same Z-score transformation on separately. In other words, we have one function that we wish to execute on either every row or every column of a matrix and return the result. This is a form of iteration that can be implemented in a functional style using the apply function. This is the signature of the apply function, from the RStudio help(apply) page: apply(X, MARGIN, FUN, ..., simplify = TRUE) Here, X is a matrix (i.e. a rectangle of numbers) that we wish to perform a computation on for either each row or each column. MARGIN indicates whether the matrix should be traversed by rows (MARGIN=1) or columns (MARGIN=2). FUN is the name of a function that accepts a vector and returns either a vector or a scalar value that we wish to execute on either the rows or columns. apply() then executes FUN on each row or column of X and returns the result. For example: zscore &lt;- function(x) { return((x-mean(x))/sd(x)) } # construct a matrix of 50 rows by 100 columns with samples drawn from a normal distribution x_mat &lt;- matrix( rnorm(100*50, mean=20, sd=5), nrow=50, ncol=100 ) # z-transform the rows of x_mat, so that each column has mean,sd of 0,1 x_mat_zscore &lt;- apply(x_mat, 2, zscore) # we can check that all the columns of x_mat_zscore have mean close to zero with apply too x_mat_zscore_means &lt;- apply(x_mat_zscore, 2, mean) # note: due to machine precision errors, these results will not be exactly zero, but are very close # note: the all() function returns true if all of its arguments are TRUE all(x_mat_zscore_means&lt;1e-15) [1] TRUE The same approach can be used when X is a list or data frame rather than a matrix using the lapply() function (hint: the l in lapply stands for list). Here is the function signature for lapply: lapply(X, FUN, ...) Recall that lists and data frames can be thought of as vectors where each element can be its own vector. Therefore, there is only one axis along which to iterate on the elements and there is not MARGIN argument as in apply. This function returns a new list of the same dimension as the original list with elements returned by FUN: x &lt;- list( feature1=rnorm(100,mean=20,sd=10), feature2=rnorm(100,mean=50,sd=5) ) x_zscore &lt;- lapply(x, zscore) # check that the means are close to zero x_zscore_means &lt;- lapply(x_zscore, mean) all(x_zscore_means &lt; 1e-15) [1] TRUE This functional programming pattern might be counter intuitive at first, but it is well worth your while to learn. R for Data Science - Iteration 3.7 Troubleshooting/Debugging Strategies Bugs in code are normal. You are not a bad programmer if your code has bugs (thank goodness!). However, some bugs can be very difficult to fix, and some are even difficult to find. You will spend a substantial amount of time debugging your code in R, especially as you are learning the language and its many quirks. You will encounter R error and warning messages routinely during development, and not all of them are straightforward to understand. It is important that you learn how to seek the answers to the problems R reports on your own; your colleagues (and instructors!) will thank you for it. There is no standard approach to debugging, but here we borrow ideas from Hadley Wickams excellent section on debugging in his Advanced R book: Google! - copy and paste the error into google and see what comes back. Especially when starting out, the errors you receive have been encountered countless times by others before you, and solutions/explanations of them are already out there. If you arent already familiar with Stack Overflow, you will be very soon. Make it repeatable - When you encounter an error, dont change anything in your code and try again to make sure you get the same error again. This may require you to isolate the code with the error in a different setting to make it more easy to run. If you do, this means the error is repeatable, or replicable, and you can now try modifying the code in question to see if and how the error changes. Find out where the bug is* - Most bugs involve multiple lines of code, only a subset of which contains the actual error. Sometimes the exact line where the error occurs is obvious, but other times the error is a consequence of a mistake assumption made earlier in the code. Fix it and test it - When you have identified the specific issue causing the bug, modify the code so it produces the correct result and then rigorously test your fix to make sure it is correct. Sometimes making one change to code causes side effects elsewhere in your code in ways that are difficult to predict. Ideally, you have already written unit tests that explicitly test parts of your code, but if not you will need to use other means of convincing yourself that your fix worked. This debugging process will become second nature as you work more in R. Practically speaking, the most basic debugging method is to run code that isnt working the way it should, print out intermediate results to inspect the state of your variables, and make adjustments accordingly. In RStudio, the Environment Inspector in the top right of the interface makes inspecting the current values of your variables very easy. You can also easily execute lines of code from your script in the interpreter at the bottom right using Cntl-Enter and test out modifications there. Sometimes you will be working with highly nested data structures like lists of lists. These objects can be difficult to inspect due to their size. The str() function, which stands for structure, will pretty print an object with its values and its structure: nested_lists &lt;- list( a=list( item1=c(1,2,3), item2=c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) ), b=list( var1=1:10, var2=100:110, var3=1000:1010 ), c=c(10,9,8,7,6,5) ) nested_lists $a $a$item1 [1] 1 2 3 $a$item2 [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; $b $b$var1 [1] 1 2 3 4 5 6 7 8 9 10 $b$var2 [1] 100 101 102 103 104 105 106 107 108 109 110 $b$var3 [1] 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 str(nested_lists) List of 3 $ a:List of 2 ..$ item1: num [1:3] 1 2 3 ..$ item2: chr [1:3] &quot;A&quot; &quot;B&quot; &quot;C&quot; $ b:List of 3 ..$ var1: int [1:10] 1 2 3 4 5 6 7 8 9 10 ..$ var2: int [1:11] 100 101 102 103 104 105 106 107 108 109 ... ..$ var3: int [1:11] 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 ... $ c: num [1:6] 10 9 8 7 6 5 The output str is more concise and descriptive than simply printing out the object. RStudio has many more debugging tools you can use. Check out the section on debugging in Hadley Wickams Advanced R book in the Read More box for description of these tools. Hands-On Programming with R - Debugging R Code Advanced R - Debugging 3.8 Coding Style and Conventions Some very common worries among new programmers is: Is my code terrible? How do I write good code? There is no gold standard for what makes code good, but there are some questions you can ask of your code as a guide: 3.8.1 Is my code correct? Does it produce the desired output? This is pretty obviously important in principle, but it can be difficult to be sure that your code is correct. This is especially difficult if your codebase is large and complicated as it tends to become over time. While simple trial and error is an effective first approach, a more reliable albeit time- and thought-intensive strategy is to write explicit tests for your code and run them regularly. 3.8.2 Does my code follow the DRY principle? Dont Repeat Yourself (DRY) is a powerful and helpful strategy to make your code more reliable. This typically involves identifying common patterns in your code and moving them to functions or objects. 3.8.3 Did I choose concise but descriptive variable and function names? Variable and function names should be descriptive when necessary and not too long. Try to put yourself in the shoes of someone who is reading your code for the first time and see if you can figure out what it does. Better yet, offer to buy a friend a coffee in return for them looking at it! 3.8.4 Did I use indentation and naming conventions consistently throughout my code? Consistently formatted code is much easier to read (and possibly understand) than inconsistent code. Consider the following code example: calcVal &lt;- function(x, a, arg=2) { return(sum(x*a)**2)} calc_val_2 &lt;- function(x, a, b, arg) { res &lt;- sum(b+a*x)**arg return(res)} This code is inconsistent in several ways: naming conventions - calcVal is camel case, calc_val_2 is snake case new lines and whitespace - calcVal is all on one line, calc_val_2 is on multiple lines unhelpful indentation - calc_val_2 has a function body that is not indented, and the close curly brace is appended to the last line of the body unhelpful function and argument names - the function names describe very little about what the functions do, and the argument names x, a, etc are not very descriptive about what they represent unused function arguments - the arg argument in calcVal isnt used anywhere in the function the two functions appear to do something very similar and could be made simpler using a default argument A more consistent version of this code might look like: exponent_product &lt;- function(x, a, offset=0, arg=2) { return(sum(offset+a*x)**arg) } This code is much cleaner, more consistent, and easier to read. 3.8.5 Did I write comments, especially when what the code does is not obvious? Sometimes what a piece of code does is obvious from looking at it: x &lt;- x + 1 Clearly this line of code takes the value of x, whatever it is, and adds 1 to it. However, it may not be obvious why a piece of code does what it does. In these cases, it may be very helpful to record your thinking about a line of code as a comment: # add 1 as a pseudocount x &lt;- x + 1 Then when you or someone else reads the code, it will be obvious what you were thinking when you wrote it. In your career, you will encounter situations where you need to figure out what you were thinking when you wrote a piece of code. Endeavor to make future you proud of current you! 3.8.6 How easy would it be for someone else to understand my code? If someone else who has never seen my code before is asked to run and understand it, how easy would it be for them to do so? 3.8.7 Is my code easy to maintain/change? This is related to the previous question, but is distinct in that understanding what code does is just the first step in being able to make desired changes to it. "],["data-wrangling.html", "4 Data Wrangling 4.1 Basics 4.2 Arranging Data", " 4 Data Wrangling 4.1 Basics 4.1.1 Overview 4.1.2 Learning Objectives 4.1.3 Skill List 4.1.4 The Tidyverse 4.1.5 Importing Data 4.1.6 Tidying Data 4.1.7 Rectangular Data in Biology 4.2 Arranging Data 4.2.1 Overview 4.2.2 Learning Objectives 4.2.3 Skill List "],["data-science.html", "5 Data Science 5.1 Distributions &amp; Tests 5.2 Data summarization &amp; cleaning 5.3 Clustering 5.4 Data Modeling 5.5 Network Analysis", " 5 Data Science 5.1 Distributions &amp; Tests 5.2 Data summarization &amp; cleaning 5.3 Clustering 5.4 Data Modeling 5.5 Network Analysis "],["biology-bioinformatics.html", "6 Biology &amp; Bioinformatics 6.1 R in Biology 6.2 Bioconductor 6.3 Gene Expression 6.4 Differential Expression: Microarrays 6.5 Gene Set Enrichment Analysis 6.6 High Throughput Sequencing 6.7 Differential Expression: RNASeq 6.8 Biological Network Analysis 6.9 Single Cell Sequencing Analysis", " 6 Biology &amp; Bioinformatics 6.1 R in Biology 6.1.1 Overview 6.1.2 Learning Objectives 6.1.3 Skill List 6.2 Bioconductor 6.2.1 Overview 6.2.2 Learning Objectives 6.2.3 Skill List 6.3 Gene Expression 6.3.1 Overview 6.3.2 Learning Objectives 6.3.3 Skill List 6.4 Differential Expression: Microarrays 6.4.1 Overview 6.4.2 Learning Objectives 6.4.3 Skill List 6.5 Gene Set Enrichment Analysis 6.5.1 Overview 6.5.2 Learning Objectives 6.5.3 Skill List 6.6 High Throughput Sequencing 6.6.1 Overview 6.6.2 Learning Objectives 6.6.3 Skill List 6.7 Differential Expression: RNASeq 6.7.1 Overview 6.7.2 Learning Objectives 6.7.3 Skill List 6.8 Biological Network Analysis 6.8.1 Overview 6.8.2 Learning Objectives 6.8.3 Skill List 6.9 Single Cell Sequencing Analysis 6.9.1 Overview 6.9.2 Learning Objectives 6.9.3 Skill List "],["data-visualization.html", "7 Data Visualization 7.1 Grammar of Graphics 7.2 Responsible Plotting 7.3 Publication Ready Plots 7.4 Network Viz", " 7 Data Visualization 7.1 Grammar of Graphics 7.1.1 Overview 7.1.2 Learning Objectives 7.1.3 Skill List 7.2 Responsible Plotting Good plots empower us to ask good questions. - Alberto Cairo, How Charts Lie 7.2.1 Overview 7.2.2 Learning Objectives 7.2.3 Skill List 7.3 Publication Ready Plots ggpubr - publication ready plotting with ggplot 7.4 Network Viz 7.4.1 Overview 7.4.2 Learning Objectives 7.4.3 Skill List "],["engineering.html", "8 EngineeRing 8.1 Unit Testing 8.2 Toolification 8.3 Pipelines &amp; Workflows 8.4 Parallel Processing 8.5 R Packages", " 8 EngineeRing 8.1 Unit Testing 8.2 Toolification 8.3 Pipelines &amp; Workflows 8.4 Parallel Processing 8.5 R Packages "],["rshiny.html", "9 RShiny 9.1 Overview 9.2 Learning Objectives 9.3 Skill List", " 9 RShiny 9.1 Overview 9.2 Learning Objectives 9.3 Skill List "],["communicating-with-r.html", "10 Communicating with R 10.1 RMarkdown &amp; knitr", " 10 Communicating with R 10.1 RMarkdown &amp; knitr "],["contribution-guide.html", "11 Contribution Guide 11.1 Custom blocks", " 11 Contribution Guide This page contains some conventions and tips for writing material for this book. 11.1 Custom blocks There are a number of custom blocks that can be used for highlighting salient information throughout the text. The code inside the corresponding block below can be used to insert different standout boxes: ::: {.box .tldr} This is a tl;dr ::: ::: {.box .note} This is a note ::: ::: {.box .hint} This is a hint ::: ::: {.box .important} This is an important ::: ::: {.box .warning} This is a warning ::: "],["assignments.html", "12 Assignments 12.1 Overview Assignment 6", " 12 Assignments 12.1 Overview GitHub repo templates here Assignment 2 Problem Statement Learning Objectives Skill List Instructions Our main focus for this assignment are installing packages, manipulating data, and plotting our manipulated data. We will be borrowing from the same started CSV of expression data from BF528s Project 1, available here. The project is laid out as such: main.R test_main.R report.Rmd A skeleton of the functions you need to complete is in main.R. Tests have been pre-written to test your code and help you ensure it is running correctly, these are in test_main.R. Finally, we are also introducing the concept of R Markdown, which for this assignment is report.Rmd. The document itself goes into greater detail, but you will: 1. Complete the functions in main.R and use testthat:test_file('test_main.R') to ensure they work correctly. 2. Read the R Markdown file and complete the section called Assignment. To do this, you can source('main.R') to bring over the functions you wrote in step one. 3. Finally, annotate the functions you wrote and Knit the R Markdown report, complete with your additional comments and code execution. This page will go into detail on how the functions and their associated tests should work. Function Details 1. Bioconductor While many useful R packages can be loaded through CRAN using the install.packages() syntax, a lot of specifically bioinformatics packages are exclusively released on Bioconductor. For this assignment we only need the package called biomaRt. R programmers fancy themselves very clever, so Rs show up a lot. Naturally, we want to load our packages at the beginning of our script so all of the code we write beneath can access it as it runs. However, if a user or ourselves already has this package installed we dont want to waste their time installing it again. The function require() can help us avoid unnecessary installation time and will help us develop faster. The Bioconductor link above has an example of this method. This section is untested. 2. load_expression() Perhaps the most integral part of using the many data wrangling abilities of R is actually entering your data into the R environment. While there are many ways to do this in R, we ultimately want this data to be in a tibble, which means the current form of the CSV will make R very angry if you attempt to load it in. This is because tibbles dont support row names very well, and the first column of our data doesnt have a name. Try to use the load_expression() data to load data from a filename parameter and return a tibble of that information. I called my firs column probeids. Tests The tests for this function are: test_that(&quot;loading csv correctly&quot;, { result_tib &lt;- load_expression(&quot;/project/bf528/project_1/data/example_intensity_data.csv&quot;) expect_equal(dim(result_tib), c(54675, 36)) expect_true(is_tibble(result_tib)) }) This test uses the load_expression() you write to store the returned tibble in result_tib. Heads up Note that this test is using the same file for data input as you are, but this may not always be the case. The test then compares the dimensions of that result, and expects 54,675 rows and 36 columns. These are the dimensions of the input CSV. It also checks to confirm it is a tibble object (because tibbles are better than dataframes). 3. filter_15() In order to filter the numerous rows we have for this data, we introduce a function that filters the probe IDs in the tibble our data is stored in. We want to capture probes that have a suitably high level of expression, so we are setting log2(15) as the cutoff for an expression level. We will keep a row if 15% of the values in that row exceed log2(15) (about 3.9). Since we may want to examine the probe IDs we find, the function simple returns the values of the probe IDs (column 1) instead of returning the entire tibble. This function presents an important concept in R: using built-ins to speed up our code. Built-ins are functions and packages that are optimized to process data in a certain way. Since were looking at each row of a table, we could simply use a for loop to iterate one row at a time. This is slow, though, and for this function might take 5-10 seconds to run (a long time for a program like this!). Instead, you could use a function like apply() or lapply() to filter every row at once. This solution takes mere moments. Tests library(tibble) test_that(&quot;the correct rows are filtered out in filter_15()&quot;, { test_tib &lt;- tibble(probeids=c(&#39;1_s_at&#39;, &#39;2_s_at&#39;, &#39;3_s_at&#39;, &#39;4_s_at&#39;), GSM1=c(1.0, 3.95, 4.05, 0.5), GSM2=rep(1.6, 4), GSM3=rep(2.5, 4), GSM4=rep(3.99, 4), GSM5=rep(3.0, 4), GSM6=rep(1.0, 4), GSM7=rep(0.5, 4)) expect_equal(pull(filter_15(test_tib)), c(&quot;2_s_at&quot;, &quot;3_s_at&quot;)) }) In order to test this function, we create a small sample tibble of expression data containing only seven samples and four IDs. Two of the rows do have more than 15% of their values exceeding log2(15), the other two do not. This test ensures that filter_15() selects the correct rows. Creating a small sample table like this can be very useful when testing your own code since you dont need to look at a large amount of data to see if its working correctly or not. 4. affy_to_hgnc() This is an important, but sometimes painful, part of using R. There is a great built-in package for connecting to Ensembl (a database of genomic information for many species) called biomaRt. We will use biomaRt to connect the affymetrix probe IDs to more recognizable HGNC gene IDs. The problem is that biomaRt depends on an external API (application program interface) to retrieve data, and this connection sometimes (oftentimes) doesnt work. While their may be more nuanced approaches to an unstable resource like this like automatically retrying failed connections, the best advice for the time being is to try running this function a few times if it doesnt work at first. The errors are clear when it comes to a failed connection, so know that when you get to this stage it likely isnt your codes fault. To build a biomaRt query, read the documentation in section 3 here. The biomart you should use is ENSEMBL_MART_ENSEMBL, the data set hsapiens_gene_ensembl, and you want to find the attributes c(\"affy_hg_u133_plus_2\", \"hgnc_symbol\"). The data you filter using filter_15() returns a list of affy_hg_u133_plus_2 probe IDs, and the gene names were interested in are stored in hgnc_symbol. This function should return a tibble, but biomaRts getBM() will only accept and return a data.frame. You can use dplyr::pull() to turn a tibble into a simple character vector, and dplyr::as_tibble() to go from a data frame to a tibble. Tests test_that(&quot;affy ids can be converted to HGNC names properly using affy_to_hgnc()&quot;, { # biomaRt super buggy so we can try to account for not connecting well response &lt;- try(affy_to_hgnc(tibble(&#39;1553551_s_at&#39;)), TRUE) if (grepl(&quot;Error&quot;, response[1])) { expect_warning(warning(&quot;Could not connect to ENSEMBL.&quot;)) } else { expect_equal(response$hgnc_symbol, c(&quot;MT-ND1&quot;, &quot;MT-TI&quot;, &quot;MT-TM&quot;, &quot;MT-ND2&quot;)) } }) As fun as it is to try to get biomaRt function to connect correctly, it is even more fun to test them. Since a failure to connect doesnt indicate an actual failure in our code we must use a try() block in order to capture if there is a connection error. Note The try() function is a part of programming called error-handling which extends to many other languages. We often expect errors when running our programs (such as right now) but dont want to shut down our entire operation if its an error we can expect. Using try, except, and finally (the latter two not appearing here) we can account for issues outside of our control and adapt our code to change the outcome. Using try except is not a replacement for writing code that doesnt generate errors. If you can avoid an error in the first place, that is far better than using error-handling. In this case we try() to use affy_to_hgnc() to connect to Ensembl and store the resulting error in response. We then check: if there is an Error then we throw a warning() to our testing output. This doesnt stop further testing from happening, but it ensures we know that something isnt quite right. If the response does not contain an error, we simply test that it returned the correct gene symbols for our random affy probe ID of choice. 5. reduce_data() We have one final step in manipulating our data before we plot it. We have our original data, the probe IDs and their associated HGNC symbols, and a list of good gene names and bad gene names. reduce_data takes these four inputs and returns a tibble that has reduced our expression data to only the genes of interest and has a column describing which set of genes it belongs to (good or bad). Changing the shape of the data is incredibly useful for ggplot, the tidyverse package we will use for plotting. While there is flexibility when using ggplot to plot data, having data in long format is typically ideal. Once again, there are multiple ways to reorganize data in this way. We used the base function match() to connect our probe IDs to with our HGNC IDs in name_ids. We then used tibble::add_column() to insert the new data in the correct location. Finally, we created two tibbles of good and bad genes using which() and the %in% modifier. Which evaluates true conditions across a range of data, so we can pass the list of genes we want and select the correct ones. For instance: library(tibble) tib &lt;- tibble(gene = c(&quot;gene1&quot;, &quot;gene2&quot;, &quot;gene3&quot;, &quot;gene4&quot;), affy = c(&quot;a_s_1&quot;, &quot;a_s_2&quot;, &quot;a_s_3&quot;, &quot;a_s_4&quot;)) which(tib$gene %in% c(&quot;gene2&quot;, &quot;gene3&quot;)) # returns the index of the TRUE rows ## [1] 2 3 tib$affy[which(tib$gene %in% c(&quot;gene2&quot;, &quot;gene3&quot;))] # use [] to get data back ## [1] &quot;a_s_2&quot; &quot;a_s_3&quot; Once again, there are many ways to reshape this data (some maybe more elegant than this!) and all we need is the data to be correctly shaped when it is returned. Tests test_that(&quot;reduce_data() is correctly changing the size and shape of the tibble&quot;, { t_tibble &lt;- tibble(probeids = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;, &quot;3_s_at&quot;), GSM1 = c(9.5, 7.6, 5.5), GSM2 = c(9.7, 7.2, 2.9), GSM3 = c(6.9, 4.3, 6.8)) names &lt;- tibble(affy_hg_u133_plus_2 = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;, &quot;3_s_at&quot;), hgnc_symbol = c(&quot;A-REAL-GENE&quot;, &quot;SONIC&quot;, &quot;UTWPU&quot;)) good &lt;- c(&quot;A-REAL-GENE&quot;) bad &lt;- c(&quot;SONIC&quot;) reduce_test &lt;- reduce_data(t_tibble, names, good, bad) result &lt;- tibble(probeids = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;), hgnc_symbol = c(&quot;A-REAL-GENE&quot;, &quot;SONIC&quot;), gene_set = c(&quot;good&quot;, &quot;bad&quot;), GSM1 = c(9.5, 7.6), GSM2 = c(9.7, 7.2), GSM3 = c(6.9, 4.3)) expect_equal(reduce_test, result) }) In order to test a function that changes a tibble, we need to use a tibble. We only create a test table that is three rows by four columns, but that is enough to get the gist of the function. We simply pass the four parameters to reduce_data() and we expect it to create a tibble like result. Ensure your output column names are the same here or your tests my fail. While not crucial to the success of your assingment, maintaining correct column names across multiple data transformations is an important skill. Assignment 6 Problem Statement Plotting data is a useful but oftentimes complicated skill set. While R has many tools to help simplify and create attractive and useful graphs, they can be difficult to utilize and have many pitfalls. This assignment will teach you the most important parts of plotting with R using the package ggplot. Learning Objectives The basic operations of ggplot and The Grammar of Graphics. Using differential gene expression analysis packages to generate data for plotting. Creating simple plots and not using the default colors and themes. Combining multiple plots into one image. Skill List A tempered heart and mind that understands a plot may not ever look exactly how you want it to. An intermediate understanding of Rs most popular plotting package, ggplot. Further understanding of differential expression analysis and its plotting. A sense of superiority whenever you see a publication use the base R plotting package for figures. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
