[["index.html", "BF591 - R for Biological Sciences Syllabus Course Schedule Course Values and Policies", " BF591 - R for Biological Sciences Syllabus Semester: Spring 2022 Location: TBD Time: M/W 8:00AM - 9:45AM Contents: Course Schedule [Instructors] Course Values and Policies This course introduces the R programming language through the lens of practitioners in the biological sciences, particularly biology and bioinformatics. Key concepts and patterns of the language are covered, including: RStudio Data wrangling with tidyverse Data visualization with ggplot Essential biological data shapes and formats Core bioconductor packages Basic data exploration, including elementary statistical modeling and summarization Elementary Data Science concepts Toolifying R scripts Communicating R code and results with RMarkdown Buidling R packages and unit testing strategies Building interactive tools with RShiny About 1/3 of the materials are inspired by the online textbook R for Data Science, while the rest has been developed by practicing bioinformaticians based on their experiences. Weekly programming assignments will help students apply these techniques to realistic problems involving analysis and visualization of biological data. Students will be introduced to a unit testing paradigm that will help them write correct code and deposit all their code into github for evaluation. Students will implement an end-to-end project that begins with one of a set of provided datasets, implements a set of data summarization and exploration operations on it, and allows interaction with an RShiny app. The course materials are aligned with BF528 Applications in Translational Bioinformatics and are intended to be taken in tandem, but the materials also stand alone as an independent class. Course Schedule NB: Subject to change Week Dates Topics Assignment 1 1/24 &amp; 1/26 Preliminaries R Programming: Basics Data Wrangling: Basics [Assignment 1] 2 1/31 &amp; 2/2 Data Wrangling: Arranging Data Viz: Grammar of Graphics Bio: R in Biology Bio: Bioconductor Basics Assignment 2 3 2/7 &amp; 2/9 Data Sci: Data Modeling Bio: Gene Expression Bio: Differential Expression: Microarrays Data Sci: Clustering [Assignment 3] 3 2/14 &amp; 2/16 Data Sci: Distributions &amp; Tests R Programming: Structures &amp; Iteration Data Sci: Summarization &amp; Cleaning Bio: Gene Set Enrichment Analysis [Assignment 4] 5 2/22 &amp; 2/23 Bio: High Throughput Sequencing Bio: Differential Expression: RNASeq Bio: Gene Set Enrichment Analysis R Programming: Style &amp; Conventions [Assignment 5] 6 2/28 &amp; 3/2 Data Viz: Responsible Plotting Assignment 6 7 3/14 &amp; 3/16 Bio: Single Cell Sequencing R Programming: Pipelines &amp; Workflows [Assignment 7] 8 3/21 &amp; 3/23 RShiny [Assignment 8] 9 3/28 &amp; 3/30 Bio: Biological Networks Data Sci: Network Analysis Data Viz: Network Viz [Project] 10 4/4 &amp; 4/6 Toolification 11 4/11 &amp; 4/13 EngineeRing 12 4/20 Communicating with R 13 4/25 &amp; 4/27 No class - Project work 14 5/2 &amp; 5/4 No class - Project work Project due NB: struckout - class not held, reserved for project work Course Values and Policies Everyone is welcome. Every background, race, color, creed, religion, ethnic origin, age, sex, sexual orientation, gender identity, nationality is welcome and celebrated in this course. Everyone deserves respect, patience, and kindness. Disrespectful language, discrimination, or harassment of any kind are not tolerated, and may result in removal from class or the University. This is not merely BU policy. The instructors deem these principles to be inviolable human rights. Students should feel safe reporting any and all instances of discrimination or harassment to the instructor, to any of the Bioinformatics Program leadership, or the BU Equal Opportunity Office. Everyone brings value. Each of us brings unique experiences, skills, and creativity to this course. Our diversity is our greatest asset. Collaboration is highly encouraged. All students are encouraged to work together and seek out any and all available resources when completing projects in all aspects of the course, including sharing coding ideas and strategies with each other as well as those found on the internet. Any and all available resources may be brought to bear. However, consistent with BU policy, the bulk of your code and your final reports should be written in your own words and represent your own work and understanding of the material. Copying/pasting large sections of code is not acceptable and will be investigated as cheating (we check). A safe space for dissent. For complex topics such as those covered in this class, there is seldom one correct answer, approach, or solution. Disagreement fosters innovation. All in the course, including students and TAs, are encouraged to express constructive criticism and alternative ideas on any aspect of the content. We are always learning. Our knowledge and understanding is always incomplete. Even experts are fallible. The bioinformatics field evolves rapidly, and Rome was not built in a day. Be kind to yourself and to others. You are always smarter and more knowledgable today than you were yesterday. "],["introduction-overview.html", "1 Introduction &amp; Overview 1.1 Who This Book Is For 1.2 A Note About Reinventing the Wheel 1.3 Sources and References", " 1 Introduction &amp; Overview Since the publication of the first draft human genome in 2001, data has driven biological discovery at an exponential rate. Rapid technological innovations in data-generating biochemical instruments, computational resource availability, data storage, and analytical approaches including artificial intelligence, machine learning, and data science more generally have combined and synergized to enable advances in our understanding of biological systems by orders of magnitude. As the rate of development of these technologies has increased, so are practitioners of biological inquiry expected to keep up with the rapidly expanding set of knowledge, skills, and tools required to use them. Modern biological data analysis entails combining knowledge and skills from many domains, including not only biological concepts like molecular biology, genetics, genomics, and biochemistry, but also in computational and quantitative skills including statistics, mathematics, programming and software engineering, high performance and cloud computing, data visualization, and computer science. No one person can be expert in all of these areas, but modern software tools and packages made available by subject matter experts enable us to perform cutting edge analysis with a conceptual understanding of the topics. One such tool is the R programming language, a statistical programming language and environment specifically designed to run statistical analyses and visualize data. R became popular in biological data analysis in the early to mid 2000s, when microarray technology came into widespread use enabling researchers to look for statistical differences in gene expression for thousands of genes across large numbers of samples. As a result of this popularity, a community of biological researchers and data analysts created a collection of software packages called Bioconductor, which made a vast array of cutting edge statistical and bioinformatic methodologies widely available. Today R is one of the two most popular programming languages in biological data analysis and bioinformatics (the other being python). A major innovation in the R language came with the introduction of the tidyverse, as set of open-source data manipulation and visualization packages, first developed by Hadley Wickham and now improved, supported, and maintained by his team of data scientists and software engineers and other individuals. The tidyverse is a collection of packages that specialize in different aspects of data manipulation with the goal of enabling powerful, consistent, and accurate data operations in the broad field of data science. While not changing the structure of the language per se, the tidyverse packages define a set of consistent programming conventions and patterns that are tailored to the types of manipulations required to make data tidy and, therefore, easier and more consistent to work with. The tidyverse therefore is something of its own language that is compatible with but distinct in convention from the base R language. This book and accompanying course focus on how to use R and its related package ecosystems to analyze, visualize, and communicate biological data analyses. As noted above, effective biological data analysis employs skills that span several knowledge domains. This book covers many of these topics in relatively shallow depth, but with the intent of presenting just enough in each to enable the learner to become proficient in most day-to-day biological analysis tasks. 1.1 Who This Book Is For This book was written for the practicing biologist wishing to learn how to use R to analyze biological data. A basic working knowledge of basic genetics, genomics, molecular biology, and biochemistry is assumed, but we endeavored to include enough pertinent background to understand the analysis concepts presented in the text. Basic knowledge of statistics is assumed, but again some background is provided as necessary to understand the analyses and concepts in the text. No further knowledge is assumed. 1.2 A Note About Reinventing the Wheel Many topics in this book are covered elsewhere in greater detail and depth. The content in each section is intended to stand alone, but may not provide a high level of detail that has been done better by others in online materials. These sections provide links to these other resources that provide more information, in case the instructions in this book are too terse or unclear. Wikipedia - Reinventing the Wheel 1.3 Sources and References The materials of this book were inspired and informed by a large number of sources, including books and freely available online materials. The authors would like to thank the generosity of the creators and maintainers of these resources for making their valuable contributions: R for Data Science, by Hadley Wickam, Garrett Grolemund, et al STAT 545 - Data wrangling, exploration, and analysis with R What They Forgot to Teach You About R Reproducible Analysis with R, by State of Alaskas Salmon and People Project, NCEAS How Charts Lie: Getting Sparter about Visual Information, by Alberto Cairo "],["preliminaries.html", "2 Preliminaries 2.1 The R Language 2.2 RStudio 2.3 git + github", " 2 Preliminaries 2.1 The R Language R is a free programming language and environment where that language can be used. More specifically, R is a statistical programming language, designed for the express and exclusive purpose of conducting statistical analyses and visualizing data. Said differently, R is not a general purpose programming language unlike other languages such as python, Java, C, etc. As such, the languages real strengths are in the manipulation, analysis, and visualization of data and statistical procedures, though it is often used for other purposes (for example, web applications with RShiny, or writing books like this one with bookdown). You may download R for free from the Comprehensive R Archive Network. The effective biological analysis practitioner knows how to use multiple tools for their appropriate purposes. The most common programming languages in this field are python, R, and scripting languages like The Bourne Again Shell - bash. While it is beyond the scope of this book to cover which tools are best used where, R is appropriate wherever data analysis and visualization are needed. Any operations that do not involve these aspects (e.g. manipulating text files, programming web servers, etc) are likely more suitable for other languages and software. R Project Home Page Section on R from R for Data Science 2.2 RStudio This course assumes the learner is using the RStudio software platform for all analyses, unless otherwise noted. RStudio is a freely available and fully featured integrated development environment (IDE) for R, and has many convenient capabilities when learning R. RStudio may be downloaded and installed for free from the site above. By default, RStudio preserves your R environment when you shut it down and restores it when you start it again. This is very bad practice! The state of your R environment, which includes the values stored in variables, the R packages loaded, etc. from previously executed code is transient and may not reflect the results your code produces when run alone. Open the Tools &gt; Global Options menu and: 1. Uncheck Restore .RData into workspace at startup 2. Set Save workspace to .RData on exit: to Never Never save workspace The book R for Data Science has an excellent chapter on why this is a problem and how to change the RStudio setting to avoid it. RStudio Education - Beginners Guide ModernDrive R Series - Getting Started R for Data Science - What is real, and where does your analysis live? What They Forgot To Teach You About R - Always start R with a blank slate R for Data Science - RStudio 2.3 git + github 2.3.1 Motivation Biological analysis entails writing code, which changes over time as you develop it, gain insight from your data, and identify new questions and hypotheses. A common pattern when developing scripts is to make copies of older code files to preserve them before making new changes to them. While it is a good idea to maintain a record of all the code you previously ran, over time this practice often leads to disorganized, cluttered, untidy analysis directories. For example, say you are working on a script named my_R_script.R and decide you want to add a new analysis that substantially changes the code. You might be tempted to make a copy of the current version of the code into a new file named my_R_script_v2.R that you then make changes to, leaving your original script intact and untouched going forward. You make your changes to your new script, produce some stunning and fascinating plots, present the analysis at a group meeting, only to discover later there was a critical bug in your code that made the plots misleading and requires substantial redevelopment. Bugs happen. There are two types of bugs: Syntax bugs: bugs due to incorrect language usage, which R will tell you about and can (usually) be easily identified and fixed Logic bugs: the code you write is syntactically correct, but does something other than what you intend Bugs are normal. The scenario described above, where you present results only to discover your code wasnt doing what you thought it was doing, is extremely common and it will happen to you. This is normal, and finding a bug in your code does not mean you are a bad programmer. Rather than edit your version 2 of your script directly, you decide it is sensible to copy the file to my_R_script_v2_BAD.R and edit the version 2 script to fix the bug. You are satisfied with your new version 2 script, and so make a new copy my_R_script_v2_final.R. Upon review of your analysis, you are asked to implement new changes to the script based on reviewer feedback. You make a new copy of your script to my_R_script_v2_final_revision.R and make the requested changes. Perhaps now your script is final, but in your directory you now have five different versions of your analysis: my_R_script.R my_R_script_v2.R my_R_script_v2_BAD.R my_R_script_v2_final.R my_R_script_v2_final_revision.R When you write your code, you may know which scripts are which, and if you follow good programming practice and carefully commented your code you or your successors may be able to sleuth what was done. However, as time passes, the intimate knowledge you thought you had about your code will be replaced by other more immediately important things; eventually you may not even understand or even recognize your own code, let alone someone else trying to understand it. Not an ideal situation in any case. A better solution involves recording changes to code over time in such a way that you can recover old code if needed, but dont clutter your analytical workspace with unneeded files. git provides an efficient solution to this problem. 2.3.2 git git is a free, open source version control software program. Version control software is used to track and record changes to code over time, potentially by many developers working on the same software project concurrently from different parts of the world. The base git software can be used on the command line, or with graphical user interface applications for popular operating systems. There are many excellent tutorials online (some linked below) that teach how to use git but the basic concepts are described below. The command line commands are listed, but the same operations apply in the graphical clients. A repository (or repo) is a collection of files in a directory that you have asked git to track (run git init in a new directory) Each file you wish to track must be explicitly added to the repo (run git add &lt;filename&gt; from within the git repo directory) When you modify a tracked file, git will notice those differences and show them to you with git status You may tell git to track the changes to the explicit files that changed (also run git add &lt;filename&gt; to record changes) A set of tracked changes is stored in the repo by making a commit. A commit takes a snapshot of all the tracked files in the repo at the time the commit is made (run git commit -c &lt;commit message&gt; with a concise commit message that describes what was done) Each commit has a date and time associated with it. The files in the repo can be reset to exactly the state they were in at any commit, thus preserving all previous versions of code. For the vast majority of use cases, the git init, git status, git add, and git commit operations are all you will need to use git effectively. Two more commands, git push and git pull are needed when sharing your code with others as described in the next section. Official git tutorial videos Official git book Git Immersion - a guided tour through git commands DataCamp - Git for data scientists 2.3.3 Git hosting platforms (GitHub) The basic git software only works on your local computer with local repositories. To share this code with others, and receive others contributions, a copy of the repo must be made available in a centralized location that everyone can access. One such place is github.com, which is a free web application that hosts git repos. bitbucket.org is another popular free git repo hosting service. These two services are practically the same, so we will focus on GitHub. There is no formal relationship between git and GitHub. git is an open source software project maintained by hundreds of developers around the world (and is hosted on GitHub). GitHub is an independently provided web service and application. The only connection between GitHub and git is that GitHub hosts git repos. As with git, there are many excellent tutorials on how to use GitHub, but the basic concepts are described below. First you must create an account on GitHub if you dont have one already Then, create a new repo on GitHub that you wish to contain your code The next step depends on whether you have an existing local repo or not: If you do not already have a local git repo: Follow the instructions on GitHub to clone your GitHub repo and create a local copy that is connected to the one on GitHub If you already have a local git repo: Follow the instructions on the GitHub to connect your local repo to the GitHub one (this is called adding a remote) Now, your local repo is connected to the same repo on GitHub, and the changes you make to your local files can be sent, or pushed to the repo on GitHub: Make changes to your local files, and git add and git commit them as above Update the remote repo on GitHub by pushing your local commits with git push Running git status will indicate whether your local repo is up to date with your remote GitHub repo When you are working on a team of contributors to a GitHub repo, your local files will become out of date as others push their changes. To ensure your local repo is up to date with the GitHub repo, you must pull your changes from GitHub with git pull. git was designed to automatically combine changes made to a code base by different developers whenever possible. However, if two people make changes to the same parts of the same file, git may not be able to resolve those changes on its own and the developers must communicate and decide what the code should be. These instances are called merge conflicts and can be challenging to resolve. Dealing with merge conflicts is beyond the scope of this book, but some resources are linked below for further reading. All the content and code for this book are stored and available on GitHub, as are the assignment code templates. Official GitHub Tutorial FreeCodeCamp - Git and GitHub For Beginners Official GitHub Tutorial on Merge Conflicts "],["r-programming.html", "3 R Programming 3.1 Before you begin 3.2 The R Script 3.3 The Scripting Workflow 3.4 Basics 3.5 Data Structures &amp; Iteration 3.6 Coding Style and Conventions 3.7 Pipelines &amp; Workflows", " 3 R Programming 3.1 Before you begin All the examples and instructions in this chapter assume you have installed R are using RStudio. Be sure to turn off automatic environment saving in RStudio! Because this is so important, here it is again: By default, RStudio preserves your R environment when you shut it down and restores it when you start it again. This is very bad practice! The state of your R environment, which includes the values stored in variables, the R packages loaded, etc. from previously executed code is transient and may not reflect the results your code produces when run alone. Open the Tools &gt; Global Options menu and: 1. Uncheck Restore .RData into workspace at startup 2. Set Save workspace to .RData on exit: to Never Never save workspace The book R for Data Science has an excellent chapter on why this is a problem and how to change the RStudio setting to avoid it. What They Forgot To Teach You About R - Always start R with a blank slate 3.2 The R Script Before we cover the R language itself, we should talk about how you should run your code and where it should live. As mentioned, R is both a programming language and an environment where you can run code written in that language. The environment is a program (confusingly also called R) that allows you to interact with it and run simple lines of code one at a time. This environment is very useful for learning how the language works and troubleshooting, but it is not suitable for recording and running large, complex analyses that require many lines of code. Therefore, all important R code should be written and saved in a file before you run it! The code may not be correct, and the interactive R environment is helpful for debugging and troubleshooting, but as soon as the code works it should be saved to the file and rerun from there. With this in mind, the basic unit of an R analysis is the R script. An R script is a file that contains lines of R code that run sequentially as a unit to complete one or more tasks. Every R script file has a name, which you choose and should be descriptive but concise about what the script does; script.R, do_it.R, and a_script_that_implements_my_very_cool_but_complicated_analysis_and_plots.R are generally poor names for scripts, whereas analyze_gene_expression.R might be more suitable. In RStudio, you can create a new script file in the current directory using the File -&gt; New File -&gt; R Script menu item or the new R Script button at the top of the screen: New R Script Your RStudio configuration should now enable you to write R code into the (currently unsaved) file in the top left portion of the screen (labeled in the figure as File Editor). Basic RStudio Interface You are now nearly ready to start coding in R! How to name files Some useful and advanced tips on how to name files 3.3 The Scripting Workflow But hold on, were still not quite ready. As mentioned above, all important R code should be written and saved in a file before you run it! Your scripts will very quickly contain many lines of code that are meant to be run in sequential order. While developing your code it is very helpful to run each individual line separately, building up your script incrementally over time. To illustrate how to do this, we will begin with a simple R code that stores the result of an arithmetic expression to a new variable: # stores the result of 1+1 into a variable named &#39;a&#39; a &lt;- 1+1 The concepts in this line of code will be covered in greater depth later, but for now an intuitive understanding will suffice to explain the development workflow in RStudio. When developing, this is the suggested sequence of operations: Save your file (naming if necessary on the first save) with Ctrl-s on Windows or Cmd-s on Mac Execute the line or lines of code in your script you wish to evaluate using Ctrl-Enter on Windows or Cmd-Enter on Mac. By default only the line with the cursor is executed; you may click and drag with the mouse to select multiple lines to execute if needed. The executed code will be evaluated in the Console window, where you may inspect the result and modify the code if necessary. You may inspect the definitions of any variables you have declared in the Environment tab at the upper right. When you have verified that the code you executed does what you intend, ensure the code in the file you started from is updated appropriately. Go to step 1 The above steps are depicted in the following figure: RStudio workflow Over time, you will gain comfort with this workflow and become more flexible with how you use RStudio. If you followed the instructions above and prevented RStudio from saving your environment when you exit the program (which you should! Did I mention you should?!), none of the results of code you previously ran will be available upon starting a new RStudio session. Although this may seem inconvenient, this is an excellent opportunity to verify that your script in its current state does what you intend for it to do. It is extremely easy to ask R to do things you dont mean for it to do! Rerunning your scripts from the beginning in a new RStudio session is an excellent way to guard against this kind of error. This short page summarizes this very well, you should read it: What They Forgot To Teach You About R - Always start R with a blank slate R for Data Science - Workflow: scripts RStudio IDE cheatsheet (scroll down the page to find the cheatsheet entitled RStudio IDE cheatsheet) 3.4 Basics As with other subjects covered so far, the basic syntax of R is covered very well in other free online materials. Some of those excellent resources are linked at the end of this section, but a brief overview of the syntax is covered here. The code examples below can be written into a script and evaluated as described above or entered on the R Console directly and run by pressing Enter. 3.4.1 R Syntax Basics At its core, R (like all programming languages) is basically a fancy calculator. The syntax of most basic arithmetic operations in R should be familiar to you: # addition, subtraction, multiplication, division 1 + 2 [1] 3 3 - 2 [1] 1 4 * 2 [1] 8 4 / 2 [1] 2 1.234 + 2.345 - 3.5*4.9 # numbers can have decimals [1] -13.571 1.234 + (2.345 - 3.5)*4.9 # expressions can contain parentheses [1] -4.4255 # exponentiation 2**2 [1] 4 4**(1/2) # square root [1] 2 9**(1/3) # cube root [1] 3 The [1] lines above are the output given by R when the preceding expression is executed. Any portion of a line starting with a # is a comment and ignored by R. R also supports storing values into symbolic placeholders called variables, or objects. An expression like those above can be assigned into a variable with a name using the &lt;- operator: new_var &lt;- 1 + 2 Variables that have been assigned a value can be placed in subsequent expressions anywhere where their value is evaluated: new_var - 2 [1] 1 another_var &lt;- new_var * 4 The correct way to assign a value to a variable in R is with the &lt;- syntax, unlike many other programming languages which use =. However, although the = assignment syntax does work in R: new_var = 2 # works, but is not common convention! this is considered bad practice and may cause confusion later. You should always use the &lt;- syntax when assigning values to variables! 3.4.2 Functions A function is a R provides a large number of 3.4.3 DRY: Dont Repeat Yourself 3.4.4 Unit Testing 3.4.5 Troubleshooting/Debugging Strategies 3.4.6 RMarkdown &amp; knitr R for Data Science - Workflow basics 3.5 Data Structures &amp; Iteration 3.6 Coding Style and Conventions 3.7 Pipelines &amp; Workflows "],["data-wrangling.html", "4 Data Wrangling 4.1 Basics 4.2 Arranging Data", " 4 Data Wrangling 4.1 Basics 4.1.1 Overview 4.1.2 Learning Objectives 4.1.3 Skill List 4.1.4 The Tidyverse 4.1.5 Importing Data 4.1.6 Tidying Data 4.1.7 Rectangular Data in Biology 4.2 Arranging Data 4.2.1 Overview 4.2.2 Learning Objectives 4.2.3 Skill List "],["data-science.html", "5 Data Science 5.1 Distributions &amp; Tests 5.2 Data summarization &amp; cleaning 5.3 Clustering 5.4 Data Modeling 5.5 Network Analysis", " 5 Data Science 5.1 Distributions &amp; Tests 5.2 Data summarization &amp; cleaning 5.3 Clustering 5.4 Data Modeling 5.5 Network Analysis "],["biology-bioinformatics.html", "6 Biology &amp; Bioinformatics 6.1 R in Biology 6.2 Bioconductor 6.3 Gene Expression 6.4 Differential Expression: Microarrays 6.5 Gene Set Enrichment Analysis 6.6 High Throughput Sequencing 6.7 Differential Expression: RNASeq 6.8 Biological Network Analysis 6.9 Single Cell Sequencing Analysis", " 6 Biology &amp; Bioinformatics 6.1 R in Biology 6.1.1 Overview 6.1.2 Learning Objectives 6.1.3 Skill List 6.2 Bioconductor 6.2.1 Overview 6.2.2 Learning Objectives 6.2.3 Skill List 6.3 Gene Expression 6.3.1 Overview 6.3.2 Learning Objectives 6.3.3 Skill List 6.4 Differential Expression: Microarrays 6.4.1 Overview 6.4.2 Learning Objectives 6.4.3 Skill List 6.5 Gene Set Enrichment Analysis 6.5.1 Overview 6.5.2 Learning Objectives 6.5.3 Skill List 6.6 High Throughput Sequencing 6.6.1 Overview 6.6.2 Learning Objectives 6.6.3 Skill List 6.7 Differential Expression: RNASeq 6.7.1 Overview 6.7.2 Learning Objectives 6.7.3 Skill List 6.8 Biological Network Analysis 6.8.1 Overview 6.8.2 Learning Objectives 6.8.3 Skill List 6.9 Single Cell Sequencing Analysis 6.9.1 Overview 6.9.2 Learning Objectives 6.9.3 Skill List "],["data-visualization.html", "7 Data Visualization 7.1 Grammar of Graphics 7.2 Responsible Plotting 7.3 Network Viz", " 7 Data Visualization 7.1 Grammar of Graphics 7.1.1 Overview 7.1.2 Learning Objectives 7.1.3 Skill List 7.2 Responsible Plotting Good plots empower us to ask good questions. - Alberto Cairo, How Charts Lie 7.2.1 Overview 7.2.2 Learning Objectives 7.2.3 Skill List 7.3 Network Viz 7.3.1 Overview 7.3.2 Learning Objectives 7.3.3 Skill List "],["engineering.html", "8 EngineeRing 8.1 Toolification 8.2 Parallel Processing 8.3 R Packages", " 8 EngineeRing 8.1 Toolification 8.1.1 Overview 8.1.2 Learning Objectives 8.1.3 Skill List 8.2 Parallel Processing 8.2.1 Overview Consider not using R for any legitimate development. 8.2.2 Learning Objectives 8.2.3 Skill List 8.3 R Packages 8.3.1 Overview 8.3.2 Learning Objectives 8.3.3 Skill List "],["rshiny.html", "9 RShiny 9.1 Overview 9.2 Learning Objectives 9.3 Skill List", " 9 RShiny 9.1 Overview 9.2 Learning Objectives 9.3 Skill List "],["communicating-with-r.html", "10 Communicating with R 10.1 Overview 10.2 Learning Objectives 10.3 Skill List", " 10 Communicating with R 10.1 Overview 10.2 Learning Objectives 10.3 Skill List "],["contribution-guide.html", "11 Contribution Guide 11.1 Custom blocks 11.2 ", " 11 Contribution Guide This page contains some conventions and tips for writing material for this book. 11.1 Custom blocks There are a number of custom blocks that can be used for highlighting salient information throughout the text. The code inside the corresponding block below can be used to insert different standout boxes: ::: {.box .tldr} This is a tl;dr ::: ::: {.box .note} This is a note ::: ::: {.box .hint} This is a hint ::: ::: {.box .important} This is an important ::: ::: {.box .warning} This is a warning ::: 11.2  "],["assignments.html", "12 Assignments 12.1 Overview", " 12 Assignments 12.1 Overview GitHub repo templates here "],["assignment-2.html", "Assignment 2 Problem Statement Learning Objectives Skill List Instructions Function Details", " Assignment 2 Problem Statement Learning Objectives Skill List Instructions Our main focus for this assignment are installing packages, manipulating data, and plotting our manipulated data. We will be borrowing from the same started CSV of expression data from BF528s Project 1, available here. The project is laid out as such: main.R test_main.R report.Rmd A skeleton of the functions you need to complete is in main.R. Tests have been pre-written to test your code and help you ensure it is running correctly, these are in test_main.R. Finally, we are also introducing the concept of R Markdown, which for this assignment is report.Rmd. The document itself goes into greater detail, but you will: 1. Complete the functions in main.R and use testthat:test_file('test_main.R') to ensure they work correctly. 2. Read the R Markdown file and complete the section called Assignment. To do this, you can source('main.R') to bring over the functions you wrote in step one. 3. Finally, annotate the functions you wrote and Knit the R Markdown report, complete with your additional comments and code execution. This page will go into detail on how the functions and their associated tests should work. Function Details 1. Bioconductor While many useful R packages can be loaded through CRAN using the install.packages() syntax, a lot of specifically bioinformatics packages are exclusively released on Bioconductor. For this assignment we only need the package called biomaRt. R programmers fancy themselves very clever, so Rs show up a lot. Naturally, we want to load our packages at the beginning of our script so all of the code we write beneath can access it as it runs. However, if a user or ourselves already has this package installed we dont want to waste their time installing it again. The function require() can help us avoid unnecessary installation time and will help us develop faster. The Bioconductor link above has an example of this method. This section is untested. 2. load_expression() Perhaps the most integral part of using the many data wrangling abilities of R is actually entering your data into the R environment. While there are many ways to do this in R, we ultimately want this data to be in a tibble, which means the current form of the CSV will make R very angry if you attempt to load it in. This is because tibbles dont support row names very well, and the first column of our data doesnt have a name. Try to use the load_expression() data to load data from a filename parameter and return a tibble of that information. I called my firs column probeids. Tests The tests for this function are: test_that(&quot;loading csv correctly&quot;, { result_tib &lt;- load_expression(&quot;/project/bf528/project_1/data/example_intensity_data.csv&quot;) expect_equal(dim(result_tib), c(54675, 36)) expect_true(is_tibble(result_tib)) }) This test uses the load_expression() you write to store the returned tibble in result_tib. Heads up Note that this test is using the same file for data input as you are, but this may not always be the case. The test then compares the dimensions of that result, and expects 54,675 rows and 36 columns. These are the dimensions of the input CSV. It also checks to confirm it is a tibble object (because tibbles are better than dataframes). 3. filter_15() In order to filter the numerous rows we have for this data, we introduce a function that filters the probe IDs in the tibble our data is stored in. We want to capture probes that have a suitably high level of expression, so we are setting log2(15) as the cutoff for an expression level. We will keep a row if 15% of the values in that row exceed log2(15) (about 3.9). Since we may want to examine the probe IDs we find, the function simple returns the values of the probe IDs (column 1) instead of returning the entire tibble. This function presents an important concept in R: using built-ins to speed up our code. Built-ins are functions and packages that are optimized to process data in a certain way. Since were looking at each row of a table, we could simply use a for loop to iterate one row at a time. This is slow, though, and for this function might take 5-10 seconds to run (a long time for a program like this!). Instead, you could use a function like apply() or lapply() to filter every row at once. This solution takes mere moments. Tests library(tibble) test_that(&quot;the correct rows are filtered out in filter_15()&quot;, { test_tib &lt;- tibble(probeids=c(&#39;1_s_at&#39;, &#39;2_s_at&#39;, &#39;3_s_at&#39;, &#39;4_s_at&#39;), GSM1=c(1.0, 3.95, 4.05, 0.5), GSM2=rep(1.6, 4), GSM3=rep(2.5, 4), GSM4=rep(3.99, 4), GSM5=rep(3.0, 4), GSM6=rep(1.0, 4), GSM7=rep(0.5, 4)) expect_equal(pull(filter_15(test_tib)), c(&quot;2_s_at&quot;, &quot;3_s_at&quot;)) }) In order to test this function, we create a small sample tibble of expression data containing only seven samples and four IDs. Two of the rows do have more than 15% of their values exceeding log2(15), the other two do not. This test ensures that filter_15() selects the correct rows. Creating a small sample table like this can be very useful when testing your own code since you dont need to look at a large amount of data to see if its working correctly or not. 4. affy_to_hgnc() This is an important, but sometimes painful, part of using R. There is a great built-in package for connecting to Ensembl (a database of genomic information for many species) called biomaRt. We will use biomaRt to connect the affymetrix probe IDs to more recognizable HGNC gene IDs. The problem is that biomaRt depends on an external API (application program interface) to retrieve data, and this connection sometimes (oftentimes) doesnt work. While their may be more nuanced approaches to an unstable resource like this like automatically retrying failed connections, the best advice for the time being is to try running this function a few times if it doesnt work at first. The errors are clear when it comes to a failed connection, so know that when you get to this stage it likely isnt your codes fault. To build a biomaRt query, read the documentation in section 3 here. The biomart you should use is ENSEMBL_MART_ENSEMBL, the data set hsapiens_gene_ensembl, and you want to find the attributes c(\"affy_hg_u133_plus_2\", \"hgnc_symbol\"). The data you filter using filter_15() returns a list of affy_hg_u133_plus_2 probe IDs, and the gene names were interested in are stored in hgnc_symbol. This function should return a tibble, but biomaRts getBM() will only accept and return a data.frame. You can use dplyr::pull() to turn a tibble into a simple character vector, and dplyr::as_tibble() to go from a data frame to a tibble. Tests test_that(&quot;affy ids can be converted to HGNC names properly using affy_to_hgnc()&quot;, { # biomaRt super buggy so we can try to account for not connecting well response &lt;- try(affy_to_hgnc(tibble(&#39;1553551_s_at&#39;)), TRUE) if (grepl(&quot;Error&quot;, response[1])) { expect_warning(warning(&quot;Could not connect to ENSEMBL.&quot;)) } else { expect_equal(response$hgnc_symbol, c(&quot;MT-ND1&quot;, &quot;MT-TI&quot;, &quot;MT-TM&quot;, &quot;MT-ND2&quot;)) } }) As fun as it is to try to get biomaRt function to connect correctly, it is even more fun to test them. Since a failure to connect doesnt indicate an actual failure in our code we must use a try() block in order to capture if there is a connection error. Note The try() function is a part of programming called error-handling which extends to many other languages. We often expect errors when running our programs (such as right now) but dont want to shut down our entire operation if its an error we can expect. Using try, except, and finally (the latter two not appearing here) we can account for issues outside of our control and adapt our code to change the outcome. Using try except is not a replacement for writing code that doesnt generate errors. If you can avoid an error in the first place, that is far better than using error-handling. In this case we try() to use affy_to_hgnc() to connect to Ensembl and store the resulting error in response. We then check: if there is an Error then we throw a warning() to our testing output. This doesnt stop further testing from happening, but it ensures we know that something isnt quite right. If the response does not contain an error, we simply test that it returned the correct gene symbols for our random affy probe ID of choice. 5. reduce_data() We have one final step in manipulating our data before we plot it. We have our original data, the probe IDs and their associated HGNC symbols, and a list of good gene names and bad gene names. reduce_data takes these four inputs and returns a tibble that has reduced our expression data to only the genes of interest and has a column describing which set of genes it belongs to (good or bad). Changing the shape of the data is incredibly useful for ggplot, the tidyverse package we will use for plotting. While there is flexibility when using ggplot to plot data, having data in long format is typically ideal. Once again, there are multiple ways to reorganize data in this way. We used the base function match() to connect our probe IDs to with our HGNC IDs in name_ids. We then used tibble::add_column() to insert the new data in the correct location. Finally, we created two tibbles of good and bad genes using which() and the %in% modifier. Which evaluates true conditions across a range of data, so we can pass the list of genes we want and select the correct ones. For instance: library(tibble) tib &lt;- tibble(gene = c(&quot;gene1&quot;, &quot;gene2&quot;, &quot;gene3&quot;, &quot;gene4&quot;), affy = c(&quot;a_s_1&quot;, &quot;a_s_2&quot;, &quot;a_s_3&quot;, &quot;a_s_4&quot;)) which(tib$gene %in% c(&quot;gene2&quot;, &quot;gene3&quot;)) # returns the index of the TRUE rows ## [1] 2 3 tib$affy[which(tib$gene %in% c(&quot;gene2&quot;, &quot;gene3&quot;))] # use [] to get data back ## [1] &quot;a_s_2&quot; &quot;a_s_3&quot; Once again, there are many ways to reshape this data (some maybe more elegant than this!) and all we need is the data to be correctly shaped when it is returned. Tests test_that(&quot;reduce_data() is correctly changing the size and shape of the tibble&quot;, { t_tibble &lt;- tibble(probeids = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;, &quot;3_s_at&quot;), GSM1 = c(9.5, 7.6, 5.5), GSM2 = c(9.7, 7.2, 2.9), GSM3 = c(6.9, 4.3, 6.8)) names &lt;- tibble(affy_hg_u133_plus_2 = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;, &quot;3_s_at&quot;), hgnc_symbol = c(&quot;A-REAL-GENE&quot;, &quot;SONIC&quot;, &quot;UTWPU&quot;)) good &lt;- c(&quot;A-REAL-GENE&quot;) bad &lt;- c(&quot;SONIC&quot;) reduce_test &lt;- reduce_data(t_tibble, names, good, bad) result &lt;- tibble(probeids = c(&quot;1_s_at&quot;, &quot;2_s_at&quot;), hgnc_symbol = c(&quot;A-REAL-GENE&quot;, &quot;SONIC&quot;), gene_set = c(&quot;good&quot;, &quot;bad&quot;), GSM1 = c(9.5, 7.6), GSM2 = c(9.7, 7.2), GSM3 = c(6.9, 4.3)) expect_equal(reduce_test, result) }) In order to test a function that changes a tibble, we need to use a tibble. We only create a test table that is three rows by four columns, but that is enough to get the gist of the function. We simply pass the four parameters to reduce_data() and we expect it to create a tibble like result. Ensure your output column names are the same here or your tests my fail. While not crucial to the success of your assingment, maintaining correct column names across multiple data transformations is an important skill. "],["assignment-6.html", "Assignment 6 Problem Statement Learning Objectives Skill List", " Assignment 6 Problem Statement Plotting data is a useful but oftentimes complicated skill set. While R has many tools to help simplify and create attractive and useful graphs, they can be difficult to utilize and have many pitfalls. This assignment will teach you the most important parts of plotting with R using the package ggplot. Learning Objectives The basic operations of ggplot and The Grammar of Graphics. Using differential gene expression analysis packages to generate data for plotting. Creating simple plots and not using the default colors and themes. Combining multiple plots into one image. Skill List A tempered heart and mind that understands a plot may not ever look exactly how you want it to. An intermediate understanding of Rs most popular plotting package, ggplot. Further understanding of differential expression analysis and its plotting. A sense of superiority whenever you see a publication use the base R plotting package for figures. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
