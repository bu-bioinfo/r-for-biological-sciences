[{"path":"index.html","id":"r-for-biological-sciences","chapter":"R for Biological Sciences","heading":"R for Biological Sciences","text":"Semester: Spring 2023Location: HAR 211 zoomTime: M/W 2:30PM - 4:15PMSite last updated 1/4/2023Contents:Course ScheduleInstructorsOffice HoursCourse Values PoliciesThis course introduces R programming language lens \npractitioners biological sciences, particularly biology \nbioinformatics. Key concepts patterns language covered,\nincluding:RStudioData wrangling tidyverseData visualization ggplotEssential biological data shapes formatsCore bioconductor packagesBasic data exploration, including elementary statistical modeling summarizationElementary Data Science concepts“Toolifying” R scriptsCommunicating R code results RMarkdownBuidling R packages unit testing strategiesBuilding interactive tools RShinyAbout 1/3 materials inspired online textbook R Data\nScience, rest developed \npracticing bioinformaticians based experiences.Weekly programming assignments help students apply techniques \nrealistic problems involving analysis visualization biological data.\nStudents introduced unit testing paradigm help write\ncorrect code deposit code github evaluation. Students\nimplement end--end project begins one set provided\ndatasets, implements set data summarization exploration operations \n, allows interaction RShiny app.course materials aligned BF528 Applications Translational\nBioinformatics intended taken tandem,\nmaterials also stand alone independent class.","code":""},{"path":"index.html","id":"course-schedule","chapter":"R for Biological Sciences","heading":"Course Schedule","text":"Key:bold - class held zoomstruckout - class held, reserved project workFollow Week N links detailed list topic sections.Assignments assigned due first day class week unless\nmentioned otherwisePreliminariesData BiologyR Programming BasicsData Wrangling & tidyverse BasicsR BiologyBio: Bioconductor BasicsData Viz: Grammar GraphicsData Sci: Data ModelingBio: Gene ExpressionBio: MicroarraysBio: Differential Expression: MicroarraysData Sci: PCA & ClusteringData Vis: Heatmaps & DendrogramsR Programming: Structures & IterationBio: High Throughput SequencingBio: RNASeqBio: Count DataBio: Differential Expression: RNASeqBio: Gene Set Enrichment AnalysisData Sci: DistributionsData Sci: Statistical TestsR Programming: Style & ConventionsData Viz: Responsible PlottingRShinyEngineeRingBio: Biological NetworksData Sci: Network AnalysisData Viz: Network VizProject work, classProject work, classProject work, classCourse wrap & feedbackFirst day exam period","code":""},{"path":"index.html","id":"instructors","chapter":"R for Biological Sciences","heading":"Instructors","text":"Primary instructor: Adam Labadorf (labadorf bu DOT edu)following lovely people helping course, particular\norder (ok, guess ’s alphabetical):Regan Conrad (BU BF PhD Candidate)Aubrey (Brie) Odom-Mabey (BU BF PhD Candidate)","code":""},{"path":"index.html","id":"office-hours","chapter":"R for Biological Sciences","heading":"Office Hours","text":"Arranged needed. consistent need required, fixed hours \nestablished.","code":""},{"path":"index.html","id":"course-values-and-policies","chapter":"R for Biological Sciences","heading":"Course Values and Policies","text":"Everyone welcome. Every background, race, color, creed, religion, ethnic\norigin, age, sex, sexual orientation, gender identity, nationality welcome\ncelebrated course. Everyone deserves respect, patience, \nkindness. Disrespectful language, discrimination, harassment kind \ntolerated, may result removal class University. \nmerely BU policy.\ninstructors deem principles inviolable human rights. Students\nfeel safe reporting instances discrimination \nharassment instructor, Bioinformatics Program leadership,\nBU Equal Opportunity Office.Everyone brings value. us brings unique experiences, skills, \ncreativity course. diversity greatest asset.Collaboration highly encouraged. students encouraged work\ntogether seek available resources completing projects\naspects course, including sharing coding ideas strategies \nwell found internet. available\nresources may brought bear. However, consistent BU policy, bulk\ncode final reports written words \nrepresent work understanding material. Copying/pasting large\nsections code acceptable investigated cheating (check).safe space dissent. complex topics covered \nclass, seldom one correct answer, approach, solution. Disagreement\nfosters innovation. course, including students TAs, \nencouraged express constructive criticism alternative ideas \naspect content.always learning. knowledge understanding always\nincomplete. Even experts fallible. bioinformatics field evolves\nrapidly, Rome built day. kind others.\nalways smarter knowledgeable today yesterday.","code":""},{"path":"index.html","id":"acknowledgements-contributions","chapter":"R for Biological Sciences","heading":"Acknowledgements & Contributions","text":"materials possible without contributions \nDakota Hawkins, Vanessa Li, Taylor Falk, Mae Rose Gott, Joey Orofino.Former valiant (probably still) attractive TAs:","code":""},{"path":"index.html","id":"section","chapter":"R for Biological Sciences","heading":"2022","text":"Taylor Falk (BU BF MS Alumnus ’21) Bioinformatics Developer working\nVA PTSD Brain Bank, developing infrastructure support data\ngenerated brains. championing assignment strategy \navailable help issues.Mae Rose Gott (BU BF MS Alumna ’21) Research Staff member working\nnumber different projects across many different areas. \nhelping organize course materials go forward.Vanessa Li (BU BF PhD Candidate) PhD candidate Dr. Stefano Monti’s\nlab Computational Biomedicine. primarily helping grading\nassignments.Joey Orofino (BU BF MS Alumnus ’18) Research Scientist working \nidentifying small RNA based biomarkers Parkinson’s Disease","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"SlidesSince publication first draft human genome 2001, data driven\nbiological discovery exponential rate. Rapid technological innovations \ndata-generating biochemical instruments, computational resource availability,\ndata storage, analytical approaches including artificial intelligence,\nmachine learning, data science generally combined synergized\nenable advances understanding biological systems orders \nmagnitude. rate development technologies increased, \npractitioners biological inquiry expected keep rapidly\nexpanding set knowledge, skills, tools required use .Modern biological data analysis entails combining knowledge skills \nmany domains, including biological concepts like molecular biology,\ngenetics, genomics, biochemistry, also computational quantitative\nskills including statistics, mathematics, programming software engineering,\nhigh performance cloud computing, data visualization, computer science.\none person can expert areas, modern software tools \npackages made available subject matter experts enable us perform cutting\nedge analysis conceptual understanding topics.One tool R programming language, \nstatistical programming language environment specifically designed run\nstatistical analyses visualize data. Today\nR one two popular programming languages biological data\nanalysis bioinformatics (python).major innovation R language came introduction \ntidyverse, set open-source data manipulation \nvisualization packages, first developed Hadley Wickham\nnow improved, supported, maintained team data scientists \nsoftware engineers individuals. tidyverse collection \npackages specialize different aspects data manipulation goal\nenabling powerful, consistent, accurate data operations broad\nfield data science. changing structure language per\nse, tidyverse packages define set consistent programming conventions\npatterns tailored types manipulations required make\ndata “tidy” , therefore, easier consistent work . \ntidyverse therefore something “language” compatible \ndistinct convention base R language.book accompanying course focus use R related package\necosystems analyze, visualize, communicate biological data analyses. \nnoted , effective biological data analysis employs skills span several\nknowledge domains. book covers many topics relatively shallow\ndepth, intent presenting just enough enable \nlearner become proficient day--day biological analysis tasks.","code":""},{"path":"introduction.html","id":"who-this-book-is-for","chapter":"1 Introduction","heading":"1.1 Who This Book Is For","text":"SlidesThis book written practicing biologist wishing learn use\nR analyze biological data. basic working knowledge genetics, genomics,\nmolecular biology, biochemistry assumed, endeavored include\nenough pertinent background understand analysis concepts presented \ntext. Basic knowledge statistics assumed, background \nprovided necessary understand analyses concepts text. \nknowledge assumed.","code":""},{"path":"introduction.html","id":"a-note-about-reinventing-the-wheel","chapter":"1 Introduction","heading":"1.2 A Note About Reinventing the Wheel","text":"Many topics book covered elsewhere greater detail depth. \ncontent section intended stand alone, may provide high\nlevel detail done better others online materials. \nsections provide links resources provide information,\ncase instructions book terse unclear.Wikipedia - Reinventing Wheel","code":""},{"path":"introduction.html","id":"sources-and-references","chapter":"1 Introduction","heading":"1.3 Sources and References","text":"materials book inspired informed large number \nsources, including books freely available online materials. authors\nlike thank creators maintainers resources \ngenerosity making valuable contributions:R MaterialsHands-Programming R, Garrett GrolemundR Data Science, Hadley Wickam, Garrett Grolemund, et alAdvanced R, Hadley WickamSTAT 545 - Data wrangling, exploration, analysis RWhat Forgot Teach RReproducible Analysis R, State Alaska’s Salmon People Project, NCEASData Science Psychologists, Hansjörg NethData visualizationHow Charts Lie: Getting Smarter Visual Information, Alberto CairoThe Functional Art - Introduction Information Graphics Visualization, Alberto CairoThe Truthful Art - Data, Charts, Maps Communication, Alberto Cairo","code":""},{"path":"data-bio.html","id":"data-bio","chapter":"2 Data in Biology","heading":"2 Data in Biology","text":"","code":""},{"path":"data-bio.html","id":"a-brief-history-of-data-in-molecular-biology","chapter":"2 Data in Biology","heading":"2.1 A Brief History of Data in Molecular Biology","text":"SlidesMolecular biology became data science 1953 structure DNA \ndetermined. Prior advance, biochemical assays biological systems\nmake general statements characteristics composition \nbiological macromolecules (e.g. two types nucleic acids - \nmade ribose (RNA) deoxyribose (DNA)) quantitative statements\ncompositions (e.g. roughly equal concentrations purines\n- adenine, guanine - pyrimidines - cytosine, thymine - single\nchromosome). However, shown nucleic acid molecule \nspecific (eventually\nmeasurable) sequence, \nopened possibility defining genetic signature every living thing\nEarth , principle, enable us understand life works \nbasic components. tantalizing prospect, say least.perhaps happy coincidence computational data storage\ntechnologies began developing around time molecular biology\nadvances made. mechanical\ncomputers existed \nhundred years arguably\nlonger, first modern\ncomputer, Atanasoff-Berry\nComputer (ABC), \ninvented John Vincent Atanasoff Clifford Berry 1942 now\nIowa State University. following decades, speed, sophistication,\nreliability computing machines increased exponentially, enabling ever\nlarger faster computations performed.development computational capabilities necessitated technologies \nstored information machines use, instructions tell\ncomputers operations perform data use \nperform . 1950s, commonly available mechanical data\nstorage technologies like\nwriting, phonographic\ncylinders disks (.k..\nrecords), punch cards \nimpractical unsuitable create store amount data needed \ncomputers. newer technology, magnetic storage, originally proposed 1888 \nOberlin\nSmith\nquickly became standard way digital computers read stored information.technological advances, second half 20th century saw rapid\nadvances ability determine study properties function \nbiological molecular sequences, primarily DNA RNA (although first\nbiological sequences scientists determined proteins composed amino acids\nusing methods independently invented Frederick\nSanger\nPehr Edman).1970, Pauline Hogeweg \nBen Hesper defined new discipline bioinformatics “study \ninformatic processes biotic systems” (fact, original term \nproposed Dutch,\nHogeweg Hesper’s native language). early form bioinformatics \nsubfield theoretical\nbiology,\nstudied recognized biological systems, much like computer\nsystems, can viewed information storage processing systems .broad definition bioinformatics began narrowing practice study\ngenetic information amount molecular sequence data collected\ngrew. early 1980s, biological sequence data stored magnetic tape \ncreated studied using new pattern recognition\nalgorithms computers,\nbecoming widely available academic research institutions.\ntime, idea determining complete sequence human\ngenome born, leading inception Human Genome\nProject present modern\npost genomic era.Biological Data Timeline - Setting Stage","code":""},{"path":"data-bio.html","id":"biology-as-a-mature-data-science","chapter":"2 Data in Biology","heading":"2.2 Biology as a Mature Data Science","text":"SlidesThe completion first draft human genome ushered revolution \nunderstand humans, evolutionary history, ancestry, \ntraits, health. provided fundamentally new empirical tools \napproaches human genetic biomedical research, technologies \ntechniques developed completion draft sequence formed\nfoundation genetic research non-human systems well.Biological Data Timeline - Human Genome EraWhile focus human genome project determining DNA sequence\nhuman genome, sequence technologies used ascertain \nprovide us opportunities learn many properties genomes \nbiological systems analyzing data different approaches. example,\nknowing complete sequence genome also provides information \nnumber genes contains, repetitive sequence , combined\ngenetic sequences individuals organisms, closely related\ngenes even organisms whole . Thanks central\ndogma molecular\nbiology, \ngene sequences also give us information intermediate RNA molecule \nresultant proteins encoded genome, creating opportunities new ideas,\nhypotheses, experiments, even new data-generating assays approaches.\nadvances causing exponential growth different types biological\ndata volume, necessitating ever powerful sophisticated\ncomputational resources analytical methods signs slowing.biochemical instruments used produce data continually improving\nprecision, accuracy, throughput, cost output operations.Biologist’s Tools","code":""},{"path":"preliminaries.html","id":"preliminaries","chapter":"3 Preliminaries","heading":"3 Preliminaries","text":"","code":""},{"path":"preliminaries.html","id":"prelim-r","chapter":"3 Preliminaries","heading":"3.1 The R Language","text":"SlidesR free programming language environment language can \nused. specifically, R statistical programming language, designed\nexpress exclusive purpose conducting statistical analyses \nvisualizing data. Said differently, R general purpose programming\nlanguage unlike languages python, Java, C, etc. , \nlanguage’s real strengths manipulation, analysis, visualization\ndata statistical procedures, though often used purposes\n(example, web applications RShiny, writing books like one\nbookdown). may download R \nfree Comprehensive R Archive Network.effective biological analysis practitioner knows use multiple tools\nappropriate purposes. common programming languages \nfield python, R, scripting languages like\nBourne Shell - bash. \nbeyond scope book cover tools best used , R \nappropriate wherever data analysis visualization needed. operations\ninvolve aspects (e.g. manipulating text files, programming web\nservers, etc) likely suitable languages software.R Project Home PageHands-Programming R - Installing R RStudioSection R R Data Science","code":""},{"path":"preliminaries.html","id":"prelim-rstudio","chapter":"3 Preliminaries","heading":"3.2 RStudio","text":"SlidesThis course assumes learner using RStudio\nsoftware platform analyses, unless otherwise noted. RStudio freely\navailable fully featured integrated development environment (IDE) R, \nmany convenient capabilities learning R. RStudio may downloaded \ninstalled free site .examples instructions book assume installed\nR using RStudio. sure turn automatic environment\nsaving RStudio! important, :default, RStudio preserves R environment shut \nrestores start . bad practice!state R environment, includes values stored variables,\nR packages loaded, etc. previously executed code transient may\nreflect results code produces run alone.Open Tools > Global Options… menu :Uncheck “Restore .RData workspace startup”Set “Save workspace .RData exit:” “Never”Never save workspaceThe book R Data Science excellent\nchapter \nproblem change RStudio setting avoid .Hands-Programming R - Installing R RStudioRStudio Education - Beginners GuideModernDrive R Series - Getting StartedR Data Science - real, analysis “live”?Forgot Teach R - Always start R blank slateR Data Science - RStudio","code":""},{"path":"preliminaries.html","id":"prog-the-r-script","chapter":"3 Preliminaries","heading":"3.3 The R Script","text":"SlidesBefore cover R language , talk run\ncode live. mentioned, R \nprogramming language environment can run code written \nlanguage. environment program (confusingly also called R) allows\ninteract run simple lines code one time. \nenvironment useful learning language works \ntroubleshooting, suitable recording running large, complex\nanalyses require many lines code. Therefore, important R code\nwritten saved file run ! code may \ncorrect, interactive R environment helpful debugging \ntroubleshooting, soon code works saved file\nrerun .mind, basic unit R analysis R script. R\nscript file contains lines R code run sequentially unit\ncomplete one tasks. Every R script file name, choose\ndescriptive concise script ; script.R,\ndo_it.R, \na_script_that_implements_my_very_cool_but_complicated_analysis_and_plots.R\ngenerally poor names scripts, whereas analyze_gene_expression.R might\nsuitable.RStudio, can create new script file current directory using \nFile -> New File -> R Script menu item new R Script button top\nscreen:New R ScriptYour RStudio configuration now enable write R code \n(currently unsaved) file top left portion screen (labeled \nfigure “File Editor”).Basic RStudio InterfaceYou now nearly ready start coding R!name files \nuseful advanced tips name files","code":""},{"path":"preliminaries.html","id":"prog-workflow","chapter":"3 Preliminaries","heading":"3.4 The Scripting Workflow","text":"SlidesBut hold , ’re still quite ready start coding. mentioned ,\nimportant R code written saved file run\n! scripts quickly contain many lines code meant\nrun sequential order. developing code helpful \nrun individual line separately, building script incrementally \ntime. illustrate , begin simple R code \nstores result arithmetic expression new variable:concepts line code covered greater depth later, \nnow intuitive understanding suffice explain development\nworkflow RStudio.developing, suggested sequence operations:Save file (naming necessary first save) Ctrl-s \nWindows Cmd-s MacExecute line lines code script wish evaluate using\nCtrl-Enter Windows Cmd-Enter Mac. default line \ncursor executed; may click drag mouse select\nmultiple lines execute needed. NB: can press arrow key \nrecall previously run commands console.executed code evaluated Console window, may\ninspect result modify code necessary.may inspect definitions variables declared \nEnvironment tab upper right.verified code executed intend,\nensure code file started updated appropriately.Go step 1The steps depicted following figure:RStudio workflowOver time, gain comfort workflow become flexible\nuse RStudio.followed instructions prevented RStudio\nsaving environment exit program (! \nmention ?!), none results code previously ran \navailable upon starting new RStudio session. Although may seem\ninconvenient, excellent opportunity verify script \ncurrent state intend .extremely easy ask R things don’t mean !Rerunning scripts beginning new RStudio session \nexcellent way guard kind error. short page summarizes\nwell, read :Forgot Teach R - Always start R blank\nslateR Data Science - Workflow: scriptsRStudio IDE cheatsheet\n(scroll page find cheatsheet entitled “RStudio IDE cheatsheet”)","code":"\n# stores the result of 1+1 into a variable named 'a'\na <- 1+1"},{"path":"preliminaries.html","id":"prelim-git","chapter":"3 Preliminaries","heading":"3.5 git + github","text":"Slides","code":""},{"path":"preliminaries.html","id":"motivation","chapter":"3 Preliminaries","heading":"3.5.1 Motivation","text":"SlidesBiological analysis entails writing code, changes time \ndevelop , gain insight data, identify new questions \nhypotheses. common pattern developing scripts make copies older\ncode files preserve making new changes . \ngood idea maintain record code previously ran, time\npractice often leads disorganized, cluttered, untidy analysis\ndirectories.example, say working script named my_R_script.R decide\nwant add new analysis substantially changes code. might \ntempted make copy current version code new file\nnamed my_R_script_v2.R make changes , leaving original\nscript intact untouched going forward. make changes new\nscript, produce stunning fascinating plots, present analysis \ngroup meeting, discover later critical bug code \nmade plots misleading requires substantial redevelopment.Bugs happen. two types bugs:Syntax bugs: bugs due incorrect language usage, R tell \ncan (usually) easily identified fixedLogic bugs: code write syntactically correct, something\nintendBugs normal. scenario described , present results \ndiscover code wasn’t thought , extremely\ncommon happen . normal, finding bug \ncode mean bad programmer.Rather edit version 2 script directly, decide \nsensible copy file my_R_script_v2_BAD.R edit version 2\nscript fix bug. satisfied new version 2 script, \nmake new copy my_R_script_v2_final.R. Upon review analysis, \nasked implement new changes script based reviewer feedback. \nmake new copy script my_R_script_v2_final_revision.R make\nrequested changes. Perhaps now script final, directory\nnow five different versions analysis:write code, may know scripts , \nfollow good programming practice carefully commented \ncode successors may able sleuth done. However, \ntime passes, intimate knowledge thought code \nreplaced immediately important things; eventually may even\nunderstand even recognize code, let alone someone else trying \nunderstand . ideal situation case. better solution involves\nrecording changes code time way can recover old code\nneeded, don’t clutter analytical workspace unneeded files.\ngit provides efficient solution problem.","code":"my_R_script.R\nmy_R_script_v2.R\nmy_R_script_v2_BAD.R\nmy_R_script_v2_final.R\nmy_R_script_v2_final_revision.R"},{"path":"preliminaries.html","id":"git","chapter":"3 Preliminaries","heading":"3.5.2 git","text":"Slidesgit free, open source version\ncontrol software program.\nVersion control software used track record changes code time,\npotentially many developers working software project\nconcurrently different parts world. base git software can \nused command line, graphical user interface\napplications popular operating systems.many excellent tutorials online (linked ) teach \nuse git basic concepts described . command line\ncommands listed, operations apply graphical clients.repository (repo) collection files directory \nasked git track (run git init new directory)file wish track must explicitly added repo (run\ngit add <filename> within git repo directory)modify tracked file, git notice differences show\ngit statusYou may tell git track changes explicit files changed (also\nrun git add <filename> record changes)set tracked changes stored repo making commit.\ncommit takes snapshot tracked files repo \ntime commit made (run git commit -c <commit message> concise\ncommit message describes done)commit date time associated . files \nrepo can reset exactly state commit,\nthus preserving previous versions code.vast majority use cases, git init, git status, git add, git commit operations need use git\neffectively. Two commands, git push git pull needed \nsharing code others described next section.Official git tutorial videosOfficial git bookGit Immersion - guided tour git\ncommandsDataCamp - Git data scientists","code":""},{"path":"preliminaries.html","id":"git-hosting-platforms-github","chapter":"3 Preliminaries","heading":"3.5.3 Git hosting platforms (GitHub)","text":"SlidesThe basic git software works local computer local\nrepositories. share code others, receive others’ contributions,\ncopy repo must made available centralized location \neveryone can access. One place github.com, \nfree web application hosts git repos.\nbitbucket.org another popular free git repo hosting\nservice. two services practically , focus \nGitHub.formal relationship git GitHub. git open source\nsoftware project maintained hundreds developers around world (\nhosted GitHub). GitHub independently\nprovided web service application. connection GitHub git\nGitHub hosts git repos.git, many excellent tutorials use GitHub, \nbasic concepts described .First must create account GitHub don’t one alreadyThen, create new repo GitHub wish contain codeThe next step depends whether existing local repo :already local git repo: Follow instructions \nGitHub clone GitHub repo create local copy connected\none GitHubIf already local git repo: Follow instructions GitHub\nconnect local repo GitHub one (called “adding \nremote”)Now, local repo connected repo GitHub, changes\nmake local files can sent, pushed repo GitHub:Make changes local files, git add git commit \naboveUpdate remote repo GitHub pushing local commits git   pushRunning git status indicate whether local repo date\nremote GitHub repoWhen working team contributors GitHub repo, local\nfiles become date others push changes. ensure local\nrepo date GitHub repo, must pull changes \nGitHub git pull.git designed automatically combine changes made code base \ndifferent developers whenever possible. However, two people make changes \nparts file, git may able resolve changes\ndevelopers must communicate decide code \n. instances called merge conflicts can challenging \nresolve. Dealing merge conflicts beyond scope book, \nresources linked reading.content code book stored available \nGitHub, \nassignment code templates.Official GitHub TutorialFreeCodeCamp - Git GitHub BeginnersOfficial GitHub Tutorial Merge Conflicts","code":""},{"path":"prog-basics.html","id":"prog-basics","chapter":"4 R Programming","heading":"4 R Programming","text":"","code":""},{"path":"prog-basics.html","id":"before-you-begin","chapter":"4 R Programming","heading":"4.1 Before you begin","text":"done already, sure follow R Language,\nRStudio, R Script, \nScripting Workflow sections working \nchapter. guidance sections set success!","code":""},{"path":"prog-basics.html","id":"introduction-1","chapter":"4 R Programming","heading":"4.2 Introduction","text":"subjects covered far, basic syntax R covered \nwell free online materials. excellent resources \nlinked end section, brief overview concepts \nsyntax covered . code examples can written script\nevaluated described entered R Console directly run \npressing Enter.","code":""},{"path":"prog-basics.html","id":"prog-r-syntax","chapter":"4 R Programming","heading":"4.3 R Syntax Basics","text":"core, R (like programming languages) basically fancy calculator.\nsyntax basic arithmetic operations R familiar :[1] lines output given R preceding expression\nexecuted. portion line starting # comment ignored\nR.R also supports storing values symbolic placeholders called variables, \nobjects. expression like can assigned variable \nname using <- operator:Variables assigned value can placed subsequent\nexpressions anywhere value evaluated:correct way assign value variable R <- syntax,\nunlike many programming languages use =. However, although =\nassignment syntax work R:considered bad practice may cause confusion later. always\nuse <- syntax assigning values variables!R, period . special meaning like many \nlanguages like python, C, javascript, etc. Therefore, new.var valid\nvariable name just like new_var, even though may look strange \nfamiliar languages. including . R variable\nnames valid, results use programs written \nlanguages meaning character. Therefore, good\npractice avoid using . characters variable names reduce \nchances conflicts later.Hands-Programming RR Data Science - Workflow basics","code":"1 + 2 # addition\n[1] 3\n3 - 2 # subtraction\n[1] 1\n4 * 2 # multiplication\n[1] 8\n4 / 2 # division\n[1] 2\n1.234 + 2.345 - 3.5*4.9 # numbers can have decimals\n[1] -13.571\n1.234 + (2.345 - 3.5)*4.9 # expressions can contain parentheses\n[1] -4.4255\n2**2 # exponentiation\n[1] 4\n4**(1/2) # square root\n[1] 2\n9**(1/3) # cube root\n[1] 3\nnew_var <- 1 + 2new_var - 2\n[1] 1\nanother_var <- new_var * 4\nnew_var = 2 # works, but is not common convention!"},{"path":"prog-basics.html","id":"prog-types","chapter":"4 R Programming","heading":"4.4 Basic Types of Values","text":"common type value R number, e.g. 1.0 1e-5 \n\\(10^{-5}\\). practical purposes, R distinguish numbers\nfractional parts (e.g. 1.123) integers (e.g. 1); number \nnumber. addition numbers, types values \nspecial R:logical boolean values - TRUE FALSE. Internally, R stores TRUE\nnumber 1 FALSE number 0. Generally, R interprets\nnon-zero numbers TRUE 0 FALSE, good practice supply\ntokens TRUE FALSE argument expects logical value.missing values - NA. NA special value indicates value \nmissing.missing vectors - NULL. Similar NA, NULL indicates vector,\nrather value, missing. Vectors described next section\ndata strutures.factors - Factors complex type used statistical models \ncovered greater detail laterinfinity - Inf -Inf. values encode R understands \npositive negative infinity, number divided 0.impossible values - NaN. value corresponds mathematically\n‘impossible’ undefined value 0/0.character data - \"value\". R can store character data form \nstrings. Note R interpret string values default, \"1\" 1\ndistinct.dates times - R basic type store dates times (together\ntermed datetime, includes components). Internally, R stores\ndatetimes fractional number days since January 1, 1970, using negative\nnumbers earlier dates.complex numbers - R can store complex numbers using complex\nfunction.Unsurprisingly, R perform computations NA, NaN, Inf values.\nvalues ‘infectious’ quality , \nmixed values, result computation reverts first\nvalues encountered:code produces values numbers expect, suggests\none values input, need handled explicitly.difference NA NaN RR Data Science - Dates date-timesDates times RComplex numbers R","code":"# this how to create a vector of 4 values in R\nx <- c(1,2,3,NA)\nmean(x) # compute the mean of values that includes NA\n[1] NA\nmean(x,na.rm=TRUE) # remove NA values prior to computing mean\n[1] 2\nmean(c(1,2,3,NaN))\n[1] NaN\nmean(c(NA,NaN,1))\n[1] NA"},{"path":"prog-basics.html","id":"prog-struct","chapter":"4 R Programming","heading":"4.5 Data Structures","text":"","code":""},{"path":"prog-basics.html","id":"vectors","chapter":"4 R Programming","heading":"4.5.1 Vectors","text":"Data structures R (languages) ways storing organizing\none value together. basic data structure R one\ndimensional sequence values called \nvector:vector R special property values contained vector\nmust type, list described . \nconstructing vector, R coerce\nvalues \ngeneral type encounters values different types:addition single type, vectors also length, \ndefined number elements vector:Internally, R much efficient operating vectors individual\nelements separately. numeric vectors, can perform arithmetic operations\nvectors compatible size just easily individual values:example , multiplied vector length 2 vector length\n3:Rather raise error aborting, R merely emits warning message \nvectors divisible lengths. R decide third value\n5? R cycles vector multiplies values\nelement-wise longest vector operation performed \nvalues:R sometimes work ways don’t expect. careful read warnings \ncheck code expect!","code":"# the c() function creates a vector\nx <- c(1,2,3)\n[1] 1 2 3c(1,2,\"3\")\n[1] \"1\" \"2\" \"3\"\nc(1,2,TRUE,FALSE)\n[1] 1 2 1 0\nc(1,2,NA) # note missing values stay missing\n[1] 1 2 NA\nc(\"1\",2,NA,NaN) # NA stays, NaN is cast to a character type\n[1] \"1\" \"2\" NA \"NaN\"x <- c(1,2,3)\nlength(x)\n[1] 3c(1,2) + c(3,4)\n[1] 4 6\nc(1,2) * c(3,4)\n[1] 3 8\nc(1,2) * c(3,4,5) # operating on vectors of different lengths raises warning, but still works\n[1] 3 8 5\nWarning message:\nIn c(1, 2) * c(3, 4, 5) :\n  longer object length is not a multiple of shorter object lengthc(1,2) * c(3,4,5) # operating on vectors of different lengths raises warning, but still works\n[1] 3 8 5\nWarning message:\nIn c(1, 2) * c(3, 4, 5) :\n  longer object length is not a multiple of shorter object lengthc(1,2) * c(3,4,5) # yields: 1*3 2*4 1*5\n[1] 3 8 5\nWarning message:\nIn c(1, 2) * c(3, 4, 5) :\n  longer object length is not a multiple of shorter object length\nc(1,2) * c(3,4,5,6) # yields: 1*3 2*4 1*5 2*6\n[1] 3 8 5 12"},{"path":"prog-basics.html","id":"factors","chapter":"4 R Programming","heading":"4.5.2 Factors","text":"Factors objects R uses handle categorical variables, .e. variables\ncan take one distinct set values sample. example,\nvariable indicating whether subject disease control \nencoded using factor values Disease Control. Consider example\ndataset six subjects three disease three control, \ncreate factor corresponding variable character strings using \nfactor() function:factor case_status prints vector labels, either Disease \nControl. distinct values factor called levels, \nfactor two: Control Disease. Internally, factor stored \nvector integers level value:default, R assigns integers levels alphanumeric order; since “Control”\ncomes lexicographically “Disease”, Control level assigned \ninteger 1 Disease assigned 2. value factor corresponds\nintegers, since Disease came Control, numeric values\nfactor (2, 2, 2, 1, 1, 1). integer values assigned level\nallow factor sorted:Note order factor levels changed controls, \nvalue 1, precede disease, value 2. integers assigned \nlevel can specified explicitly creating factor:base R functions reading CSV Files load columns character\nvalues factors default (may turn \nstringsAsFactors=FALSE read.csv()), situations may \nfactors created functions need integer values\nchanged. process called releveling factor, may accomplished\npassing factor factor() function specifying new levels:Controlling order levels factor important number \nsituations. One specifying reference category categorical\nvariables constructing model matrices pass statistical models, \ndetails beyond scope book. second order\ncategorical variables passed ggplot, \ncovered greater detail [Reordering 1-D Data Elements] \nGrammar Graphics chapter. forcats\ntidyverse package provides powerful functions working categorical\nvariables stored factors.\n### MatricesA matrix R simply 2 dimensional version vector. , \nrectangle values type, e.g. number, character,\nlogical, etc. matrix may constructed using vector notation described\nspecifying number rows columns matrix , \nInstead length like vector, \\(m \\times n\\) dimensions:matrix 2 dimensional, can transposed \\(m \\times n\\) \n\\(n \\times m\\) using t() function:Hands-Programming R - Atomic Vectors\nR Data Science - Vectors\nAdvanced R - Vectors","code":"case_status <- factor(\n  c('Disease','Disease','Disease',\n    'Control','Control','Control'\n  )\n)\ncase_status\n[1] Disease Disease Disease Control Control Control\nLevels: Control Diseaseas.numeric(case_status)\n[1] 2 2 2 1 1 1\nstr(case_status)\n Factor w/ 2 levels \"Control\",\"Disease\": 2 2 2 1 1 1sort(case_status)\n[1] Control Control Control Disease Disease Disease\nLevels: Control Diseasecase_status <- factor(\n  c('Disease','Disease','Disease','Control','Control','Control'),\n  levels=c('Disease','Control')\n)\ncase_status\n[1] Disease Disease Disease Control Control Control\nLevels: Control Disease\nstr(case_status)\n Factor w/ 2 levels \"Disease\",\"Control\": 1 1 1 2 2 2str(case_status)\n Factor w/ 2 levels \"Disease\",\"Control\": 1 1 1 2 2 2\nfactor(case_status, levels=c(\"Control\",\"Disease\"))\n Factor w/ 2 levels \"Control\",\"Disease\": 2 2 2 1 1 1# create a matrix with two rows and three columns containing integers\nA <- matrix(c(1,2,3,4,5,6)\n       nrow = 2, ncol = 3, byrow=1\n      )\nA\n[,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\ndim(A) # the dim function prints out the dimensions of the matrix, rows first\n[1] 2 3# A defined above as a 2 x 3 matrix\nt(A)\n[,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\ndim(t(A))\n[1] 3 2"},{"path":"prog-basics.html","id":"lists-and-data-frames","chapter":"4 R Programming","heading":"4.5.3 Lists and data frames","text":"Vectors matrices special property items must \ntype, e.g. numbers. Lists data frames data structures \nrequirement. Similar vectors, lists data frames one\ndimensional sequences values, values can mixed types. \ninstance, first item list may vector numbers, second\nvector character strings. flexible data structures \nR, among commonly used.Lists can created using list() function:arguments passed list() define values order list.\nexample, list two elements: one vector 3 numbers one\nvector 4 character strings. Note can access individual items list\nusing [[N]] syntax, N 1-based index element.Lists can also defined indexed name:elements list assigned names numbers categories\ncreating list, though valid R identifier names can used. \nelements associated names can accessed using list$name\nsyntax.Lists data frames underlying data structure, however differ \none important respect: elements data frame must \nlength, elements list . may create data frame \ndata.frame() function:Note data frame printed matrix element names columns \nautomatically numbered rows. may access specific elements data frame \nnumber ways:examples , operation extracting different parts \nvector, matrix, list, data frame called subsetting. R provides many\ndifferent ways subset data structure discussing beyond\nscope book. However, mastering subsetting help code \nconcise correct. See Read link Subsetting :Advanced R - SubsettingAdvanced R - Data Structures\nAdvanced R - Subsetting","code":"my_list <- list(\n  c(1,2,3),\n  c(\"A\",\"B\",\"C\",\"D\")\n)\nmy_list\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] \"A\" \"B\" \"C\" \"D\"\nmy_list[[1]] # access the first item of the list\n[1] 1 2 3\nmy_list[[2]] # access the second item of the list\n[1] \"A\" \"B\" \"C\" \"D\"my_list <- list(\n  numbers=c(1,2,3),\n  categories=c(\"A\",\"B\",\"C\",\"D\")\n)\nmy_list\n$numbers\n[1] 1 2 3\n\n$categories\n[1] \"A\" \"B\" \"C\" \"D\"\nmy_list$numbers # access the first item of the list\n[1] 1 2 3\nmy_list$categories # access the second item of the list\n[1] \"A\" \"B\" \"C\" \"D\"my_df <- data.frame( # recall '.' has no special meaning in R\n  numbers=c(1,2,3),\n  categories=c(\"A\",\"B\",\"C\",\"D\")\n)\nError in data.frame(c(1, 2, 3), c(\"A\", \"B\", \"C\", \"D\")) :\n  arguments imply differing number of rows: 3, 4\nmy_df <- data.frame(\n  numbers=c(1,2,3),\n  categories=c(\"A\",\"B\",\"C\")\n)\nmy_df\n  numbers categories\n1       1          A\n2       2          B\n3       3          C\nmy_df$numbers\n[1] 1 2 3\nmy_df[1] # numeric indexing also works, and returns a subset data frame\n  numbers\n1       1\n2       2\n3       3\nmy_df[1]$numbers\n[1] 1 2 3\n# this syntax is [<row>,<column>], and if either is omitted return all\nmy_df[,1] # return all rows of the first column as a vector\n[1] 1 2 3\nmy_df$categories\n[1] \"A\" \"B\" \"C\"my_df$numbers[1] # extract the first value of the numbers column\n[1] 1\nmy_df[1,1] # same as above, recall the [<row>,<column>] syntax\n[1] 1\nmy_df$categories[3] # extract the third value of the categories column\n[1] \"C\""},{"path":"prog-basics.html","id":"logical-tests-and-comparators","chapter":"4 R Programming","heading":"4.6 Logical Tests and Comparators","text":"mentioned , R recognizes logical values distinct type. R provides\nconventional infix logical operators:operators also work vectors, albeit caveats vector\nlength noted earlier:R also provides many functions form .X X type \ncondition (recall . special character R):Quick-R - Operators","code":"1 == 1 # equality\n[1] TRUE\n1 != 1 # inequality\n[1] FALSE\n1 < 2 # less than\n[1] TRUE\n1 > 2 # greater than\n[1] FALSE\n1 <= 2 # less than or equal to\n[1] TRUE\n1 >= 2 # greater than or equal tox <- c(1,2,3)\nx == 2\n[1] FALSE TRUE FALSE\nx < 1\n[1] FALSE FALSE FALSE\nx < 3\n[1] TRUE TRUE FALSE\nc(1,2) == c(1,3)\n[1] TRUE FALSE\nc(1,2) != c(1,3)\n[1] FALSE TRUE\nc(1,2) == c(1,2,3)\n[1] TRUE TRUE FALSE\nWarning message:\nIn c(1, 2, 3) == c(1, 2) :\n  longer object length is not a multiple of shorter object lengthis.numeric(1) # is the argument numeric?\n[1] TRUE\nis.character(1) # is the argument a string?\n[1] FALSE\nis.character(\"ABC\")\n[1] TRUE\nis.numeric(c(1,2,3)) # recall a vector has exactly one type\n[1] TRUE\nis.numeric(c(1,2,\"3\"))\n[1] FALSE\nis.na(c(1,2,NA))\n[1] FALSE FALSE TRUE"},{"path":"prog-basics.html","id":"functions","chapter":"4 R Programming","heading":"4.7 Functions","text":"Just variable symbolic representation value, function \nsymbolic representation code. words, function allows \nsubstitute short name, e.g. mean, set operations given input,\ne.g. sum set numbers divided number numbers. R provides \nlarge number functions common operations \ndefault environment, functions provided packages can\ninstall separately.Encapsulating many lines code function useful (least) five\ndistinct reasons:Make code concise readableAllow avoid writing code (.e. reuse )Allow systematically test pieces code make sure \nintendAllow share code easily othersProgram using functional programming style (see note box )core, R functional programming language. details \nmeans outside scope book, name implies refers \nlanguage structured around use functions. property\ntechnical implications structure language, important\nconsequence style programming entails. functional programming\nstyle (paradigm) many advantages, including generally producing\nprograms concise, predictable, provably correct, performant.\nprovides good starting point learning functional programming.order anything useful, function must generally able accept\nexecute different inputs; e.g. mean function wouldn’t \nuseful didn’t accept value! terminology used R many \nprogramming languages function must accept allow \npass arguments. R, functions accept arguments using following\n*pattern:, arg1 arg2 arguments actual arguments, indicating\nfunction accepts two arguments. name function (.e.\nfunction_name) pattern arguments accepts called \nfunction’s signature. Every function least one signature, \ncritical understand order use function properly.example, arg1 arg2 positional arguments. means\norder arguments important. Calling function like\nfunction_name(arg2, arg1) produce different result \nfunction_name(arg1, arg2). see shortly, functions names \narguments definition can used specify arguments \norder.functions, arguments required. means function\nexecute without two arguments provided raise error\ntry otherwise:know arguments function requires? functions provided \nbase R many packages include detailed documentation can \naccessed directly RStudio using either ? help():RStudio - help function signaturesThe second signature mean function introduces two new types syntax:Default argument values - e.g. trim = 0. formal arguments \ndefault value provided explicitly.Variable arguments - .... means mean function can accept\narguments explicitly listed signature. syntax called\ndynamic dots.definitions, can now understand Arguments section help\ndocumentation:Arguments mean functionIn words:x vector values (next section data\nstructures) wish compute arithmetic mean fortrim fraction (.e. number 0 0.5) instructs R \nremove portion largest smallest values x prior computing\nmean.na.rm logical value (.e. either TRUE FALSE) instructs R \nremove NA values x computing mean.function arguments can specified name, regardless whether \ndefault value . instance, following two mean calls \nequivalent:borrow Zen \nPython, “Explicit better \nimplicit.” explicit variables passed \narguments almost always make code easier read likely \nintend.... argument catchall can dangerous. allows provide\narguments function meaning, R raise error.\nConsider following call mean:spot mistake? trim argument name misspelled \ntirm, R report error. Compare value mean without \ntypo:value get different, R recognizes trim tirm \nchanges behavior accordingly. functions ... catchall \nsignatures, many must diligent supplying\narguments function calls!R Data Science - Functionsrdrr.io - Base R Function ReferenceThis tutorialAdvanced R - Functional Programming","code":"\n# a basic function signature\nfunction_name(arg1, arg2) # function accepts 2 argumentsmean() # compute the arithmetic mean, but of what?\nError in mean.default() : argument \"x\" is missing, with no default# this generates 100 normally distributed samples with mean 0 and standard deviation 1\nmy_vals <- rnorm(100,mean=0,sd=1)\nmean(my_vals)\n[1] -0.05826857\nmean(x=my_vals)\n[1] -0.05826857# this generates 100 normally distributed samples with mean 0 and standard deviation 1\nmy_vals <- rnorm(100,mean=0,sd=1)\nmean(x=my_vals,tirm=0.1)\n[1] -0.05826857mean(x=my_vals,trim=0.1)\n[1]-0.02139839"},{"path":"prog-basics.html","id":"prog-dry","chapter":"4 R Programming","heading":"4.7.1 DRY: Don’t Repeat Yourself","text":"Sometimes find writing code perform\noperation different data. example, one common data\ntransformation standardization normalization entails taking \nseries numbers, subtracting mean numbers , \ndividing standard deviation numbers:Later code, may need standardize different set values, \ndecide copy paste code replace variable name\nreflect new data:Notice mistake? forgot change variable name my_vals \nmy_other_vals pasted code, produced incorrect result. Good\nthing checked!general, copying pasting code one part script \nanother, repeating lot work sure \nmodified copy correctly. Copying pasting code tempting \nefficiency standpoint, introduces may opportunities (often undetected!)\nerrors.Don’t Repeat \n(DRY) principle software development emphasizes recognizing \navoiding writing code encapsulating code. R, \neasily done functions. notice copying pasting\ncode, writing pattern code , excellent\nopportunity write function avoid repeating !","code":"# 100 normally distributed samples with mean 20 and standard deviation 10\nmy_vals <- rnorm(100,mean=20,sd=10)\nmy_vals_norm <- (my_vals - mean(my_vals))/sd(my_vals)\nmean(my_vals_norm)\n[1] 0\nsd(my_vals_norm)\n[1] 1# new samples with mean 40 and standard deviation 5\nmy_other_vals <- rnorm(100,mean=40,sd=5)\nmy_other_vals_norm <- (my_other_vals - mean(my_other_vals))/sd(my_vals)\nmean(my_other_vals_norm)\n[1] 0\nsd(my_other_vals_norm) # this should be 1!\n[1] 0.52351"},{"path":"prog-basics.html","id":"writing-your-own-functions","chapter":"4 R Programming","heading":"4.7.2 Writing your own functions","text":"R allows define function using following syntax:define name function, number arguments accepts \nnames, code within function, also called function\nbody. Taking example , define function named standardize\naccepts vector numbers, subtracts mean values, \ndivides standard deviation:Notice assigning value standardize function new\nvariables. R languages, result function returned \nfunction called; value returned called return value. \nreturn() function makes clear function returning.return() function strictly necessary R; result last\nline code body function returned default. However, \nborrow Zen \nPython, “Explicit better \nimplicit.” explicit function returns using return()\nfunction make code less error prone easier understand.","code":"\nfunction_name <- function(arg1, arg2, ...) {\n  # code that does something with arg1, arg2, etc\n  return(some_result)\n}standardize <- function(x) {\n  res <- (x - mean(x))/sd(x)\n  return(res)\n}\n\nmy_vals <- rnorm(100,mean=20,sd=10)\nmy_vals_std <- standardize(my_vals)\nmean(my_vals_std)\n[1] 0\nsd(my_vals_std)\n[1] 1\n\nmy_other_vals <- rnorm(100,mean=40,sd=5)\nmy_other_vals_std <- standardize(my_other_vals)\nmean(my_other_vals_std)\n[1] 0\nsd(my_other_vals_std)\n[1] 1"},{"path":"prog-basics.html","id":"scope","chapter":"4 R Programming","heading":"4.7.3 Scope","text":"programming, critically important concept called scope. Every\nvariable function define program scope, defines\nrest code variable can accessed. R, variables\ndefined outside function universal top level scope, .e. \ncan accessed anywhere script. However, variables defined\ninside functions can accessed within function. example:Notice variable x accessible within function\nmultiply_x_by_two, variable y accessible outside \nfunction. reason x accessible within function \nmultiply_x_by_two inherits scope defined, case\ntop level scope script, includes x. scope y \nlimited body function { } curly braces defining\nfunction.Accessing variables within functions outside function’s scope \nbad practice! Functions self contained possible, \nvalues need passed parameters. better way write \nfunction follows:Every variable function define subject scope rules .\nScope critical concept understand programming, grasping \nworks make code predictable less error prone.Scope R","code":"x <- 3\nmultiply_x_by_two <- function() {\n  y <- x*2 # x is not defined as a parameter to the function, but is defined outside the function\n  return(y)\n}\nx\n[1] 3\nmultiply_x_by_two()\n[1] 6\ny\nError: object 'y' not foundx <- 3\nmultiply_by_two <- function(x) {\n  y <- x*2 # x here is defined as whatever is passed to the function!\n  y\n}\nx\n[1] 3\nmultiply_by_two(6)\n[1] 12\nx # the value of x in the outer scope remains the same, because the function scope does not modify it\n[1] 3"},{"path":"prog-basics.html","id":"iteration","chapter":"4 R Programming","heading":"4.8 Iteration","text":"programming, iteration refers stepping sequentially set \ncollection objects, vector numbers, columns matrix, etc.\nnon-functional languages like python, C, etc. particular control\nstructures implement iteration, commonly called loops. \nworked languages, may familiar loops,\niteration control structures. However, R designed \nexecute iteration different way languages, provides\ntwo forms iteration: vectorized operations, functional programming \napply().Note R loop support language. However,\nloop structures can poor performance, generally \navoided favor functional style iteration described .Avoid Loops RIf really, really want learn use loops R, read , \ndon’t say didn’t warn code slows crawl unknown reasons:R Data Science - loops","code":""},{"path":"prog-basics.html","id":"vectorized-operations","chapter":"4 R Programming","heading":"4.8.1 Vectorized operations","text":"simplest form iteration R comes vectorized computation. sounds\nfancy, just means R intrinsically knows perform many operations \nvectors matrices well individual values. already seen examples\nperforming arithmetic operations vectors:addition simple arithmetic operations, R also syntax \nvector-vector, matrix-vector, matrix-matrix operations, like matrix\nmultiplication dot products:forms implicit iteration powerful, R program \nhighly optimized perform operations quickly. can cast \niteration vector matrix multiplication, good idea .\ncomplex custom iteration, must first talk briefly \nfunctional programming.","code":"x <- c(1,2,3,4,5)\nx + 3 # add 3 to every element of vector x\n[1] 4 5 6 7 8\nx * x # elementwise multiplication, 1*1 2*2 etc\n[1] 1 4 9 16 25\nx_mat <- matrix(c(1,2,3,4,5,6),nrow=2,ncol=3)\nx_mat + 3 # add 3 to every element of matrix x_mat\n[,1] [,2] [,3]\n[1,]    4    6    8\n[2,]    5    7    9\n# the * operator always means element-wise\nx_mat * x_mat\n     [,1] [,2] [,3]\n[1,]    1    9   25\n[2,]    4   16   36# the %*% operator stands for matrix multiplication\nx_mat %*% c(1,2,3) # [ 2x3 ] * [ 3 ]\n     [,1]\n[1,]   22\n[2,]   28\nx_mat %*% t(x_mat) # recall t() is the transpose function, making [ 2x3 ] * [ 3x2 ]\n     [,1] [,2]\n[1,]   35   44\n[2,]   44   56"},{"path":"prog-basics.html","id":"functional-programming","chapter":"4 R Programming","heading":"4.8.2 Functional programming","text":"R functional programming language core, means designed\naround use functions. previous section, saw functions \ndefined assigned names just like variables. means functions can\npassed functions just like variables! Consider following\nexample.Let’s consider general formulation vector transformation:\\[\n\\bar{\\mathbf{x}} = \\frac{\\mathbf{x} - t_r(\\mathbf{x})}{s(\\mathbf{x})}\n\\], \\(\\mathbf{x}\\) vector real numbers, \\(\\bar{\\mathbf{x}}\\) \ndefined vector length value average\ncentral value \\(t_r(\\mathbf{x})\\) subtracted , divided scaling\nfactor \\(s(\\mathbf{x})\\) control range resulting values. \n\\(t_r(\\mathbf{x})\\) \\(s(\\mathbf{x})\\) scalars (.e. individual numbers) \ndependent upon values \\(\\mathbf{x}\\). \\(t_r\\) arithmetic mean \\(s\\) \nstandard deviation, defined standardization transformation mentioned\nearlier examples:However, many different ways define central value set \nnumbers:arithmetic meangeometric meanmedianmodeand many moreEach central value methods accepts vector numbers, \nbehaviors different, appropriate different situations. Likewise,\nmany possible scaling strategies might consider:standard deviationrescaling factor (e.g. set data range -1 1)scaling unit length (values sum 1)othersWe may wish explore different methods without writing entirely new code\ncombination trying different transformation techniques.R functional languages, can easily accomplish passing\nfunctions arguments functions. Consider following R function:look familiar equation presented earlier, except now code\narguments t_r s passed arguments. wished transform\nusing Z-score normalization,\ncall my_transform follows:my_transform function call, second third arguments \nnames mean sd functions, respectively. definition \nmy_transform use syntax t_r(x) s(x) indicate \narguments treated functions. Using strategy, just \neasily define transformation using median sum t_r s \nwished :can also write functions pass get my_transform\nfunction desired behavior. following scales values x \nrange \\([0,1]\\):data_range function simply subtracts minimum value x \nmaximum value returns result.feature passing functions arguments functions \nfundamental property functional programming languages. Now ready \nfinally talk iteration performed R.Advanced R - Functional ProgrammingFunctional Programming Tutorial","code":"\nx <- rnorm(100, mean=20, sd=10)\nx_zscore <- (x - mean(x))/sd(x)\n# note R already has a built in function named \"transform\"\nmy_transform <- function(x, t_r, s) {\n  return((x - t_r(x))/s(x))\n}x <- rnorm(100,mean=20,sd=10)\nx_zscore <- my_transform(x, mean, sd)\nmean(x_zscore)\n[1] 0\nsd(x_zscore)\n[1] 1x <- rnorm(100,mean=20,sd=10)\nx_transform <- my_transform(x, median, sum)\nmedian(x_transform)\n[1] 0\nsum(x_transform) # this quantity does not have an a priori known value (or meaning for that matter, it's just an example)\n[1] 0.013data_range <- function(x) {\n  return(max(x) - min(x))\n}\n# my_transform computes: (x - min(x))/(max(x) - min(x))\nx_rescaled <- my_transform(x, min, data_range)\nmin(x_rescaled)\n[1] 0\nmax(x_rescaled)\n[1] 1"},{"path":"prog-basics.html","id":"apply-and-friends","chapter":"4 R Programming","heading":"4.8.3 apply() and friends","text":"working lists matrices R, often times want\nperform computation every row every column separately. common\nexample data science mentioned feature standardization.\nEarlier wrote Z-score\ntransformation accepts \nvector, subtracts mean element, divides result \nstandard deviation data. ensures data mean standard\ndeviation 0 1, respectively. However, function operates \nsingle vector numbers. Large datasets many features, may\nindividual vectors, desire perform Z-score\ntransformation separately. words, one function wish\nexecute either every row every column matrix return \nresult. form iteration can implemented functional\nstyle using apply function.signature apply function, RStudio help(apply)\npage:, X matrix (.e. rectangle numbers) wish perform \ncomputation either row column. MARGIN indicates whether\nmatrix traversed rows (MARGIN=1) columns (MARGIN=2).\nFUN name function accepts vector returns either \nvector scalar value wish execute either rows columns.\napply() executes FUN row column X returns \nresult. example:approach can used X list data frame rather \nmatrix using lapply() function (hint: l lapply stands \n“list”). function signature lapply:Recall lists data frames can thought vectors \nelement can vector. Therefore, one axis along \niterate elements MARGIN argument apply. \nfunction returns new list dimension original list \nelements returned FUN:functional programming pattern might counter intuitive first, \nwell worth learn.R Data Science - Iteration","code":"\napply(X, MARGIN, FUN, ..., simplify = TRUE)zscore <- function(x) {\n  return((x-mean(x))/sd(x))\n}\n# construct a matrix of 50 rows by 100 columns with samples drawn from a normal distribution\nx_mat <- matrix(\n  rnorm(100*50, mean=20, sd=5),\n  nrow=50,\n  ncol=100\n)\n# z-transform the rows of x_mat, so that each column has mean,sd of 0,1\nx_mat_zscore  <- apply(x_mat, 2, zscore)\n# we can check that all the columns of x_mat_zscore have mean close to zero with apply too\nx_mat_zscore_means <- apply(x_mat_zscore, 2, mean)\n# note: due to machine precision errors, these results will not be exactly zero, but are very close\n# note: the all() function returns true if all of its arguments are TRUE\nall(x_mat_zscore_means<1e-15)\n[1] TRUE\nlapply(X, FUN, ...)x <- list(\n  feature1=rnorm(100,mean=20,sd=10),\n  feature2=rnorm(100,mean=50,sd=5)\n)\nx_zscore <- lapply(x, zscore)\n# check that the means are close to zero\nx_zscore_means <- lapply(x_zscore, mean)\nall(x_zscore_means < 1e-15)\n[1] TRUE"},{"path":"prog-basics.html","id":"installing-packages","chapter":"4 R Programming","heading":"4.9 Installing Packages","text":"Advanced functionality R provided packages written supported\nR community members. exception bioconductor\npackages, R packages hosted Comprehensive R\nArchive Network (CRAN) web site. time \nwriting, 18,000\npackages hosted CRAN\ncan install. install package CRAN, use install.packages\nfunction R console:mentioned , many packages used biological data analysis \nhosted CRAN, Bioconductor. Bioconductor\nproject’s mission “develop, support, disseminate free open source\nsoftware facilitates rigorous reproducible analysis data \ncurrent emerging biological assays.” Practically, means Bioconductor\npackages subject stricter standards documentation, coding conventions\nstructure, standards compliance compared relatively lax\nCRAN package submission process.","code":"\n# install one package\ninstall.packages(\"tidyverse\")\n# install multiple packages\ninstall.packages(c(\"readr\",\"dplyr\"))"},{"path":"prog-basics.html","id":"saving-and-loading-r-data","chapter":"4 R Programming","heading":"4.10 Saving and Loading R Data","text":"always good idea save results tabular form CSV Files,\nsometimes convenient save complicated R objects data structures\nlike lists file can read back R easily. can done \nsaveRDS() readRDS()\nfunctions:functions convenient saving results complicated analyses\nreading back later, especially analyses time\nconsuming.R also functions\nsave()\nload().\nfunctions similar saveRDS readRDS, except allow\nloading individual objects new variables. Instead, save accepts one \nvariable names global namespace saves \nfile:later file saved_variables.rda load()ed, \nvariables saved file loaded namespace saved\nvalues:requires programmer remember names variables \nsaved file. Also, variables already exist \ncurrent environment also saved file, variable values \noverridden possibly without programmers knowledge intent. \nway change variable names variable saved way. \nreasons, saveRDS loadRDS generally safer use can \nexplicit saving loading.saveRDS() readRDS() R manualreadRDS() saveRDS() official documentation","code":"\na_complicated_list <- list(\n    categories = c(\"A\",\"B\",\"C\"),\n    data_matrix = matrix(c(1,2,3,4,5,6),nrows=2,ncols=3),\n    nested_list = list(\n      a = c(1,2,3),\n      b = c(4,5,6)\n    )\n)\nsaveRDS(a_complicated_list, \"a_complicated_list.rda\")\n\n# later, possibly in a different script\na_complicated_list <- readRDS(\"a_complicated_list.rda\")\nvar_a <- \"a string\"\nvar_b <- 1.234\n\nsave(var_a, var_b, file=\"saved_variables.rda\")# assume var_a and var_b are not defined yet\nload(\"saved_variables.rda\")\nvar_a\n[1] \"a string\"\nvar_b\n[1] 1.234"},{"path":"prog-basics.html","id":"prog-debugging","chapter":"4 R Programming","heading":"4.11 Troubleshooting and Debugging","text":"Bugs code normal. bad programmer code bugs\n(thank goodness!). However, bugs can difficult fix, \neven difficult find. spend substantial amount time debugging\ncode R, especially learning language many quirks.\nencounter R error warning messages routinely development,\nstraightforward understand. important \nlearn seek answers problems R reports ; \ncolleagues (instructors!) thank .standard approach debugging, borrow ideas \nHadley Wickam’s excellent section debugging Advanced R\nbook:Google! - copy paste error google see comes back.\nEspecially starting , errors receive encountered\ncountless times others , solutions/explanations \nalready . aren’t already familiar Stack\nOverflow, soon.Make repeatable - encounter error, don’t change anything\ncode try make sure get error . \nmay require isolate code error different setting \nmake easy run. , means error repeatable, \nreplicable, can now try modifying code question see \nerror changes.Find bug * - bugs involve multiple lines code,\nsubset contains actual error. Sometimes exact line\nerror occurs obvious, times error consequence\nmistake assumption made earlier code.Fix test - identified specific issue causing\nbug, modify code produces correct result \nrigorously test fix make sure correct. Sometimes making one\nchange code causes side effects elsewhere code ways \ndifficult predict. Ideally, already written unit\ntests explicitly test parts code, \nneed use means convincing fix worked.debugging process become second nature work R.Practically speaking, basic debugging method run code isn’t\nworking way , print intermediate results inspect state\nvariables, make adjustments accordingly. RStudio, Environment\nInspector top right interface makes inspecting current values\nvariables easy. can also easily execute lines code \nscript interpreter bottom right using Cntl-Enter test \nmodifications .Sometimes working highly nested data structures like lists \nlists. objects can difficult inspect due size. str()\nfunction, stands structure, pretty print object \nvalues structure:output str concise descriptive simply printing \nobject.RStudio many debugging tools can use. Check section \ndebugging Hadley Wickam’s Advanced R book Read box \ndescription tools.Hands-Programming R - Debugging R Code\nAdvanced R - Debugging","code":"nested_lists <- list(\n  a=list(\n      item1=c(1,2,3),\n      item2=c('A','B','C')\n  ),\n  b=list(\n      var1=1:10,\n      var2=100:110,\n      var3=1000:1010\n  ),  \n  c=c(10,9,8,7,6,5)\n)\nnested_lists\n$a\n$a$item1\n[1] 1 2 3\n\n$a$item2\n[1] \"A\" \"B\" \"C\"\n\n\n$b\n$b$var1\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$b$var2\n [1] 100 101 102 103 104 105 106 107 108 109 110\n\n$b$var3\n [1] 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010\nstr(nested_lists)\nList of 3\n $ a:List of 2\n  ..$ item1: num [1:3] 1 2 3\n  ..$ item2: chr [1:3] \"A\" \"B\" \"C\"\n $ b:List of 3\n  ..$ var1: int [1:10] 1 2 3 4 5 6 7 8 9 10\n  ..$ var2: int [1:11] 100 101 102 103 104 105 106 107 108 109 ...\n  ..$ var3: int [1:11] 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 ...\n $ c: num [1:6] 10 9 8 7 6 5"},{"path":"prog-basics.html","id":"prog-style","chapter":"4 R Programming","heading":"4.12 Coding Style and Conventions","text":"common worries among new programmers : “code terrible? \nwrite good code?” gold standard makes code “good”, \nquestions can ask code guide:","code":""},{"path":"prog-basics.html","id":"is-my-code-correct","chapter":"4 R Programming","heading":"4.12.1 Is my code correct?","text":"produce desired output? pretty obviously important \nprinciple, can difficult sure code correct. \nespecially difficult codebase large complicated tends \nbecome time. simple trial error effective\nfirst approach, reliable albeit time- thought-intensive strategy \nwrite explicit tests code run regularly.","code":""},{"path":"prog-basics.html","id":"does-my-code-follow-the-dry-principle","chapter":"4 R Programming","heading":"4.12.2 Does my code follow the DRY principle?","text":"Don’t Repeat (DRY) powerful helpful strategy make \ncode reliable. typically involves identifying common patterns \ncode moving functions objects.","code":""},{"path":"prog-basics.html","id":"did-i-choose-concise-but-descriptive-variable-and-function-names","chapter":"4 R Programming","heading":"4.12.3 Did I choose concise but descriptive variable and function names?","text":"Variable function names descriptive necessary \nlong. Try put shoes someone reading code \nfirst time see can figure . Better yet, offer \nbuy friend coffee return looking !","code":""},{"path":"prog-basics.html","id":"did-i-use-indentation-and-naming-conventions-consistently-throughout-my-code","chapter":"4 R Programming","heading":"4.12.4 Did I use indentation and naming conventions consistently throughout my code?","text":"Consistently formatted code much easier read (possibly understand)\ninconsistent code. Consider following code example:code inconsistent several ways:naming conventions - calcVal camel case, calc_val_2 snake casenew lines whitespace - calcVal one line, calc_val_2 \nmultiple linesunhelpful indentation - calc_val_2 function body indented,\nclose curly brace appended last line bodyunhelpful function argument names - function names describe \nlittle functions , argument names x, , etc \ndescriptive representunused function arguments - arg argument calcVal isn’t used\nanywhere functionthe two functions appear something similar made\nsimpler using default argumentA consistent version code might look like:code much cleaner, consistent, easier read.","code":"\ncalcVal <- function(x, a, arg=2) { return(sum(x*a)**2)}\ncalc_val_2 <- function(x, a, b, arg) {\nres <- sum(b+a*x)**arg\nreturn(res)}\nexponent_product <- function(x, a, offset=0, arg=2) {\n  return(sum(offset+a*x)**arg)\n}"},{"path":"prog-basics.html","id":"did-i-write-comments-especially-when-what-the-code-does-is-not-obvious","chapter":"4 R Programming","heading":"4.12.5 Did I write comments, especially when what the code does is not obvious?","text":"Sometimes piece code obvious looking :Clearly line code takes value x, whatever , adds 1 \n. However, may obvious piece code . \ncases, may helpful record thinking line code\ncomment:someone else reads code, obvious \nthinking wrote . career, encounter situations\nneed figure thinking wrote piece \ncode. Endeavor make future proud current !","code":"\nx <- x + 1\n# add 1 as a pseudocount\nx <- x + 1"},{"path":"prog-basics.html","id":"how-easy-would-it-be-for-someone-else-to-understand-my-code","chapter":"4 R Programming","heading":"4.12.6 How easy would it be for someone else to understand my code?","text":"someone else never seen code asked run understand\n, easy ?","code":""},{"path":"prog-basics.html","id":"is-my-code-easy-to-maintainchange","chapter":"4 R Programming","heading":"4.12.7 Is my code easy to maintain/change?","text":"related previous question, distinct understanding\ncode just first step able make desired changes \n.","code":""},{"path":"prog-basics.html","id":"the-styler-package","chapter":"4 R Programming","heading":"4.12.8 The styler package","text":"Consistently formatted code generally much easier read inconsistently\nformatted code. Consistent formatting may also allow identify syntax \nlogic errors much easily might otherwise. styler\npackage\nR package can automatically format code make consistent \nnumber ways, including indentation, code block conventions . \ninstall styler install.packages(\"styler\") RStudio, new entry \navailable Addins menu:styler Addin menuThese Addins allow let styler format code according \nreasonable (albeit arbitrary) conventions.","code":""},{"path":"data-wrangling.html","id":"data-wrangling","chapter":"5 Data Wrangling","heading":"5 Data Wrangling","text":"","code":""},{"path":"data-wrangling.html","id":"the-tidyverse","chapter":"5 Data Wrangling","heading":"5.1 The Tidyverse","text":"tidyverse “opinionated collection R\npackages designed data science.” packages designed work\ntogether unified approach helps code look consistent neat. \nopinion author, tidyverse practically changes R language\nprincipally statistical programming language efficient \nexpressive data science language. still important understand \nlanguage fundamentals presented chapter R programming\nlanguage, tidyverse uses distinct set coding conventions\nlets achieve greater expressiveness, conciseness, correctness\nrelative base R language.data science language, R+tidyverse (referred simply tidyverse \nbook) strongly focused operations related loading, manipulating,\nvisualizing, summarizing, analyzing data sets many domains. \nmajor strength tidyverse community, means many\neducational materials written general use case, \npracticing biological data analysis. general data manipulation\noperations often biological data analysis general\ncase examples, biological analysis practitioners must nonetheless translate\nconcepts general cases common data analysis tasks must\nperform. analytical patterns common biological data analysis\nothers, materials focus subset operations book\naid learning applying concepts problems directly \npossible.R Data Science - Data Wrangling IntroductionModernDive - Data Wrangling","code":""},{"path":"data-wrangling.html","id":"dw-basics","chapter":"5 Data Wrangling","heading":"5.2 Tidyverse Basics","text":"Since tidyverse set packages work together, often want \nload multiple packages time. tidyverse authors recognize ,\ndefined set reasonable packages load loading \nmetapackage (.e. package contains multiple packages):packages Attaching packages section loaded default,\npackages adds unique set functions R environment.\nmention functions many packages go \nchapter, now table packages briefly :Notice dplyr::filter() syntax Conflicts section. filter \ndefined function dplyr package well base R stats\npackage. stats package loaded default run R, thus \nfilter function defined (specifically, performs linear filtering \ntime series\ndata).\nHowever, dplyr loaded, also filter function overrides\ndefinition stats package. tidyverse package\nreports conflict loaded:tidyverse telling filter function redefined \nmake sure aware .However, want use original stats defined filter function,\nmay still access using :: namespace operator. operator lets\n“look inside” loaded package, example dplyr, access function\nwithin namespace package:functions defined package can accessed using :: operator, \noften good idea , ensure calling right function.","code":"library(tidyverse)\n-- Attaching packages --------------------------------------------- tidyverse 1.3.1 --\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.1     v forcats 0.5.1\n-- Conflicts ------------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()-- Conflicts ------------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nlibrary(dplyr)\n# the filter() function definition is now from the dplyr package, and not from the stats package\n# the following two lines of code execute the same function\nfilter(df)\ndplyr::filter(df)\n# to access the stats definition of the filter function, use the namespace operator\nstats::filter(df)"},{"path":"data-wrangling.html","id":"importing-data","chapter":"5 Data Wrangling","heading":"5.3 Importing Data","text":"first operation typically must perform analyzing data reading \ndata file R. readr package \ndefault tidyverse packages contains following similar\nfunctions import\ndata delimited text files:CSV files can large may \ncompressed save\nspace. many different file compression algorithms, common\ndata science biology gzip \nbzip. readr file reading functions can work\ncompressed files directly, need decompress first.functions returns special data frame called \ntibble, explained next section.Note readr also functions writing delimited\nfiles. functions\nbehave similarly read_X functions instead creating tibble \nfile, create file tibble. frequently need export \nresults analysis share collaborators also part larger\nworkflows use tools R.R Data Science - Data Importreadr - read_delim referencereadr - write_delim reference","code":""},{"path":"data-wrangling.html","id":"data-tibble","chapter":"5 Data Wrangling","heading":"5.4 The tibble","text":"Data tidyverse organized primarily special data frame object called \ntibble. tibble() function defined tibble\npackage tidyverse:tibble stores rectangular data, .e. grid data elements \nevery column number rows. can access individual columns \nway base R data frames:tibbles (regular data frames) typically names columns. \nexample, column names x y, accessed using \ncolnames function:Column names may changed using function:see later, can also use dplyr::rename rename columns \nwell:tibbles dataframes also row names well column names:However, tibble support row names included compatibility\nbase R data frames generally avoided. reason row\nnames basically character column different semantics every\ncolumn, authors tidyverse believe row names better stored\nnormal column.tibble - working row namesThe tibble package provides convenient way construct simple tibbles\nmanually tribble() function, stands “transposed\ntibble”:made-dataset includes statistics p-values two different\nstatistical tests (, made ) three human genes. use \nexample Arranging Data section.tibble documentationR Data Science - tibbles","code":"library(tibble) # or\nlibrary(tidyverse)\ntbl <- tibble(\n    x = rnorm(100, mean=20, sd=10),\n    y = rnorm(100, mean=50, sd=5)\n)\ntbl\n# A tibble: 100 x 2\n       x     y\n   <dbl> <dbl>\n 1 16.5   54.6\n 2 14.4   54.3\n 3  7.87  53.7\n 4  8.06  50.8\n 5 37.2   57.1\n 6 16.5   51.9\n 7 15.8   50.1\n 8 40.3   44.3\n 9 12.0   49.8\n10 23.8   50.1\n# ... with 90 more rowstbl$x\n[1] 29.572549 12.015877 15.235536 23.071761 32.254703 48.048651 21.905756\n[8] 15.511768 34.872685 21.352433 12.515230 23.608096  6.778630 12.342237\n...\ntbl[1,\"x\"] # access the first element of x\n# A tibble: 1 x 1\n    x\n  <dbl>\n1  29.6\ntbl$x[1]\n[1] 29.57255colnames(tbl)\n[1] \"x\" \"y\"colnames(tbl) <- c(\"a\",\"b\")\ntbl\n# A tibble: 100 x 2\n       a     b\n   <dbl> <dbl>\n 1 16.5   54.6\n 2 14.4   54.3\n 3  7.87  53.7\n 4  8.06  50.8\n 5 37.2   57.1\n 6 16.5   51.9\n 7 15.8   50.1\n 8 40.3   44.3\n 9 12.0   49.8\n10 23.8   50.1\n# ... with 90 more rowsdplyr::rename(tbl,\n  a = x,\n  b = y\n)rownames(tbl)\n[1] \"1\" \"2\" \"3\"...\ngene_stats <- tribble(\n    ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p,\n   \"apoe\", 12.509293,   0.1032,   34.239521,   1.3e-5,\n   \"hoxd1\",  4.399211,   0.6323,   16.332318,   0.0421,\n   \"snca\", 45.748431,   4.2e-9,    0.757188,   0.9146,\n)\ngene_stats## # A tibble: 3 x 5\n##   gene  test1_stat      test1_p test2_stat  test2_p\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>\n## 1 apoe       12.5  0.103            34.2   0.000013\n## 2 hoxd1       4.40 0.632            16.3   0.0421  \n## 3 snca       45.7  0.0000000042      0.757 0.915"},{"path":"data-wrangling.html","id":"tidy-data","chapter":"5 Data Wrangling","heading":"5.5 Tidy Data","text":"tidyverse packages designed operate -called “tidy data”. \ntidy data section R Data\nScience book, following rules make data tidy:variable must column.observation must row.value must cell., variable quantity property every observation dataset\n, observation separate instance variable (e.g. \ndifferent sample, subject, etc). gene_stats example tibble , \ncolumns referring different test statistics variables, gene\nrow “observation”, cell value; can therefore say\ndataset tidy:row “observation” somewhat abstract case; say\n“observed” test genes tibble. Depending \ndataset working , sometimes flexible \nconceptualization constitutes variables observations.R Data Science book depicts rules following illustration:Tidy data - R Data ScienceThese rules pretty generic, general dataset might require work\nmanipulate tidy form. Fortunately us working biology\nbioinformatics, many datasets already provided format\nclose tidy, tools use process biological data\nus. reason, details tidying operations might\nneeded data general case left reading tidy data\nsection R Data Science book.one big common exception claim biological\ndata usually already tidy . Briefly, illustration , tidy\ndata observations rows variables columns. However, datasets\noften use, e.g. gene expression matrices, organized \nvariables (e.g. genes) rows observations (e.g. samples) columns.\nmeans operations summarize variables across observations, \ncommon compute, easily done tidyverse functions like\nmutate(). describe work around difference section\nBiological data Tidy!.Tidy data - R Data Science","code":"\ngene_stats## # A tibble: 3 x 5\n##   gene  test1_stat      test1_p test2_stat  test2_p\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>\n## 1 apoe       12.5  0.103            34.2   0.000013\n## 2 hoxd1       4.40 0.632            16.3   0.0421  \n## 3 snca       45.7  0.0000000042      0.757 0.915"},{"path":"data-wrangling.html","id":"pipes","chapter":"5 Data Wrangling","heading":"5.6 pipes","text":"One key tidyverse programming patterns chaining manipulations \ntibbles together using %>% operator. often want perform\nserial operations data frame, example read file, rename one \ncolumns, subset rows based criteria, compute summary statistics\nresult. might implement operations using variable \nassignment:repeated use data intermediate data_grouped variable may\nunnecessary ’re interested summarized result. code \nalso straightforward read. Using %>% operator, can write\nsequence operations much concise manner:Note function calls piped example data\nvariable passed explicitly. %>% operator passes \nresult function immediately preceding first argument \nnext function automatically. convention allows us focus writing \nimportant parts code perform logic analysis, \navoid unnecessary potentially distracting additional characters make\ncode less readable.R Data Science - Pipes%>% operator documentation magrittr package","code":"# data_file.csv has two columns: bad_cOlumn_name and numeric_column\ndata <- readr::read_csv(\"data_file.csv\")\ndata <- dplyr::rename(data, \"better_column_name\"=bad_cOlumn_name)\ndata <- dplyr::filter(data, better_column_name %in% c(\"condA\",\"condB\"))\ndata_grouped <- dplyr::group_by(data, better_column_name)\nsummarized <- dplyr::summarize(data_grouped, mean(numeric_column))data <- readr::read_csv(\"data_file.csv\") %>%\n      dplyr::rename(\"better_column_name\"=bad_cOlumn_name) %>%\n      dplyr::filter(better_column_name %in% c(\"condA\",\"condB\")) %>%\n      dplyr::group_by(better_column_name) %>%\n      dplyr::summarize(mean(numeric_column))"},{"path":"data-wrangling.html","id":"arranging-data","chapter":"5 Data Wrangling","heading":"5.7 Arranging Data","text":"loaded data file tibble, often need \nmanipulate various ways make values amenable desired\nanalysis. manipulations might include renaming poorly named columns,\nfiltering certain records, deriving new columns using values others,\nchanging order rows etc. operations may collectively termed\narranging data many provided \n*dplyr package. cover \ncommon data arranging functions , many dplyr\npackage worth knowing.examples , make use following made-tibble \ncontains fake statistics p-values three human genes:gene_stats tibble simple example common type \ndata work biology; namely instead raw data, work \nstatistics computed using raw data help us interpret \nresults. statistics may ‘data’ per se, can still use \nfunctions strategies tidyverse work .tidyverse big place. RStudio created many helpful\ncheatsheets aid looking \ncertain operations. cheatsheet \ndplyr\nlots useful information use many functions package.","code":"\ngene_stats## # A tibble: 3 x 5\n##   gene  test1_stat      test1_p test2_stat  test2_p\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>\n## 1 apoe       12.5  0.103            34.2   0.000013\n## 2 hoxd1       4.40 0.632            16.3   0.0421  \n## 3 snca       45.7  0.0000000042      0.757 0.915"},{"path":"data-wrangling.html","id":"dplyrmutate---create-new-columns-using-other-columns","chapter":"5 Data Wrangling","heading":"5.7.1 dplyr::mutate() - Create new columns using other columns","text":"Many biological analysis procedures perform kind statistical test \ncollection features (e.g. genes) produce p-values indicate \n“surprising” feature according test. p-values tibble\nnominal, .e. adjusted multiple\nhypotheses. Briefly, run multiple tests like\nthree genes, chance tests \nsmall p-value simply chance. Multiple testing\ncorrection\nprocedures adjust nominal p-values account possibility number\ndifferent ways, common procedure biological analysis \nBenjamini-Hochberg False Discovery Rate\n(FDR) procedure. R, \np.adjust function can perform several procedures, including FDR:Notice adjusted p-values larger nominal ones; \neffect multiple testing procedure. Since two sets p-values,\nmust compute FDR , can call \nmutate():Another common operation create new columns derived values \nmultiple columns. (wetlab colleagues) might decide \nconvenient new column TRUE FALSE based whether \ngene significant either test. column make easy filter\ngenes just ones might interesting tools like Excel. can\ncreate new columns multiple columns just easily using mutate()\nfunction:Recall | & operators execute ‘’ ‘’ logic,\nrespectively. example required creation new variable\ngene_stats_mutated columns test1_padj \ntest2_padj need tibble computing new fields. However,\nmutate(), columns created first function call available later\ncolumns. following example, note test1_padj created first \nused create signif columns:alternative split two mutate() commands, first\ncreating adjusted p-value columns second creating significance\ncolumns. dplyr recognizes common build new variables \nnew variables mutate() command, therefore provides convenient\nbehavior.mutate() can also used modify columns place. official convention\nhuman gene symbols upper case, reason \ntibble contains lower case gene symbols. can correct using mutate()\nfirst talk stringr\npackage makes working strings much\neasier base R functions.R Data Science - Add new variables mutate()dplyr mutate() reference","code":"\ndplyr::mutate(gene_stats,\n  test1_padj=p.adjust(test1_p,method=\"fdr\")\n)## # A tibble: 3 x 6\n##   gene  test1_stat      test1_p test2_stat  test2_p   test1_padj\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>        <dbl>\n## 1 apoe       12.5  0.103            34.2   0.000013 0.155       \n## 2 hoxd1       4.40 0.632            16.3   0.0421   0.632       \n## 3 snca       45.7  0.0000000042      0.757 0.915    0.0000000126\ngene_stats_mutated <- dplyr::mutate(gene_stats,\n  test1_padj=p.adjust(test1_p,method=\"fdr\"),\n  test2_padj=p.adjust(test2_p,method=\"fdr\")\n)\ngene_stats_mutated## # A tibble: 3 x 7\n##   gene  test1_stat      test1_p test2_stat  test2_p   test1_padj test2_padj\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>        <dbl>      <dbl>\n## 1 apoe       12.5  0.103            34.2   0.000013 0.155          0.000039\n## 2 hoxd1       4.40 0.632            16.3   0.0421   0.632          0.0632  \n## 3 snca       45.7  0.0000000042      0.757 0.915    0.0000000126   0.915\ndplyr::mutate(gene_stats_mutated,\n  signif_either=(test1_padj < 0.05 | test2_padj < 0.05),\n  signif_both=(test1_padj < 0.05 & test2_padj < 0.05)\n)## # A tibble: 3 x 9\n##   gene  test1_stat      test1_p test2_stat  test2_p   test1_padj test2_padj\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>        <dbl>      <dbl>\n## 1 apoe       12.5  0.103            34.2   0.000013 0.155          0.000039\n## 2 hoxd1       4.40 0.632            16.3   0.0421   0.632          0.0632  \n## 3 snca       45.7  0.0000000042      0.757 0.915    0.0000000126   0.915   \n## # ... with 2 more variables: signif_either <lgl>, signif_both <lgl>\ndplyr::mutate(gene_stats,\n  test1_padj=p.adjust(test1_p,method=\"fdr\"), # test1_padj created\n  test2_padj=p.adjust(test2_p,method=\"fdr\"),\n  signif_either=(test1_padj < 0.05 | test2_padj < 0.05), #test1_padj used\n  signif_both=(test1_padj < 0.05 & test2_padj < 0.05)\n)## # A tibble: 3 x 9\n##   gene  test1_stat      test1_p test2_stat  test2_p   test1_padj test2_padj\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>        <dbl>      <dbl>\n## 1 apoe       12.5  0.103            34.2   0.000013 0.155          0.000039\n## 2 hoxd1       4.40 0.632            16.3   0.0421   0.632          0.0632  \n## 3 snca       45.7  0.0000000042      0.757 0.915    0.0000000126   0.915   \n## # ... with 2 more variables: signif_either <lgl>, signif_both <lgl>"},{"path":"data-wrangling.html","id":"stringr---working-with-character-values","chapter":"5 Data Wrangling","heading":"5.7.2 stringr - Working with character values","text":"Base R convenient functions working character\nstrings (just strings). due original intent statistical\nprogramming language, string manipulation (principle) common\noperation. However, practice, must frequently manipulate strings \nloading, cleaning, analyzing datasets. stringr\npackage aims make working strings “\neasy possible.”package includes many useful functions operating strings, including\nsearching patterns, mutating strings, lexicographical sorting, combining\nmultiple strings together (.e. concatenation), performing complex\nsearch/replace operations. far many useful functions cover \nbecome comfortable reading stringr\ndocumentation \nhelpful stringr\ncheatsheet.previous section, noted gene symbols tibble lower\ncase official gene symbols upper case. can use stringr\nfunction stringr::str_to_upper() dplyr::mutate() function \nperform adjustment easily:Now gene symbols appropriate case, wetlab colleagues won’t\ncomplain . :)","code":"\ndplyr::mutate(gene_stats,\n  gene=stringr::str_to_upper(gene)\n)## # A tibble: 3 x 5\n##   gene  test1_stat      test1_p test2_stat  test2_p\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>\n## 1 APOE       12.5  0.103            34.2   0.000013\n## 2 HOXD1       4.40 0.632            16.3   0.0421  \n## 3 SNCA       45.7  0.0000000042      0.757 0.915"},{"path":"data-wrangling.html","id":"regular-expressions","chapter":"5 Data Wrangling","heading":"5.7.2.1 Regular expressions","text":"Many string operations stringr package use regular\nexpression syntax. regular\nexpression sequence characters describes patterns text. Regular\nexpressions written sort mini programming language certain\ncharacters special meaning help defining search patterns \nidentifies location sequences characters text follow \nparticular pattern specified regular expression. similar \n“Find” functionality many word processors, powerful due \nflexibility patterns can found.simple example helpful. Let’s say tibble containing \nresult (made-) statistical test genes genome:Now let’s say ’re interested examining results BRCA family \ngenes, BRCA1 BRCA2. can use filter() data frame look \nindividually:isn’t bad, can thing \nstringr::str_detect()\n, returns TRUE provided pattern matches input FALSE\notherwise, regular expression, [dplyr::filter() function], \ndescribed greater detail later section:argument \"^BRCA[12]$\" regular expression searches \nfollowing:Search genes start characters BRCA - ^ \nbeginning pattern stands start stringFor genes start BRCA, look genes next character\neither 1 2 [12] - characters [] searched\nexplicitly, character encountered listed \nresults non-matchFor genes start BRCA followed either 1 2, match\nsuccessfully number end name - $ end \npattern stands end stringWe can use principles find genes complex naming conventions.\nHomeobox (HOX) genes encode\nDNA binding proteins regulate gene expression genes involved \nmorphgenesis cell differentiation vertebrates. humans, HOX genes \norganized 4 clusters paralogs result three DNA\nduplication events distant evolutionary past(Abbasi 2015), \ncluster encodes subset 13 distinct HOX genes placed next .\nclusters assigned letter identifier -D (e.g. HOXA,\nHOXB, HOXC, HOXD) paralogous gene within cluster assigned\nnumber (e.g. HOXA4, HOXB4, HOXC4, HOXD4 paralogs). 13\nHOX genes total, though genes remain clusters (e.g. HOXA1,\nHOXB1, HOXD1 exist HOXC1 lost time). following figure\ndepicts human HOX gene clusters:Human HOX gene clusters - Veraksa, .; Del Campo, M.; McGinnis, W. Developmental Patterning Genes Conserved Functions: Model Organisms Humans. Mol. Genet. Metab. 2000, 69 (2), 85–100.Let’s say want extract HOX genes gene statistics. \ncan write regular expression matches pattern described :query used two new regular expression features:within [] specified range characters -D 0-9 \nmatch characters D (.e. , B, C, D) 0 9\nrespectivelythe + character means “match one preceding expression”, \ncase [0-9]. allows us match genes single\nnumber (e.g. HOXA1) well double digit numbers (e.g. HOXA10).Since know cluster identifier part HOX gene names (.e. \n[-D] part) exactly one character long, alternatively write \nregular expression follows, using special . character:, . character interpreted regex match single\ncharacter, regardless , HOX part number\npart. also requires exist exactly one character two\nparts; gene symbol HOX1 matched, 1 match\n., number remains match [0-9]+ part.Sometimes want search text characters considered special \nregular expression language. example, list filenames:wanted limit just .txt extension, need match\nusing literal . character:Inside [], characters usual regular expression meaning,\ntherefore [.] match literal . character. Instead using \n[] syntax, may also escape literal characters using two back\nslashes:Regular expressions powerful, can much \ndescribed . See regular expression tutorial linked readmore box\nlearn details.R Data Science - Stringsstringr\ndocumentationstringr\ncheatsheetRegexOne - regular expression tutorial","code":"\nde_genes## # A tibble: 39,535 x 4\n##    hgnc_symbol  mean        p    padj\n##    <chr>       <int>    <dbl>   <dbl>\n##  1 MT-TF        5799 0.000910 0.00941\n##  2 MT-RNR1       153 0.0272   0.0342 \n##  3 MT-TV         115 0.0228   0.0301 \n##  4 MT-RNR2       495 0.00318  0.0123 \n##  5 MT-TL1      20201 0.000377 0.00841\n##  6 MT-ND1        160 0.0179   0.0258 \n##  7 MT-TI        3511 0.00247  0.0115 \n##  8 MT-TQ         772 0.00376  0.0129 \n##  9 MT-TM         301 0.00325  0.0124 \n## 10 MT-ND2         12 0.107    0.111  \n## # ... with 39,525 more rows\nde_genes %>% filter(hgnc_symbol == \"BRCA1\" | hgnc_symbol == \"BRCA2\")## # A tibble: 2 x 4\n##   hgnc_symbol  mean      p   padj\n##   <chr>       <int>  <dbl>  <dbl>\n## 1 BRCA1          41 0.0321 0.0386\n## 2 BRCA2         447 0.0140 0.0223\ndplyr::filter(de_genes, str_detect(hgnc_symbol,\"^BRCA[12]$\"))## # A tibble: 2 x 4\n##   hgnc_symbol  mean      p   padj\n##   <chr>       <int>  <dbl>  <dbl>\n## 1 BRCA1          41 0.0321 0.0386\n## 2 BRCA2         447 0.0140 0.0223\ndplyr::filter(de_genes, str_detect(hgnc_symbol,\"^HOX[A-D][0-9]+$\")) %>%\n  dplyr::arrange(hgnc_symbol)## # A tibble: 39 x 4\n##    hgnc_symbol  mean        p    padj\n##    <chr>       <int>    <dbl>   <dbl>\n##  1 HOXA1        8734 0.000858 0.00934\n##  2 HOXA10       4149 0.00152  0.0102 \n##  3 HOXA11        567 0.0101   0.0188 \n##  4 HOXA13        411 0.0105   0.0191 \n##  5 HOXA2         554 0.00600  0.0151 \n##  6 HOXA3          18 0.0919   0.0959 \n##  7 HOXA4         475 0.0113   0.0199 \n##  8 HOXA5         434 0.0127   0.0211 \n##  9 HOXA6        3983 0.00252  0.0115 \n## 10 HOXA7         897 0.00961  0.0183 \n## # ... with 29 more rows\ndplyr::filter(de_genes, str_detect(hgnc_symbol,\"^HOX.[0-9]+$\")) %>%\n  dplyr::arrange(hgnc_symbol)## # A tibble: 39 x 4\n##    hgnc_symbol  mean        p    padj\n##    <chr>       <int>    <dbl>   <dbl>\n##  1 HOXA1        8734 0.000858 0.00934\n##  2 HOXA10       4149 0.00152  0.0102 \n##  3 HOXA11        567 0.0101   0.0188 \n##  4 HOXA13        411 0.0105   0.0191 \n##  5 HOXA2         554 0.00600  0.0151 \n##  6 HOXA3          18 0.0919   0.0959 \n##  7 HOXA4         475 0.0113   0.0199 \n##  8 HOXA5         434 0.0127   0.0211 \n##  9 HOXA6        3983 0.00252  0.0115 \n## 10 HOXA7         897 0.00961  0.0183 \n## # ... with 29 more rows\nfilenames <- tribble(\n  ~name,\n  \"annotation.csv\",\n  \"file1.txt\",\n  \"file2.txt\",\n  \"results.csv\"\n)\nfilter(filenames, stringr::str_detect(name,\"[.]txt$\"))## # A tibble: 2 x 1\n##   name     \n##   <chr>    \n## 1 file1.txt\n## 2 file2.txt\nfilter(filenames, stringr::str_detect(name,\"\\\\.txt$\"))## # A tibble: 2 x 1\n##   name     \n##   <chr>    \n## 1 file1.txt\n## 2 file2.txt"},{"path":"data-wrangling.html","id":"dplyrselect---subset-columns-by-name","chapter":"5 Data Wrangling","heading":"5.7.3 dplyr::select() - Subset Columns by Name","text":"mutate() operations created number new columns tibble,\nspecify tibble new columns go. Let’s\nconsider mutated tibble created four new columns:readability standpoint, might helpful columns \ntest grouped together, rather look end \ntibble find .dplyr::select()\nfunction allows pick\nspecific columns larger tibble whatever order choose:explicitly selected statistics columns. dplyr also helper\nfunctions allow\nflexible selection columns. example, columns \nwished select ended _stat, use ends_with() helper\nfunction:desire, select() allows renaming selected columns:knew test statistics actually corresponded kind \nt-test \\(\\chi\\)-squared test, naming columns tibble appropriately\nmay help others (possibly ) understand code better.can use dplyr::select() function obtain desired column order:Now order columns clear convenient. necessary \nlist columns test statistic line, author thinks\nmakes code easier read understand.R Data Science - Select columns select()dplyr referenceselect helper functions","code":"\ndplyr::mutate(gene_stats,\n  test1_padj=p.adjust(test1_p,method=\"fdr\"),\n  test2_padj=p.adjust(test2_p,method=\"fdr\"),\n  signif_either=(test1_padj < 0.05 | test2_padj < 0.05),\n  signif_both=(test1_padj < 0.05 & test2_padj < 0.05),\n  gene=stringr::str_to_upper(gene)\n)## # A tibble: 3 x 9\n##   gene  test1_stat      test1_p test2_stat  test2_p   test1_padj test2_padj\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>        <dbl>      <dbl>\n## 1 APOE       12.5  0.103            34.2   0.000013 0.155          0.000039\n## 2 HOXD1       4.40 0.632            16.3   0.0421   0.632          0.0632  \n## 3 SNCA       45.7  0.0000000042      0.757 0.915    0.0000000126   0.915   \n## # ... with 2 more variables: signif_either <lgl>, signif_both <lgl>\nstats <- dplyr::select(gene_stats, test1_stat, test2_stat)\nstats## # A tibble: 3 x 2\n##   test1_stat test2_stat\n##        <dbl>      <dbl>\n## 1      12.5      34.2  \n## 2       4.40     16.3  \n## 3      45.7       0.757\nstats <- dplyr::select(gene_stats, ends_with(\"_stat\"))\nstats## # A tibble: 3 x 2\n##   test1_stat test2_stat\n##        <dbl>      <dbl>\n## 1      12.5      34.2  \n## 2       4.40     16.3  \n## 3      45.7       0.757\nstats <- dplyr::select(gene_stats,\n  t=test1_stat,\n  chisq=test2_stat\n)\nstats## # A tibble: 3 x 2\n##       t  chisq\n##   <dbl>  <dbl>\n## 1 12.5  34.2  \n## 2  4.40 16.3  \n## 3 45.7   0.757\ndplyr::mutate(gene_stats,\n  test1_padj=p.adjust(test1_p,method=\"fdr\"),\n  test2_padj=p.adjust(test2_p,method=\"fdr\"),\n  signif_either=(test1_padj < 0.05 | test2_padj < 0.05),\n  signif_both=(test1_padj < 0.05 & test2_padj < 0.05),\n  gene=stringr::str_to_upper(gene)\n) %>%\ndplyr::select(\n    gene,\n    test1_stat, test1_p, test1_padj,\n    test2_stat, test2_p, test2_padj,\n    signif_either,\n    signif_both\n)## # A tibble: 3 x 9\n##   gene  test1_stat      test1_p   test1_padj test2_stat  test2_p test2_padj\n##   <chr>      <dbl>        <dbl>        <dbl>      <dbl>    <dbl>      <dbl>\n## 1 APOE       12.5  0.103        0.155            34.2   0.000013   0.000039\n## 2 HOXD1       4.40 0.632        0.632            16.3   0.0421     0.0632  \n## 3 SNCA       45.7  0.0000000042 0.0000000126      0.757 0.915      0.915   \n## # ... with 2 more variables: signif_either <lgl>, signif_both <lgl>"},{"path":"data-wrangling.html","id":"dplyrfilter---pick-rows-out-of-a-data-set","chapter":"5 Data Wrangling","heading":"5.7.4 dplyr::filter() - Pick rows out of a data set","text":"Often, first step interpreting analysis identify features\nsignificant adjusted p-value threshold. First \nsave mutated tibble another variable, aid demonstration:Now can use dplyr::filter() function select rows based whether\nsignificant either test example.filtering result genes nominal p-value less\n0.05 remain. Note filter two tests separately, can also\ncombine tests using logical\noperators achieve\ndifferent results:APOE SCNA significant least one tests.looks like don’t genes significant tests.\nFiltering results like one common operations \nresults biological analyses.R Data Science - Filter rows filter()dplyr - filter() reference","code":"\ngene_stats_mutated <- dplyr::mutate(gene_stats,\n  test1_padj=p.adjust(test1_p,method=\"fdr\"),\n  test2_padj=p.adjust(test2_p,method=\"fdr\"),\n  signif_either=(test1_padj < 0.05 | test2_padj < 0.05),\n  signif_both=(test1_padj < 0.05 & test2_padj < 0.05),\n  gene=stringr::str_to_upper(gene)\n) %>%\ndplyr::select(\n    gene,\n    test1_stat, test1_p, test1_padj,\n    test2_stat, test2_p, test2_padj,\n    signif_either,\n    signif_both\n)\ndplyr::filter(gene_stats_mutated, test1_padj < 0.05)## # A tibble: 1 x 9\n##   gene  test1_stat      test1_p   test1_padj test2_stat test2_p test2_padj\n##   <chr>      <dbl>        <dbl>        <dbl>      <dbl>   <dbl>      <dbl>\n## 1 SNCA        45.7 0.0000000042 0.0000000126      0.757   0.915      0.915\n## # ... with 2 more variables: signif_either <lgl>, signif_both <lgl>\ndplyr::filter(gene_stats_mutated, test2_padj < 0.05)## # A tibble: 1 x 9\n##   gene  test1_stat test1_p test1_padj test2_stat  test2_p test2_padj\n##   <chr>      <dbl>   <dbl>      <dbl>      <dbl>    <dbl>      <dbl>\n## 1 APOE        12.5   0.103      0.155       34.2 0.000013   0.000039\n## # ... with 2 more variables: signif_either <lgl>, signif_both <lgl>\n# | means \"logical or\", meaning the row is retained if either condition is true\ndplyr::filter(gene_stats_mutated, test1_padj < 0.05 | test2_padj < 0.05)## # A tibble: 2 x 9\n##   gene  test1_stat      test1_p   test1_padj test2_stat  test2_p test2_padj\n##   <chr>      <dbl>        <dbl>        <dbl>      <dbl>    <dbl>      <dbl>\n## 1 APOE        12.5 0.103        0.155            34.2   0.000013   0.000039\n## 2 SNCA        45.7 0.0000000042 0.0000000126      0.757 0.915      0.915   \n## # ... with 2 more variables: signif_either <lgl>, signif_both <lgl>\n# & means \"logical and\", meaning the row is retained only if both conditions are true\ndplyr::filter(gene_stats_mutated, test1_padj < 0.05 & test2_padj < 0.05)## # A tibble: 0 x 9\n## # ... with 9 variables: gene <chr>, test1_stat <dbl>, test1_p <dbl>,\n## #   test1_padj <dbl>, test2_stat <dbl>, test2_p <dbl>, test2_padj <dbl>,\n## #   signif_either <lgl>, signif_both <lgl>"},{"path":"data-wrangling.html","id":"dplyrarrange---order-rows-based-on-their-values","chapter":"5 Data Wrangling","heading":"5.7.5 dplyr::arrange() - Order rows based on their values","text":"Another common operation working biological analysis results \nordering meaningful value. Like , p-values often used \nprioritize results simply sorting ascending order. arrange()\nfunction perform sorting tidyverse:Note sorting nominal p-value , adjusted p-value. general,\nsorting nominal adjusted p-value results order results. \nexception , due way FDR procedure works, adjusted\np-values identical, making relative order tests \nFDR meaningless. contrast, rare nominal p-values \nidentical, since induce ordering results, sorting\nanalysis results advantages using nominal p-value, rather \nadjusted p-value.general, larger magnitude statistic, smaller p-value\n(two-tailed tests), desired induce similar ranking \narranging data statistic descending order:first apply base R abs() function compute absolute value \ntest 1 statistic specify want sort largest first. Note\nalthough don’t negative values dataset, assume\ngeneral, safer us complete add absolute value\ncall case later decide copy paste code another analysis.\n’s pretty much arrange().R Data Science - Arrange rows witharrange()dplyr arrange() reference","code":"\nstats_sorted_by_test1_p <- dplyr::arrange(gene_stats, test1_p)\nstats_sorted_by_test1_p## # A tibble: 3 x 5\n##   gene  test1_stat      test1_p test2_stat  test2_p\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>\n## 1 snca       45.7  0.0000000042      0.757 0.915   \n## 2 apoe       12.5  0.103            34.2   0.000013\n## 3 hoxd1       4.40 0.632            16.3   0.0421\n# desc() is a helper function that causes the results to be sorted in descending\n# order for the given column\ndplyr::arrange(gene_stats, desc(abs(test1_stat)))## # A tibble: 3 x 5\n##   gene  test1_stat      test1_p test2_stat  test2_p\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>\n## 1 snca       45.7  0.0000000042      0.757 0.915   \n## 2 apoe       12.5  0.103            34.2   0.000013\n## 3 hoxd1       4.40 0.632            16.3   0.0421"},{"path":"data-wrangling.html","id":"putting-it-all-together","chapter":"5 Data Wrangling","heading":"5.7.6 Putting it all together","text":"previous sections, performed following operations:Created new columns computing false discovery rate nominal p-values\nusing dplyr::mutate() p.adjust functionsCreated new columns indicate patterns significance gene\nusing dplyr::mutate()Mutated gene symbol case using stringr::str_to_upper \ndplyr::mutate()Reordered columns group related variables select()Filtered genes based whether adjusted p-value less 0.05\neither statistical tests using dplyr::filter()Sorted results p-value using dplyr::arrange()sake illustration, steps presented separately, \ntogether represent single unit data processing thus might\nprofitably done R command using %>%:complete pipeline now contains manipulations mutated\ntibble can passed downstream analysis collaborators.","code":"\ngene_stats <- dplyr::mutate(gene_stats,\n  test1_padj=p.adjust(test1_p,method=\"fdr\"),\n  test2_padj=p.adjust(test2_p,method=\"fdr\"),\n  signif_either=(test1_padj < 0.05 | test2_padj < 0.05),\n  signif_both=(test1_padj < 0.05 & test2_padj < 0.05),\n  gene=stringr::str_to_upper(gene)\n) %>%\ndplyr::select(\n    gene,\n    test1_stat, test1_p, test1_padj,\n    test2_stat, test2_p, test2_padj,\n    signif_either,\n    signif_both\n) %>%\ndplyr::filter(\n  test1_padj < 0.05 | test2_padj < 0.05\n) %>%\ndplyr::arrange(\n  test1_p\n)\ngene_stats## # A tibble: 2 x 9\n##   gene  test1_stat      test1_p   test1_padj test2_stat  test2_p test2_padj\n##   <chr>      <dbl>        <dbl>        <dbl>      <dbl>    <dbl>      <dbl>\n## 1 SNCA        45.7 0.0000000042 0.0000000126      0.757 0.915      0.915   \n## 2 APOE        12.5 0.103        0.155            34.2   0.000013   0.000039\n## # ... with 2 more variables: signif_either <lgl>, signif_both <lgl>"},{"path":"data-wrangling.html","id":"grouping-data","chapter":"5 Data Wrangling","heading":"5.8 Grouping Data","text":"Sometimes interested summarizing subsets data defined \ngrouping variable. Consider following made-sample metadata set \nindividuals Alzheimer’s disease (AD) study:typical setup metadata types experiments. \nsample ID column uniquely identifies subject, condition variable\nindicating group subject , clinical information like age \ndeath, Braak stage (measure Alzheimer’s disease pathology brain),\ndiploid APOE genotype (e2 associated reduced risk AD, e3 \nbaseline, e4 confers increased risk).important experimental design consideration match sample attributes\ngroups well possible avoid confounding comparison \ninterest. case, age death one variable wish match\ngroups. Although values look pretty well matched AD \nControl groups, better check explicitly. can using\ndplyr::group_by() group rows together based condition \ndplyr::summarize() compute mean age death group:dplyr::group_by() accepts tibble column name \ncolumn contains categorical variable (.e. variable discrete\nvalues like AD Control) separates rows tibble groups\naccording distinct values column. dplyr::summarize()\nfunction accepts grouped tibble creates new tibble contents\ndefined function values columns group.example , see mean age death indeed different\ntwo groups, much. can go one step compute\nstandard deviation age range investigate:Note use summarized variables defined first used variables\ndefined later. Now can see age ranges defined +/- one standard\ndeviation clearly overlap, gives us confidence average age\ndeath AD Control significantly different.used +/- one standard deviation define likely mean age range \narithmetic mean simplicity example . proper way \nassess whether distributions significantly different use \nappropriate statistical test like t-test.Like functions dplyr, dplyr::summarize() helper functions\ngive additional functionality. One useful helper function n(),\ndefined number records within group. add one\ncolumn summarized sample metadata reports number\nsubjects within condition:now column number subjects condition.Hadley Wickham New Zealand, uses British, rather American,\nEnglish. Therefore, many places, spellings supported \ntidyverse; e.g. summarise() summarize() supported.R Data Science - Grouped summaries summarise()dplyr summarise() reference","code":"\nmetadata <- tribble(\n    ~ID, ~condition, ~age_at_death, ~Braak_stage, ~APOE_genotype,\n  \"A01\",        \"AD\",            78,            5,       \"e4/e4\",\n  \"A02\",        \"AD\",            81,            6,       \"e3/e4\",\n  \"A03\",        \"AD\",            90,            5,       \"e4/e4\",\n  \"A04\",   \"Control\",            80,            1,       \"e3/e4\",\n  \"A05\",   \"Control\",            79,            0,       \"e3/e3\",\n  \"A06\",   \"Control\",            81,            0,       \"e2/e3\"\n)\ndplyr::group_by(metadata,\n  condition\n) %>% dplyr::summarize(mean_age_at_death = mean(age_at_death))## # A tibble: 2 x 2\n##   condition mean_age_at_death\n##   <chr>                 <dbl>\n## 1 AD                       83\n## 2 Control                  80\ndplyr::group_by(metadata,\n  condition\n) %>% dplyr::summarize(\n  mean_age_at_death = mean(age_at_death),\n  sd_age_at_death = sd(age_at_death),\n  lower_age = mean_age_at_death-sd_age_at_death,\n  upper_age = mean_age_at_death+sd_age_at_death,\n)## # A tibble: 2 x 5\n##   condition mean_age_at_death sd_age_at_death lower_age upper_age\n##   <chr>                 <dbl>           <dbl>     <dbl>     <dbl>\n## 1 AD                       83            6.24      76.8      89.2\n## 2 Control                  80            1         79        81\ndplyr::group_by(metadata,\n  condition\n) %>% dplyr::summarize(\n  num_subjects = n(),\n  mean_age_at_death = mean(age_at_death),\n  sd_age_at_death = sd(age_at_death),\n  lower_age = mean_age_at_death-sd_age_at_death,\n  upper_age = mean_age_at_death+sd_age_at_death\n)## # A tibble: 2 x 6\n##   condition num_subjects mean_age_at_death sd_age_at_death lower_age upper_age\n##   <chr>            <int>             <dbl>           <dbl>     <dbl>     <dbl>\n## 1 AD                   3                83            6.24      76.8      89.2\n## 2 Control              3                80            1         79        81"},{"path":"data-wrangling.html","id":"rearranging-data","chapter":"5 Data Wrangling","heading":"5.9 Rearranging Data","text":"Sometimes shape format data convenient \nperforming certain operations , even tidy. Let’s say\nconsidering range statistics computed \ngenes gene_stats tibble, wanted compute average statistic\ngenes tests. Recall tibble separate columns \ntest:convenience, desire output table form, one row per test\nstatistics test columns. manually like :gets job done, clearly ugly, error prone, require\nsignificant work later added statistics columns.Instead typing values desire manually, can pivot tibble\nusing tidyr::pivot_longer() function, column values placed\nnew column corresponding values placed yet another column.\nprocess illustrated following figure:Pivot longer moves columns values two new columnsIn figure, original tibble genes along rows samples columns.\nsample columns pivoted, value column name placed \nnew column named “Sample” repeated many rows \ntibble. second new column “Value” populated corresponding values\ncolumn. gene associated value preserved \nrepeated vertically table columns values pivoted.\nprocess pivoting transforms tibble called “long” form.Returning gene_stats example, can apply operations \ntibble easily perform summarization much \nexpressive manner:see now instead X_stat columns, column names \nvalues put test stat columns, respectively. Now \nsummarize statistics test, simply group_by() \ntest column compute summaries stat column:may verify numbers identical pivoted tibble one\nmanually created earlier. pivoting method produce desired\noutput regardless number tests include table, long \ncolumn names end \"_test\".inverse pivot_longer() \npivot_wider(). \nvariables gathered single columns like produced \npivot_longer() can reverse process function create \ntibble variables columns.Pivoting - R Data Sciencepivot_longer() reference","code":"\ngene_stats <- tribble(\n    ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p,\n   \"APOE\",   12.509293,   0.1032,   34.239521,   1.3e-5,\n  \"HOXD1\",    4.399211,   0.6323,   16.332318,   0.0421,\n   \"SNCA\",   45.748431,   4.2e-9,    0.757188,   0.9146,\n)\ngene_stats## # A tibble: 3 x 5\n##   gene  test1_stat      test1_p test2_stat  test2_p\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>\n## 1 APOE       12.5  0.103            34.2   0.000013\n## 2 HOXD1       4.40 0.632            16.3   0.0421  \n## 3 SNCA       45.7  0.0000000042      0.757 0.915\ntribble(\n   ~test_name, ~min, ~mean, ~max,\n   \"test1_stat\", min(gene_stats$test1_stat), mean(gene_stats$test1_stat), max(gene_stats$test1_stat),\n   \"test2_stat\",  min(gene_stats$test2_stat), mean(gene_stats$test2_stat), max(gene_stats$test2_stat),\n)## # A tibble: 2 x 4\n##   test_name    min  mean   max\n##   <chr>      <dbl> <dbl> <dbl>\n## 1 test1_stat 4.40   20.9  45.7\n## 2 test2_stat 0.757  17.1  34.2\nlong_gene_stats <- tidyr::pivot_longer(\n  gene_stats,\n  ends_with(\"_stat\"),\n  names_to=\"test\",\n  values_to=\"stat\"\n)\nlong_gene_stats## # A tibble: 6 x 5\n##   gene       test1_p  test2_p test         stat\n##   <chr>        <dbl>    <dbl> <chr>       <dbl>\n## 1 APOE  0.103        0.000013 test1_stat 12.5  \n## 2 APOE  0.103        0.000013 test2_stat 34.2  \n## 3 HOXD1 0.632        0.0421   test1_stat  4.40 \n## 4 HOXD1 0.632        0.0421   test2_stat 16.3  \n## 5 SNCA  0.0000000042 0.915    test1_stat 45.7  \n## 6 SNCA  0.0000000042 0.915    test2_stat  0.757\nlong_gene_stats %>%\n  dplyr::group_by(test) %>%\n  dplyr::summarize(min = min(stat), mean = mean(stat), max = max(stat))## # A tibble: 2 x 4\n##   test         min  mean   max\n##   <chr>      <dbl> <dbl> <dbl>\n## 1 test1_stat 4.40   20.9  45.7\n## 2 test2_stat 0.757  17.1  34.2"},{"path":"data-wrangling.html","id":"relational-data","chapter":"5 Data Wrangling","heading":"5.10 Relational Data","text":"mentioned section types biological data, \noften need combine different sources data together aid \ninterpretation results. redefine tibble gene statistics\nproperly capitalized gene symbols:gene identifiers data frame gene\nsymbols ,\nconvenient human brains remember, can change time \nmany aliases (e.g. APOE gene also called AD2, LDLCQ5, APO-E, \nApoE4 listed \ngenecard). can\nmake writing code refers specific gene difficult, since \nmany possible names look . Fortunately, alternative gene\nidentifier systems better job maintaining stable, consistent gene\nidentifiers, one popular Ensembl.\nEnsembl gene IDs always take form ENSGNNNNNNNNNNN Ns digits.\nIDs much stable predictable gene symbols, \npreferable working genes code.wish add Ensembl IDs genes gene_stats result \ntibble new column. Now suppose obtained another file cross\nreferenced gene identifiers like following:Imagine file contains mappings ~60,000 genes human\ngenome. might simple look three genes file \nannotate manually, easier ask dplyr us. can \nusing dplyr::left_join() function accepts two data frames \nnames columns share common values:Notice additional columns gene_map involved \njoin (.e. ENSG gene_name) appended gene_stats tibble. \nwanted use pipes, implement join follows:hood, dplyr::left_join() function took values \ngene_stats$gene looked corresponding row gene_map \nvalue symbol column. appends additional columns\ngene_map matching rows returns result. added \nidentifier mapping desired lines code. ’s ,\ncode work matter many genes gene_stats long \ngene_map contains mapping value values gene_stats$gene.happens genes gene_stats don’t exist \nmapping? example, use left join want include\nrows gene_stats regardless whether mapping exists \ngene_map. case fine genes gene_stats\ncorresponding row mapping. However, notice mapping\nadditional gene, BRCA1 \ngene statistics tibble. reverse order join,\nobserve happens:Now order rows gene_map, columns \nmissing gene BRCA1 filled NA. left join work, \nrecord gene_map included regardless whether corresponding\nvalue found gene_stats.additional types joins besides left joins. Right joins simply\nopposite left joins:result left join gene_map except order \nresulting columns different.Inner joins return results rows match two tibbles.\nRecall left join gene_map included BRCA1 even though found\ngene_stats. inner join include row, match \ngene_stats found:One last type join \ndplyr::full_join()\n(also sometimes called outer join). may expect, full join \nreturn rows tibbles whether match table found\n.","code":"\ngene_stats <- tribble(\n    ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p,\n   \"APOE\",   12.509293,   0.1032,   34.239521,   1.3e-5,\n  \"HOXD1\",    4.399211,   0.6323,   16.332318,   0.0421,\n   \"SNCA\",   45.748431,   4.2e-9,    0.757188,   0.9146,\n)\ngene_stats## # A tibble: 3 x 5\n##   gene  test1_stat      test1_p test2_stat  test2_p\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl>\n## 1 APOE       12.5  0.103            34.2   0.000013\n## 2 HOXD1       4.40 0.632            16.3   0.0421  \n## 3 SNCA       45.7  0.0000000042      0.757 0.915\ngene_map <- tribble(\n    ~symbol,            ~ENSGID,                    ~gene_name,\n     \"APOE\",  \"ENSG00000130203\",            \"apolipoprotein E\",\n     \"BRCA1\",  \"ENSG00000012048\", \"BRCA1 DNA repair associated\",\n    \"HOXD1\",  \"ENSG00000128645\",                 \"homeobox D1\",\n     \"SNCA\",  \"ENSG00000145335\",             \"synuclein alpha\",\n)\ngene_map## # A tibble: 4 x 3\n##   symbol ENSGID          gene_name                  \n##   <chr>  <chr>           <chr>                      \n## 1 APOE   ENSG00000130203 apolipoprotein E           \n## 2 BRCA1  ENSG00000012048 BRCA1 DNA repair associated\n## 3 HOXD1  ENSG00000128645 homeobox D1                \n## 4 SNCA   ENSG00000145335 synuclein alpha\ndplyr::left_join(\n    x=gene_stats,\n    y=gene_map,\n    by=c(\"gene\" = \"symbol\")\n)## # A tibble: 3 x 7\n##   gene  test1_stat      test1_p test2_stat  test2_p ENSGID          gene_name   \n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl> <chr>           <chr>       \n## 1 APOE       12.5  0.103            34.2   0.000013 ENSG00000130203 apolipoprot~\n## 2 HOXD1       4.40 0.632            16.3   0.0421   ENSG00000128645 homeobox D1 \n## 3 SNCA       45.7  0.0000000042      0.757 0.915    ENSG00000145335 synuclein a~\ngene_stats %>% dplyr::left_join(\n  gene_map,\n  by=c(\"gene\" = \"symbol\")\n)## # A tibble: 3 x 7\n##   gene  test1_stat      test1_p test2_stat  test2_p ENSGID          gene_name   \n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl> <chr>           <chr>       \n## 1 APOE       12.5  0.103            34.2   0.000013 ENSG00000130203 apolipoprot~\n## 2 HOXD1       4.40 0.632            16.3   0.0421   ENSG00000128645 homeobox D1 \n## 3 SNCA       45.7  0.0000000042      0.757 0.915    ENSG00000145335 synuclein a~\ngene_map %>% dplyr::left_join(\n  gene_stats,\n  by=c(\"symbol\" = \"gene\")\n)## # A tibble: 4 x 7\n##   symbol ENSGID          gene_name       test1_stat  test1_p test2_stat  test2_p\n##   <chr>  <chr>           <chr>                <dbl>    <dbl>      <dbl>    <dbl>\n## 1 APOE   ENSG00000130203 apolipoprotein~      12.5   1.03e-1     34.2    1.3 e-5\n## 2 BRCA1  ENSG00000012048 BRCA1 DNA repa~      NA    NA           NA     NA      \n## 3 HOXD1  ENSG00000128645 homeobox D1           4.40  6.32e-1     16.3    4.21e-2\n## 4 SNCA   ENSG00000145335 synuclein alpha      45.7   4.2 e-9      0.757  9.15e-1\ngene_stats %>% dplyr::right_join(\n  gene_map,\n  by=c(\"gene\" = \"symbol\")\n)## # A tibble: 4 x 7\n##   gene  test1_stat       test1_p test2_stat   test2_p ENSGID          gene_name \n##   <chr>      <dbl>         <dbl>      <dbl>     <dbl> <chr>           <chr>     \n## 1 APOE       12.5   0.103            34.2    0.000013 ENSG00000130203 apolipopr~\n## 2 HOXD1       4.40  0.632            16.3    0.0421   ENSG00000128645 homeobox ~\n## 3 SNCA       45.7   0.0000000042      0.757  0.915    ENSG00000145335 synuclein~\n## 4 BRCA1      NA    NA                NA     NA        ENSG00000012048 BRCA1 DNA~\ngene_map %>% dplyr::inner_join(\n  gene_stats,\n  by=c(\"symbol\" = \"gene\")\n)## # A tibble: 3 x 7\n##   symbol ENSGID          gene_name        test1_stat  test1_p test2_stat test2_p\n##   <chr>  <chr>           <chr>                 <dbl>    <dbl>      <dbl>   <dbl>\n## 1 APOE   ENSG00000130203 apolipoprotein E      12.5   1.03e-1     34.2   1.3 e-5\n## 2 HOXD1  ENSG00000128645 homeobox D1            4.40  6.32e-1     16.3   4.21e-2\n## 3 SNCA   ENSG00000145335 synuclein alpha       45.7   4.2 e-9      0.757 9.15e-1"},{"path":"data-wrangling.html","id":"dealing-with-multiple-matches","chapter":"5 Data Wrangling","heading":"5.10.1 Dealing with multiple matches","text":"example , one--one relationship gene\nsymbols tibbles. made joined tibble tidy. However, \none--many relationship exists, .e. one gene symbol one tibble multiple\nrows , can lead appears duplicate rows \njoined result. Due relative instability gene symbols, \ncommon multiple Ensembl genes associated single gene symbol. \nfollowing takes gene mapping Ensembl IDs gene symbols identifies\ncases multiple Ensembl IDs map single gene symbol:7,000 Ensembl IDs map gene symbol another\nEnsembl ID. 10% Ensembl IDs. now let’s create new\ngene_stats tibble one gene symbols join map \nsee happens:Notice two rows DEAF1 gene identical values\nexcept Stable Gene ID column. common problem \nmapping gene symbols gene identifiers general solution\npicking “best” mapping, short manually inspecting \nduplicates choosing one appropriate (\nobviously huge amount work). However, desire remove \nduplicated rows. case, since values besides Ensembl ID \n, effectively doesn’t matter duplicate rows eliminate. \ncan using duplicated() function, returns TRUE \nfirst row set duplicated values:However, general, must careful identifying types \none--many mapping issues also mitigate .R Data Science - Relational Datadplyr - mutate joins","code":"\nreadr::read_tsv(\"mart_export.tsv\") %>%\n  dplyr::filter(\n    `HGNC symbol` != \"NA\" & # many unstudied genes have Ensembl IDs but no official symbol\n    `HGNC symbol` %in% `HGNC symbol`[duplicated(`HGNC symbol`)]) %>%\n  dplyr::arrange(`HGNC symbol`)## # A tibble: 7,268 x 3\n##    `Gene stable ID` `HGNC symbol` `Gene name`                                \n##    <chr>            <chr>         <chr>                                      \n##  1 ENSG00000261846  AADACL2       arylacetamide deacetylase like 2           \n##  2 ENSG00000197953  AADACL2       arylacetamide deacetylase like 2           \n##  3 ENSG00000262466  AADACL2-AS1   <NA>                                       \n##  4 ENSG00000242908  AADACL2-AS1   <NA>                                       \n##  5 ENSG00000276072  AATF          apoptosis antagonizing transcription factor\n##  6 ENSG00000275700  AATF          apoptosis antagonizing transcription factor\n##  7 ENSG00000281173  ABCB10P1      <NA>                                       \n##  8 ENSG00000280461  ABCB10P1      <NA>                                       \n##  9 ENSG00000274099  ABCB10P1      <NA>                                       \n## 10 ENSG00000282479  ABCB10P3      <NA>                                       \n## # ... with 7,258 more rows\ngene_map <- readr::read_tsv(\"mart_export.tsv\")\ngene_stats <- tribble(\n    ~gene, ~test1_stat, ~test1_p, ~test2_stat, ~test2_p,\n   \"APOE\",   12.509293,   0.1032,   34.239521,   1.3e-5,\n  \"HOXD1\",    4.399211,   0.6323,   16.332318,   0.0421,\n   \"SNCA\",   45.748431,   4.2e-9,    0.757188,   0.9146,\n  \"DEAF1\",    0.000000,      1.0,           0,      1.0\n) %>% left_join(gene_map, by=c(\"gene\" = \"HGNC symbol\"))\ngene_stats## # A tibble: 5 x 7\n##   gene  test1_stat      test1_p test2_stat  test2_p `Gene stable ID` `Gene name`\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl> <chr>            <chr>      \n## 1 APOE       12.5  0.103            34.2   0.000013 ENSG00000130203  apolipopro~\n## 2 HOXD1       4.40 0.632            16.3   0.0421   ENSG00000128645  homeobox D1\n## 3 SNCA       45.7  0.0000000042      0.757 0.915    ENSG00000145335  synuclein ~\n## 4 DEAF1       0    1                 0     1        ENSG00000282712  DEAF1 tran~\n## 5 DEAF1       0    1                 0     1        ENSG00000177030  DEAF1 tran~\nfilter(gene_stats, !duplicated(gene))## # A tibble: 4 x 7\n##   gene  test1_stat      test1_p test2_stat  test2_p `Gene stable ID` `Gene name`\n##   <chr>      <dbl>        <dbl>      <dbl>    <dbl> <chr>            <chr>      \n## 1 APOE       12.5  0.103            34.2   0.000013 ENSG00000130203  apolipopro~\n## 2 HOXD1       4.40 0.632            16.3   0.0421   ENSG00000128645  homeobox D1\n## 3 SNCA       45.7  0.0000000042      0.757 0.915    ENSG00000145335  synuclein ~\n## 4 DEAF1       0    1                 0     1        ENSG00000282712  DEAF1 tran~"},{"path":"data-science.html","id":"data-science","chapter":"6 Data Science","heading":"6 Data Science","text":"Data science enormous \nrapidly growing field incorporates elements statistics, computer\nscience, software engineering, high performance cloud computing, big\ndata management, well syntheses knowledge social \nphysical science fields model predict phenomena captured data\ncollected “real world”. Many books written \nsubject, one pretend even attempt give area \nadequate treatment. Like rest book, topics covered \nopinionated presented lens biological analyst practitioner\nenough high level conceptual details general principles \nhopefully useful context.","code":""},{"path":"data-science.html","id":"data-modeling","chapter":"6 Data Science","heading":"6.1 Data Modeling","text":"goal data modeling describe dataset using relatively small\nnumber mathematical relationships. Said differently, model uses parts\ndataset try accurately predict parts dataset way\nuseful us.Models human inventions; reflect beliefs way universe\nworks. successful model identifies patterns within dataset \nresult causal relationships universe led phenomena \nmeasured accounting noise data. However, model \nidentify even accurately reflect causal effects. model\nmerely summarizes patterns scientists left interpret \npatterns design follow experiments investigate nature \ncausal relationships using prior knowledge world.several principles keep mind modeling data:Data never wrong. data collected using processes \ndevices designed implemented humans, always biases make\nassumptions. data measure something universe, “true” \nsense word. intended measure actually\nmeasured thing, due errors collection \ninterpretation, due data wrong. approach dataset \nparticular set hypotheses data don’t support hypotheses, \nbeliefs world understanding dataset wrong,\ndata .Data never wrong. data collected using processes \ndevices designed implemented humans, always biases make\nassumptions. data measure something universe, “true” \nsense word. intended measure actually\nmeasured thing, due errors collection \ninterpretation, due data wrong. approach dataset \nparticular set hypotheses data don’t support hypotheses, \nbeliefs world understanding dataset wrong,\ndata .data useful. Just data isn’t wrong, doesn’t mean\nuseful. may systematic errors collection \ndata makes interpreting difficult. Data collected one purpose may\nuseful purposes. sometimes, dataset collected \nparticular purpose may simply information needed answer \nquestions; measure relationship wish predict,\ndata useful - though knowledge measured \ndetectable effect thing wish predict may useful!data useful. Just data isn’t wrong, doesn’t mean\nuseful. may systematic errors collection \ndata makes interpreting difficult. Data collected one purpose may\nuseful purposes. sometimes, dataset collected \nparticular purpose may simply information needed answer \nquestions; measure relationship wish predict,\ndata useful - though knowledge measured \ndetectable effect thing wish predict may useful!“models wrong, useful.” George Box, renowned\nBritish statistician, famously asserted 1976 paper Journal \nAmerican Statistical Association. (Box 1976). meant every\nmodel create simplification system seeking model, \ndefinition identical system. perfectly model system, \nmodel need precisely system modeling, longer\nmodel system . Fortunately, even though know models \nalways wrong degree, may nonetheless useful \nwrong. models may indeed wrong, though.“models wrong, useful.” George Box, renowned\nBritish statistician, famously asserted 1976 paper Journal \nAmerican Statistical Association. (Box 1976). meant every\nmodel create simplification system seeking model, \ndefinition identical system. perfectly model system, \nmodel need precisely system modeling, longer\nmodel system . Fortunately, even though know models \nalways wrong degree, may nonetheless useful \nwrong. models may indeed wrong, though.Data contain causal information. “Correlation mean\ncausation.” Data measurements results process universe\nwish understand; data possibly reflective process,\ncontain information process . infer\ncausal relationships dataset alone. must construct possible causal\nmodels using knowledge world first, apply data model\nalternative models compare relative plausibility.Data contain causal information. “Correlation mean\ncausation.” Data measurements results process universe\nwish understand; data possibly reflective process,\ncontain information process . infer\ncausal relationships dataset alone. must construct possible causal\nmodels using knowledge world first, apply data model\nalternative models compare relative plausibility.data noise. usefulness model describe dataset \nrelated relative strength patterns noise dataset \nviewed lens model; conceptually, -called “signal \nnoise ratio” data. fundamental concern statistics quantifying\nuncertainty (.e. noise), separating real signal, though different\nstatistical approaches (e.g. frequentist vs Bayesian) reason uncertainty\ndifferent ways.data noise. usefulness model describe dataset \nrelated relative strength patterns noise dataset \nviewed lens model; conceptually, -called “signal \nnoise ratio” data. fundamental concern statistics quantifying\nuncertainty (.e. noise), separating real signal, though different\nstatistical approaches (e.g. frequentist vs Bayesian) reason uncertainty\ndifferent ways.Modeling begins (begin) posing one scientific models\nprocess phenomenon wish understand. scientific model \nconceptual; reflects belief universe proposes causal\nexplanation phenomenon. decide map scientific model\nonto statistical model, mechanical procedure quantifies\nwell scientific model explains dataset. scientific model \nstatistical model related independent choices make. may \nmany valid statistical models represent given scientific model. However,\nsometimes practice lack sufficient knowledge process propose\nscientific models first, requiring data exploration summarization first \nsuggest reasonable starting points.section pertains primarily models specified explicitly humans. \nanother class models, namely created certain machine learning\nalgorithms like neural networks deep learning, discover models \ndata. models fundamentally different designed human\nminds, often accurate therefore useful, can \ndifficult impossible understand work. \nimportant types models fall umbrella data science, limit\ncontent chapter human designed statistical models.","code":""},{"path":"data-science.html","id":"a-worked-modeling-example","chapter":"6 Data Science","heading":"6.1.1 A Worked Modeling Example","text":"example, let’s consider scenario wish assess whether \nthree genes can help us distinguish patients Parkinson’s\nDisease don’t measuring relative activity genes \nblood samples. following made-dataset:imaginary dataset 100 Parkinson’s 100 control subjects. \nsamples, sample ID, Disease Status Parkinson's Control,\nnumeric measurements three genes. violin\nplots (made-) data set three genes:inspection, appears Gene 1 relationship disease; may\nsafely eliminate gene consideration. Gene 2 appears \ndifferent profile depending disease status, control individuals \nhigher average expression lower variance. Unfortunately, despite \nqualitative difference, gene may useful telling whether someone\ndisease - ranges completely overlap. Gene 3 appears \ndiscriminate disease control. overlap two\nexpression distributions, certain expression value data\nsuggest high degree predictive accuracy may obtained gene.\nMeasuring gene may therefore useful, results dataset\ngeneralize people Parkinson’s Disease.far, done modeling, instead relied plotting \neyes. quantitative question might : much higher Gene 3\nexpression Parkinson’s Disease control? Another way posing \nquestion : know patient Parkinson’s Disease, Gene 3 expression\nvalue expect ? Written way, turned question\nprediction problem: information patient \nParkinson’s Disease, predicted expression value Gene 3?Another way pose prediction question opposite (arguably\nuseful) direction: knew person Gene 3 gene\nexpression, likely person Parkinson’s Disease? \ngene expression predictive enough person’s disease status, may \nviable biomarker disease thus\nmight useful clinical setting, example identifying\npresymptomatic individuals assessing efficacy pharmacological\ntreatment.Although may seems obvious, beginning model dataset, must\nstart posing scientific question concisely possible, \ndone . questions help us identify modeling techniques \nappropriate help us ensure interpret results correctly.use example dataset throughout chapter illustrate key\nconcepts.","code":"\ngene_exp## # A tibble: 200 x 5\n##    sample_name `Disease Status` `Gene 1` `Gene 2` `Gene 3`\n##    <chr>       <fct>               <dbl>    <dbl>    <dbl>\n##  1 P1          Parkinson's          257.     636.     504.\n##  2 P2          Parkinson's          241.     556.     484.\n##  3 P3          Parkinson's          252.     426.     476.\n##  4 P4          Parkinson's          271.     405.     458.\n##  5 P5          Parkinson's          248.     482.     520.\n##  6 P6          Parkinson's          275.     521.     460.\n##  7 P7          Parkinson's          264.     415.     568.\n##  8 P8          Parkinson's          276.     450.     495.\n##  9 P9          Parkinson's          274.     490.     496.\n## 10 P10         Parkinson's          251.     584.     573.\n## # ... with 190 more rows"},{"path":"data-science.html","id":"data-summarization","chapter":"6 Data Science","heading":"6.1.2 Data Summarization","text":"Broadly speaking, data summarization process finding \nlower-dimensional representation larger dataset. many ways \nsummarize set data; approach emphasize different aspects \ndataset, varying degrees accuracy. Consider gene expression \nGene 1 individuals example , plotted distribution \nhistogram:","code":"\nggplot(gene_exp, aes(x=`Gene 1`)) +\n  geom_histogram(bins=30,fill=\"#a93c13\")"},{"path":"data-science.html","id":"point-estimates","chapter":"6 Data Science","heading":"6.1.2.1 Point Estimates","text":"data concentrated around value 250, become less common \nlarger smaller values. Since extents left right middle\ndistribution appear equally distant, perhaps arithmetic mean\ngood way identify middle distribution:eye, mean seem correspond well value among \nfrequent, successfully captures important aspect data: \ncentral tendency. Summaries compute single number called point\nestimates. Point estimates collapse data one singular point, one\nvalue.arithmetic mean just one measure central tendency, computed taking\nsum values dividing number values. mean may \ngood point estimate central tendency dataset, sensitive\noutlier samples. Consider following examples:median another measure central tendency, found identifying\nvalue divides samples equal halves sorted smallest\nlargest. median robust presence outliers.","code":"\nggplot(gene_exp, aes(x=`Gene 1`)) +\n  geom_histogram(bins=30,fill=\"#a93c13\") +\n  geom_vline(xintercept=mean(gene_exp$`Gene 1`))\nlibrary(patchwork)\nwell_behaved_data <- tibble(data = rnorm(1000))\ndata_w_outliers <- tibble(data = c(rnorm(800), rep(5, 200))) # oops I add some outliers :^)\n\ng_no_outlier <- ggplot(well_behaved_data, aes(x = data)) +\n  geom_histogram(fill = \"#56CBF9\", color = \"grey\", bins = 30) +\n  geom_vline(xintercept = mean(well_behaved_data$data)) +\n  ggtitle(\"Mean example, no outliers\")\n\ng_outlier <- ggplot(data_w_outliers, aes(x = data)) +\n  geom_histogram(fill = \"#7FBEEB\", color = \"grey\", bins = 30) +\n  geom_vline(xintercept = mean(data_w_outliers$data)) +\n  ggtitle(\"Mean example, big outliers\")\n\ng_no_outlier | g_outlier\ng_no_outlier <- ggplot(well_behaved_data, aes(x = data)) +\n  geom_histogram(fill = \"#AFBED1\", color = \"grey\", bins = 30) +\n  geom_vline(xintercept = median(well_behaved_data$data)) +\n  ggtitle(\"Median example\")\n\ng_outlier <- ggplot(data_w_outliers, aes(x = data)) +\n  geom_histogram(fill = \"#7FBEEB\", color = \"grey\", bins = 30) +\n  geom_vline(xintercept = median(data_w_outliers$data)) +\n  ggtitle(\"Median example, big outliers\")\n\ng_no_outlier | g_outlier"},{"path":"data-science.html","id":"dispersion","chapter":"6 Data Science","heading":"6.1.2.2 Dispersion","text":"Central tendencies important aspects data don’t describe \ndata values outside point estimate central tendency; \nwords, expressed spread, dispersion data.decide perhaps computing standard\ndeviation data may\ncharacterize spread well, since appears symmetric around mean.\ncan layer information plot well inspect :width horizontal line proportional mean +/- one standard\ndeviation around mean, placed arbitrarily y axis y = 10 show range covers data histogram. +/- 1 standard\ndeviation around mean visually describes spread data reasonably\nwell.Measures spread data, typically around perceived center (mean).\nOften related distribution data.Standard deviation: measure close values mean. Bigger\nstandard deviations mean data spread .Variance:\nSimilar SD (’s square SD), variance measures far random value\nmean.","code":"\ng1_mean <- mean(gene_exp$`Gene 1`)\ng1_sd <- sd(gene_exp$`Gene 1`)\nggplot(gene_exp, aes(x=`Gene 1`)) +\n  geom_histogram(bins=30,fill=\"#a93c13\") +\n  geom_vline(xintercept=g1_mean) +\n  geom_segment(x=g1_mean-g1_sd, y=10, xend=g1_mean+g1_sd, yend=10)\ndata <- tibble(data = c(rnorm(1000, sd=1.75)))\nggplot(data, aes(x = data)) +\n  geom_histogram(fill = \"#EAC5D8\", color = \"white\", bins = 30) +\n  geom_vline(xintercept = c(-2, -1, 0, 1, 2) * sd(data$data)) +\n  xlim(c(-6, 6)) +\n  ggtitle(\"Standard deviations aplenty\", paste(\"SD:\", sd(data$data)))\ndata <- tibble(data = c(rnorm(1000, sd=0.5)))\nggplot(data, aes(x = data)) +\n  geom_histogram(fill = \"#DBD8F0\", color = \"white\", bins = 30) +\n  geom_vline(xintercept = mean(data$data)) +\n  xlim(c(-6, 6)) +\n  ggtitle(\"Same mean as SD plot, but different variance\",\n          paste(\"Variance:\", sd(data$data)))"},{"path":"data-science.html","id":"distributions","chapter":"6 Data Science","heading":"6.1.2.3 Distributions","text":"two pieces knowledge - mean accurately describes center \ndata standard deviation describes spread - now recognize \ndata may normally distributed, therefore\ncan potentially describe dataset mathematically. decide visually\ninspect possibility layering normal distribution top data\nusing\nstat_function:Note histogram bars scaled \naes(y=[after_stat](https://ggplot2.tidyverse.org/reference/aes_eval.html)(density))\ndensity values bin make bar heights sum 1 \ny scale matches normal distribution.now created first model: chose express dataset normal\ndistribution parameterized mean standard deviation standard\ndeviation data. Using values 254 11\nmean standard deviation, respectively, can express model\nmathematically follows:\\[\nGene\\;1 \\sim \\mathcal{N}(254, 11)\n\\]\\(\\sim\\) symbol means “distributed ” \\(\\mathcal{N}(\\mu,\\sigma)\\)\nrepresents normal distribution mean \\(\\mu\\) standard deviation \n\\(\\sigma\\). mathematical formulation means thing saying \nmodeling Gene 1 expression normal distribution mean 254 \nstandard deviation 11. Without additional information new sample,\nexpect expression gene 254, although may vary \nvalue.normal distribution common distribution observed nature, \nhardly one. proposed distributions instead\nsummarize data:\naddition normal distribution, also plotted samples drawn \ncontinuous uniform\ndistribution\n0 1, logistic\ndistribution \nsimilar normal distribution heavier “tails”, exponential\ndistribution.\nmany distributions , many discovered\narise nature encode different types processes relationships.notes data modeling example move :model choice totally subjective. looked data \ndecided normal distribution reasonable choice. many\nchoices made, equally valid, though\nmay describe data equally well.model choice totally subjective. looked data \ndecided normal distribution reasonable choice. many\nchoices made, equally valid, though\nmay describe data equally well.can’t know “correct” model data. eye, \nappears reasonably accurate summary. However, thing \ncorrect model; models simply better describing data \nothers. Recall models wrong, models useful. model\nmay useful, definitely wrong extent.can’t know “correct” model data. eye, \nappears reasonably accurate summary. However, thing \ncorrect model; models simply better describing data \nothers. Recall models wrong, models useful. model\nmay useful, definitely wrong extent.don’t know well model describes data yet. far ’ve\nused eyes choose model might good starting point\nconsidering data simple, yet quantified well \nmodel describes data, compared alternative models see \nbetter. discussed briefly later section.don’t know well model describes data yet. far ’ve\nused eyes choose model might good starting point\nconsidering data simple, yet quantified well \nmodel describes data, compared alternative models see \nbetter. discussed briefly later section.","code":"\ng1_mean <- mean(gene_exp$`Gene 1`)\ng1_sd <- sd(gene_exp$`Gene 1`)\nggplot(gene_exp, aes(x=`Gene 1`)) +\n  geom_histogram(\n    aes(y=after_stat(density)),\n    bins=30,\n    fill=\"#a93c13\"\n  ) +\n  stat_function(fun=dnorm, args = list(mean=g1_mean, sd=g1_sd), size=2)\ng_norm <- ggplot(tibble(data = rnorm(5000)), aes(x = data)) +\n  geom_histogram(fill = \"#D0FCB3\", bins = 50, color = \"gray\") +\n  ggtitle(\"Normal distribution\", \"rnorm(n = 1000)\")\n\ng_unif <- ggplot(tibble(data = runif(5000)), aes(x = data)) +\n  geom_histogram(fill = \"#271F30\", bins = 50, color = \"white\") +\n  ggtitle(\"Uniform distribution\", \"runif(n = 1000)\")\n\ng_logistic <- ggplot(tibble(data = rlogis(5000)), aes(x = data)) +\n  geom_histogram(fill = \"#9BC59D\", bins = 50, color = \"black\") +\n  ggtitle(\"Logistic distribution\", \"rlogis(n = 1000)\")\n\ng_exp <- ggplot(tibble(data = rexp(5000, rate = 1)), aes(x = data)) +\n  geom_histogram(fill = \"#6C5A49\", bins = 50, color = \"white\") +\n  ggtitle(\"Exponential distribution\", \"rexp(n = 1000, rate = 1)\")\n\n(g_norm | g_unif) / (g_logistic | g_exp)"},{"path":"data-science.html","id":"linear-models","chapter":"6 Data Science","heading":"6.1.3 Linear Models","text":"choice normal distribution model Gene 1 gene expression \ndescriptive; low-dimensional summary dataset. However, \ninformative; doesn’t tell us anything useful Gene 1\nexpression respect scientific question distinguishing \nParkinson’s Disease Control individuals. , need find \nmodel can make predictions may find useful receive new\ndata. , introduce new type model: linear model.linear model statistical model relates one outcome variable \nlinear combination (.e. sum) one explanatory variables. may \nexpressed mathematically follows:\\[\nY_i = \\beta_0 + \\beta_1 \\phi_1 ( X_{i1} ) + \\beta_2 \\phi_2 ( X_{i2} ) + \\ldots + \\beta_p \\phi_p ( X_{ip} ) + \\epsilon_i\n\\], \\(Y_i\\) outcome response variable wish model, \\(X_{ij}\\) \nexplanatory predictor variable \\(j\\) observatoin \\(\\), \\(\\beta_j\\) \ncoefficients estimated minimize difference predicted outcome\n\\(\\hat{Y_i}\\) observed \\(Y_i\\) observations. \\(\\phi_j\\) possibly\nnon-linear transformation explanatory variable \\(X_ij\\); note \nfunctions may non-linear long predicted outcome modeled \nlinear combination transformed variables. rest section \ndedicated worked example linear model gene expression data.Let us begin beeswarm plot plot Gene 3:expression values within disease status look like might \nnormally distributed just like Gene 1, let’s summarize group \narithmetic mean standard deviation , instead plot \ndistributions histograms:can make point estimate difference simply subtracting means:words, point estimate suggests average Parkinson’s\npatients 164.1 greater expression Controls. can\nplot relationship relatively simply:However, point estimate tells us nothing confident \ndifference. can using linear\nregression modeling Gene 3 \nfunction disease status:coefficient associated disease status Parkinson’s disease\nalmost exactly equal difference means. also note \ncoefficient labeled (Intercept) nearly equal mean control\nsamples (334.6). hood, simple linear model \ncalculation subtracting means group estimated \nmeans using data , instead point estimates. Another advantage\nusing lm() point estimate method model can estimate \nconfident model difference mean Parkinson’s \ncontrols subjects. Let’s print information model \n:see coefficient estimates intercept (.e. mean control\nexpression) increase Parkinson’s, also number terms,\nparticular Pr(>|t|) Multiple R-squared. former \np-value associated coefficient estimates, \nsmall, indicating us model confident \nestimated values. latter, multiple R-squared \\(R^2\\), describes much \nvariance data explained model found fraction\n0 1. model explains 77.7% variance data, \nsubstantial. \\(R^2\\) value also associated p-value, also \nsmall. Overall, statistics suggest model fits data well.can plot results linear model genes relatively\neasily:can also compute corresponding linear model fits, confirm \ncoefficients agree directions observed plot, well \nassociations significant FDR < 0.05:just performed first elementary differential expression analysis\nusing linear model. Specifically, examined genes \nstatistical relationship Parkinson’s disease. mechanics \nanalysis beyond scope book, later consider\ndifferential expression analysis packages chapter\n7 pattern familiar example.familiar logistic\nregression, might \nwondered didn’t model disease status, binary variable, \nfunction gene expression like Disease Status ~ Gene 3. several\nreasons , complete\nseparation principally\namong . logistic regression, complete separation occurs \npredictor values (e.g. gene expression) one outcome group greater \nsmaller ; .e. overlap values groups.\ncauses logistic regression fail converge, leaving genes \nstatistics even though genes potentially interesting! \nmethods (e.g. Firth’s Logistic\nRegression) \novercome problem, methods model gene expression function \noutcome variables developed first remain popular.Modeling Basics - R Data Science\nLeast squares regression","code":"\nlibrary(ggbeeswarm)\nggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) +\n  geom_beeswarm()\nexp_summ <- pivot_longer(\n  gene_exp,\n  c(`Gene 3`)\n) %>%\n  group_by(`Disease Status`) %>%\n  summarize(mean=mean(value),sd=sd(value))\n\npd_mean <- filter(exp_summ, `Disease Status` == \"Parkinson's\")$mean\nc_mean <- filter(exp_summ, `Disease Status` == \"Control\")$mean\n\nggplot(gene_exp, aes(x=`Gene 3`, fill=`Disease Status`)) +\n  geom_histogram(bins=20, alpha=0.6,position=\"identity\") +\n  annotate(\"segment\", x=c_mean, xend=pd_mean, y=20, yend=20, arrow=arrow(ends=\"both\", angle=90)) +\n  annotate(\"text\", x=mean(c(c_mean,pd_mean)), y=21, hjust=0.5, label=\"How different?\")\npd_mean - c_mean## [1] 164.0942\nggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) +\n  geom_beeswarm() +\n  annotate(\"segment\", x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean)\nfit <- lm(`Gene 3` ~ `Disease Status`, data=gene_exp)\nfit## \n## Call:\n## lm(formula = `Gene 3` ~ `Disease Status`, data = gene_exp)\n## \n## Coefficients:\n##                 (Intercept)  `Disease Status`Parkinson's  \n##                       334.6                        164.1\nsummary(fit)## \n## Call:\n## lm(formula = `Gene 3` ~ `Disease Status`, data = gene_exp)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -124.303  -25.758   -2.434   30.518  119.348 \n## \n## Coefficients:\n##                             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                  334.578      4.414   75.80   <2e-16 ***\n## `Disease Status`Parkinson's  164.094      6.243   26.29   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 44.14 on 198 degrees of freedom\n## Multiple R-squared:  0.7773, Adjusted R-squared:  0.7761 \n## F-statistic:   691 on 1 and 198 DF,  p-value: < 2.2e-16\npd_mean <- mean(filter(gene_exp,`Disease Status`==\"Parkinson's\")$`Gene 1`)\nc_mean <- mean(filter(gene_exp,`Disease Status`==\"Control\")$`Gene 1`)\ng1 <- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 1`, color=`Disease Status`)) +\n  geom_beeswarm() +\n  annotate(\"segment\", x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) +\n  theme(legend.position=\"none\")\n\npd_mean <- mean(filter(gene_exp,`Disease Status`==\"Parkinson's\")$`Gene 2`)\nc_mean <- mean(filter(gene_exp,`Disease Status`==\"Control\")$`Gene 2`)\ng2 <- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 2`, color=`Disease Status`)) +\n  geom_beeswarm() +\n  annotate(\"segment\", x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) +\n  theme(legend.position=\"none\")\n\npd_mean <- mean(filter(gene_exp,`Disease Status`==\"Parkinson's\")$`Gene 3`)\nc_mean <- mean(filter(gene_exp,`Disease Status`==\"Control\")$`Gene 3`)\ng3 <- ggplot(gene_exp, aes(x=`Disease Status`, y=`Gene 3`, color=`Disease Status`)) +\n  geom_beeswarm() +\n  annotate(\"segment\", x=0, xend=3, y=2*c_mean-pd_mean, yend=2*pd_mean-c_mean) +\n  theme(legend.position=\"none\")\ng1 | g2 | g3\nfit1 <- lm(`Gene 1` ~ `Disease Status`, data=gene_exp)\nfit2 <- lm(`Gene 2` ~ `Disease Status`, data=gene_exp)\nfit3 <- lm(`Gene 3` ~ `Disease Status`, data=gene_exp)\n\ngene_stats <- bind_rows(\n  c(\"Gene 1\",coefficients(fit1),summary(fit1)$coefficients[2,4]),\n  c(\"Gene 2\",coefficients(fit2),summary(fit2)$coefficients[2,4]),\n  c(\"Gene 3\",coefficients(fit3),summary(fit3)$coefficients[2,4])\n)\ncolnames(gene_stats) <- c(\"Gene\",\"Intercept\",\"Parkinson's\",\"pvalue\")\ngene_stats$padj <- p.adjust(gene_stats$pvalue,method=\"fdr\")\ngene_stats## # A tibble: 3 x 5\n##   Gene   Intercept        `Parkinson's`     pvalue                   padj\n##   <chr>  <chr>            <chr>             <chr>                   <dbl>\n## 1 Gene 1 250.410735540151 6.95982194659045  3.92131170913765e-06 3.92e- 6\n## 2 Gene 2 597.763814010886 -93.6291659250254 2.37368730897756e-13 3.56e-13\n## 3 Gene 3 334.57788193953  164.094204856583  1.72774902843408e-66 5.18e-66"},{"path":"data-science.html","id":"flavors-of-linear-models","chapter":"6 Data Science","heading":"6.1.4 Flavors of Linear Models","text":"linear model implemented termed linear\nregression due way \nmodels relationship predictor variables outcome.\nSpecifically, linear regression makes strong assumptions \nrelationship may always hold datasets. address \nlimitation, flexible class linear models called generalized linear\nmodels allow \nassumptions relaxed changing mathematical relationship \npredictors outcome using link\nfunction,\n/mathematically transforming predictors . \ncommon generalized linear models listed :Logistic regression - models binary outcome (e.g. disease vs control) \nlinear combination predictor variables using logistic\nfunction link function.Multinomial regression - models multinomial (.e. categorical variable\ntwo categories) using multinomial logit function link\nfunctionPoisson regression - models outcome variable count data \nlinear combination predictors using logarithm link function;\nmodel commonly used modeling certain types high throughput\nsequencing dataNegative binomial regression - also models outcome variable \ncount data relaxes assumptions Poisson regression, namely \nmean-variance equality constraint; negative binomial regression commonly used\nmodel gene expression estimated RNASeq data.Generalized linear models flexible, many types \nmodels used biology data science. important aware \ncharacteristics outcome wish model choose modeling\nmethod suitable.Generalized linear models","code":""},{"path":"data-science.html","id":"assessing-model-accuracy-.","chapter":"6 Data Science","heading":"6.1.5 Assessing Model Accuracy .","text":"","code":""},{"path":"data-science.html","id":"statistical-distributions","chapter":"6 Data Science","heading":"6.2 Statistical Distributions","text":"Biological data, like data, uncertain. Measurements always contain noise,\ncollection measurements always contain signal. field \nstatistics grew recognition mathematics can used quantify\nuncertainty help us reason whether signal exists within dataset,\ncertain signal. core, statistics \nseparating signal noise dataset rigorous precise way.One fundamental statistical tools used estimating uncertainty \nstatistical distribution, probability distribution, simply\ndistribution. two broad classes distributions: statistical, \ntheoretical, distributions empirical distributions. section \ndiscuss general properties distributions, briefly describe common\nprobability distributions, explain specify use \ndistributions R.","code":""},{"path":"data-science.html","id":"random-variables","chapter":"6 Data Science","heading":"6.2.1 Random Variables","text":"random variable object quantity depends upon random events \ncan samples drawn . example, six-sided die random\nvariable six possible outcomes, single roll die yield one\noutcomes probability (equal probability, die fair).\ncoin also random variable, heads tails outcome. gene’s\nexpression RNASeq experiment random variable, possible\noutcomes non-negative integer corresponding number reads \nmap . examples, outcome simple category real number,\nrandom variables can associated complex outcomes well,\nincluding trees, sets, shapes, sequences, etc. probability outcome\nspecified distribution associated random variable.Random variables usually notated capital letters, like \\(X,Y,Z,\\) etc. \nsample drawn random variable usually notated lowercase \nletter, e.g. \\(x\\) sample drawn random variable \\(X\\). \ndistribution random variable usually described like “\\(X\\) follows \nbinomial distribution” “\\(Y\\) normally distributed random variable”.probability random variable taking one possible values \nusually written \\(P(X = x)\\). value \\(P(X = x)\\) defined \ndistribution \\(X\\). see later p-values section, sometimes\nalso interested probability observing value \\(x\\) larger\n(smaller). probabilities written \\(P(X > x)\\) (\\(P(X < x)\\)). \nprobabilities computed described next section.","code":""},{"path":"data-science.html","id":"statistical-distribution-basics","chapter":"6 Data Science","heading":"6.2.2 Statistical Distribution Basics","text":"definition, statistical distribution function maps possible\nvalues variable often occur. Said differently, statistical\ndistribution used compute probability seeing single value, \nrange values, relative possible values, assuming random\nvariable question follows statistical distribution. following \nvisualization theoretical distribution normally distributed random\nvariable:plot depicts probability density function (PDF) normal\ndistribution mean zero standard deviation one. PDF defines\nprobability associated every possible value \\(x\\), \nnormal distribution real numbers. PDFs closed mathematical\nform. PDF normal distribution :\\[\nP(X = x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma}}\n\\]notation \\(P(X = x|\\mu,\\sigma)\\) read like “probability random\nvariable \\(X\\) takes value \\(x\\), given mean \\(\\mu\\) standard deviation\n\\(\\sigma\\)”. normal distribution example parametric distribution\nPDF requires two parameters - mean standard deviation - \ncompute probabilities. choice parameter values \\(\\mu\\) \\(\\sigma\\) \nnormal distribution determine probability value \\(x\\).Probability density functions defined continuous distributions ,\ndescribed later section. Discrete distributions probability\nmass function (PMF) instead PDF, since probability distribution \nsubdivided set different categories. PMFs different mathematical\ncharacteristics PDFs (e.g. continuous therefore \ndifferentiable), serve purpose.probability theory, plausible event probability zero, \nmean event can never occur. fact, every specific value \ndistribution supports real numbers probability zero (.e. one\nspecific value infinite number values). Instead, probability\ndistribution function allows us reason relative likelihood \nobserving values one range distribution compared others.\nvalues extremely small relative probabilities, never \nequal zero. due asymptotic properties probability\ndistributions, every supported\nvalue distribution\nnon-zero value definition, though many values may close \nzero.PDF provides probability density specific values within \ndistribution, sometimes interested probability value\nless equal particular value. compute using \ncumulative distribution function (CDF) sometimes just distribution\nfunction. plot , PDF CDF plotted normal\ndistribution mean zero standard deviation 1:value CDF corresponds area density curve \ncorresponding value \\(x\\); 1 minus value area curve\ngreater value. following figures illustrate :91% probability density less (arbitrary) value \\(x=1.37\\),\nlikewise 9% greater. p-values calculated, described\nlater.CDF also useful generating samples distribution. \nfollowing plot, 1000 random samples drawn uniform distribution \ninterval \\((0,1)\\) plotted using inverse CDF function:histograms margins show distributions \\(p\\) \\(x\\) values\nscatter points. \\(p\\) coordinates uniform, \\(x\\)\ndistribution normal mean zero standard deviation one. \nway, can draw samples normal distribution, distribution\ninvertible CDF.","code":"\ntibble(\n  x = seq(-4,4,by=0.1),\n  `Probability Density`=dnorm(x,0,1)\n) %>%\n  ggplot(aes(x=x,y=`Probability Density`)) +\n  geom_line() +\n  labs(title=\"Probability Density Function for a Normal Distribution\")\ntibble(\n  x = seq(-4,4,by=0.1),\n  PDF=dnorm(x,0,1),\n  CDF=pnorm(x,0,1)\n) %>%\n  ggplot() +\n  geom_line(aes(x=x,y=PDF,color=\"PDF\")) +\n  geom_line(aes(x=x,y=CDF,color=\"CDF\"),linetype=\"dashed\")"},{"path":"data-science.html","id":"distributions-in-r","chapter":"6 Data Science","heading":"6.2.3 Distributions in R","text":"four key operations performed normal distribution \nprevious section:Calculate probabilities using PDFCalculate cumulative probabilities using CDFCalculate value associated cumulative probabilitySample values parameterized distributionIn R, operations dedicated function different\ndistribution, prefixed d, p, q, r. normal distribution,\nfunctions dnorm, pnorm, qnorm, rnorm:dnorm(x, mean=0, sd=1) - PDF normal distributionpnorm(q, mean=0, sd=1) - CDF normal distributionqnorm(p, mean=0, sd=1) - inverse CDF; accepts quantiles 0 1 returns \nvalue distribution quantilesrnorm(n, mean=0, sd=1) - generate n samples normal distributionR uses scheme base distributions, include:many distributions\nimplemented R beyond , follow scheme.next two sections cover examples distributions.\nGenerally, statistical distributions divided two categories: discrete\ndistributions continuous distributions.","code":""},{"path":"data-science.html","id":"discrete-distributions","chapter":"6 Data Science","heading":"6.2.4 Discrete Distributions","text":"Discrete distributions defined countable, possibly infinite sets.\nCommon discrete distributions include binomial (e.g. coin flips), multinomial\n(e.g. dice rolls), Poisson (e.g. number WGS sequencing reads map \nspecific locus), etc. describe detail.","code":""},{"path":"data-science.html","id":"bernoulli-random-trail-and-more","chapter":"6 Data Science","heading":"6.2.4.1 Bernoulli random trail and more","text":"One examples discrete random variable distribution Bernoulli\nfunction. Bernoulli trail 2 outcomes probability \\(p\\) \\((1-p)\\).\nConsider flipping fair coin, random variable X can take value 0 1\nindicating get head tail. ’s fair coin, expect \n\\(Pr(X = 0) = Pr(X = 1) = 0.5\\). , throw die record X = 1 \nget six, X = 0 otherwise, \\(Pr(X = 0) = 5/6\\) \\(Pr(X = 1) = 1/6\\).Now consider slightly complicated situation: throwing die\n\\(n\\) times like analyze total number six, say \\(x\\), get\n\\(n\\) throws? Now, leads us binomial distribution.\n’s fair die, say proportion parameter \\(p = 1/6\\), means\nprobability getting six 1/6 throw.\\[\n\\begin{equation}\nf(x) = \\frac {n!} {x!(n-x)!}p^x (1-p) ^{(n-x)}\n\\end{equation}\n\\]geometric random variable, similar binomial, also \nsequence random Bernoulli trials constant probability parameter \\(p\\).\ntime, define random variable \\(X\\) number consecutive\nfailures first success. case, probability \\(x\\)\nconsecutive failures followed success trial \\(x+1\\) :\\[\n\\begin{equation}\nf(x) = p * (1-p)^x\n\\end{equation}\n\\]negative binomial distribution goes one step forward. time, \nstill performing sequence independent Bernoulli random trials \nconstant probability success equal \\(p\\). now like record\nrandom variable \\(Z\\) total number failures finally get\n\\(r^{th}\\) success. words, get \\(r^{th}\\) success, \n\\(x+r\\) Bernoulli random trails, \\(x\\) times failed \\(r\\) times\nsucceeded.\\[\n\\begin{equation}\nf(x) = \\frac {x+r-1} {r-1} p^r {(1-p)}^x\n\\end{equation}\n\\]","code":""},{"path":"data-science.html","id":"poisson","chapter":"6 Data Science","heading":"6.2.4.2 Poisson","text":"Poisson distribution used express probability given number\nevents occurring fixed interval time space, events\noccur known constant mean rate independently time since\nlast event. one understands definition.formula Poisson distribution :\\[\n\\begin{equation}\nf(k; \\lambda) = Pr(X=k) = \\frac {\\lambda^k e^{-\\lambda}} {k!}\n\\end{equation}\n\\]lambda expected value random variable Xk number occurrencese Euler’s number (e=2.71828)okay, formula makes even confusing.Imagine working mail reception center, responsibility\nreceive incoming letters. Assume number incoming letters \naffected day week season year. expected get\n20 letters average day. , actual number letters receive\nday perfectly 20.recorded number letters receive day month (30 days).\nfollowing plot, dot represents day. x-axis calender day,\ny-axis number letters receive day. Although average\nreceiving 20 letters day, actual number letters day\nvary lot.Now, let’s plot density plot data. \\(x\\)-axis number \nletters single day, \\(y\\)-axis probability.Since 30 data points, doesn’t look like good curve. ,\nworked mail reception 5000 days, becomes much closer\ntheoretical Poisson distribution lambda = 20.theoretical Poisson distribution lambda = 20:want know ’s probability receive, example, 18 letters,\ncan use dpois() function.want know probability receiving 18 less letters,\nuse ppois() function.cumulative area colored following plot:qpois() like “reverse” ppois().Let’s review definition Poisson distribution.“events occur fixed interval time space”, day;“events occur known constant mean rate”, 20 letters;“independently time since last event”, means number \nletters receive today independent letter receive tomorrow.\nwork today don’t guarantee less work tomorrow, just like real lifeNow let’s re-visit formula.\\[\n\\begin{equation}\nf(k; \\lambda) = Pr(X=k) = \\frac {\\lambda^k e^{-\\lambda}} {k!}\n\\end{equation}\n\\]lambda expected value \\(X\\), 20 letters example.\\(k\\) number occurrences, number letters get \nspecific day.e Euler’s number (e=2.71828)","code":"\nset.seed(2)\nmy_letter <- rpois(n = 30, lambda = 20)\nplot(my_letter,\n  main = \"Letters received each day\",\n  xlab = \"day of the month\", ylab = \"number of letters\",\n  pch = 19, col = \"royalblue\"\n)\nabline(a = 20, b = 0, lwd = 2, lty = 3, col = \"salmon\")\nplot(density(my_letter),\n  lwd = 2, col = \"royalblue\",\n  main = \"Probability of number of letters each day\",\n  xlab = \"number of letters\"\n)\nset.seed(3)\nplot(density(rpois(n = 5000, lambda = 20)),\n  lwd = 2, col = \"royalblue\",\n  main = \"Probability of number of letters each day\",\n  xlab = \"number of letters\"\n)\nplot(dpois(c(1:40), lambda = 20),\n  lwd = 2, type = \"l\", col = \"royalblue\",\n  ylab = \"probability\", main = \"Poisson lambda=20\"\n)\ndpois(x = 18, lambda = 20)## [1] 0.08439355\nppois(q = 18, lambda = 20, lower.tail = T)## [1] 0.3814219\nplot(dpois(c(1:40), lambda = 20),\n  lwd = 2, type = \"l\", col = \"royalblue\",\n  ylab = \"probability\", main = \"Poisson lambda=20\"\n)\npolygon(\n  x = c(1:18, 18),\n  y = c(dpois(c(1:18), lambda = 20), 0),\n  border = \"royalblue\", col = \"lightblue1\"\n)\nsegments(x0 = 18, y0 = 0, y1 = 0.08439355, lwd = 2, lty = 2, col = \"salmon\")\nqpois(p = 0.3814219, lambda = 20)## [1] 18"},{"path":"data-science.html","id":"continuous-distributions","chapter":"6 Data Science","heading":"6.2.5 Continuous Distributions","text":"contrast discrete distributions, continuous distributions defined\ninfinite, possibly bounded domains, e.g. real numbers. \nnumber different continuous distributions. focus Normal\ndistribution (Gaussian distribution). lot variables real life \nnormally distributed, common examples include people’s height, blood pressure,\nstudents’ exam score.normal distribution mean = 0 standard deviation = 1\nlooks like:Similar Poisson distribution , several functions work\nnormal distribution, including rnorm(), dnorm(), pnorm(), \nqnorm().rnorm() used draw random data points normal distribution \ngiven mean standard deviation.dnorm() density given quantile. instance, normal\ndistribution (mean=0, standard deviation=1) , probability density \n0.5 roughly 0.35.pnorm() gives distribution function. , can think \ncumulative left side density function given value,\narea colored light blue following plot.qnorm() gives quantile function. can think “reverse” \npnorm(). instance:Similarly, distributions chi-square distribution, \nset functions: rchisq(), dchisq(), pchisq() qchisq() etc.","code":"\nset.seed(2)\nnorm <- rnorm(n = 50000, mean = 0, sd = 1)\nplot(density(norm),\n  main = \"A Normal distribution\", xlab = \"x\",\n  lwd = 2, col = \"royalblue\"\n)\nset.seed(2)\nrnorm(\n  n = 6, # number of data points to draw\n  mean = 0, # mean\n  sd = 1 # standard deviation\n) ## [1] -0.89691455  0.18484918  1.58784533 -1.13037567 -0.08025176  0.13242028\ndnorm(x = 0.5, mean = 0, sd = 1)## [1] 0.3520653\nplot(density(norm),\n  main = \"A Normal distribution\", xlab = \"x\",\n  lwd = 2, col = \"royalblue\"\n)\nsegments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 3, col = \"salmon\")\nsegments(x0 = -5, y0 = 0.3520653, x1 = 0.5, lwd = 2, lty = 3, col = \"salmon\")\npoints(x = 0.5, y = 0.3520653, cex = 1.5, lwd = 2, col = \"red\")\ntext(x = 1.1, y = 0.36, labels = \"0.3521\", col = \"red2\")\npnorm(q = 0.5, mean = 0, sd = 1)## [1] 0.6914625\nplot(density(norm),\n  main = \"A Normal distribution\", xlab = \"x\",\n  lwd = 2, col = \"royalblue\"\n)\npolygon(\n  x = c(density(norm)$x[density(norm)$x <= 0.5], 0.5),\n  y = c(density(norm)$y[density(norm)$x <= 0.5], 0),\n  border = \"royalblue\", col = \"lightblue1\"\n)\nsegments(x0 = 0.5, y0 = 0, y1 = 0.3520653, lwd = 2, lty = 2, col = \"salmon\")\npnorm(q = 0.5, mean = 0, sd = 1)## [1] 0.6914625\nqnorm(p = 0.6914625, mean = 0, sd = 1)## [1] 0.5000001"},{"path":"data-science.html","id":"empirical-distributions","chapter":"6 Data Science","heading":"6.2.6 Empirical Distributions","text":"Empirical distributions describe relative frequency observed values \ndataset. Empirical distributions may shape, may visualized using\nmethods described Visualizing Distributions section, e.g. \ndensity plot. following beeswarm density plot \nvisualize made-dataset:Note \\(y\\) axis plot right scaled density, rather \ncount. distribution plotted density ensures values sum 1, thus\nmaking probability distribution. empirical distribution looks quite\ncomplicated, easily captured single mean confidence\ninterval.","code":""},{"path":"data-science.html","id":"statistical-tests","chapter":"6 Data Science","heading":"6.3 Statistical Tests","text":"","code":""},{"path":"data-science.html","id":"what-is-a-statistical-test","chapter":"6 Data Science","heading":"6.3.1 What is a statistical test","text":"statistical test application mathematics used analyze\nquantitative data. can described way understand \nindependent variable(s) study affect outcome dependent\nvariable. kind statistical test appropriate study depends \nnumber factors including variables characteristics, study design, data\ndistribution. independent variable () variable(s) controlled\nstudy determine effects dependent variable(s). \nstudies independent variable(s).","code":""},{"path":"data-science.html","id":"study-design","chapter":"6 Data Science","heading":"Study design","text":"number conditions study corresponds number levels \nindependent variable , example 1 condition = 1 level, 2 conditions = 2\nlevels, . different number independent variables \nstudy . study multiple independent variables, \nnumber levels. Statistical tests can categorized whether can\nhandle 1, 2, 2+ levels.2 levels, type grouping need \nconsidered: -subjects* repeated-measures**. -subject\ndesign means participant undergoes one condition whereas \nrepeated-measures design participant undergo conditions. third\ntype exists called matched groups, different subjects undergo different\nconditions matched important way, may \ntreated repeated-measure design selecting statistical test\n*Independent design = -subjects = -groups; confused variable.\n**Dependent design = repeated-measures = within-subjects = within-groups; also confused variable.\n","code":""},{"path":"data-science.html","id":"variables","chapter":"6 Data Science","heading":"Variables","text":"characteristics variables, independent (predictor) \ndependent (descriptor), help choose statistical test use.\nSpecifically, number , type, level measurement, \nhelp narrow choices select appropriate test.","code":""},{"path":"data-science.html","id":"number-of-variables","chapter":"6 Data Science","heading":"Number of Variables","text":"number independent dependent variables affect test\nselect. Test selection differ whether 1 2+ dependent\nvariables 0, 1, 2+ independent variables.","code":""},{"path":"data-science.html","id":"types-of-data-continuous-and-categorical","chapter":"6 Data Science","heading":"Types of Data (Continuous and Categorical)","text":"Typically bioinformatics, data subdivided 3 categories: Continuous,\nCategorical, Binary. Continuous data data can take real number\nvalues within either finite infinite range. example, height \ncontinuous variable: 152.853288cm (60.1783 ), 182.9704cm (72.0356 ),\n172.7cm(68in), 163cm(64.2in) 145.647cm (57.3413 ) technically\nvalid heights. Categorical data data can divided categories \ngroups (high, medium, low) (under10, 10-29, 30+). Binary\ndata, data can either one thing another (T/D, Y/N, 0/1, etc) falls\nCategorical data umbrella though may get mentioned separately\nelsewhere. Data exists sequential, positive integer form- number\nsiblings- called Discrete data (bonus 4th category!) typically ends \ntreated categorical data.","code":""},{"path":"data-science.html","id":"levels-of-measurement","chapter":"6 Data Science","heading":"Levels of Measurement","text":"main levels measurement use statistics Ratio, Interval,\nNominal, Ordinal. Ratio Interval levels distance \nmeasurements defined; biggest difference two Ratio\nmeasurements meaningful zero value (negative numbers). Height \ninches cm, Kelvin, number students class Ratio\nmeasurements. Interval measurements meaningful zero. Celsius \nFahrenheit arbitrary 0s- freezing point pure (specific\nkind ) ocean water- making standard temperature measurements type\nInterval. Ordinal measurements meaningful order values \nvariable/imprecise distances measurements, like socioeconomic status\n(upper, middle, lower) days since diagnosis (10, 10-39, 40-99,\n100+). Nominal measurements meaningful order values, like\ncountry origin yes/. Ratio Interval measurements continuous\nvariables Nominal Ordinal measurements categorical variables.","code":""},{"path":"data-science.html","id":"data-distribution-parametric-vs-nonparametric","chapter":"6 Data Science","heading":"Data Distribution (Parametric vs Nonparametric)","text":"third factor keep mind test selection applicable\nNON-nominal dependent variables. Parametric tests make assumption \npopulation data sourced normal Gaussian distribution.\npowerful tests data distribution assumption, \ndownside able used select cases. Nonparametric tests \nassume normal distribution therefore can used wide range \ncases, thought less likely find significance results.","code":""},{"path":"data-science.html","id":"common-statistical-tests","chapter":"6 Data Science","heading":"6.3.2 Common Statistical Tests","text":"common statistical tests quick overview run\nR. like information specific test, links \nincluded descriptionsThese statistical tests. ’s link can find \nmoreAnd ’s link Wikipedia’s list statistical tests want \noverload statistical tests ’s new\nhobby","code":""},{"path":"data-science.html","id":"one-sample-t-test","chapter":"6 Data Science","heading":"One-Sample T-Test","text":"Dependent Variables: 1, continuous [ratio interval]Independent Variables: 0 variablesDesign: 1 group, 1 levelParametric: YesMore t-test information shamelessly linked SPH\nNull Alternate (Research) Hypotheses\nZ-values(mentioned texts fully explain t-tests)\nOne-Sample t-tests come 3 flavors: two-tailed test, one-tailed test (upper tail), one-tailed test (lower tail).t.test() function allows multiple kinds t-tests. use \nfunction perform one sample t-test, need numeric vector \ndata want run data , mean want compare , \nconfidence level (1 - alpha), alternative hypothesis. t-test, like\nkinds t-tests, compare means calculating t-statistic,\np-value confidence intervals data’s mean.t: calculated t-statistic, uses formula : (mean(x) - mu)/(sd(x)*sqrt(n_samples))df: Degrees Freedom. Calculated : n_samples-1p: p-value t-test determined t-statistic df t-distribution table(conf.level) percent confidence interval: calculated interval true mean(x) conf.level % certainty.sample estimates, mean x: calculated mean xAccepting rejecting null hypothesis matter determining mean(x) outside percent confidence interval range, p-value determine significance results.t.test_x0, running two-tailed t-test vector 100 doubles\ngenerated rnorm() intended mean 0. Since \ntwo-tailed test, alternate hypothesis mean(x0) != 0 null\nhypothesis mean(x0) = 0Since mu=0 0 within range (-.323, 0.050), failed reject\nnull hypothesis set data","code":"\n# One-sample t-test\nt.test(x, #numeric vector of data \n       mu = 0, #True value of the mean. Defaults to 0\n       alternative = \"two.sided\", # (\"two.sided\", \"less\", \"greater\") depending on which you want to use. Defaults to \"two.sided\n       conf.level = 0.95 #1-alpha. Defaults to 0.95\n)\n#Example\nset.seed(10)\nx0 <- rnorm(100, 0, 1) #rnorm(n_samples, true_mean, standard_deviation)\n\nttestx0 <- t.test(x0, mu = 0, alternative = \"two.sided\", conf.level = 0.95) #actual running of the t-test\n#t.test(x0) yeilds the same results bc I used default values for mu, alternative, and conf.level\n\ntlessx0 <- t.test(x0, mu = 0, alternative = \"less\", conf.level = 0.90)\n\nknitr::knit_print(ttestx0) #display t-test output via knitr## \n##  One Sample t-test\n## \n## data:  x0\n## t = -1.4507, df = 99, p-value = 0.15\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -0.32331056  0.05021268\n## sample estimates:\n##  mean of x \n## -0.1365489"},{"path":"data-science.html","id":"unpaired-t-test","chapter":"6 Data Science","heading":"Unpaired T-Test","text":"Dependent Variables: 1, continuous [ratio interval]Independent Variables: 1Design: 2 groups, 2 levelsParametric: YesThe unpaired t-test functions similarly one-sample t-test except instead\nmu, uses dataset y. Variance assumed equal dataset\nx y.Moer information unpaired t-tests \nAnother R guide upaired t-testsOutput unpaired two sample t-test similar output \none-sample. Mu typically 0 (though one change ‘mu = n’ \nfunction call) test uses confidence interval (mean(x)-mean(y)),\n(-3.308, -2.775). Since mu exist within confidence\ninterval, null hypothesis can rejected.","code":"\n# One-sample t-test\nt.test(x, #numeric vector of data 1\n       y, #numeric vector of data 2\n       alternative = \"two.sided\", # (\"two.sided\", \"less\", \"greater\") depending on which you want to use. Defaults to \"two.sided\n       conf.level = 0.95 #1-alpha. Defaults to 0.95\n)\n#Unpaired t-test example\nset.seed(10)\nx <- rnorm(100, 0, 1)\ny <- rnorm(100, 3, 1)\n\nunpaired <- t.test(x, y)\n\nknitr::knit_print(unpaired)## \n##  Welch Two Sample t-test\n## \n## data:  x and y\n## t = -22.51, df = 197.83, p-value < 2.2e-16\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.308051 -2.775122\n## sample estimates:\n##  mean of x  mean of y \n## -0.1365489  2.9050374"},{"path":"data-science.html","id":"paired-t-test","chapter":"6 Data Science","heading":"Paired T-Test","text":"Dependent Variables: 1, continuous [ratio interval]Independent Variables: 1Design: 1 group, 2 levelsParametric: YesThe paired t-test works matched repeated-measures designs. Like \nunpaired t-test, second data vector need ‘mu’. \nrunning paired t-test make sure specify paired = TRUE \ncalling function x y length.information paired t-tests\nhereSimilar unpaired t-test, paired t-test looks differences\nx y. Unlike unpaired t-test, statistic creates runs \ntest third set data created differences x y. x\ny length n, r create third data set new = ((x1-y1), (x2-y2),...(xn-1-yn-1), (xn-yn)) run testing mean(new) sd(new)","code":"\n# One-sample t-test\nt.test(x, #numeric vector of data 1\n       y, #numeric vector of data 2\n       paired = TRUE, #defaults to FALSE\n       alternative = \"two.sided\", # (\"two.sided\", \"less\", \"greater\") depending on which you want to use. Defaults to \"two.sided\n       conf.level = 0.95 #1-alpha. Defaults to 0.95\n)\n#Unpaired t-test example\nset.seed(10)\nx <- rnorm(100, 0, 13)\ny <- rnorm(100, 3, 1)\n\nunpaired <- t.test(x, y, paired = TRUE)\n\nknitr::knit_print(unpaired)## \n##  Paired t-test\n## \n## data:  x and y\n## t = -3.7956, df = 99, p-value = 0.0002539\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -7.126787 -2.233560\n## sample estimates:\n## mean of the differences \n##               -4.680174"},{"path":"data-science.html","id":"chi-squared-test","chapter":"6 Data Science","heading":"Chi-Squared test","text":"Dependent Variables: 1, categoricalIndependent Variables: 1Design: 2 groups, 2 + levelsParametric: NoMore Information Chi-Squared tests","code":"\n#How to Run a chi-squared test\nchisq.test(x, # numeric vector, matrix, or factor\n           y # numeric vector; ignored if x is a matrix; factor of the same length as x if x is a factor\n)"},{"path":"data-science.html","id":"chi-square-goodness-of-fit_-_-","chapter":"6 Data Science","heading":"6.3.2.1 __Chi-Square Goodness-Of-Fit_ _{-}","text":"Dependent Variables: 1, categoricalIndependent Variables: 0Design: 1 groupParametric: NoMore Information Chi-Squared test one sampleThe chi-squared goodness--fit can compare one set outcomes given set probabilities determine likelihood test probabilities differing given. set given probabilities constitutes null hypothesis. example, dataset tested outcome frequencies: number times x[1], x[2], x[3], x[4] observed. null hypothesis says 4 observed equal number times, resulting chi-square test p-value p<0.001","code":"\n#How to Run a chi-square goodness of fit\nchisq.test(x, # numeric vector\n           p = #list of probabilities, like c(10, 10, 10, 70)/100), same length as x\n)\nx <- c(10,25, 18, 92)\n\nchisq.test(x, p=c(25, 25, 25, 25)/100)## \n##  Chi-squared test for given probabilities\n## \n## data:  x\n## X-squared = 117.43, df = 3, p-value < 2.2e-16"},{"path":"data-science.html","id":"wilcoxon-mann-whitney-test","chapter":"6 Data Science","heading":"Wilcoxon-Mann Whitney test","text":"Dependent Variables: 1, (ordinal, interval, ratio)Independent Variables: 1Design: 2 groups, 2 levelsParametric: NoMore information ","code":"wilcox.test(x,#Numeric vector of data\n            y #optional numeric vector of data)"},{"path":"data-science.html","id":"wilcoxon-signed-ranks-test","chapter":"6 Data Science","heading":"Wilcoxon Signed Ranks test","text":"Dependent Variables: 1, (ordinal, interval, ratio)Independent Variables: 1Design: 1 groups, 2 levelsParametric: NoMore information ","code":"\nwilcox.test(x,#Numeric vector of data\n            y, #numeric vector of data from 2nd level- same length\n            paired = TRUE)\n"},{"path":"data-science.html","id":"one-way-anova","chapter":"6 Data Science","heading":"One-Way ANOVA","text":"Dependent Variables: 1, continuousIndependent Variables: 1Design: 2+ groups, 2+ levelsParametric: YesMore information ","code":"summary(aov(formula, #formula specifying the model\n            data #data frame containing the variables in the formula))"},{"path":"data-science.html","id":"regressions","chapter":"6 Data Science","heading":"Regressions","text":"","code":""},{"path":"data-science.html","id":"factor-analysis","chapter":"6 Data Science","heading":"Factor Analysis","text":"","code":""},{"path":"data-science.html","id":"choosing-a-test","chapter":"6 Data Science","heading":"6.3.3 Choosing a Test","text":"Look Dependent Variable(s). many ? (1 2+)\nones exist continuum, fraction within valid range technically possible, though actual data might rounded? (Continuous vs Categorical)\nCategorical: values definite order ’s Nominal. Else ’s probably Ordinal\nones exist continuum, fraction within valid range technically possible, though actual data might rounded? (Continuous vs Categorical)Categorical: values definite order ’s Nominal. Else ’s probably OrdinalLook Independent Variable(s). many ? (0, 1, 2+)\nDefine types level measurement described 1a 2b 2c 1 independent variables\nmany conditions independent variable ? (= # levels)\nsubject (equivalent subjects) undergo condition/level? (‘Yes’ -> repeated-measures; ‘’ -> -groups\nDefine types level measurement described 1a 2b 2c 1 independent variablesHow many conditions independent variable ? (= # levels)subject (equivalent subjects) undergo condition/level? (‘Yes’ -> repeated-measures; ‘’ -> -groupsIf nominal data, can assume data normal distribution measured general population? (‘Yes’-> parametric; ‘’ -> non-parametric)","code":""},{"path":"data-science.html","id":"p-values","chapter":"6 Data Science","heading":"6.3.4 P-values","text":"statistics, p-value probability obtaining observed results assuming null hypothesis correct. words, p-value probability results happening chance selection normal population rather test condition.instance, imagine game randomly picks one four colors. game developer decides add 5th color, yellow, color now 1/5 chance winning 4/5 chance losing. many first games yellow lose row suspect dev forgot code ability yellow win? want wait < 5% chance yellow losing every single game row normal circumstances pointing dev, yellow lose 14/14 games (4.4% likelihood).visualize 14 losses row game chance 1/5 success rate occur less 5% instances, can run simulation 1000 instances 14 games 1/5 chance success displaying results frequencies success:situation, comparing yellow’s win rate significantly lower yellow equal chance winning. research hypothesis alternative hypothesis case yellow’s chance winning significantly lower rest colors null hypothesis yellow’s chance winning rest colors.\np-value test 0.043 43 1000 instances 14 rolls 1/5 chance success totaled 0 successes, number successes saw. instead looking 1 success less 14 rolls, p-value 0.19, simulation ran 147 instances 1 success 42 instances 0 success.Since p-values calculated percentage outcomes higher one observed, normal distribution may look something like :observed value 6, happens exactly 2 standard deviations mean. compute p-value instance, area curve need calculated can easily done looking appropriate values z-table. link finding area normal curveSPH’s information p-values ","code":"\nroll <- function(nrolls, noptions){\n outcomes <- sample(1:noptions, nrolls, TRUE)\n return(length(outcomes[outcomes == 1]))\n}\n\ndataset <- function(sample, nrolls, noptions){\n  simulations <- c()\n  for(i in 1:sample){simulations[i] <- roll(nrolls, noptions)}\n  return(simulations)\n}\n\nset.seed(18)\nYellow_Wins <- dataset(1000, 14, 5)\nhist(Yellow_Wins, freq=FALSE, right=FALSE)\nset.seed(2)\nnorm <- rnorm(n = 50000, mean = 4, sd = 1)\nplot(density(norm),\n     main = \"P-Values on a Normal Distribution\", xlab = \"X-Values\",\n     lwd = 2, col = \"royalblue\"\n)\npolygon(\n  x = c(density(norm)$x[density(norm)$x >= 6], 6),\n  y = c(density(norm)$y[density(norm)$x >= 6], 0),\n  border = \"royalblue\", col = \"lightblue1\"\n)\nsegments(x0 = 6, y0 = 0, y1 = 0.4, lwd = 2, lty = 2, col = \"salmon\")\ntext(7, 0.2, \"Observed Value (@ 2sd)\")\ntext(3.9, 0.01, \"Instances higher than crit. val ->\")"},{"path":"data-science.html","id":"multiple-hypothesis-testing-.","chapter":"6 Data Science","heading":"6.3.5 Multiple Hypothesis Testing .","text":"multiply hypothesis testing adjustment? important? \ncommon adjustment methods (Bonferroni (FWER) Benjamini-Hochberg (FDR))?\ninterpret adjusted p-values (depends adjustment method)? \ncompute adjusted p-values R?","code":""},{"path":"data-science.html","id":"statistical-power","chapter":"6 Data Science","heading":"6.3.6 Statistical power","text":"Statistical Power probability null hypothesis actually false data enough evidence reject . short, failure reject null hypothesis presence significant effect. Type II error, also known false negative. counterpart, Type error, probability null hypothesis rejected significant effect.chance Type error represented alpha, alpha use calculate significance level (1-alpha). Type II errors represented beta. relationship statistical power beta similar relationship significance level alpha: statistical power calculated 1-beta.Read relationship Type II Type errorsTo calculate beta, find area curve research hypothesis distribution opposite side critical value line alpha calculated. find area tail distribution, need reference Z-score chart, expected course. wish read link finding area normal curve z-scores, one linked Code sourced :\ncaracal, “find probability type II error”, stats.stackexchange.com, Feb 19, 2011, link, Date Accessed: Feb 28, 2022 graph shows relationship alpha beta. Since alternative hypothesis looking higher mean, alpha calculated area null hypothesis curve right side critical value beta calculated area alternative hypothesis left critical value line. experiment get higher p-value critical value, Beta increase relationship Alpha decreasing.","code":""},{"path":"data-science.html","id":"applications-of-statistical-power","chapter":"6 Data Science","heading":"Applications of Statistical Power","text":"One applications statistical power, besides measure Type II error probability, allow run power analysis. power analysis involves using relationship Effect Size, Sample Size, Significance, Statistical Power three four parts order find value fourth. typically used find effect size study, often hardest estimate, find sample size corresponds desired effect size designing study.\nreading statistical power power analysis","code":""},{"path":"data-science.html","id":"effect-size","chapter":"6 Data Science","heading":"Effect Size","text":"effect size measurement magnitude experimental effects. number ways calculate effect size including limited Cohen’s D, Pearson’s R, Odds Ratio. like read BU’s SPH say , SPH module link\nCitations:\nDorey FJ. Statistics brief: Statistical power: used?. Clin Orthop Relat Res. 2011;469(2):619-620. doi:10.1007/s11999-010-1435-0Parab S, Bhalerao S. Choosing statistical test. Int J Ayurveda Res. 2010;1(3):187-191. doi:10.4103/0974-7788.72494\nIntroduction SAS. UCLA: Statistical Consulting Group. https://stats.idre.ucla.edu/sas/modules/sas-learning-moduleintroduction---features--sas/ (accessed February 24, 2022)Ranganathan P, Gogtay NJ. Introduction Statistics - Data Types, Distributions Summarizing Data. Indian J Crit Care Med. 2019 Jun;23(Suppl 2):S169-S170. doi: 10.5005/jp-journals-10071-23198. PMID: 31485129; PMCID: PMC6707495.","code":""},{"path":"data-science.html","id":"exploratory-data-analysis","chapter":"6 Data Science","heading":"6.4 Exploratory Data Analysis","text":"far, discussed methods chose explicit model summarize\ndata. However, sometimes don’t enough information data \npropose reasonable models, earlier exploring distribution \nimaginary gene expression dataset. may patterns data \nemerge compute different summaries ask whether non-random\nstructure individual samples features compare one another.\ntypes methods use take dataset examine structure\nwithout prefigured hypothesis called exploratory analysis key\napproach working biological data.","code":""},{"path":"data-science.html","id":"principal-component-analysis","chapter":"6 Data Science","heading":"6.4.1 Principal Component Analysis","text":"common method exploratory analysis principal component\nanalysis PCA.\nPCA statistical procedure identifies called directions \northogonal variance capture covariance different dimensions \ndataset. approach captures covariance features \narbitrary dimension, often used -called dimensionality\nreduction, \nlarge amount variance dataset potentially large number \ndimensions may expressed terms set basis\nvectors smaller dimension.\nmathematical details approach beyond scope book, \nexplain general terms intuition behind PCA , \npresent example used biological context.PCA decomposes dataset set orthonormal basis\nvectors collectively\ncapture variance dataset, first basis vector, called\nfirst principal component explains largest fraction variance,\nsecond principal component explains second largest fraction, .\nalways equal number principal components dimensions\ndataset number samples, whichever smaller. Typically \nsmall number components needed explain variance.principal component \\(p\\)-dimensional unit\nvector (.e. vector magnitude\n1), \\(p\\) number features dataset, values \nweights describe component’s direction variance. multiplying \ncomponent values sample, obtain projection \nsample respect basis component. projections \nsample made principal component produces rotation dataset \n\\(p\\) dimensional space. figure presents geometric intuition PCA.Principal Component Analysis - Geometric Intuition IllustrationMany biological datasets, especially make genome-wide measurements\nlike gene expression assays, many thousands features (e.g. genes)\ncomparatively samples. Since PCA can determine maximum number \nprincipal components smaller number features samples, \nalmost always many components samples. demonstrate ,\nperform PCA using \nstats::prcomp()\nfunction example microarray gene expression intensity dataset:result prcomp() list five members:sdev - standard deviation (.e. square root variance) componentrotation - matrix principal components columnsx - projections original data, aka rotated data matrixcenter - center=TRUE passed, vector feature meansscale - scale=TRUE passed, vector feature variancesRecall principal component explains fraction overall variance\ndataset. sdev variable returned prcomp() may used first\ncalculate variance explained component squaring , dividing\nsum:first component explains nearly 20% variance dataset, followed\nsecond component 12%, third component 9%, . \ncumulative variance plot right shows top 15 components \nrequired capture 90% variance dataset. suggests \nsample contributes significant amount variance overall dataset, \nstill features covary among .important use PCA results identification outlier samples. \naccomplished plotting projections sample examining \nresult eye identify samples “far away” samples.\nusually done inspection outlier samples chosen subjectively;\ngeneral rules decide sample outlier method.\nprojections components 1 2 plotted \nscatter plot:eye, samples appear obvious outliers. However, plot just one\nmany pairs projections. plot pairs first six\ncomponents using ggpairs() function \nGGally package:already nearly unintelligible mostly uninformative. \ncommon projections pairs components plotted, alternative way\nvisualize projections across samples plotting distribution \nprojections component using beeswarm plot:Now can see projections components plot, although\nsee relate one another.projection plots may become useful layer additional\ninformation samples. two types samples dataset (\nlabelled C. can color pairwise scatter plot type like :Little pattern obvious plot, can plot pairs components \nnow type information:Examining PC3 PC4, now observe may indeed genes \ndistinguish types based separation projection scores \ntwo types. Finally, can also color beeswarm plot type:approaches plotting results PCA complementary, \nmay useful understanding structure dataset.","code":"\n# intensities contains microarray expression data for ~54k probesets x 20 samples\n\n# transpose expression values so samples are rows\nexpr_mat <- intensities %>%\n  pivot_longer(-c(probeset_id),names_to=\"sample\") %>%\n  pivot_wider(names_from=probeset_id)\n\n# PCA expects all features (i.e. probesets) to be mean-centered,\n# convert to dataframe so we can use rownames\nexpr_mat_centered <-  as.data.frame(\n  lapply(dplyr::select(expr_mat,-c(sample)),function(x) x-mean(x))\n)\nrownames(expr_mat_centered) <- expr_mat$sample\n\n# prcomp performs PCA\npca <- prcomp(\n  expr_mat_centered,\n  center=FALSE, # prcomp centers features to have mean zero by default, but we already did it\n  scale=TRUE # prcomp scales each feature to have unit variance\n)\n\n# the str() function prints the structure of its argument\nstr(pca)## List of 5\n##  $ sdev    : num [1:20] 101.5 81 71.2 61.9 57.9 ...\n##  $ rotation: num [1:54675, 1:20] 0.00219 0.00173 -0.00313 0.00465 0.0034 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:54675] \"X1007_s_at\" \"X1053_at\" \"X117_at\" \"X121_at\" ...\n##   .. ..$ : chr [1:20] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n##  $ center  : logi FALSE\n##  $ scale   : Named num [1:54675] 0.379 0.46 0.628 0.272 0.196 ...\n##   ..- attr(*, \"names\")= chr [1:54675] \"X1007_s_at\" \"X1053_at\" \"X117_at\" \"X121_at\" ...\n##  $ x       : num [1:20, 1:20] -67.94 186.14 6.07 -70.72 9.58 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:20] \"A1\" \"A2\" \"A3\" \"A4\" ...\n##   .. ..$ : chr [1:20] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n##  - attr(*, \"class\")= chr \"prcomp\"\nlibrary(patchwork)\npca_var <- tibble(\n  PC=factor(str_c(\"PC\",1:20),str_c(\"PC\",1:20)),\n  Variance=pca$sdev**2,\n  `% Explained Variance`=Variance/sum(Variance)*100,\n  `Cumulative % Explained Variance`=cumsum(`% Explained Variance`)\n)\n\nexp_g <- pca_var %>%\n  ggplot(aes(x=PC,y=`% Explained Variance`,group=1)) +\n  geom_point() +\n  geom_line() +\n  theme(axis.text.x=element_text(angle=90,hjust=0,vjust=0.5)) # make x labels rotate 90 degrees\n\ncum_g <- pca_var %>%\n  ggplot(aes(x=PC,y=`Cumulative % Explained Variance`,group=1)) +\n  geom_point() +\n  geom_line() +\n  ylim(0,100) + # set y limits to [0,100]\n  theme(axis.text.x=element_text(angle=90,hjust=0,vjust=0.5))\n\nexp_g | cum_g\nas_tibble(pca$x) %>%\n  ggplot(aes(x=PC1,y=PC2)) +\n  geom_point()\nlibrary(GGally)\nas_tibble(pca$x) %>%\n  dplyr::select(PC1:PC6) %>%\n  ggpairs()\nas_tibble(pca$x) %>%\n  pivot_longer(everything(),names_to=\"PC\",values_to=\"projection\") %>%\n  mutate(PC=fct_relevel(PC,str_c(\"PC\",1:20))) %>%\n  ggplot(aes(x=PC,y=projection)) +\n  geom_beeswarm() + labs(title=\"PCA Projection Plot\") +\n  theme(axis.text.x=element_text(angle=90,hjust=0,vjust=0.5))\nas_tibble(pca$x) %>%\n  mutate(type=stringr::str_sub(rownames(pca$x),1,1)) %>%\n  ggplot(aes(x=PC1,y=PC2,color=type)) +\n  geom_point()\nlibrary(GGally)\nas_tibble(pca$x) %>%\n  mutate(type=stringr::str_sub(rownames(pca$x),1,1)) %>%\n  dplyr::select(c(type,PC1:PC6)) %>%\n  ggpairs(columns=1:6,mapping=aes(fill=type))\nas_tibble(pca$x) %>%\n  mutate(\n    sample=rownames(pca$x),\n    type=stringr::str_sub(sample,1,1)\n  ) %>%\n  pivot_longer(PC1:PC20,names_to=\"PC\",values_to=\"projection\") %>%\n  mutate(PC=fct_relevel(PC,str_c(\"PC\",1:20))) %>%\n  ggplot(aes(x=PC,y=projection,color=type)) +\n  geom_beeswarm() + labs(title=\"PCA Projection Plot\") +\n  theme(axis.text.x=element_text(angle=90,hjust=0,vjust=0.5))"},{"path":"data-science.html","id":"cluster-analysis","chapter":"6 Data Science","heading":"6.4.2 Cluster Analysis","text":"Cluster analysis simple clustering task grouping objects together\nobjects within group, cluster, similar \nobjects clusters. type exploratory data\nanalysis seeks \nidentify structure organization dataset without making strong\nassumptions data testing specific hypothesis. Many different\nclustering algorithms\ndeveloped employ different computational strategies \ndesigned data different types properties. choice algorithm\noften dependent upon type data clustered also \ncomparative performance different clustering algorithms applied \ndata. Different algorithms may identify different types patterns, \none clustering method better general.goal clustering may easily illustrated following plot:, goal clustering take unlabeled data \nstructure like left label data points similar one another\ncluster. full treatment cluster analysis beyond \nscope book, discuss common general\nclustering algorithms used biology bioinformatics.","code":"\nunclustered <- ggplot(well_clustered_data, aes(x=f1,y=f2)) +\n  geom_point() +\n  labs(title=\"Unclustered\")\nclustered <- ggplot(well_clustered_data, aes(x=f1,y=f2,color=cluster,shape=cluster)) +\n  geom_point() +\n  labs(title=\"Clustered\")\n\nunclustered | clustered"},{"path":"data-science.html","id":"hierarchical-clustering","chapter":"6 Data Science","heading":"6.4.2.1 Hierarchical Clustering","text":"Hierarchical clustering form clustering clusters data points\ntogether nested, hierarchical, groups based dissimilarity \none another. two broad strategies accomplish :Agglomerative: data points start groups, groups \niteratively merged hierarchically larger groups based similarity\ndata points added groupDivisive: data points start group, recursively\nsplit smaller groups based dissimilarityWhichever approach used, critical step clustering choosing \ndistance function \nquantifies dissimilar two data points . Distance functions, metrics\nnon-negative real numbers whose magnitude proportional notion \ndistance pair points. many different distance\nfunctions\nchoose , choice made based type data \nclustered. numeric data, euclidean\ndistance, corresponds\nlength line segment drawn two points \\(n\\)-dimensional\nEuclidean space often \nreasonable choice.divisive strategy conceptually inverse \nagglomerative clustering, limit description agglomerative\nclustering. distance function chosen, distances \npairs points computed two nearest points merged group.\nnew group points used computing distances points \nset using linkage function\ndefines group summarized single point purposes\ndistance calculation. linkage function distinguishes different\nclustering algorithms, include:Single-linkage - distance two groups computed distance two nearest members groupsComplete-linkage - distance two groups computed distance two farthest members groupsUnweighted Pair Group Method Arithmetic mean (UPGMA) - distance two groups average distance pairs points groupsWeighted Pair Group Method Arithmetic mean (WPGMA) - similar UPGMA, weights distances pairs groups evenly mergingThe choice linkage function (therefore clustering algorithm) \ndetermined based knowledge dataset assessment clustering\nperformance. general, one linkage function always\nperform well.following figure illustrates simple example clustering 1 dimensional\nset points using WPGMA:Conceptual illustration agglomerative hierarchical clusteringBelow cluster synthetic dataset introduced R:synthetic dataset two distinct groups samples drawn \nmultivariate normal samples. hierarchically cluster samples, use \ndist()\nhclust()\nfunctions:hclust() return object describes clustering tree can \nvisualized using dendrogram:\ndata indeed cluster well, samples group cluster\ntogether perfectly. See dendrogram section data\nvizualization chapter detail plot clustering\nresults.sometimes desirable use split hierarchical clustering groups based\npattern. clustering, three discrete clusters corresponding\nsample groups clearly visible. wished separate three groups,\ncan use \ncutree\ndivide tree three groups using k=3:can use samples labels color original plot desired:able recover correct clustering dataset easy \ncluster construction. Real data seldom well behaved.","code":"\nggplot(well_clustered_data, aes(x=f1,y=f2)) +\n  geom_point() +\n  labs(title=\"Unclustered\")\n# compute all pairwise distances using euclidean distance as the distance metric\neuc_dist <- dist(dplyr::select(well_clustered_data,-ID))\n\n# produce a clustering of the data using the hclust for hierarchical clustering\nhc <- hclust(euc_dist, method=\"ave\")\n\n# add ID as labels to the clustering object\nhc$labels <- well_clustered_data$ID\n\n\nstr(hc)## List of 7\n##  $ merge      : int [1:59, 1:2] -50 -33 -48 -46 -44 -21 -11 -4 -36 -3 ...\n##  $ height     : num [1:59] 0.00429 0.08695 0.23536 0.24789 0.25588 ...\n##  $ order      : int [1:60] 8 9 7 6 19 18 11 16 4 14 ...\n##  $ labels     : chr [1:60] \"A1\" \"A2\" \"A3\" \"A4\" ...\n##  $ method     : chr \"average\"\n##  $ call       : language hclust(d = euc_dist, method = \"ave\")\n##  $ dist.method: chr \"euclidean\"\n##  - attr(*, \"class\")= chr \"hclust\"\nlibrary(ggdendro)\nggdendrogram(hc)\nlabels <- cutree(hc,k=3)\nlabels##  A1  A2  A3  A4  A5  A6  A7  A8  A9 A10 A11 A12 A13 A14 A15 A16 A17 A18 A19 A20 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n##  B1  B2  B3  B4  B5  B6  B7  B8  B9 B10 B11 B12 B13 B14 B15 B16 B17 B18 B19 B20 \n##   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 \n##  C1  C2  C3  C4  C5  C6  C7  C8  C9 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 \n##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n# we turn our labels into a tibble so we can join them with the original tibble\nwell_clustered_data %>%\n  left_join(\n    tibble(\n      ID=names(labels),\n      label=as.factor(labels)\n    )\n  ) %>%\n  ggplot(aes(x=f1,y=f2,color=label)) +\n  geom_point() +\n  labs(title=\"Clustered\")"},{"path":"data-science.html","id":"k-means-.","chapter":"6 Data Science","heading":"6.4.3 k-means .","text":"","code":""},{"path":"data-science.html","id":"others","chapter":"6 Data Science","heading":"6.4.4 Others","text":"","code":""},{"path":"data-science.html","id":"network-analysis","chapter":"6 Data Science","heading":"6.5 Network Analysis","text":"basic components network vertex (node), \nfeatures subjects interested . edges connections \nvertices. general context, one example relationship network. \nvertices people. two people friends, draw edge connect .\nbiology context, vertices may genes, edges represent whether\ncorrelations two genes.Vertices edges can attributes. example, social interaction\nnetwork, may assign different colors vertices according student’s\nmajor. connect two students know , can also assign\nweights edges represent often interact .igraph package provides functions represent \nanalyze networks several language, including R. \nexamples , use igraph package demonstrate concepts\ndescribed section.sample_gnm(n,m) function\ngenerates random network \\(n\\) nodes \\(m\\) edges . \nuse function generate random networks different types.cyclic network least one directed path starts ends \nnode.acyclic network , hand, cycle path.edges directed network point one specific direction (, edges\ncan point directions, equivalent undirected edges). One\ncommon example network representing cell metabolic process, certain\nproteins biochemically modify, e.g. phosphorylate, proteins.edges indirect network direction. Common examples\ninclude social interaction network gene correlation network.","code":"\nlibrary(igraph)\nset.seed(2)\nsamp_g <- igraph::sample_gnm(n = 4, m = 4)\nvertex_attr(samp_g) <- list(color = rep(\"slategray1\", gorder(samp_g)))\nplot(samp_g)\nset.seed(1)\nsamp_g <- igraph::sample_gnm(n = 5, m = 4)\nvertex_attr(samp_g) <- list(color = rep(\"slategray1\", gorder(samp_g)))\nplot(samp_g)\nset.seed(2)\nsamp_g <- sample_gnm(n = 5, m = 7, directed = TRUE)\nvertex_attr(samp_g) <- list(color = rep(\"slategray1\", gorder(samp_g)))\nplot(samp_g)\nset.seed(3)\nsamp_g <- sample_gnm(n = 5, m = 7, directed = FALSE)\nvertex_attr(samp_g) <- list(color = rep(\"slategray1\", gorder(samp_g)))\nplot(samp_g)"},{"path":"data-science.html","id":"basic-network-analysis","chapter":"6 Data Science","heading":"6.5.1 Basic network analysis","text":"","code":""},{"path":"data-science.html","id":"shortest-path","chapter":"6 Data Science","heading":"6.5.1.1 Shortest path","text":"shortest path two vertices … yes, shortest path can go one vertex . may one shortest paths two vertices.following network, many ways go “Moon_Capital” “Crystal_Corridor”, ’s one shortest path, “Moon_Capital” -> “Gryphon’s_Nest” -> “Crystal_Corridor”. hand, want go “Moon_Capital” “Icy_Turret”, two possible ways: one “Gryphon’s_Nest”, one “Agate_Trademart”.igraph, can calculated using shortest_paths().One thing keep mind , weights assigned edges considered, shortest path may path goes least number vertices. example, consider weight assigned edge connecting “Moon_Capital” “Gryphon’s_Nest” substantially large. Royal Priest warned Gryphons live top hill, ’s hard climb.Let’s color edges according weights. Green means 1, blue means 2 red means 3. Now, shortest path “Moon_Capital” “Crystal_Corridor” becomes “Moon_Capital” -> “Agate_Trademart” -> “Icy_Turret” -> “Crystal_Corridor”.can conveniently find shortest path weights assigned!","code":"\nshortest_paths(samp_g, from = \"Moon_Capital\", to = \"Crystal_Corridor\")$vpath## [[1]]\n## + 3/5 vertices, named, from 6b2bdd0:\n## [1] Moon_Capital     Gryphon's_Nest   Crystal_Corridor\nset.seed(4)\nnames <- c(\"Moon_Capital\", \"Icy_Turret\", \"Crystal_Corridor\", \"Gryphon's_Nest\", \"Agate_Trademart\")\nsamp_g <- sample_gnm(n = 5, m = 7)\nvertex_attr(samp_g) <- list(name = names, color = rep(\"slategray1\", gorder(samp_g)))\nedge_attr(samp_g) <- list(\n  color = c(\"deepskyblue\", \"salmon\", \"deepskyblue\", \"deepskyblue\", \"palegreen3\", \"palegreen3\", \"salmon\"),\n  weights = c(2, 3, 2, 2, 1, 1, 3)\n)\nplot(samp_g,\n  vertex.label.cex = 1,\n  vertex.label.dist = 3, edge.width = c(3, 1, 1, 1, 3, 3, 1)\n)\nshortest_paths(samp_g,\n  from = \"Moon_Capital\", to = \"Crystal_Corridor\",\n  weights = edge_attr(samp_g, \"weights\")\n)$vpath## [[1]]\n## + 4/5 vertices, named, from 6b4e4d4:\n## [1] Moon_Capital     Agate_Trademart  Icy_Turret       Crystal_Corridor"},{"path":"data-science.html","id":"vertex-centrality","chapter":"6 Data Science","heading":"6.5.1.2 Vertex centrality","text":"degree vertex number edges connected . igraph, can calculated using degree().centrality vertex indicates important network. instance, seeing social interaction network, one question can ask “person influential power community?”. many different types centrality, go basics.extent, degree can used evaluate importance vertex. vertex connected lot vertices, naturally appears important ability pass information vertices. Assume just opened new -line shop want attract new customers many possible, start reaching person know largest number friends., sometimes, people largest number friends may necessarily one can “spread word” whole community efficiently. example , although H lot connections, fact, seems like pass information G, whole community influenced quickly.leads us closeness centrality, sum length shortest paths node nodes graph. Although H lot friends, takes long reach side graph. calculating closeness centrality using closeness() function calculate closeness centrality.Betweenness centrality number shortest paths pass vertex. example, G also one highest betweenness centrality.","code":"\ng <- graph_from_data_frame(data.frame(\n  \"from\" = c(\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"A\", \"G\", \"H\", \"H\", \"H\", \"H\", \"H\", \"M\", \"I\", \"G\", \"N\", \"N\"),\n  \"to\" = c(\"B\", \"C\", \"D\", \"E\", \"F\", \"D\", \"E\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"K\", \"J\", \"N\", \"O\", \"P\")\n), directed = F)\nvertex_attr(g) <- list(name = vertex_attr(g, \"name\"), color = rep(\"slategray1\", gorder(g)))\nplot(g)\ndegree(g)## A B C G H M I N D E F J K L O P \n## 6 2 2 3 6 2 2 3 2 2 1 2 2 1 1 1\ncloseness(g)##          A          B          C          G          H          M          I \n## 0.03225806 0.02272727 0.02272727 0.03703704 0.03225806 0.02272727 0.02272727 \n##          N          D          E          F          J          K          L \n## 0.02702703 0.02272727 0.02272727 0.02222222 0.02272727 0.02272727 0.02222222 \n##          O          P \n## 0.01960784 0.01960784\nbetweenness(g)##  A  B  C  G  H  M  I  N  D  E  F  J  K  L  O  P \n## 58  0  0 72 58  0  0 27  0  0  0  0  0  0  0  0"},{"path":"data-science.html","id":"create-your-own","chapter":"6 Data Science","heading":"6.5.2 Create your own","text":"Check network visualization section see detailed illustration create network scratch, customize !","code":""},{"path":"data-visualization.html","id":"data-visualization","chapter":"7 Data Visualization","heading":"7 Data Visualization","text":"Data visualization core component exploring data communicating\nresults others. goal data visualization present data \ngraphical way shows reader patterns otherwise \nvisible. Despite ubiquity importance, effective data visualization \nchallenging, many tools approaches exist “gold\nstandard” follow meaningful sense. Rather, effective visualization\nfollowing properties:Depicts accurate dataDepicts data accuratelyShows enough, much, data viewer gain insightIs self contained - additional information (except caption) required\nunderstand contents figureBeyond , great visualization additional properties:Exposes patterns data easily observable methodsInvites viewer ask questions dataIn chapter, explore principles visualizing data \npresent many examples plots data visualization strategies commonly used\nbiological data analysis bioinformatics.","code":""},{"path":"data-visualization.html","id":"responsible-plotting","chapter":"7 Data Visualization","heading":"7.1 Responsible Plotting","text":"“Good plots empower us ask good questions.” - Alberto Cairo, Charts LiePlots primary vehicles communicating scientific results. \ncommon useful approach writing scientific papers decide many\nfigures needed tell story, create plots necessary \nconvey story. text plays supporting role figures. Effective\ndata visualization immense power communicate ideas (beliefs) \nchart creator. Consider famous “hockey stick graph” depicting average\nglobal temperatures created Jerry Mahlman describe pattern global\nwarming shown Mann, Bradkely & Hughes 1999:“Hockey stick graph” Jerry Mahlman, showing trend temperature rise past 1000 years northern hemisphereThe data models underlying graph complex, story \nclear: past 100 years seen dramatic increase average temperature \nnorthern hemisphere. chart one main drivers behind renewed\nmain stream awareness climate change, small part easy \nunderstand.Given power visualization , increasing amount data created \nrequired daily life, ever important data visualization\ndesigners understand principles pitfalls effective plotting. \neasy solution accomplishing , Alberto Cairo Enrico Bertini,\ntwo journalists specialize data visualization infographics, lay \nfive qualities effective visualization, Cairo’s book truthful\nart:truthful - message conveyed visualization expresses \ntruth underlying data (extent anything “true”)functional - visualization accurate, allows reader\nsomething useful insights gainIt insightful - visualization reveal patterns data\notherwise difficult impossible see understandIt enlightening - reader leave experience engaging\nvisualization convinced something, ideally designer intendsIt beautiful - visually appealing target audience, \nhelps user easily spend time engage visualizationA full treatment ideas included , among others, three excellent\nbooks Alberto Cairo, linked end section. sections \nfollow describe ideas concepts make visualization\nchallenging offer guidance books mentioned produce\neffective visualizations.three books written Alberto Cairo:Charts Liethe funcitonal artthe truthful art","code":""},{"path":"data-visualization.html","id":"visualization-principles","chapter":"7 Data Visualization","heading":"7.1.1 Visualization Principles","text":"purpose data visualization illustrate (latin illustrare,\nmeaning light illuminate) relationships quantities. Effective\ndata visualization takes advantage highly evolved visual perception\nsystem communicate \nrelationships cognitive decision making systems. Therefore, \nvisualization strategies take knowledge perceptual biases \naccount deciding present data. Poor data visualization choices may\nmake interpretation difficult, worse lead us incorrect inferences \nconclusions.","code":""},{"path":"data-visualization.html","id":"human-visual-perception","chapter":"7 Data Visualization","heading":"7.1.2 Human Visual Perception","text":"human visual perceptual system intrinsically integrated \ncognitive system. makes predictive interpretive system, \npatterns light received eye pre-processed relatively\nslower executive function parts brains can make judgements.\ncompare perception world around us cinematic film, \nmakes visual perception less raw video footage like director’s\ncommentary, add interpretations, motivations, experiences, \nanecdotal annotations images cognitively interpret . Said\ndifferently, visuo-cognitive system performs pattern recognition \ninformation receive, annotates information patterns \nthinks sees. often, cognition predicts patterns accurate,\ntherefore may useful us making decisions, e.g. move \ntrajectory falling tree branch. However, sometimes pattern\nrecognition system, always trying match patterns, sees patterns\ndon’t exist. well known well studied examples \nperceptual errors optical illusions.Optical illusions images animations reveal errors bias \nperceptual system. illusions static images appear move.\nOthers distort true relations lines shapes, lead incorrect\ninconsistent perception color. Consider famous checkerboard shadow\nillusion created \nEdward H. Adelson 1995:Checkerboard shadow illusion, squares marked B colorThe image right clearly shows two squares B shade\ngrey, though appears darker B left image. \nperceptual system identifies color contrasting adjacent colors. Since \ncolors adjacent B squares differ, perception colors also\ndiffer. example illustrates contrast bias according value, \nlightness, color, principle also applies different hues:three squares drawn top image colorAn illusion shows perceptual system distorts relationships \ngeometry follows:horizontal vertical pattern appears “wavy” fact form straight linesThe pattern light dark squares intersecting grid makes lines\nappear “wavy” horizontal vertical lines fact straight,\nparallel, perpendicular.final example illusion relevant data visualization demonstrates\nsimilar contextual bias relative area:blue dots diameterThe two blue dots image radius, although dot within\nlarge black circles appears smaller.illusions distort perception shapes \n, also illusions convince us existence shapes\n. following illusion illustrates point:shapes drawn suggest triangle middle, though triangle actually drawnSeeing shapes aren’t nothing new humans, course.Clouds resemble shapes - author sees running dogEach perceptual biases relate interpret many common data\nvisualization strategies. avoid making interpretive errors via\ncognition alone, can use knowledge design plots\nmitigate effects. quickly summarize perceptual biases \ndemonstrated:perceive value hue colors contrast adjacent colorsCertain types geometry interfere ability assess accurate\nrelationships geometric shapesWe perceive size (area) shapes relative nearby shapesWe may perceive shapes none drawn, based nearby shapesWe may perceive shapes similar familiar shapes, \ntrue relationshipEach biases influence perceive interpret plots, \nsee rest chapter.","code":""},{"path":"data-visualization.html","id":"visual-encodings","chapter":"7 Data Visualization","heading":"7.1.3 Visual Encodings","text":"Data visualization process encoding numbers positions, shapes, \ncolors two- sometimes three-dimensional space. many different\ngeometric properties can use create encoding, including:Length, width, heightPositionAreaAngle/proportional areaShapeHue value colorsExamples encodings depicted fictitious dataset \nvalues zero one following figure:Different encodings data. figure inspired Alberto Cairo book truthful art.example encodings figure among common recognizable\nelements used scientific plotting. Every time reader examines plot, \nmust decode (hopefully) accurate mental model quantities \nunderlie . Using familiar encodings reader already know \ninterpret makes decoding cognitively easier, thereby enabling plot \nquickly understood.Every quantity wish plot must mapped visualization using one \ngeometric encodings. combining different variables \ndifferent encodings, construct complex plots. Consider scatter plot,\ncontains two continuous values per datum, mapped \nvisualization position encoding:might layer another encoding introducing new variable \nabsolute value product \\(x\\) \\(y\\) make area datum\nproportional value:layer yet encodings color marker proportional value\n0 1, also change shape based category:Let’s say sum \\(x\\) \\(y\\) meaningful well, interested \norder data along axis. can connect points lines\nusing length encoding adjacent data points sorting \\(x+y\\):plot becoming busy difficult interpret \nencoded six different dimensions plot:\\(x\\) value positional encoding\\(y\\) value another positional encodingThe product \\(xy\\) area encoding\\(z\\) quantitative color value encodingcategory categorical encoding shapeThe adjacency coordinates along \\(x + y\\) axis length encodingVery complex plots may built thinking explicitly dimension\ndata encoded, designer (.e. ) must carefully consider\nwhether chosen encodings faithfully depict relationships data.common encodings depicted earlier figure \npossibilities mapping visual properties set values. Consider \nviolin plot :, geom_violin() preprocessed data map number values\nacross full range values density. density encoded \nwidth encoding, width violin across range values,\nencoded position encoding.common chart types simply combinations variables different\nencodings. following table contains encodings employed different\ntypes plots:","code":"\ntibble(\n  x=rnorm(100),\n  y=rnorm(100,0,5)\n) %>%\n  ggplot(aes(x=x,y=y)) +\n  geom_point()\ndata <-  tibble(\n  x=rnorm(100),\n  y=rnorm(100,0,5),\n  `x times y`=abs(x*y)\n)\ndata %>%\n  ggplot(aes(x=x,y=y,size=`x times y`)) +\n  geom_point()\ndata <- mutate(data,\n    z=runif(100),\n    category=sample(c('A','B','C'), 100, replace=TRUE)\n  )\ndata %>%\n  ggplot(aes(x=x, y=y, size=`x times y`, color=z, shape=category)) +\n  geom_point()\ndata <- mutate(data,\n  `x + y`=x+y,\n) %>%\n  arrange(`x + y`) %>%\n  mutate(\n    xend=lag(x,1),\n    yend=lag(y,1)\n  )\ndata %>%\n  ggplot() +\n  geom_segment(aes(x=x,xend=xend,y=y,yend=yend,alpha=0.5)) +\n  geom_point(aes(x=x, y=y, size=`x times y`, color=z, shape=category))\nset.seed(1337)\ntibble(\n  value=rnorm(100),\n  category='A'\n) %>%\n  ggplot(aes(x=category,y=value,fill=category)) +\n  geom_violin()"},{"path":"data-visualization.html","id":"elementary-perceptual-tasks","chapter":"7 Data Visualization","heading":"7.1.3.1 Elementary Perceptual Tasks","text":"purpose visualizations enable reader construct accurate\nmental model relationships ideas contained within data. \ndiscussed Human Visual Perception section, visualizations allow\neasy accurate judgments kind due biases biological\nvisuo-cognitive biases. However, encodings data allow accurate\njudgments others. William Cleveland Robert McGill, two statisticians,\nproposed hierarchy visual encodings span spectrum precision \nability accurately interpret visual information (Cleveland McGill 1984). \ndefined act mapping specific type encoding cognitive model \nelementary perceptual task. hierarchy depicted following figure:Elementary Perceptual Task Hierarchy - inspired Alberto Cairo truthful art, inspired Willian Cleveland Robert McGillThe hierarchy oriented least cognitively difficult decoding\ntasks, easier tasks lend easier comparisons \njudgements. example, easiest perceptual task compare visual\nelements position encoding aligned across charts; \npositional decoding chart, even use different\nstylistic elements. hierarchy, cognitive effort \nrequired assess precise relationships elements \nplot, therefore possibly reducing accuracy judgements.say always better reader make highly\naccurate estimates. General trends patterns may readily (\nbeautifully) apparent less accurate encodings. Consider following\nclustered heatmap:toy example, may sufficient reader’s comprehension show\nsamples , B, C cluster together, D, E, F, without requiring\nknow precisely different . case, clustering\nsuggested visualization presented way \nprecise well, example statistical comparison results reported\ntable, confirm eyes tell us.usually good idea plot data multiple ways, ensure patterns \nsee one encoding also seen another. might consider plotting\ndata using parallel coordinates plot:simplicity, added new variable group consistent \nclustering observed heatmap, might extracted\ncomputationally cutting row dendrogram two flat clusters. \nparallel coordinate plot also visually confirms interpretation two\ndistinct clusters. may now feel comfortable including one \nplot figure, though author finds parallel coordinate plot \naccurate enlightening. Recall elementary perceptual task hierarchy:\naligned position encodings enable accurate estimates data. Heatmaps,\nuse color value hue, enable least accurate estimates.","code":"\n# these data have two groups of samples with similar random data profiles\ndata <- as.matrix(\n  tibble(\n    A=c(rnorm(10),rnorm(10,2)),\n    B=c(rnorm(10),rnorm(10,2)),\n    C=c(rnorm(10),rnorm(10,2)),\n    D=c(rnorm(10,4),rnorm(10,-1)),\n    E=c(rnorm(10,4),rnorm(10,-1)),\n    F=c(rnorm(10,4),rnorm(10,-1))\n  )\n)\nrownames(data) <- paste0('G',seq(nrow(data)))\nheatmap(data)\nlibrary(GGally)\n\nas_tibble(data) %>% mutate(\n  group=c(rep('Group 1',10),rep('Group 2',10))\n) %>%\n  ggparcoord(columns=1:6,groupColumn=7)"},{"path":"data-visualization.html","id":"some-opinionated-rules-of-thumb","chapter":"7 Data Visualization","heading":"7.1.4 Some Opinionated Rules of Thumb","text":"Effective data visualization challenging, simple solutions\nsingle approaches well. sense, data visualization \nmuch art science. author rules thumb applies \nvisualizing data:Visualize data multiple ways. help ensure \naren’t unduly influenced quirks certain encodings inherent\nvisuo-cognitive biases.Perform precise statistical analysis confirm findings identified \ninspecting plots. Examine underlying data devise statistical tests \nprocedures verify quantify patterns see.Informative better beautiful. Although may tempting make\nplots beautiful, expense clarity. Make clear\nfirst, make beautiful.plot better useless plot. plots, pretty, \nuseless literal sense; information contained within plot \nuse either devising followup experiments gaining insight data.Sometimes table right way present data. Tables may \nconsidered boring, accurate representation data\n(.e. data). Especially relatively small datasets, table may\njob communicating results better plot.text plot, able read . Many results\nbiological data analysis hundreds thousands individual points,\nlabel sort. font size labels small\nread, substantially overlap making illegible, either\nplot needs change labels drawn. goes axis\nlabels.(Almost) every plot axis labels, tick marks labels, \nlegend, title. exceptions , plot \nmostly understandable without help text caption. isn’t,\n’s probably poor visualization strategy can improved.using colors, aware color blindness. several known\nforms colorblindness humans. default color schemes “colorblind\nfriendly” sometimes color combinations can counterinutitive. Check\nsites like ColorBrewer\nfind color schemes likely colorblind friendly.Make differences visually appear big mean. Scales plots can\ndistort meaning underlying data. example, consider following\ntwo plots identical data:left plot, y axis limited values data, make \ndifferences samples seem visually much larger actually .\ndifference 87 93 percent fact meaningful, might\nok, context matters.identify strategies time practice data\nvisualization, rules thumb might work . one\nright way data visualization, long dedicated \nrepresenting data accurately, wrong way.","code":"\nlibrary(patchwork)\ndata <- tibble(\n  percent=c(86,88,87,90,93,89),\n  ID=c('A','B','C','D','E','F')\n)\ng <- ggplot(data, aes(x=ID,y=percent)) +\n  geom_bar(stat=\"identity\")\n\ng + coord_cartesian(ylim=c(85,95)) | g"},{"path":"data-visualization.html","id":"dv-gg","chapter":"7 Data Visualization","heading":"7.2 Grammar of Graphics","text":"grammar graphics system rules describes data \ngraphical aesthetics (e.g. color, size, shape, etc) combined form\ngraphics plots. First popularized book Grammar \nGraphics Leland\nWilkinson co-authors 1999, grammar major contribution \nstructural theory statistical graphics. 2005, Hadley Wickam wrote \nimplementation grammar graphics R called\nggplot2 (gg stands grammar graphics).grammar graphics, every plot combination three types \ninformation: data, geometry, aesthetics. Data data \nwish plot. Geometry type geometry wish use depict data\n(e.g. circles, squares, lines, etc). Aesthetics connect data geometry\ndefines data controls way selected geometry looks.simple example help explain. Consider following made sample\nmetadata tibble study subjects died Alzheimer’s Disease (AD)\nneuropathologically normal controls:context, tau protein \namyloid beta peptides amyloid precursor\nprotein\naggregate neurofibrillary tangles -beta plaques, respectively, \nbrains people AD. Generally, amount pathologies \nassociated severe disease. Braak\nstage neuropathological\nassessment amount pathology brain associated \nseverity disease, 0 indicates absence pathology 6 \nwidespread involvement multiple brain regions. Aggregation tau also \nconsequence normal aging, must\naccompany neurological symptoms dementia indicate AD diagnosis\npost mortem. Note control samples well AD.Tauopathy: tau protein accumulates cell bodies affected neurons -\nWikipediaThe histology measures tau, abeta, iba1, gfap quantified\nusing digital microscopy, brain sections stained \nimmunohistochemistry identify location degree pathology; \nmeasures table number pixels 400 x 400 pixel image \npiece brain tissue fluoresce stained corresponding\nantibody. Tau -beta antibodies specialized types aggregated\nproteins mentioned provide quantification level overall\nAD pathology. Ionized calcium binding adaptor molecule 1 (IBA1) marker \nactivated microglia, resident\nmacrophages brain, \nindication neuroinflammation. Glial fibrillary acidic protein (GFAP) \nmarker activated astrocytes,\nspecialized cells derive neuron lineage, critical \nmaintaining blood brain\nbarrier, also\ninvolved neuroinflammatory response.Let’s say wished visualize relationship age death \namount tau pathology. scatter plot marker subject \\(x\\)\n\\(y\\) position corresponding age_at_death tau respectively. \nfollowing R code creates plot ggplot2:ggplot2 plots begin ggplot() function call, passed \ntibble data plotted. define aesthetics \ndefined mapping x coordinate age_at_death column y\ncoordinate tau column aes(x = age_at_death, y=tau). Finally, \ngeometry ‘point’ geom_point(), meaning marks made pairs\nx,y coordinates. plot shows expect given knowledge \nrelationship age amount tau; two look positively\ncorrelated.However, capturing whole story: know AD\nControl subjects dataset. condition relate \nrelationship see? can layer additional aesthetic color add\ninformation plot:looks little clearer, showing Control subjects generally \nearlier age death lower amount tau pathology. might \nproblem, however, since age distributions AD Control groups \ndifferent might pose problem \nconfounding. investigate\n.Instead plotting age death tau , examine \ndistributions variables AD Control samples separately.\nuse violin\ngeometry \ngeom_violin() look distributions age_at_death:can see immediately big differences age\ndistributions two groups. ideal, perhaps can adjust\neffects downstream analyses. ’d like look tau\ndistributions well, nice two plots side side\nplot. , use another library called\npatchwork, allows independent\nggplot2 plots arranged together simple expressive syntax:confirms suspicion, also reveals serious problem \nsamples: strong confounding tau age death AD \nControl samples. means look differences AD \nControl, won’t know difference due amount tau pathology\ndue age subjects. sample set, simply \nconfidently answer question. Just simple plots alerted us \nproblem; hopefully expensive datasets already generated \nsamples, hopefully different subjects available \navoid confounding.biological data analysis oriented tutorial plotting meant \nillustrate principles grammar graphics. Namely, every plot \ndata, geometry, aesthetics can independently controlled\nproduce many types plots. Many plots names, like scatter\nplots boxplots, compose different types geometries \naesthetics together may find generating plots aren’t easily\nnamed.next sections chapter kind “cook book” different kinds\nplots can generate data different shapes. intended \ncomprehensive, helpful guide trying decide \nvisualize datasets.want go directly comprehensive documentation many types\nggplot2 plots, peruse R Graph Gallery\nsite.ggplot2 - Elegant Graphics Data Analysis, Hadley WickamR Data Science - Data VizualizationR Graph Gallerypatchwork package","code":"\nad_metadata## # A tibble: 20 x 8\n##    ID    age_at_death condition    tau  abeta   iba1   gfap braak_stage\n##    <chr>        <dbl> <fct>      <dbl>  <dbl>  <dbl>  <dbl> <fct>      \n##  1 A1              81 AD        141017 230227  32959  26196 6          \n##  2 A2              78 AD        141082 214944 204381  26739 6          \n##  3 A3              80 AD         40788  46663      0  29308 2          \n##  4 A4              85 AD         78770 136101  98074  41177 3          \n##  5 A5              81 AD        110573  42893 140591  75334 5          \n##  6 A6              79 AD        125934 199602 133705  91069 5          \n##  7 A7              70 AD         32826  31016  34544  27905 1          \n##  8 A8              76 AD         95281  92308 116275 143759 4          \n##  9 A9              80 AD         55035 154453  62074 126360 2          \n## 10 A10             94 AD         53040   9099  39297 137833 2          \n## 11 C1              78 Control    35684      0  38523  59819 1          \n## 12 C2              77 Control    62182  29663  73422  52276 3          \n## 13 C3              73 Control    49062 106332      0  73822 2          \n## 14 C4              70 Control    10123      0  13962  96704 0          \n## 15 C5              74 Control     1530   2169   2002  83280 0          \n## 16 C6              73 Control    25514  49980  25771  53798 1          \n## 17 C7              81 Control    24367  48786  23961  17561 1          \n## 18 C8              69 Control    43628  36442  19467  41970 2          \n## 19 C9              78 Control    48923  64880  16367 110464 2          \n## 20 C10             77 Control     9688   3818  12424  59021 0\nggplot(data=ad_metadata, mapping = aes(x = age_at_death, y=tau)) +\n  geom_point()\nggplot(data=ad_metadata, mapping = aes(x = age_at_death, y=tau, color=condition)) +\n  geom_point()\nggplot(data=ad_metadata, mapping = aes(x=condition, y=age_at_death)) +\n  geom_violin()\nlibrary(patchwork)\nage_boxplot <- ggplot(data=ad_metadata, mapping = aes(x=condition, y=age_at_death)) +\n  geom_boxplot()\ntau_boxplot <- ggplot(data=ad_metadata, mapping=aes(x=condition, y=tau)) +\n  geom_boxplot()\n\nage_boxplot | tau_boxplot # this puts the plots side by side"},{"path":"data-visualization.html","id":"ggplot-mechanics","chapter":"7 Data Visualization","heading":"7.2.1 ggplot mechanics","text":"ggplot two key concepts give great flexibility: layers \n**scales*.Every plot one layers contain type geometry \nrepresents data encoding. general, layer one geometry\ntype, e.g. points lines, geometry might complex, e.g. density\nplots. layers added plot form stack, layers added first \nbeneath added later. geometry layer may draw \ndata, may . layer may also share aesthetic mapping\nggplot() call, may . ggplot()\nfunction individual geom_X() function can accept data aesthetic\nmappings. package comes large number geometries described \nreference documentation.geometry layer maps data values visual properties using\nscales. scale may map data range pixel range, color color\ngradient, one set discrete colors shapes. ggplot provides\nreasonable default scales geometry type. can override \ndefaults using scale_X\nfunctions.ggplot2 book excellent resource \nthings ggplot2.","code":""},{"path":"data-visualization.html","id":"plotting-one-dimension","chapter":"7 Data Visualization","heading":"7.3 Plotting One Dimension","text":"simplest plots involve plotting single vector numbers, several \nvectors (e.g. different samples). value vector typically\ncorresponds category fixed value, example tau column \nexample pairs (ID, tau value). order numbers can \nchanged, vector remains one dimensional 1-D.","code":""},{"path":"data-visualization.html","id":"dv-bar","chapter":"7 Data Visualization","heading":"7.3.1 Bar chart","text":"Bar charts map length (.e. height width box) scalar value \nnumber. difference visual length can help viewer notice consistent\npatterns groups bars, depending arranged:Note stat=\"identity\" argument required default geom_bar\ncounts number values value x, case ever\none. plot particularly helpful, let’s change fill color \nbars based condition:Slightly better, maybe can see even clearly sort tibble\ntau first. Sorting elements 1-D charts somewhat complicated, \nexplained [Reordering 1-D Data Elements] section .Bar charts can also plot negative numbers. following example, center\ntau measurements subtracting mean value plotting:","code":"\nggplot(ad_metadata, mapping = aes(x=ID,y=tau)) +\n  geom_bar(stat=\"identity\")\nggplot(ad_metadata, mapping = aes(x=ID,y=tau,fill=condition)) +\n  geom_bar(stat=\"identity\")\nmutate(ad_metadata, tau_centered=(tau - mean(tau))) %>%\n  ggplot(mapping = aes(x=ID, y=tau_centered, fill=condition)) +\n  geom_bar(stat=\"identity\")"},{"path":"data-visualization.html","id":"lollipop-plots","chapter":"7 Data Visualization","heading":"7.3.2 Lollipop plots","text":"Similar bar charts, -called “lollipop plots” replace bar\nline segment circle. length line segment proportional\nmagnitude number, point marks length segment \nheight y length x axis, depending orientation.Note aes() mappings can made ggplot() object \nindividual geometry function call, specify different mappings based \ngeometry.","code":"\nggplot(ad_metadata) +\n  geom_point(mapping=aes(x=ID, y=tau)) +\n  geom_segment(mapping=aes(x=ID, xend=ID, y=0, yend=tau))"},{"path":"data-visualization.html","id":"stacked-area-charts","chapter":"7 Data Visualization","heading":"7.3.3 Stacked Area charts","text":"Stacked area charts can visualize multiple 1D data share common\ncategorical axis. charts consist one line per variable vertices \ncorrespond x y values similar bar \nlollipop plots. variable plotted using previous\none baseline, height data points category \nproportional sum. space lines variable \nprevious one filled color. following plot visualizes amount\nmarker stain four genes individaul:notice subject A4 highest overall level marker intensity,\nfollowed A1, A7, etc. control samples overall less intensity across\nmarkers. Certain samples, A2 C5, little abeta aggregation,\nC6 little tau.Stacked area plots require three pieces data:x - numeric categorical axis vertical alignmenty - numeric axis draw vertical proportionsgroup - categorical variable indicates (x,y) pairs correspond\nlineIn example , needed pivot tibble different\nmarkers values placed columns Marker Intensity,\nrespectively. Data stacked bar charts usually need ‘long’\nformat, described Rearranging Data.Sometimes helpful view relative proportion values \ncategory rather actual values. result called proportional\nstacked area plots. distinct plot type, can create one \npreprocessing data dividing value column sum:\nNow values subject normalized sum 1. \nway, might note relative proportion abeta seems greater\nAD samples Controls, may true tau. \nobservations may inspire us ask questions rigorously \ndone far inspection.","code":"\npivot_longer(\n  ad_metadata,\n  c(tau,abeta,iba1,gfap),\n  names_to='Marker',\n  values_to='Intensity'\n  ) %>%\n  ggplot(aes(x=ID,y=Intensity,group=Marker,fill=Marker)) +\n    geom_area()\npivot_longer(\n  ad_metadata,\n  c(tau,abeta,iba1,gfap),\n  names_to='Marker',\n  values_to='Intensity'\n  ) %>%\n  group_by(ID) %>% # we want to divide each subjects intensity values by the sum of all four markers\n  mutate(\n    `Relative Intensity`=Intensity/sum(Intensity)\n  ) %>%\n  ungroup() %>% # ungroup restores the tibble to its original number of rows after the transformation\n  ggplot(aes(x=ID,y=`Relative Intensity`,group=Marker,fill=Marker)) +\n    geom_area()"},{"path":"data-visualization.html","id":"parallel-coordinate-plots","chapter":"7 Data Visualization","heading":"7.3.4 Parallel Coordinate plots","text":"","code":""},{"path":"data-visualization.html","id":"visualizing-distributions","chapter":"7 Data Visualization","heading":"7.4 Visualizing Distributions","text":"distribution one important properties set numbers.\ndistribution describes general “shape” numbers, .e. \nrelative frequency values, ranges values, within data.\nUnderstanding distribution data set critical choosing methods\napply, since many methods appropriate data distributed \ncertain ways, e.g. linear\nregression assumes \nresponse variable normally\ndistributed, otherwise \nresult model interpreted properly. Often, don’t know \ndata distributed obtain must examine distribution\nempirically. visualizations section used purpose \ndepicting distribution set numbers.","code":""},{"path":"data-visualization.html","id":"histogram","chapter":"7 Data Visualization","heading":"7.4.1 Histogram","text":"common way plot distribution 1-D set data \nhistogram. histogram divides \nrange dataset minimum maximum bins usually \nwidth tabulates number values fall within bin. \nhistogram age_at_death measurement samples:Note histogram look complete, 20\nvalues data. can mitigate somewhat increasing number \nbins data range divided :little bit better, still bins (76-79, 84-87) \nvalues. Compare following synthetic dataset 1000\nnormally distributed values:distributions small number samples, histograms might \nbest visualization. continue synthetic normally distributed dataset\nremaining examples.ggplot allows easily plot multiple distributions plot:alpha=0.6, position=\"identity\" arguments makes bars partially\ntransparent can see overlap clearly.ggplot2 geom_histogram referenceR Graph Gallery - histograms","code":"\nggplot(ad_metadata) +\n  geom_histogram(mapping=aes(x=age_at_death))\nggplot(ad_metadata) +\n  geom_histogram(mapping=aes(x=age_at_death),bins=10)\ntibble(\n  x=rnorm(1000)\n) %>%\n  ggplot() +\n  geom_histogram(aes(x=x))\ntibble(\n  x=c(rnorm(1000),rnorm(1000,mean=4)),\n  type=c(rep('A',1000),rep('B',1000))\n) %>%\n  ggplot(aes(x=x,fill=type)) +\n  geom_histogram(bins=30, alpha=0.6, position=\"identity\")"},{"path":"data-visualization.html","id":"density","chapter":"7 Data Visualization","heading":"7.4.2 Density","text":"Another way describe distribution density\nplot. Instead binning \nvalues intervals drawing bars height proportional number \nvalues bin, density plot draws smoothly interpolated line \napproximates distribution instead. key difference histogram \ndensity plot density plot always normalized integral \ncurve approximately 1, whereas histogram may either counts , \ncounts bin divided total number data points, \nproportion.Compare histogram density plots\nage_at_death variable example tibble:Notice overall shape two distributions similar, highest\nvalues around age 77. density plot smoother representation \nhistogram, accuracy still highly sensitive number \nmeasurements used construct . Compare histogram density plots \ntwo sets 1000 normally distributed samples different means:two types plots depict similar distributions, although \ndifferent enough possibly suggest different interpretations. general,\ndensity plots might preferable histograms data noisy \nsparse produce cleaner plots, potentially expense \naccuracy number samples low.ggplot2 geom_density referenceR Graph Gallery - density plots","code":"\nlibrary(patchwork)\nhist_g <- ggplot(ad_metadata) +\n  geom_histogram(mapping=aes(x=age_at_death),bins=30)\ndensity_g <- ggplot(ad_metadata) +\n  geom_density(mapping=aes(x=age_at_death),fill=\"#c9a13daa\")\n\nhist_g | density_g\nlibrary(patchwork)\nnormal_samples <- tibble(\n  x=c(rnorm(1000),rnorm(1000,mean=4)),\n  type=c(rep('A',1000),rep('B',1000))\n)\nhist_g <- ggplot(normal_samples) +\n  geom_histogram(\n    mapping=aes(x=x,fill=type),\n    alpha=0.6,\n    position=\"identity\",\n    bins=30\n)\ndensity_g <- ggplot(normal_samples) +\n  geom_density(\n    mapping=aes(x=x,fill=type),\n    alpha=0.6,\n    position=\"identity\"\n  )\n\nhist_g | density_g"},{"path":"data-visualization.html","id":"boxplot","chapter":"7 Data Visualization","heading":"7.4.3 Boxplot","text":"Box plots, box whisker plots, extremely common used describe\ndistributions. boxplot age death divided condition:Boxplots drawn assuming data \nunimodal (.e. shaped like hill,\npossibly slanted one side ), extents box\nrepresent 1st 3rd quartile data, central line median,\nwhiskers drawn 1.5 times value outside 1st 3rd quartiles.\nSometimes individual values extreme whiskers drawn\nindividually identify outliers.Boxplot anatomy. IQR stands “inner quartile range”, distance \n1st 3rd quartile - WikipediaHowever, boxplots significant shortcomings. Primarily, rectangle\ninner quartile range describe actual distribution \nsamples within . Although median can give sense skewness, \ndata unimodal may misleading. Consider following\ndistributions plotted boxplots violin plots (described next\nsection):two distributions look almost identical boxplot figure; however \ndramatically different visualized using method like violin plot\ncontours entire distribution depicted. Unless \ncertain data unimodal, one distribution visualization\nmethods section likely accurately depict data \nboxplot.ggplot2 geom_boxplot referenceR Graph Gallery - boxplots","code":"\nggplot(ad_metadata) +\n  geom_boxplot(mapping=aes(x=condition,y=age_at_death))\nlibrary(patchwork)\nnormal_samples <- tibble(\n  x=c(rnorm(1000),rnorm(1000,4),rnorm(1000,2,3)),\n  type=c(rep('A',2000),rep('B',1000))\n)\ng <- ggplot(normal_samples, aes(x=type,y=x,fill=type))\nboxplot_g <- g +  geom_boxplot()\nviolin_g <- g + geom_violin()\n\nboxplot_g | violin_g"},{"path":"data-visualization.html","id":"violin-plot","chapter":"7 Data Visualization","heading":"7.4.4 Violin plot","text":"seen last section, violin\nplot another way\ndepict distribution producing shape width proportional \nvalue along x y axis, depending orientation. “violin” shape \nsimilar principle histogram density plot, describes \ncontour data distribution, just quantiles extents,\nbox plot. violin plot tau measures example\ntibble:violin plot less descriptive boxplot; depict\nentire distribution data, also doesn’t include features like\nmedian default.ggplot2 geom_violin referenceR Graph Gallery - violin plots","code":"\nggplot(ad_metadata) +\n  geom_violin(aes(x=condition,y=tau,fill=condition))"},{"path":"data-visualization.html","id":"beeswarm-plot","chapter":"7 Data Visualization","heading":"7.4.5 Beeswarm plot","text":"beeswarm plot similar \nviolin plot, instead plotting contours data, plots \ndata points like scatter plot. individual values \ndistribution organized vertically spaced points don’t\noverlap. plot, distribution age death plotted kind\nsample markers colored amount tau:may noticed AD samples big gap ages\n74 81; since beeswarm plot displays data, can see \neasily .Beeswarm plots typically useful number values within \nrange; many . example close \nvalues per group plot useful, consider following \nmany samples:plot likely many samples right choice (’s also ugly),\ngive idea distribution data.previous examples markers group also determined color \ngroup. makes chart bit easier read pleasing \neye, technically redundant. can use however profitably however \ncolor markers value might interest. Consider final\nexample markers colored another randomly generated variable:now effectively visualizing three dimensions may provide insight\ndata.beeswarm package reference","code":"\nlibrary(ggbeeswarm)\nggplot(ad_metadata) +\n  geom_beeswarm(aes(x=condition,y=age_at_death,color=condition),cex=2,size=2)\nnormal_samples <- tibble(\n  x=c(rnorm(1000),rnorm(1000,4),rnorm(1000,2,3)),\n  type=c(rep('A',2000),rep('B',1000))\n)\nggplot(normal_samples, aes(x=type,y=x,color=type)) +\n  geom_beeswarm()\nnormal_samples <- tibble(\n  x=c(rnorm(100),rnorm(100,4),rnorm(100,2,3)),\n  type=c(rep('A',200),rep('B',100)),\n  category=sample(c('healthy','disease'),300,replace=TRUE)\n)\nggplot(normal_samples, aes(x=type,y=x,color=category)) +\n  geom_beeswarm()"},{"path":"data-visualization.html","id":"ridgeline","chapter":"7 Data Visualization","heading":"7.4.6 Ridgeline","text":"many non-trivial distributions like user \ncompare, good option ridgeline\nchart. \nridgeline plot simply multiple density plots drawn different variables\nwithin plot. Like beeswarm plot, ridgeline plots provided \nanother package outside ggplot2.Many distributions may plotted:R Graph Gallery - ridgeline plotsggridges package vignetteggridges package index CRAN","code":"\nlibrary(ggridges)\n\ntibble(\n  x=c(rnorm(100),rnorm(100,4),rnorm(100,2,3)),\n  type=c(rep('A',200),rep('B',100)),\n) %>%\n  ggplot(aes(y=type,x=x,fill=type)) +\n  geom_density_ridges()\ntibble(\n  x=rnorm(10000,mean=runif(10,1,10),sd=runif(2,1,4)),\n  type=rep(c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"),1000)\n) %>%\n  ggplot(aes(y=type,x=x,fill=type)) +\n  geom_density_ridges(alpha=0.6,position=\"identity\")"},{"path":"data-visualization.html","id":"plotting-two-or-more-dimensions","chapter":"7 Data Visualization","heading":"7.5 Plotting Two or More Dimensions","text":"Multi-dimensional plots allow relationships different quantities \ndepicted visually. mentioned [Elemental Perceptual Tasks] section,\ninvolves mapping data visual elements using different encodings. \nplot types section examples different approaches \ncombining visual data encodings.","code":""},{"path":"data-visualization.html","id":"scatter-plots","chapter":"7 Data Visualization","heading":"7.5.1 Scatter Plots","text":"Scatter plots visualize pairs quantities, usually continuous, points \ntwo dimensional space. commonly, points plotted cartesian\ncoordinates, \npolar coordinates \ntypes coordinate systems.geom_point() geometry draws shape, circle default, pixel\ncoordinates proportional \\((x,y)\\) pair. shape may changed \nspecifying shape=N parameter geom_point, N integer ID\nshape. following plot shows IDs correspond shapes:However, geom_points() choose six different shapes \nmap categorical encoding shape aesthetic:\nControlling shapes mapped value can accomplished \nscale_shape_manual():Color encodings can also added using either continuous discrete values:","code":"\nggplot(ad_metadata,mapping=aes(x=abeta, y=tau)) +\n  geom_point(size=3)\ng <- ggplot()\nfor(x in 0:5) {\n  for(y in 0:4) {\n    if(x+y*6 < 26) {\n      g <- g + geom_point(aes(x=x,y=y),tibble(x=x,y=y),shape=x+y*6,size=8) +\n       geom_label(aes(x=x,y=y+0.5,label=label),tibble(x=x,y=y,label=x+y*6))\n    }\n  }\n}\ng\nggplot(ad_metadata,mapping=aes(x=abeta, y=tau, shape=condition)) +\n  geom_point(size=3)\nggplot(ad_metadata,mapping=aes(x=abeta, y=tau, shape=condition)) +\n  geom_point(size=3) +\n  scale_shape_manual(values=c(3,9))\nlibrary(patchwork)\ng <- ggplot(ad_metadata)\ng_condition <- g + geom_point(mapping=aes(x=abeta, y=tau, color=condition),size=3)\ng_age <- g + geom_point(mapping=aes(x=abeta, y=tau, color=age_at_death),size=3)\ng_condition / g_age"},{"path":"data-visualization.html","id":"bubble-plots","chapter":"7 Data Visualization","heading":"7.5.2 Bubble Plots","text":"close relative scatter plot bubble plot. bubble plot \nscatter plot area point markers proportional third\ncontinuous dimension. Consider plot , age death encoded \narea point:Recall Human Visual Perception section difficult \nhumans accurately assess relative sizes shapes near \n. Bubble plots may useful instances, used\ncarefully.","code":"\nggplot(ad_metadata,mapping=aes(x=abeta, y=tau, size=age_at_death)) +\n  geom_point(alpha=0.5)"},{"path":"data-visualization.html","id":"connected-scatter-plots","chapter":"7 Data Visualization","heading":"7.5.3 Connected Scatter Plots","text":"Another close relative scatter plot connected scatter plot. \nconnected scatter plot simply scatter plot certain pairs points\nconnected line addition markers. pairs points may\nlines connecting , often line indicates dimension \ntime, .e. two points sequential time connected. \nconnected scatter plot line drawn youngest oldest age \ndeath:plot suggests perhaps AD Control samples might closer age\nwithin groups . can confirm using another type plot, e.g.\ndensity grouped condition.","code":"\narrange(ad_metadata,age_at_death) %>%\n  mutate(\n    x=abeta,\n    xend=lag(x,1),\n    y=tau,\n    yend=lag(y,1)\n  ) %>%\n  ggplot() +\n  geom_segment(aes(x=abeta, xend=xend, y=tau, yend=yend)) +\n  geom_point(aes(x=x,y=y,shape=condition,color=condition),size=3)"},{"path":"data-visualization.html","id":"line-plots","chapter":"7 Data Visualization","heading":"7.5.4 Line Plots","text":"Line plots connect pairs points line without drawing symbol \npoint. geom_line() function draw lines pairs points sorted\n\\(x\\) axis default:may useful plotting multiple lines using group aesthetic\nmapping:plot similar parallel coordinate plot,\nfour dimensions sample displayed using position\nencoding.","code":"\nggplot(ad_metadata,mapping=aes(x=abeta, y=tau)) +\n    geom_line()\npivot_longer(ad_metadata,\n  c(tau,abeta,iba1,gfap),\n  names_to='Marker',\n  values_to='Intensity'\n  ) %>%\n  ggplot(ad_metadata,mapping=aes(x=ID, y=Intensity, group=Marker, color=Marker)) +\n    geom_line()"},{"path":"data-visualization.html","id":"parallel-coordinate-plots-1","chapter":"7 Data Visualization","heading":"7.5.5 Parallel Coordinate Plots","text":"Parallel coordinate plots line plots possibly many continuous\nvariables given vertical position encodings sample \nline. Unlike line plot , vertical axis can different scale,\nthus allowing plots variables different value ranges. following\nparallel coordinate plot AD metadata:variables scaled centered mean zero standard\ndeviation 1, values plotted using position encoding vertically long\n. line different subject, colored based type\nsubject, either AD control. encoding allows us make number \nobservations samples, e.g. except gfap, AD samples \nconsistently higher values Controls.common way specify colors six digit\nhexadecimal code, e.g. #2ca9f3.\ncodes form triplet (rr,gg,bb), rr value \ncolor red, gg value green, bb blue, examples:#000000 color black, value zero three colors#ffffff color white, maximum value three colors#ff0000 color red, full red value value green blue#0000ff color blue, full blue value value red green#2ca9f3 color, combination red, green, blueAll colors tidyverse may specified hexadecimal color codes. See\nweb colors information \nspecify colors way.","code":"\nlibrary(GGally)\n\nggparcoord(ad_metadata,\n           columns=c(2,4:8),\n           groupColumn=3,\n           showPoints=TRUE\n           ) +\n  scale_color_manual(values=c(\"#bbbbbb\", \"#666666\"))"},{"path":"data-visualization.html","id":"heatmaps","chapter":"7 Data Visualization","heading":"7.5.6 Heatmaps","text":"Heatmaps visualize values associated grid points \\((x,y,z)\\) grid\ncolored rectangles, \\(x\\) \\(y\\) define grid point coordinates \n\\(z\\) continuous value. common heatmap might seen weather\nmap, plots current \npredicted weather patterns top geographic map:Weather maps heatmapsThe color grid coordinate, case spaced closely enough\ndistinguish boundaries rectangles without zooming\n, proportional type intensity weather location.\nweather map, weather values mapped color scales \ndescribe amount precipitation shown map legend.biology, heatmaps typically used visualize matrices. concept\nweather map, except instead geographic coordinates\ngrid corresponds rows columns data matrix, color \ngrid point mapped color scale chosen designer. grid \nrectangles typically overlayed anything, weather map, \ninstead entire visualization.can create heatmap R using base R\nheatmap()\nfunction. heatmap visualization histological markers \nAD example:lower right portion plot heatmap, matrix values \nvisualized color gradient light yellow dark red.precisely, heatmap() function creates clustered heatmap, \nrows columns hierarchically\nclustered separately ordered according \nsimilar one another. left top line diagrams \ndendrograms, depict similarity rows \ncolumns tree, total branch length summed one row/column \nanother proportional dissimilarity (.e. numeric distance) two.base R heatmap() function performs many different operations input\nmatrix just draw grid rectangles proportion values. \ndefault, also:performs hierarchical clustering rows \ncolumns using Euclidean distance function orders accordinglydraws dendrograms rows columns according \nclusteringscales data rows mean zero standard deviation 1Each defaults can changed passing arguments function\ncall. following turns extra functionality produces \nheatmap matrix:\nNote colors heatmap different many \nmarker/sample pairs. illustrates dangers using heatmaps,\ndescribed fully Use Heatmaps Responsibly section\n.base R heatmap function major drawback color key provided\nindicate values map colors. Another heatmap\nheatmap.2\nfunction gplots package\nsimilar interface heatmap(), allows provides \nparameters control behavior plot includes color key:extra decoration (dashed solid vertical lines called traces \npackage) provide another way understand magnitude value \ncell. Refer gplots documentation \nheatmap2\ninformation interpret trace (hint: may turn \npassing argument trace = \"none\" function call).heatmap() heatmap.2() provide useful method annotate rows \ncolumns categorical variable. subjects either AD \ncontrol, can add color bar along margin plot indicate \nstatus subject:heatmap.2:Heatmaps may also created using ggplot \ngeom_tile geometry.\nmethod generating heatmaps “manual” functions\ncovered far, flexible require work obtain certain\nfunctionality (e.g. clustering, color bars side margins, etc). geometry\nrequires data long format, one column x, y, z\nvalues:Note ggplot method scale reorder rows columns.","code":"\n# heatmap() requires a R matrix, and cannot accept a tibble or a dataframe\nmarker_matrix <- as.matrix(\n  dplyr::select(ad_metadata,c(tau,abeta,iba1,gfap))\n)\n# rownames of the matrix become y labels\nrownames(marker_matrix) <- ad_metadata$ID\n\nheatmap(marker_matrix)\nheatmap(\n  marker_matrix,\n  Rowv=NA,\n  Colv=NA,\n  scale=\"none\",\n)\nlibrary(gplots)\nheatmap.2(marker_matrix)\ncondition_colors <-\n  transmute(\n    ad_metadata,\n    color=if_else(condition == \"AD\",\"red\",\"blue\")\n  )\nheatmap(\n  marker_matrix,\n  RowSideColors=condition_colors$color\n)\nheatmap.2(\n  marker_matrix,\n  RowSideColors=condition_colors$color\n)\npivot_longer(\n  ad_metadata,\n  c(tau,abeta,iba1,gfap),\n  names_to=\"Marker\",\n  values_to=\"Intensity\"\n) %>%\n  ggplot(aes(x=Marker,y=ID,fill=Intensity)) +\n  geom_tile()"},{"path":"data-visualization.html","id":"specifying-heatmap-colors","chapter":"7 Data Visualization","heading":"7.5.6.1 Specifying Heatmap Colors","text":"colors heatmap may changed passing one native color\npalettes\nheatmap function col argument:change color using ggplot geom_tile(), use \nscale_fill_gradientn\nfunction specify different color palette:Instead color palettes, may use one \nColorBrewer\npalettes RColorBrewer\npackageThere many color palettes choose , may also create \npalettes. Creating color palettes somewhat\ncomplicated\nbase R, ggplot() provides convenient\nfunctions\nscale_fill_gradient() scale_fill_gradient2() produce gradient scales\n2 3 colors, respectively. discussion choose colors \ndescribed next section.","code":"\n# native R colors are:\n# - rainbow(n, start=.7, end=.1)\n# - heat.colors(n)\n# - terrain.colors(n)\n# - topo.colors(n)\n# - cm.colors(n)\n# the n argument specifies the number of colors (i.e. resolution) of the colormap to return\nheatmap(marker_matrix,col=cm.colors(256))\npivot_longer(\n  ad_metadata,\n  c(tau,abeta,iba1,gfap),\n  names_to=\"Marker\",\n  values_to=\"Intensity\"\n) %>%\n  ggplot(aes(x=Marker,y=ID,fill=Intensity)) +\n  geom_tile() +\n  scale_fill_gradientn(colors=cm.colors(256))\nlibrary(RColorBrewer)\ndisplay.brewer.all()"},{"path":"data-visualization.html","id":"how-to-use-heatmaps-responsibly","chapter":"7 Data Visualization","heading":"7.5.7 How To Use Heatmaps Responsibly","text":"heatmaps may seem intuitive, actually quite complicated can\ndifficult create way accurately depicts underlying matrix.\ndescribed Human Visual Perception section, visual system\nestimates hue value colors based adjacent colors. large\nheatmaps many rows columns, therefore difficult \naccurately estimate relationships across wide sections heatmap. may\nalways problematic, heatmaps might needed convey general\npatterns dataset. However, designer large degree control \nheatmap utilizes hue value mapping underlying numbers,\nchoices large impact interpretation chart. \ndata can effectively visualized heatmap, preprocessing usually\nrequired yield useful plot.four major factors influence dataset can visualized \nheatmap:type features, .e. whether features continuous discreteThe relative scales featuresThe total range dataWhether data centered","code":""},{"path":"data-visualization.html","id":"the-type-of-the-features","chapter":"7 Data Visualization","heading":"7.5.7.1 The Type of the Features","text":"two broad types features: discrete continuous. data\nelements discrete feature take one finite set values. \nexample, discrete feature might include values like 0 1, Case \nControl integers 1 6. general, primarily\ninterested know whether samples discrete value group together,\nideally colors mapped discrete features easily\ndistinguishable one another. number possible values feature\nmay take increases, finding set colors distinguishable \none another becomes increasingly difficult. Consider following three\nheatmaps show different discrete data:first second heatmaps 2 3 discrete values , colors\neasy tell apart. third heatmap 10 discrete values \ndifficult distinguish colors different parts plot.Continuous features can take (possibly infinite) range real values,\ne.g. \\(-3.252\\), \\(858,352\\), \\(0\\). values mapped colors \ngradient, two colors blended together linearly one\ncolor another, values mapped within range colors \nproportion relative magnitudes. Two gradients, one two colors \none three, shown :three example gradients comprise common types color mappings.\nfirst two one color differ color (.e. lighter darker) \nmapped largest value. third gradient called diverging palette,\nmaximum minimum values show divergence central value,\ncase 0.5. diverging color palettes,\nvalue middle gradient often important, defines\nvalues visualized color. diverging color palettes \nfollowing sections.general, types features, .e. discrete continuous, must \nheatmap meaningful. determined, values\nfeature must examined possibly transformed, described next.","code":"\nlibrary(patchwork)\nrandom_letters <- function(n) {\n  letters <- c('A','B','C','D','E','F','G','H','I','J')\n  sample(letters[1:n],10,replace=TRUE)\n}\nfeatures <- paste0('F',seq(10))\ng_binary <- tibble(\n  feature=features,\n  a=random_letters(2),\n  b=random_letters(2),\n  c=random_letters(2)\n) %>%\n  pivot_longer(c(a,b,c))  %>%\n  ggplot(aes(x=name,y=feature,fill=value)) + geom_tile()\n  \ng_trinary <- tibble(\n  feature=paste0('F',seq(10)),\n  a=random_letters(3),\n  b=random_letters(3),\n  c=random_letters(3)\n) %>%\n  pivot_longer(c(a,b,c))  %>%\n  ggplot(aes(x=name,y=feature,fill=value)) + geom_tile()\n\ng_unmanageable <- tibble(\n  feature=paste0('F',seq(10)),\n  a=random_letters(10),\n  b=random_letters(10),\n  c=random_letters(10)\n) %>%\n  pivot_longer(c(a,b,c))  %>%\n  ggplot(aes(x=name,y=feature,fill=value)) + geom_tile()\n\ng_binary | g_trinary | g_unmanageable\ng_two_up <- tibble(\n  ID='a',\n  x=seq(0,1,0.1),\n  val=x\n) %>%\n  ggplot(aes(x=x,y=ID,fill=val)) +\n  geom_tile() +\n  scale_fill_gradient(low=\"#990000\", high=\"#ffdddd\")\n\ng_two_down <- tibble(\n  ID='a',\n  x=seq(0,1,0.1),\n  val=x\n) %>%\n  ggplot(aes(x=x,y=ID,fill=val)) +\n  geom_tile() +\n  scale_fill_gradient(low=\"#ffdddd\", high=\"#990000\")\n\ng_three <- tibble(\n  ID='a',\n  x=seq(0,1,0.1),\n  val=x\n) %>%\n  ggplot(aes(x=x,y=ID,fill=val)) +\n  geom_tile() +\n  scale_fill_gradient2(low=\"#990000\", mid=\"#ffffff\", high=\"#9999ff\", midpoint=0.5)\n\ng_two_up / g_two_down / g_three"},{"path":"data-visualization.html","id":"scales-of-features","chapter":"7 Data Visualization","heading":"7.5.7.2 Scales of Features","text":"second important consideration plotting heatmap understand \nnature different features relate one another.\nSpecifically, features must comparable scale, transformed\nappropriately attain , colors heatmap visible.\nConsider following heatmap three features plotted columns 10\nsamples rows:scales different features \\(\\), \\(b\\), \\(c\\) different, namely\nmeans 0, 100, 20, respectively. clearly seen \ndata plotted beeswarm plot:data dramatically different scales, feature \nlargest spread (b) can distinguished heatmap.order visualize data meaningfully plot, features\nmust scaled comparable. Typically done \nstandardizing features mean zero standard deviation 1:Now data scale, mapped across full\nrange color palette. case, explicitly scaled feature, \npossible scale sample well:result dramatically different, possibly intended. \ncase effect somewhat obvious - values b largest,\nvalues c smallest, c . However, general \nmay case, care taken understand dimension \ndata scaled.features typically assumed scale, base R\nheatmap() heatmap2() functions automatically scale rows \ndata prior visualization.","code":"\ndata <- tibble(\n  ID=paste0('F',seq(10)),\n  a=rnorm(10,0,1),\n  b=rnorm(10,100,20),\n  c=rnorm(10,20,5)\n) %>%\n  pivot_longer(c(a,b,c))\n\nggplot(data,aes(x=name,y=ID,fill=value)) +\n  geom_tile()\nlibrary(ggbeeswarm)\nggplot(data) +\n  geom_beeswarm(aes(x=name,y=value,color=name))\ndata %>%\n  pivot_wider(id_cols='ID',names_from=name) %>%\n  mutate(\n    across(c(a,b,c),scale)\n  ) %>%\n  pivot_longer(c(a,b,c)) %>%\n  ggplot(aes(x=name,y=ID,fill=value)) +\n  geom_tile()\ndata %>%\n  pivot_wider(id_cols='ID',names_from=name) %>%\n  mutate(\n    across(c(a,b,c),scale)\n  ) %>%\n  pivot_longer(c(a,b,c)) %>%\n  ggplot() +\n  geom_beeswarm(aes(x=name,y=value,color=name))\ndata %>%\n  pivot_wider(id_cols=name,names_from=ID) %>%\n  mutate(\n    across(starts_with('F'),scale)\n  ) %>%\n  pivot_longer(starts_with('F'),names_to=\"ID\") %>%\n  ggplot(aes(x=name,y=ID,fill=value)) +\n  geom_tile()"},{"path":"data-visualization.html","id":"total-data-range","chapter":"7 Data Visualization","heading":"7.5.7.3 Total data range","text":"saw previous section scaling features, color mapping \nhighly sensitive range values data. Even features \nsamples properly scaled, extreme values features can\nrender heatmap uninformative. Consider following heatmap visualizes\ndata single extreme outlier:single extreme value compressed color values colors \nlowest end color palette. anomalous value must removed \ndifferences variables visible.issue caused fact default color gradient scales\nassume linear mapping values palette. \nequivalent assuming data relatively well distributed across linear\nrange values, normal uniform distribution. data \nfollow assumption, resulting heatmaps may suffer issue \nextreme values. Consider following heatmap data randomly\nsampled exponential distribution:data similarly compressed toward lower end scale due \nuneven distribution data. case, might consider taking log\ntransformation data prior drawing heatmap:log scale, heatmap visualizes range values linearly across \ngradient. case, knew log transform appropriate cast \ndata onto approximately linear scale, though general need \nexplored dataset.","code":"\ntibble(\n  ID=paste0('F',seq(10)),\n  a=rnorm(10,0,1),\n  b=rnorm(10,0,1),\n  c=c(rnorm(9,0,1),1e9)\n) %>%\n  pivot_longer(c(a,b,c)) %>%\n  ggplot(aes(x=name,y=ID,fill=value)) +\n  geom_tile()\ndata <- tibble(\n  ID=paste0('F',seq(10)),\n  a=10**rnorm(10,4,1),\n  b=10**rnorm(10,4,1),\n  c=10**rnorm(10,4,1)\n) %>%\n  pivot_longer(c(a,b,c))\ndata %>%\n  ggplot(aes(x=name,y=ID,fill=value)) +\n  geom_tile()\nmutate(data,\n       value=log10(value)\n       ) %>%\n  ggplot(aes(x=name,y=ID,fill=value)) +\n  geom_tile() "},{"path":"data-visualization.html","id":"whether-or-not-the-data-are-centered","chapter":"7 Data Visualization","heading":"7.5.7.4 Whether or not the data are centered","text":"final important consideration creating heatmap continuous values\ndetermining meaningful center feature. certain\nkinds data, data points certain value particular\nmeaning. example, log2 fold change values differential expression\ngreater less zero indicate gene increased decreased,\nrespectively, comparison interest. visualizing values \nheatmap, therefore important indicate central value color,\nusually white black, appropriately, colors \nvalue can set accordingly, e.g. zero red, zero blue., first 3-color diverging palette required, .e. palette \none color one side, different color side, third color \nmiddle, usually white black. following heatmap normally\ndistributed values mean 0 standard deviation 1 visualized \ndiverging color scale:Note scale_fill_gradient2() call, midpoint=0 argument. \nargument instructs gradient set value 0 middle color, \nwhite. midpoint zero default, data centered around\nvalue, failing specify midpoint produce misleading\nvisualizations:data centered mean 1, midpoint still \ndefault zero. intend 1 midpoint corresponding \ncolor white, must specify explicitly:features meaningful center, color palette scale\ngenerally two colors range either light dark \ndark light:direction color palette depends meaning data, \nsubjective choice.","code":"\ntibble(\n  ID=paste0('F',seq(10)),\n  a=rnorm(10,0,1),\n  b=rnorm(10,0,1),\n  c=rnorm(10,0,1)\n) %>%\n  pivot_longer(c(a,b,c)) %>%\n  ggplot(aes(x=name,y=ID,fill=value)) +\n  geom_tile() +\n  scale_fill_gradient2(low=\"#000099\", mid=\"#ffffff\", high=\"#990000\", midpoint=0)\ntibble(\n  ID=paste0('F',seq(10)),\n  a=rnorm(10,1,1),\n  b=rnorm(10,1,1),\n  c=rnorm(10,1,1)\n) %>%\n  pivot_longer(c(a,b,c)) %>%\n  ggplot(aes(x=name,y=ID,fill=value)) +\n  geom_tile() +\n  scale_fill_gradient2(low=\"#000099\", mid=\"#ffffff\", high=\"#990000\")\ntibble(\n  ID=paste0('F',seq(10)),\n  a=rnorm(10,1,1),\n  b=rnorm(10,1,1),\n  c=rnorm(10,1,1)\n) %>%\n  pivot_longer(c(a,b,c)) %>%\n  ggplot(aes(x=name,y=ID,fill=value)) +\n  geom_tile() +\n  scale_fill_gradient2(low=\"#000099\", mid=\"#ffffff\", high=\"#990000\", midpoint=1)\ndata <- tibble(\n  ID=paste0('F',seq(10)),\n  a=runif(10,0,10),\n  b=runif(10,0,10),\n  c=runif(10,0,10)\n) %>%\n  pivot_longer(c(a,b,c))\n\ng_up <- ggplot(data, aes(x=name,y=ID,fill=value)) +\n  geom_tile() +\n  scale_fill_gradient(low=\"#000099\",high=\"#eeeeff\")\n\ng_dn <- ggplot(data, aes(x=name,y=ID,fill=value)) +\n  geom_tile() +\n  scale_fill_gradient(low=\"#ffffff\",high=\"#000099\")\n\ng_up | g_dn"},{"path":"data-visualization.html","id":"other-kind-of-plots","chapter":"7 Data Visualization","heading":"7.6 Other Kind of Plots","text":"","code":""},{"path":"data-visualization.html","id":"dendrograms","chapter":"7 Data Visualization","heading":"7.6.1 Dendrograms","text":"Dendrograms visualizations tree structures (dendron means tree \nGreek). graph theory branch\nmathematics, tree \ndefined graph composed nodes edges two nodes \nconnected exactly one path. graph also called acyclic graph,\npaths exist can followed node back iself. full\ntreatment trees graph theory beyond scope book, \ndiscuss trees extent necessary understand read \ndendrogram.concepts dendrogram illustrated following figure:Dendrogram IllustrationThe tree drawn typical dendrogram created using output \nclustering algorithm, e.g. hierarchical clustering\ngroups data points together based dissimilarity. Briefly, \nfigure, data one dimensional scalars, distance points\nsimply absolute value difference . two closest\npoints merged single group, pairwise distances \nrecomputed remaining data points summary new group (\ngroup summarization method specified user called linkage\ncritera).\nprocess repeats, two nearest data points summarized groups\nmerged new group, new group summarized, pairwise distances\nrecomputed, etc. data points included group. \nway, data points assigned hierarchy groups. general,\nhierarchical grouping data can used generate dendrogram.Given hierarchical clustering result, dendrogram can drawn several\nways R. Using AD marker example data, hierarchically cluster \nsubjects based content markers visualize result \nfollows:dendrogram produce highly distinct clusters AD control\nsamples data.illustrate dataset strong clustering looks like, draw\nmultivariate samples two normal distributions cluster results:, B samples strongly cluster together, large distance\nclusters.Dendrograms Wikipedia pageggdendro package vignette","code":"\nlibrary(ggdendro)\n\n# produce a clustering of the data using the hclust for hierarchical clustering\n# and euclidean distance as the distance metric\neuc_dist <- dist(dplyr::select(ad_metadata,c(tau,abeta,iba1,gfap)))\nhc <- hclust(euc_dist, method=\"ave\")\n\n# add ID as labels to the clustering object\nhc$labels <- ad_metadata$ID\n\nggdendrogram(hc)\nlibrary(patchwork)\nlibrary(ggdendro)\n\nwell_clustered_data <- tibble(\n  ID=c(stringr::str_c(\"A\",1:10),stringr::str_c(\"B\",1:10)),\n  f1=c(rnorm(10,0,1),rnorm(10,10,1)),\n  f2=c(rnorm(10,0,1),rnorm(10,10,1))\n)\n\nscatter_g <- ggplot(well_clustered_data, aes(x=f1,y=f2)) + geom_point()\n\n# produce a clustering of the data using the hclust for hierarchical clustering\n# and euclidean distance as the distance metric\neuc_dist <- dist(dplyr::select(well_clustered_data,-ID))\nhc <- hclust(euc_dist, method=\"ave\")\n\n# add ID as labels to the clustering object\nhc$labels <- well_clustered_data$ID\n\ndendro_g <- ggdendrogram(hc)\nscatter_g | dendro_g"},{"path":"data-visualization.html","id":"chord-diagrams-and-circos-plots","chapter":"7 Data Visualization","heading":"7.6.2 Chord Diagrams and Circos Plots","text":"Circos software package creates circular\ndiagrams originally designed depict genomic features data. software\ncapable making beautiful, information dense figures. Circos software\nwritten Perl programming language, \ncirclize package brings\ncapabilities R. following basic circos plot created \ncirclize package:Basic circlize plotThe circle divided 8 sectors labeled letters. \nsingle track added plot, sector 2-dimensional random\ndataset plotted within arc sector’s track box. Many tracks may \nadded plot:Multiple tracks circlize plotThe arches inside circle called chords drawn indicate \nrelationship coordinates different sectors. Diagrams \nchords sectors called chord diagrams, plots \ncomposed coords:circlize chord diagramThese chord diagrams often used genomics studies illustrate\nchromosomal rearrangements:genomic links exampleRead circlize documentation\ninformation use feature rich package.","code":""},{"path":"data-visualization.html","id":"multiple-plots","chapter":"7 Data Visualization","heading":"7.7 Multiple Plots","text":"Sometimes convenient plot multiple sets data . can \naccomplished two ways: facet wrapping separate different subsets \ndataset plots shared axes, adding multiple plots \nfigure.","code":""},{"path":"data-visualization.html","id":"facet-wrapping","chapter":"7 Data Visualization","heading":"7.7.1 Facet wrapping","text":"Facet wrapping separates subsets dataset plots identical axes.\ncan useful different groups data points overlap occlude one\nanother. Consider following dataset three groups normally\ndistributed random samples:three sample data points plotted top , although \ncolored differently, difficult discern patterns group\nclearly. plot may split three, one sample group,\nusing facet_wrap()\nfunction:Now trends group clearly visible. facet_wrap() function\naccepts one required argument, variable name data \nused separate groups decorated vars() function. \ngeometry may used facet wrap. example, add \ngeom_smooth() plot estimate smoothing function within facet:default, facet wrapping fixes axes plot identical,\nenables accurate estimates comparisons plots according\nElementary Perceptual Tasks hierarchy. many aspects \nfaceting may adjusted using arguments described \ndocumentation.","code":"\nlibrary(mvtnorm) # package implementing multivariate normal distributions\nnsamp <- 100\ndata <-  rbind(\n    rmvnorm(nsamp,c(1,1),sigma=matrix(c(1,0.8,0.8,1),nrow=2)),\n    rmvnorm(nsamp,c(1,1),sigma=matrix(c(1,-0.8,-0.8,1),nrow=2)),\n    rmvnorm(nsamp,c(1,1),sigma=matrix(c(1,0,0,1),nrow=2))\n)\ncolnames(data) <- c('x','y')\ng_oneplot <- as_tibble(data) %>%\n  mutate(\n    sample_name=c(rep('A',nsamp),rep('B',nsamp),rep('C',nsamp))\n  ) %>%\n  ggplot(aes(x=x,y=y,color=sample_name)) +\n  geom_point()\ng_oneplot\ng_oneplot + facet_wrap(vars(sample_name))\ng_oneplot + facet_wrap(vars(sample_name)) +\n  geom_smooth(method=\"loess\", formula=y ~ x)"},{"path":"data-visualization.html","id":"multipanel-figures","chapter":"7 Data Visualization","heading":"7.7.2 Multipanel Figures","text":"Facet wrapping useful data separate plots \nscale, comaprisons facet groups meaningful. However, faceting\nless convenient desire place plots unrelated data \nfigure. patchwork package allows\nus compose multiple plots together using intuitive set operators. \nexample, may put two plots next separating |\noperator:Note plot saved variable composed \npatchwork operator |, puts plots side--side. also place\none plot top / operator:Plots may composed arbitrarily complex multipanel figures way\nusing () group plots together:Another possible composition:patchwork library flexible, allows designer compose plots\nmany different ways. multipanel plots make excellent starting points\nproducing publication-quality figures, \ndescribed next section.","code":"\ndata <- tibble(\n  a=rnorm(100,0,1),\n  b=rnorm(100,3,2)\n)\ng_scatter <- ggplot(data, aes(x=a, y=b)) +\n  geom_point()\ng_violin <- pivot_longer(data, c(a,b)) %>% ggplot(aes(x=name,y=value,fill=name)) +\n  geom_violin()\n\ng_scatter | g_violin\ng_scatter / g_violin\ng_scatter / ( g_scatter | g_violin)\n(g_scatter / g_scatter ) | g_violin"},{"path":"data-visualization.html","id":"publication-ready-plots","chapter":"7 Data Visualization","heading":"7.8 Publication Ready Plots","text":"ggplot powerful flexible tool creating plots, falls little\nshort producing plots directly suitable publication. primarily\ndue default styling provided package; standard ggplot formatting\neffective examining plots screens, elements, \ndefault grey plot background, unnecessary sometimes inappropriate \npolished plots printed journals.two ways take ggplots final step publication quality\nfigures. first using\nthemes provided \nggplot package customizing specific elements taste. second \nexport plots file Scalable Vector\nGraphics format, \ncan imported programs like Adobe\nIllustrator \nInkscape edited .","code":""},{"path":"data-visualization.html","id":"ggplot-themes","chapter":"7 Data Visualization","heading":"7.8.1 ggplot Themes","text":"Themes ggplot combinations styling elements applied different\ncomponents chart include everything except display styling \ndata, controlled using geometry aesthetics. default theme\nproduces plots grey background, white grid, charting area frame, \nstandard axis tick labels:fine displaying screen, plot printed \npaper, solid grey background use unnecessary ink.helpful principle plotting: use least amount ink possible\nproduce desired plot. just economic, ink-saving reasons,\nalso sparser plots without superfluous graphic elements allow \nreader focus data styling.ggplot comes themes may added plots \ntheme_X() functions.\nAnother available theme theme_bw():theme replaces grey background white, draws light grey grid lines,\nadds frame around plotting area. Plots theme might \nsuitable plots displayed projector, example presentations.Another theme theme_classic(), draws elements plot\nstrictly necessary:many -called “complete\nthemes” included \nggplot. desired, style elements controlled complete themes\ncan customized using \ntheme() function. full\ntreatment themes beyond scope chapter, may read \ndocumentation.","code":"\nbase_g <- tibble(\n  x=rnorm(100),\n  y=rnorm(100)\n) %>%\n  ggplot(aes(x=x, y=y)) +\n  geom_point()\n\nbase_g\nbase_g + theme_bw()\nbase_g + theme_classic()"},{"path":"data-visualization.html","id":"exporting-to-svg","chapter":"7 Data Visualization","heading":"7.8.2 Exporting to SVG","text":"Even custom themes multipanel figures may \nproduce figures adhere scientific journal guidelines tastes.\ninstances, ggplot plots may saved Scalable Vector\nGraphics (SVG) format.\ncontrast bitmapped file formats like PNG JPG store images \nmatrix pixels, SVG format describes shapes using mathematical\nspecification. example, circle expressed SVG format like: tag\nspecifies circle (ellipse) positioned center \ncoordinate (50, 50) radius 50. Illustration software like Adobe\nIllustrator \nInkscape can understand SVG instructions generate\nvisual representation . Text can also included directly SVG\nfiles. elements represented mathematically, illustration\nprograms understand can produce images size resolution,\nallowing designer create clear, high quality images required \njournals.Another benefit SVG illustration programs understand \ncan also used edit individual elements entire plot. \nexample, position text labels, font, text , may \narbitrarily modified. Consider following multipanel figure:figure may saved SVG format using \nggsave() function:image edited Adobe Illustrator different font annotations\nfollows:Edited multipanel figureBy combining themes ggplot SVG+illustration software strategy, \nhigh quality customized figures can created relatively little\neffort.","code":"<circle cx=\"50\" cy=\"50\" r=\"50\"/>\nlibrary(patchwork)\ng1 <- tibble(\n  x=rnorm(100),\n  y=rnorm(100)\n) %>%\n  ggplot(aes(x=x, y=y)) +\n  geom_point() +\n  theme_bw()\n\ng2 <- tibble(\n   x=rnorm(100),\n   y=5*x+3+rnorm(100,1,4)\n) %>%\n  ggplot(aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(method=\"loess\", formula=y ~ x) +\n  theme_bw()\n\ng3 <- tibble(\n  group=c(rep('A',100),rep('B',100)),\n  val=c(rnorm(100,3),rnorm(100,5,3))\n) %>%\n  ggplot(aes(x=group, y=val,fill=group)) +  \n  geom_violin() +\n  theme_bw()\n\n(g1 / g2) | g3\nggsave('multipanel.svg')"},{"path":"data-visualization.html","id":"network-visualization","chapter":"7 Data Visualization","heading":"7.9 Network visualization","text":"commonly used package visualize network igraph.First, let’s create graph 15 nodes.Use sample_gnm() generate random graph.assign color vertices, use vertex_attr(). gorder() returns number vertices.assign color edges, use vertex_attr(). gsize() returns total number edges. assigning random colors.real case, want modify color specific edge, need first find edge want modify…modify . example, modify 1st edge, “Emerald–Citrine”.visualize network, just use plot()!real-world, don’t want random network. Now let’s create network scratch.many different ways create network. One basic (easy) ways using graph_from_data_frame() function igraph.\n1. Create 2 columns data frame: . row represents edge, entries vertex names. instance, indicated row 1 , edge “Crystal Moonstone”. undirected graph, can put two vertices either way.\n2. (optional) Add another column color represent color plot edge.\n3. (optional) can also add additional information edge. example, add column weight specify weight edge. weight become useful network analysis later, example, find shortest path two vertices. can also add attributes want, like “connection_type” .Now create igraph object data frame, assign vertex color., simply use plot(g) plot network. can also fancy customization play around labels! (Remember connection_type specified data frame? edge.lty, 1 solid line, 2 dash line)see whole list available arguments available plotting network, visit igraph plot manual.case directed graph, simply put directed = TRUE creating igraph object, see cute arrows.","code":"\nlibrary(igraph)\nnames <- c(\"Opal\", \"Tourmaline\", \"Emerald\", \"Citrine\", \"Jasper\", \"Moonstone\", \"Agate\", \"Chalcedony\", \"Crystal\", \"Obsidian\", \"Quartz\", \"Topaz\", \"Beryl\", \"Sugilite\", \"Sapphire\")\nset.seed(10)\nsamp_g <- sample_gnm(\n  n = 15, # number of vertices\n  m = 20, # total number of edges\n  directed = FALSE # if it's a directed network\n)\nvertex_attr(samp_g) <- list(name = names, color = rep(\"slategray1\", gorder(samp_g)))\nedge_attr(samp_g) <- list(color = sample(\n  x = c(\"slateblue1\", \"lightpink1\", \"steelblue1\", \"palegreen3\"),\n  size = gsize(samp_g), replace = TRUE\n))\nedges(samp_g)## [[1]]\n## IGRAPH 7328255 UN-- 15 20 -- Erdos renyi (gnm) graph\n## + attr: name (g/c), type (g/c), loops (g/l), m (g/n), name (v/c), color\n## | (v/c), color (e/c)\n## + edges from 7328255 (vertex names):\n##  [1] Emerald   --Citrine    Citrine   --Jasper     Tourmaline--Moonstone \n##  [4] Emerald   --Chalcedony Emerald   --Crystal    Opal      --Obsidian  \n##  [7] Agate     --Obsidian   Opal      --Quartz     Jasper    --Quartz    \n## [10] Agate     --Quartz     Obsidian  --Quartz     Obsidian  --Topaz     \n## [13] Tourmaline--Beryl      Agate     --Beryl      Quartz    --Beryl     \n## [16] Quartz    --Sugilite   Emerald   --Sapphire   Moonstone --Sapphire  \n## [19] Agate     --Sapphire   Sugilite  --Sapphire  \n## \n## attr(,\"class\")\n## [1] \"igraph.edge\"\nedge_attr(samp_g, name = \"color\")[1] <- \"red\"\nplot(samp_g)\nset.seed(4)\nmy_gem_df <- data.frame(\n  \"from\" = c(\"Crystal\", \"Crystal\", \"Crystal\", \"Crystal\", \"Topaz\", \"Obsidian\", \"Agate\", \"Topaz\"),\n  \"to\" = c(\"Moonstone\", \"Citrine\", \"Obsidian\", \"Agate\", \"Citrine\", \"Topaz\", \"Moonstone\", \"Moonstone\"),\n  \"color\" = c(\"slateblue1\", \"slateblue1\", \"slateblue1\", \"steelblue1\", \"palegreen3\", \"steelblue1\", \"lightpink1\", \"palegreen3\"),\n  \"connection_type\" = c(rep(1, 6), 2, 2)\n)\nmy_gem_df$weight <- as.numeric(as.factor(my_gem_df$color))\nmy_gem_df##       from        to      color connection_type weight\n## 1  Crystal Moonstone slateblue1               1      3\n## 2  Crystal   Citrine slateblue1               1      3\n## 3  Crystal  Obsidian slateblue1               1      3\n## 4  Crystal     Agate steelblue1               1      4\n## 5    Topaz   Citrine palegreen3               1      2\n## 6 Obsidian     Topaz steelblue1               1      4\n## 7    Agate Moonstone lightpink1               2      1\n## 8    Topaz Moonstone palegreen3               2      2\ng <- graph_from_data_frame(my_gem_df, directed = FALSE)\nvertex_attr(g) <- list(\n  name = c(\"Crystal\", \"Topaz\", \"Obsidian\", \"Agate\", \"Moonstone\", \"Citrine\"),\n  color = c(\"slategray1\", \"slategray1\", \"slategray1\", \"lightpink\", \"slategray1\", \"lightpink\")\n)\nset.seed(4)\nplot(g,\n  edge.width = 1.5,\n  edge.lty = edge_attr(g, \"connection_type\"),\n  vertex.label.family = \"Trebuchet MS\",\n  vertex.label.font = 3,\n  vertex.label.cex = 1,\n  vertex.label.dist = 2,\n  vertex.label.degree = pi / 2\n)\ng <- graph_from_data_frame(my_gem_df, directed = TRUE)\nvertex_attr(g) <- list(\n  name = c(\"Crystal\", \"Topaz\", \"Obsidian\", \"Agate\", \"Moonstone\", \"Citrine\"),\n  color = c(\"slategray1\", \"slategray1\", \"slategray1\", \"lightpink\", \"slategray1\", \"lightpink\")\n)\nset.seed(4)\nplot(g,\n  edge.width = 1.5,\n  edge.lty = edge_attr(g, \"connection_type\"),\n  edge.arrow.size = 0.8,\n  vertex.label.family = \"Trebuchet MS\",\n  vertex.label.font = 3,\n  vertex.label.cex = 1,\n  vertex.label.dist = 2,\n  vertex.label.degree = pi / 2\n)"},{"path":"data-visualization.html","id":"interactive-network-plots","chapter":"7 Data Visualization","heading":"7.9.1 Interactive network plots","text":"Besides igraph, many packages create fancier networks. One example visNetwork. can create interactive network, ’s easy use! Check example . can implemented HTML report knitted R markdown R Shiny app interface!\nOne thing keep mind: can imagine, since interactive, visNetwork requires much memory time compared static graph. , may need make choice develop web interface R markdown HTML report.","code":""},{"path":"biology-bioinformatics.html","id":"biology-bioinformatics","chapter":"8 Biology & Bioinformatics","heading":"8 Biology & Bioinformatics","text":"","code":""},{"path":"biology-bioinformatics.html","id":"bio-r","chapter":"8 Biology & Bioinformatics","heading":"8.1 R in Biology","text":"R became popular biological data analysis early mid 2000s, \nmicroarray\ntechnology came\nwidespread use enabling researchers look statistical differences \ngene expression thousands genes across large numbers samples. \nresult popularity, community biological researchers data\nanalysts created collection software packages called\nBioconductor, made vast array \ncutting edge statistical bioinformatic methodologies widely available.Due ease use, widespread community support, rich ecosystem \nbiological data analysis packages, free, R remains one \nprimary languages biological data analysis. language’s early focus \nstatistical analysis, later transition data science general, makes \nuseful accessible perform kinds analyses new data science\nbiology required. also bridge biologists without \ncomputational background statisticians bioinformaticians, invent\nnew methods implement R packages easily accessible .\nlanguage package ecosystems communities supports continue \nmajor source biological discovery innovation.data science, biology benefits main strengths R \ntidyverse combined powerful analytical techniques\navailable Bioconductor packages, namely manipulate, visualize, analyze,\ncommunicate biological data.","code":""},{"path":"biology-bioinformatics.html","id":"biological-data-overview","chapter":"8 Biology & Bioinformatics","heading":"8.2 Biological Data Overview","text":"","code":""},{"path":"biology-bioinformatics.html","id":"bio-data-types","chapter":"8 Biology & Bioinformatics","heading":"8.2.1 Types of Biological Data","text":"general terms, five types data used biological data\nanalysis: raw/primary data, processed data, analysis results, metadata, \nannotation data.Raw/primary data. data primary observations made whatever\ninstruments/techniques using experiment. may include\nhigh-throughput sequencing data, mass/charge ratio data mass spectrometry,\n16S rRNA sequencing data metagenomic studies, SNPs genotyping assays,\netc. data can large often efficiently processed using\nR. Instead, specialized tools built outside R used first process \nprimary data form amenable analysis. common primary\nbiological data types include Microarrays, High Throughput Sequencing data,\nmass spectrometry data.Processed data. Processed data result analysis \ntransformation primary data intermediate, interpretable form.\nexample, performing gene expression analysis RNASeq, short reads\nform primary data typically aligned genome \ncounted annotated genes, resulting count gene \ngenome. Typically, counts combined many samples single\nmatrix subject downstream analyses like differential expression.Analysis results. Analysis results aren’t data per se, results\nanalysis primary data processed results. example, gene\nexpression studies, first analysis often differential expression, \ngene genome tested expression associated outcome\ninterest across many samples. gene number statistics \nrelated values, like log2 fold change, nominal adjusted p-value, etc. \nforms data usually use form interpretations datasets\ntherefore must manipulate much way dataset.Metadata. experimental settings, multiple samples \nprocessed primary data type collected. samples also \ninformation associated , termed metadata, “data\ndata”. gene expression post mortem brain experiments mentioned\n, information individuals , including age death,\nwhether person disease, measurements tissue quality, etc. \nmetadata. primary processed data metadata usually stored \ndifferent files, metadata (sample information sample data,\netc) one column indicating unique identifier (ID) sample.\nprocessed data typically columns named sample IDs.Annotation data. tremendous amount knowledge generated \nbiological entities, e.g. genes, especially since publication human\ngenome. Annotation data publicly available information features \nmeasure experiments. may come form coordinates \ngenome genes exist, known functions genes, domains found\nproteins relative sequence, gene identifier cross references across\ndifferent gene naming systems (e.g. symbol vs Ensembl ID), single nucleotide\npolymorphism genomic locations associations traits diseases, etc.\ninformation use aid interpretation experimental\ndata generally generate . Annotation data comes many\nforms, CSV format.figure contains simple diagram different types data\nwork together typical biological data analysis.Information flow biological data analysis","code":""},{"path":"biology-bioinformatics.html","id":"csv-files","chapter":"8 Biology & Bioinformatics","heading":"8.2.2 CSV Files","text":"common, convenient, flexible data file format biology \nbioinformatics character-delimited character-separated text file.\nfiles plain text files (.e. native file format \nspecific program, like Excel) generally contain rectangles data. \nformatted correctly, can think files containing grid matrix\ndata values number columns, number \nvalues. line files number data values separated \nconsistent character, commonly comma called comma-separated\nvalue, “CSV”, files\nfilenames typically end extension .csv. Note \ncharacters, especially  character, may used create valid\nfiles format, general principles apply. \nexample simple CSV file:properties principles CSV files:first line often always contains column names columnEach value delimited character, case ,Values can value, including numbers charactersWhen value contains delimiting character (e.g. HOXA1,HOXB1 contains \n,), value wrapped double quotesValues can missing, indicated sequential delimiters (.e. ,, one\n, end line, last column value missing)delimiter end linesTo well-formatted every line must number delimited\nvaluesThese properties principles apply character-separated files,\nregardless specific delimiter used. file follows principles,\ncan loaded easily R data analysis setting.\n### Common Biological Data MatricesTypically first data set work R processed data \ndescribed previous section. data transformed primary\ndata way (usually) forms numeric matrix features\nrows samples columns. first column files usually contains\nfeature identifier, e.g. gene identifier, genomic locus, probe set ID, etc \nremaining columns numerical values, one per sample. first row \nusually column names columns file. example one\nfiles microarray gene expression dataset loaded R:file 54,676 rows, consisting one header row R loads \ncolumn names, remaining probe sets, one per row. 36\ncolumns, first contains probe set ID (e.g. 1007_s_at) \nremaining 35 columns correspond samples.","code":"id,somevalue,category,genes\n1,2.123,A,APOE\n4,5.123,B,\"HOXA1,HOXB1\"\n7,8.123,,SNCAintensities <- read_csv(\"example_intensity_data.csv\")\nintensities\n# A tibble: 54,675 x 36\n   probe     GSM972389 GSM972390 GSM972396 GSM972401 GSM972409 GSM972412 GSM972413 GSM972422 GSM972429\n   <chr>         <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1 1007_s_at      9.54     10.2       9.72      9.68      9.35      9.89      9.70      9.67      9.87\n 2 1053_at        7.62      7.92      7.17      7.24      8.20      6.87      6.62      7.23      7.45\n 3 117_at         5.50      5.56      5.06      7.44      5.19      5.72      5.87      6.15      5.46\n 4 121_at         7.27      7.96      7.42      7.34      7.49      7.76      7.44      7.66      8.02\n 5 1255_g_at      2.79      3.10      2.78      2.91      3.02      2.73      2.78      3.56      2.83\n 6 1294_at        7.51      7.28      7.00      7.18      7.38      6.98      6.90      7.54      7.66\n 7 1316_at        3.89      4.36      4.24      3.94      4.20      4.34      4.06      4.24      4.11\n 8 1320_at        4.65      4.91      4.70      4.78      5.06      4.71      4.55      4.58      5.10\n 9 1405_i_at      8.03      7.47      5.42      7.21      9.48      6.79      6.57      8.50      6.36\n10 1431_at        3.09      3.78      3.33      3.12      3.21      3.27      3.37      3.84      3.32\n# ... with 54,665 more rows, and 26 more variables: GSM972433 <dbl>, GSM972438 <dbl>, GSM972440 <dbl>,\n#   GSM972443 <dbl>, GSM972444 <dbl>, GSM972446 <dbl>, GSM972453 <dbl>, GSM972457 <dbl>,\n#   GSM972459 <dbl>, GSM972463 <dbl>, GSM972467 <dbl>, GSM972472 <dbl>, GSM972473 <dbl>,\n#   GSM972475 <dbl>, GSM972476 <dbl>, GSM972477 <dbl>, GSM972479 <dbl>, GSM972480 <dbl>,\n#   GSM972487 <dbl>, GSM972488 <dbl>, GSM972489 <dbl>, GSM972496 <dbl>, GSM972502 <dbl>,\n#   GSM972510 <dbl>, GSM972512 <dbl>, GSM972521 <dbl>"},{"path":"biology-bioinformatics.html","id":"biological-data-is-not-tidy","chapter":"8 Biology & Bioinformatics","heading":"8.2.3 Biological data is NOT Tidy!","text":"mentioned tidy data section, tidyverse packages\nassume data -called “tidy format”, variables columns \nobservations rows. Unfortunately, certain forms biological data \ntypically available opposite orientation - variables rows \nobservations columns. primarily true feature data matrices,\ne.g. gene expression counts matrices, number variables (e.g. genes)\nmuch larger number samples, tend small small\ncompared number features. format convenient humans \ninteract using, e.g. spreadsheet programs like Microsoft Excel, can\nunfortunately make performing certain operations challenging \ntidyverse.example, consider microarray expression dataset previous section.\n54,676 rows probeset, 35 numeric columns \nsample. large number probesets consider, especially \nplan conduct statistical test , impose substantial\nmultiple hypothesis testing burden \nresults. may therefore wish eliminate probesets low\nvariance overall dataset, since probesets likely \ndetectable statistical difference tests. However, computing \nvariance probeset computation across columns, columns\n, tidyverse designed well. Said\ndifferently, R tidyverse operate default rows data\nframe, tibble, matrix.base R tidyverse optimized perform computations columns, \nrows. reasons buried details R program \nwritten organize data internally beyond scope book.\nconsequence design choice , can perform operations\nrows rather columns data structure, code may perform\npoorly (.e. take long time run).working datasets, couple options deal \nissue:Pivot long format. described Rearranging Data section, \ncan rearrange tibble amenable certain computations. \nearlier example, wish group measurements probeset \ncompute variance , possibly filter probesets based low\nvariance. can therefore combine pivot_longer(), group_by(),\nsummarize(), finally left_join() perform operation. Exactly\nleft exercise Assignment 1.Pivot long format. described Rearranging Data section, \ncan rearrange tibble amenable certain computations. \nearlier example, wish group measurements probeset \ncompute variance , possibly filter probesets based low\nvariance. can therefore combine pivot_longer(), group_by(),\nsummarize(), finally left_join() perform operation. Exactly\nleft exercise Assignment 1.Compute row-wise statistics using apply(). described Iteration,\nR functional programming language implements iteration \nfunctional style using apply() function. apply()   function accepts MARGIN argument 1 2 \nprovided function applied columns rows, respectively. \nmethod can used compute summary statistic row tibble \nresult saved back tibble using column set operator:\n\nintensity_variance <- apply(intensities, 2, var)\nintensities$variance <- intensity_varianceCompute row-wise statistics using apply(). described Iteration,\nR functional programming language implements iteration \nfunctional style using apply() function. apply()   function accepts MARGIN argument 1 2 \nprovided function applied columns rows, respectively. \nmethod can used compute summary statistic row tibble \nresult saved back tibble using column set operator:","code":"\nintensity_variance <- apply(intensities, 2, var)\nintensities$variance <- intensity_variance"},{"path":"biology-bioinformatics.html","id":"bio-bioconductor","chapter":"8 Biology & Bioinformatics","heading":"8.3 Bioconductor","text":"Bioconductor organized collection \nstrictly biological analysis methods packages R. packages hosted\nmaintained outside CRAN \nmaintainers enforce rigorous set coding quality, testing, \ndocumentation\nstandards \nrequired normal R packages. stricter requirements arose \nrecognition software packages generally usable \ndocumentation allows , also many users \npackages statisticians experienced computer programmers.\nRather, people like us: biological analysis practitioners may \nmay substantial coding experience must analyze data\nnonetheless. excellent documentation community support \nbioconductor ecosystem primary reason R popular language \nbiological analysis.Bioconductor divided roughly two sets packages: core maintainer\npackages user contributed packages. core maintainer packages among\ncritical, define set common objects classes (e.g.\nExpressionSet class\nBiobase package)\nBioconductor packages know work . common base provides\nconsistency among Bioconductor packages thereby streamlining learning\nprocess. User contributed maintained packages provide specialized\nfunctionality specific types analysis.Bioconductor must installed prior installing Bioconductor\npackages. [install bioconductor], can run following:Bioconductor undergoes regular releases indicated version, e.g. version = \"3.14\" time writing. Every bioconductor package also version,\nversions may may compatible specific version\nBioconductor. make matters worse, Bioconductor versions \ncompatible certain versions R. Bioconductor version 3.14 requires R\nversion 4.1.0, work older R versions. versions can cause\nmajor version compatibility issues forced use older versions \nR, may sometimes case managed compute clusters. good\ngeneral solution ensuring packages work together, general\nbest rule thumb use current version R packages \ntime write code.BiocManager package Bioconductor package installable using\ninstall.packages(). installing BiocManager package, may \ninstall bioconductor packages:One key aspect Bioconductor packages consistent helpful documentation;\nevery package page Bioconductor.org site lists block code \ninstall package, e.g. affy\npackage. \naddition, Biconductor provides three types documentation:Workflow tutorials \nperform specific analysis use casesPackage vignettes \nevery package, provide worked example use package \ncan adapted user specific caseDetailed, consistently formatted reference documentation gives precise\ninformation functionality use packageThe thorough useful documentation packages Bioconductor one \nreasons package ecosystem, R, widely used biology \nbioinformatics.base Bioconductor packages define convenient data structures storing \nanalyzing many types data. Recall earlier Types Biological Data\nsection, described five types biological data: primary, processed,\nanalysis, metadata, annotation data. Bioconductor provides convenient\nclass storing many different data types together one place,\nspecifically SummarizedExperiment class package \nname\npackage. illustration SummarizedExperiment object stores ,\nBioconductor maintainer’s paper Nature:SummarizedExperiment Schematic - Huber, et al. 2015. “Orchestrating High-Throughput Genomic Analysis Bioconductor.” Nature Methods 12 (2): 115–21.can see figure, class stores processed data (assays),\nmetadata (colData exptData), annotation data (rowData). \nSummarizedExperiment class used ubiquitously throughout Bioconductor\npackage ecosystem, core classes cover later\nchapter.","code":"\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(version = \"3.14\")\n# installs the affy bioconductor package for microarray analysis\nBiocManager::install(\"affy\")"},{"path":"biology-bioinformatics.html","id":"microarrays","chapter":"8 Biology & Bioinformatics","heading":"8.4 Microarrays","text":"Microarray Device - Thermo Fisher ScientificMicroarrays devices measure relative abundance thousands \ndistinct DNA sequences simultaneously. Short (~25 nucleotide) single-stranded\nDNA molecules called probes deposited small glass slide grid \nspots, spot contains many copies probe identical sequence.\nprobe sequences selected priori based purpose array.\nexample, gene expression microarrays probes correspond \ncoding regions genome species interest, genotyping\nmicroarrays use sequences known variants found population genomes,\noften human population. Microarrays type (e.g. human gene\nexpression) set probes. choice DNA sequence probes\ntherefore determines microarray measures interpret data.\ndesign microarray illustrated following figure.Illustration Microarray DesignA microarray device generates data applying specially prepared DNA sample\n; sample usually corresponds single biological specimen, e.g. \nindividual patient. preparation method sample depends \nmeasured:measuring DNA directly, e.g. genetic variants, DNA \nbiochemically extractedWhen measuring gene expression via RNA abundance, RNA first extracted \nreverse transcribed complementary DNA (cDNA)either case, molecules applied microarray DNA\nmolecules. extraction preparation, DNA molecules randomly\ncut shorter molecules (.e. sheared) molecule molecular\ntag biochemically ligated emit fluorescence excited \nspecific wavelength light. tagged DNA sample washed \nmicroarray chip, DNA molecules share sequence complementarity \nprobes array pair together. treatment, microarray \nwashed remove DNA molecules match array, leaving\nmolecules sequence match remain bound probes.microarray chip loaded scanning device, laser \nspecific wavelength light shone onto array, causing spots\ntagged DNA molecules associated probes fluoresce, spots\nremain dark. high resolution image taken fluorescent array, \nimage analyzed map intensity light spot numeric\nvalue proportional intensity. reason , since \nspot many individual probe molecules contained within , copies \ncorresponding DNA molecule sample, light spot\nemits. way, relative abundance probes array \nmeasured simultaneously. process generating microarray data sample\nillustrated following figure.Illustration Microarray Data Generation ProcessAfter microarray scanned, relative copy number DNA \nsample matching probes microarray expressed intensity\nfluorescence probe. raw intensity data scan \nprocessed analyzed scanner software account technical biases\nartefacts scanning instrument data generation process. data\nsingle scan processed stored file CEL\nformat,\nproprietary data format stores raw probe intensity data can \nloaded downstream analysis.","code":""},{"path":"biology-bioinformatics.html","id":"high-throughput-sequencing","chapter":"8 Biology & Bioinformatics","heading":"8.5 High Throughput Sequencing","text":"High throughput sequencing (HTS) technologies measure digitize \nnucleotide sequences thousands billions DNA molecules simultaneously.\ncontrast microarrays, identify new sequences due \npriori probe based design, HTS instruments can principle determine \nsequence DNA molecule sample. reason, HTS datasets \nsometimes called unbiased assays, sequences identify \npredetermined.popular HTS technology time writing provided \nIllumina biotechnology company.\ntechnology first developed commercialized company\nSolexa\nlate 1990s instrumental completion human\ngenome. cost generating data technology reduced several\norders magnitude since , resulting exponential growth biological\ndata fundamentally changed understanding biology health. \n, analysis data, time cost generation,\nbecome primary bottleneck biological biomedical research.Illumina HTS technology uses biotechnological technique called sequencing\nsynthesis. Briefly, technique entails biochemically ligating millions\nbillions short (~300 nucleotide long) DNA molecules glass slide,\ndenatured become single stranded, used synthesize \ncomplementary strand incorporating fluorescently tagged nucleotides. \ntagged nucleotides incorporated nucleotide addition excited\nlaser high resolution photograph taken flow cell. \nprocess repeated many times DNA molecules flow cell \ncompletely synthesized, collective images computationally\nanalyzed build complete sequence molecule.HTS methods developed use different strategies, \nutilize sequencing synthesis method. alternative\ntechnologies differ length, number, confidence per base cost \nDNA molecules sequence. time writing, two common\ntechnologies include:Pacific Biosciences (PacBio) Long Read\nSequencing - \ntechnology uses engineered polymerase performs run-amplification \ncircularized DNA molecules records signature different base \nincorporated amplification product. PacBio sequencing can generate\nreads thousands nucleotides length.Oxford Nanopore Sequencing (ONS) - technology uses\nengineered proteins form pores electrically conductive membrane,\nsingle stranded DNA molecules pass. individual nucleotides\nDNA molecule pass pore, electrical current induced \ndifferent nucleotide molecular structures recorded analyzed signal\nprocessing algorithms recover original sequence molecule. \nsynthesis occurs ONS technology.Due widespread use, Illumina sequencing data analysis covered \nbook, though additions future may warranted.DNA molecules may subjected sequencing via method; meaning \nresulting datasets goal subsequent analysis therefore\ndependent upon material studied. common\ntypes HTS experiments:Whole genome sequencing (WGS) - input sequencing randomly sheared\ngenomic DNA molecules, single organisms collection organisms, \ngoal reconstructing refining genome sequence \nconstituent organismsRNA Sequencing (RNASeq) - input complementary DNA (cDNA) \nreverse transcribed \nRNA extracted sample, goal detecting RNA transcripts\n(therefore genes) expressed sampleWhole genome bisulfite sequencing (WGBS) - input sequencing \nrandomly sheared genomic DNA undergone bisulfite treatment, \nconverts methylated cytosines thymines, thereby enabling detection \nepigenetic methylation marks DNAChromatin immunoprecipitation followed sequencing (ChIPSeq) - input\ngenomic DNA bound specific DNA-binding proteins (e.g.\ntranscription factors), allows identification locations genome\nproteins boundA complete coverage many different types sequencing assays beyond\nscope book, analysis techniques used \ndifferent types experiments covered later chapter.","code":""},{"path":"biology-bioinformatics.html","id":"raw-hts-data","chapter":"8 Biology & Bioinformatics","heading":"8.5.1 Raw HTS Data","text":"raw data produced sequencing instrument, sequencer, \ndigitized DNA sequences billions molecules captured\nflow cell. digitized nucleotide sequence molecule called\nread, therefore every run sequencing instrument produces millions\nbillions reads. data stored standardized format called \nFASTQ file format. Data \nsingle read FASTQ format shown :read four lines FASTQ file. first beginning @ \nsequencing header name, provides unique identifier read \nfile. second line nucleotide sequence detected \nsequencer. third line secondary header line starts + \nmay may include information. last line contains phred\nquality scores base\ncalls corresponding position read. Briefly, character \nphred quality score line corresponding integer proportional \nconfidence base corresponding position read called\ncorrectly. information used assess quality data. See \n-depth explanation phred\nscore information.","code":"@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65"},{"path":"biology-bioinformatics.html","id":"preliminary-hts-data-analysis","chapter":"8 Biology & Bioinformatics","heading":"8.5.2 Preliminary HTS Data Analysis","text":"Since HTS experiments generate type data (.e. reads), data\nanalyses begin processing sequences via bioinformatic algorithms;\nchoice algorithm determined experiment type biological\nquestion asked. full treatment variety bioinformatic\napproaches processing data beyond scope book. \nfocus one analysis approach, alignment, commonly employed \nproduce derived data processed using R.common HTS data, also called short read sequencing data, \ngenerated organisms already genome sequenced, e.g.\nhuman mouse. reads dataset can therefore compared \nappropriate genome sequence aid interpretation. comparison amounts \nsearching organism’s genome, -called reference genome, \nlocation locations molecule captured read originated.\nsearch process called alignment, short read sequence \naligned usually much larger reference genome identify locations\nidentical nearly identical sequence. Said differently, alignment\nprocess attempts assign read one locations reference\ngenome, reads match target genome \nassigned.alignment process computationally intensive requires specialized\nsoftware perform efficiently. may technically capable \nperforming alignment, R optimized process character data volume\ntypical datasets. Therefore, common work short read\ndata (.e. FASTQ files) directly R, processing steps required\ntransform sequencing data forms amenable \nanalysis R.end result aligning reads short read dataset assignments\nreads positions across entire reference genome. distribution \nread alignment locations across genome forms basis data\nused interpret dataset. Specifically, locus, location spanning\none bases, zero reads aligned . number reads\naligned locus, read count commonly interpreted \nproportional number molecules found original sample.\nTherefore, analysis interpretation HTS data commonly involves \nanalysis count data, discuss detail next section. \nfollowing figure illustrates concepts described section.Illustration WGS RNASeq read alignment process","code":""},{"path":"biology-bioinformatics.html","id":"count-data","chapter":"8 Biology & Bioinformatics","heading":"8.5.3 Count Data","text":"mentioned , number reads, read counts align \nlocus genome form distribution can use interpret data. Count\ndata two important properties require special attention:Count data integers - read counts assigned particular locus\nwhole numbersCounts non-negative - counts can either zero positive; \nnegativeThese two properties important consequence count data \nnormally distributed. Unlike microarray data, common statistical\ntechniques like linear regression directly applicable count data. \nmust therefore account property data prior statistical\nanalysis one two ways:Use statistical methods model counts data - generalized linear models\nmodel data using Poisson Negative Binomial distributionsTransform data normally distributed - certain statistical\ntransformations,\nregularized log rank\ntransformations can make counts data closely follow normal distributionWe discuss strategies greater detail RNASeq Gene\nExpression Data section.","code":""},{"path":"biology-bioinformatics.html","id":"gene-identifiers","chapter":"8 Biology & Bioinformatics","heading":"8.6 Gene Identifiers","text":"","code":""},{"path":"biology-bioinformatics.html","id":"gene-identifier-systems","chapter":"8 Biology & Bioinformatics","heading":"8.6.1 Gene Identifier Systems","text":"Since first genetic sequence gene determined 1965 - alanine\ntRNA yeast(Holley et al. 1965) -\nscientists trying give names. 1979, formal guidelines \nnomenclature used human genes \nproposed(Shows et al. 1979) along \nfounding Human Gene Nomenclature Committee (Bruford et al. 2020), \nresearchers common vocabulary refer genes consistent\nway. 1989, committee placed auspices Human Genome\nOrganisation (HUGO), becoming HUGO Gene Nomenclature Committee (HGNC). \nHGNC official body providing guidelines gene naming authority\nsince, HGNC gene names often called gene symbols.per Bruford et al. (2020), official human gene symbol guidelines \nfollows:gene assigned unique symbol, HGNC ID descriptive name.Symbols contain uppercase Latin letters Arabic numerals.Symbols commonly used abbreviationsNomenclature contain reference species “G” gene.Nomenclature offensive pejorative.Gene symbols human-readable system naming genes. gene\nsymbols BRCA1, APOE, ACE2 may familiar involved\ncommon diseases, namely breast cancer, Alzheimer’s Disease risk, \nSARS-COV-2, respectively. Typically, gene symbol acronym roughly\nrepresents label gene (originally found \n, many genes subsequently discovered involved entirely\nseparate processes well), e.g. APOE represents gene apolipoprotein\nE.\nconvention convenient humans reading identifying genes.\nHowever, standardized though symbol conventions may , always\neasy computers work , ambiguities can cause mapping problems.gene identifier systems developed along gene information\ndatabases. 2000, Ensembl genome browser \nlaunched Wellcome Trust, charitable foundation\nbased United Kingdom, goal providing automated annotation \nhuman genome. Ensembl Project, supports genome browser,\nrecognized even publication human genome manual\nannotation curation genes slow labor intensive process \nprovide researchers around world timely access information. \nproject therefore created set automated tools pipelines collect,\nprocess, publish rapid consistent annotation genes human\ngenome. Since initial release, Ensembl now supports 50,000 different\ngenomes.Ensembl assigns automatic, systematic ID called Ensembl Gene ID every\ngene database. Human Ensembl Gene IDs take form ENSG plus 11\ndigit number, optionally followed period delimited version number. \nexample, time writing BRCA1 gene Ensembl Gene ID \nENSG00000012048.23. stable ID portion (.e. ENSGNNNNNNNNNNN) remain\nassociated gene forever (unless gene annotation changes\n“dramatically” case \nretired). .23 version\nnumber gene annotation, meaning gene updated 22 times (plus\ninitial version) since addition database. additional version\ninformation important reproducibility biological analysis, since\nconclusions drawn results analyses usually based \ncurrent information gene continually updated time.Ensembl maintains annotations many different organisms, gene\nidentifiers genome contain codes indicate organism gene\n. codes orthologs body plan developmental gene\nHOXA1 several\ndifferent species:Ensembl Gene IDs gene symbols commonly used gene identifiers.\nGENCODE project provides consistent\nstable genome annotation releases human mouse genomes, uses \ntwo types identifiers official annotations.Ensembl provides stable identifiers transcripts well genes. Transcript\nidentifiers correspond distinct patterns exons/introns \nidentified gene, gene one distinct transcripts.\nEnsembl Transcript IDs form ENSTNNNNNNNNNNN.NN similar Gene IDs.Ensembl gene identifier system besides gene symbol. Several\ndatabases devised maintain identifiers, notably\n[Entrez Gene IDs](UCSC Gene IDs used NCBI Gene\ndatabase assigns unique integer \ngene organism, Online Mendelian Inheritance Man\n(OMIM) database, identifiers look like\nOMIM:NNNNN, OMIM ID refers unique gene human phenotype.\nHowever, primary identifiers used modern human biological research remain\nEnsembl IDs official HGNC gene symbols.","code":""},{"path":"biology-bioinformatics.html","id":"mapping-between-identifier-systems","chapter":"8 Biology & Bioinformatics","heading":"8.6.2 Mapping Between Identifier Systems","text":"common operation biological analysis map gene identifier\nsystems, e.g. given Ensembl Gene ID want map \nhuman-recognizable gene symbols. Ensembl provides service called\nBioMart allows download\nannotation information genes, transcripts, information maintained\ndatabases. also provides limited access external sources \ninformation genes, including cross references HGNC gene symbols \ngene identifier systems. Ensembl website helpful\ndocumentation \nuse BioMart download annotation data using web interface.addition downloading annotations CSV file web interface,\nEnsembl also provides biomaRt Bioconductor\npackage \nallow programmatic access information directly R. much\ninformation Ensembl\ndatabases gene\nannotation data can accessed via BioMart, provide brief\nexample extract gene information :name column provides programmatic name associated attribute\ncan used retrieve annotation information using getBM()\nfunction:Ensembl Gene ID HGNC symbol mapping hand, can combine \ntibble tibbles containing gene information joining \ntogether.BioMart/biomaRt ways map different gene identifiers. \nBioconductor package\nAnnotateDbi\nalso provides functionality flexible format independent Ensembl\ndatabases. package includes gene identifier mapping \ninformation, also identifiers technology platforms, e.g. probe set IDs\nmicroarrays, can help working types \ndata. package also allows comprehensive flexible homolog\nmapping. Bioconductor packages, \nAnnotationDbi documentation well written comprehensive, though knowledge\nrelational databases \nhelpful understanding packages work.Ensembl BioMart portalEnsembl BioMart web portal documentationbiomaRt Bioconductor documentation","code":"# this assumes biomaRt is already installed through bioconductor\nlibrary(biomaRt)\n\n# the human biomaRt database is named \"hsapiens_gene_ensembl\"\nensembl <- useEnsembl(biomart=\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n\n# listAttributes() returns a data frame, turn into a tibble to help with formatting\nas_tibble(listAttributes(ensembl))\n# A tibble: 3,143 x 3\n   name                          description                  page        \n   <chr>                         <chr>                        <chr>       \n 1 ensembl_gene_id               Gene stable ID               feature_page\n 2 ensembl_gene_id_version       Gene stable ID version       feature_page\n 3 ensembl_transcript_id         Transcript stable ID         feature_page\n 4 ensembl_transcript_id_version Transcript stable ID version feature_page\n 5 ensembl_peptide_id            Protein stable ID            feature_page\n 6 ensembl_peptide_id_version    Protein stable ID version    feature_page\n 7 ensembl_exon_id               Exon stable ID               feature_page\n 8 description                   Gene description             feature_page\n 9 chromosome_name               Chromosome/scaffold name     feature_page\n10 start_position                Gene start (bp)              feature_page\n# ... with 3,133 more rows# create a tibble with ensembl gene ID, HGNC gene symbol, and gene description\ngene_map <- as_tibble(\n  getBM(\n    attributes=c(\"ensembl_gene_id\", \"hgnc_symbol\", \"description\"),\n    mart=ensembl\n  )\n)\ngene_map\n# A tibble: 68,012 x 3\n   ensembl_gene_id hgnc_symbol description                                                               \n   <chr>           <chr>       <chr>                                                                     \n 1 ENSG00000210049 MT-TF       mitochondrially encoded tRNA-Phe (UUU/C) [Source:HGNC Symbol;Acc:HGNC:748~\n 2 ENSG00000211459 MT-RNR1     mitochondrially encoded 12S rRNA [Source:HGNC Symbol;Acc:HGNC:7470]       \n 3 ENSG00000210077 MT-TV       mitochondrially encoded tRNA-Val (GUN) [Source:HGNC Symbol;Acc:HGNC:7500]\n 4 ENSG00000210082 MT-RNR2     mitochondrially encoded 16S rRNA [Source:HGNC Symbol;Acc:HGNC:7471]       \n 5 ENSG00000209082 MT-TL1      mitochondrially encoded tRNA-Leu (UUA/G) 1 [Source:HGNC Symbol;Acc:HGNC:7~\n 6 ENSG00000198888 MT-ND1      mitochondrially encoded NADH:ubiquinone oxidoreductase core subunit 1 [So~\n 7 ENSG00000210100 MT-TI       mitochondrially encoded tRNA-Ile (AUU/C) [Source:HGNC Symbol;Acc:HGNC:748~\n 8 ENSG00000210107 MT-TQ       mitochondrially encoded tRNA-Gln (CAA/G) [Source:HGNC Symbol;Acc:HGNC:749~\n 9 ENSG00000210112 MT-TM       mitochondrially encoded tRNA-Met (AUA/G) [Source:HGNC Symbol;Acc:HGNC:749~\n10 ENSG00000198763 MT-ND2      mitochondrially encoded NADH:ubiquinone oxidoreductase core subunit 2 [So~\n# ... with 68,002 more rows"},{"path":"biology-bioinformatics.html","id":"mapping-homologs","chapter":"8 Biology & Bioinformatics","heading":"8.6.3 Mapping Homologs","text":"Sometimes necessary link datasets different organisms together \northology. example, experiment performed mice, might \ninterested comparing gene expression patterns observed mouse samples\npublicly available human dataset. contexts, must link gene\nidentifiers one organism corresponding homologs .\nBioMart enables us extract linked identifiers explicitly connecting\ndifferent biomaRt\ndatabases\ngetLDS() function:mgi_symbol field refers gene symbol assigned Mouse Genome\nInformatics database\nmaintained Jackson Laboratory.","code":"human_db <- useEnsembl(\"ensembl\", dataset = \"hsapiens_gene_ensembl\")\nmouse_db <- useEnsembl(\"ensembl\", dataset = \"mmusculus_gene_ensembl\")\north_map <- as_tibble(\n  getLDS(attributes = c(\"ensembl_gene_id\", \"hgnc_symbol\"),\n       mart = human_db,\n       attributesL = c(\"ensembl_gene_id\", \"mgi_symbol\"),\n       martL = mouse_db\n  )\n)\north_map\n# A tibble: 26,390 x 4\n   Gene.stable.ID  HGNC.symbol Gene.stable.ID.1   MGI.symbol\n   <chr>           <chr>       <chr>              <chr>     \n 1 ENSG00000198695 MT-ND6      ENSMUSG00000064368 \"mt-Nd6\"  \n 2 ENSG00000212907 MT-ND4L     ENSMUSG00000065947 \"mt-Nd4l\"\n 3 ENSG00000279169 PRAMEF13    ENSMUSG00000094303 \"\"        \n 4 ENSG00000279169 PRAMEF13    ENSMUSG00000094722 \"\"        \n 5 ENSG00000279169 PRAMEF13    ENSMUSG00000095666 \"\"        \n 6 ENSG00000279169 PRAMEF13    ENSMUSG00000094741 \"\"        \n 7 ENSG00000279169 PRAMEF13    ENSMUSG00000094836 \"\"        \n 8 ENSG00000279169 PRAMEF13    ENSMUSG00000074720 \"\"        \n 9 ENSG00000279169 PRAMEF13    ENSMUSG00000096236 \"\"        \n10 ENSG00000198763 MT-ND2      ENSMUSG00000064345 \"mt-Nd2\"  \n# ... with 26,380 more rows"},{"path":"biology-bioinformatics.html","id":"gene-expression","chapter":"8 Biology & Bioinformatics","heading":"8.7 Gene Expression","text":"Gene expression process information gene used \nsynthesis functional gene products affect phenotype. gene\nexpression studies often focused protein coding genes, many\nfunctional molecules, e.g. transfer\nRNAs,\nlncRNAs, etc, \nproduced process. gene expression process many steps \nintermediate products, depicted following figure:Gene Expression Process - overview flow information DNA\nprotein eukaryoteThe specific parts genome code genes copied, \ntranscribed RNA molecules called transcripts. lower organisms like\nbacteria, RNA molecules passed directly ribosomes, \ntranslate proteins. higher organisms like eukaryotes, \ninitial transcripts, called pre-messenger RNA (pre-mRNA) transcripts \nprocessed certain parts transcript, called introns,\nspliced flanking sequences, called exons, concatenated\ntogether. splicing process complete, pre-mRNA transcripts\nbecome mature messenger RNA (mRNA) transcripts go exported \nnucleus loaded ribosomes cytoplasm undergo translation\nproteins.gene expression studies, relative abundance, number copies, RNA\nmRNA transcripts sample measured. measurements non-negative\nnumbers proportional relative abundance transcript \nrespect reference, either another gene, case high\nthroughput assays like microarrays high throughput sequencing, relative \nmeasurements distinct transcripts examined experiment.\nConceptually, larger magnitude transcript abundance measurement,\ncopies transcript original sample.many ways measure abundance RNA transcripts; following\ncommon methods time writing:Light absorbance - RNA absorbs ultraviolet light wavelength 260 nm, \ncan used determine RNA concentration sample using specialized equipmentNorthern blot -\nmeasures relative abundance RNA specific sequencequantitative polymerase chain reaction\n(qPCR) -\nmeasures relative abundance RNA specific sequence using PCR\namplificationOligonucleotide microarrays - measures relative abundance\nthousands genes simultaneously using known DNA probe sequences \nfluorescently tagged RNA moleculesHigh throughput RNA sequencing (RNASeq) - measures thousands \nmillions RNA fragments simultaneously proportion relative\nabundanceWhile measurement methods may analyzed R, high\nthroughput methods (.e. microarray, high throughput sequencing) primary\nconcern chapter. methods generate measurements thousands \ntranscripts genes simultaneously, requiring power flexibility \nprogrammatic analysis process practical amount time. remainder \nchapter devoted understanding specific technologies, data types,\nanalytical strategies involved working data.Gene expression measurements almost always inherently relative, either due\nlimitations measurement methods (e.g. microarrays, described ) \nmeasuring molecules sample almost always prohibitively\nexpensive, diminishing returns, difficult impossible\ndetermine molecules measured. means \ngeneral associate numbers associated measurements absolute\nmolecular copy number. important implication inherent relativity \nmeasurements : absence evidence evidence absence. \nwords, transcript abundance measurement zero, \nnecessarily imply gene expressed. may gene\nindeed expressed, copy number sufficiently small \ndetected assay.","code":""},{"path":"biology-bioinformatics.html","id":"gene-expression-data-in-bioconductor","chapter":"8 Biology & Bioinformatics","heading":"8.7.1 Gene Expression Data in Bioconductor","text":"SummarizedExperiment\ncontainer\nstandard way load work gene expression data Bioconductor.\ncontainer requires following information:assays - one measurement assays (e.g. gene expression) form feature sample matrixcolData - metadata associated samples (.e. columns) assaysrowData - metadata associated features (.e. rows) assaysexptData - additional metadata experiment , like protocol, project name, etcThe figure illustrates SummarizedExperiment container \nstructured different data elements accessed:SummarizedExperiment Schematic - Huber, et al. 2015. “Orchestrating High-Throughput Genomic Analysis Bioconductor.” Nature Methods 12 (2): 115–21.Many Bioconductor packages specific types data, e.g.\nlimma create \nSummarizedExperiment objects , may also create \nassembling data data frames manually:Detailed documentation create use SummarizedExperiment available SummarizedExperiment vignette.SummarizedExperiment successor older\nExpressionSet\ncontainer. still used Bioconductor packages, \nSummarizedExperiment modern flexible, suggested use\nwhenever possible.","code":"\n# microarray expression dataset intensities\nintensities <- readr::read_delim(\"example_intensity_data.csv\",delim=\" \")\n\n# the first column of intensities tibble is the probesetId, extract to pass as rowData\nrowData <- intensities[\"probeset_id\"]\n\n# remove probeset IDs from tibble and turn into a R dataframe so that we can assign rownames\n# since tibbles don't support row names\nintensities <- as.data.frame(\n  select(intensities, -probeset_id)\n)\nrownames(intensities) <- rowData$probeset_id\n\n# these column data are made up, but you would have a sample metadata file to use\ncolData <- tibble(\n  sample_name=colnames(intensities),\n  condition=sample(c(\"case\",\"control\"),ncol(intensities),replace=TRUE)\n)\n\nse <- SummarizedExperiment(\n   assays=list(intensities=intensities),\n   colData=colData,\n   rowData=rowData\n)\nse## class: SummarizedExperiment\n## dim: 54675 35\n## metadata(0):\n## assays(1): intensities\n## rownames(54675): 1007_s_at 1053_at ... AFFX-TrpnX-5_at AFFX-TrpnX-M_at\n## rowData names(1): probeset_id\n## colnames(35): GSM972389 GSM972390 ... GSM972512 GSM972521\n## colData names(2): sample_name condition"},{"path":"biology-bioinformatics.html","id":"differential-expression-analysis","chapter":"8 Biology & Bioinformatics","heading":"8.7.2 Differential Expression Analysis","text":"Differential expression analysis seeks identify extent gene\nexpression associated one variables interest. example,\nmight interested genes higher expression subjects \ndisease, genes change response treatment.Typically, gene expression analysis requires two types data run: \nexpression matrix design matrix. expression matrix generally\nfeatures (e.g. genes) rows samples columns. design matrix \nnumeric matrix contains variables wish model covariates\nconfounders wish adjust . variables design matrix \nmust encoded way statistical procedures can understand. full\ndetails design matrices constructed beyond scope book.\nFortunately, R makes easy construct matrices tibble \nmodel.matrix()\nfunction.\nConsider following imaginary sample information tibble 10 AD\n10 Control subjects different clinical protein histology\nmeasurements assayed brain tissue:might interested identifying genes increased decreased \npeople AD compared Controls. can create model matrix \nfollows:~ condition argument R\nforumla,\nconcise description variable included model.\ngeneral format formula follows:models, design matrix intercept term ones \nadditional colums variables model. might wish adjust\neffect age including age_at_death covariate model:model matrix now includes column age death well. model\nmodel matrix now suitable pass differential expression software\npackages look genes associated disease status.mentioned , modern gene expression assays measure thousands genes\nsimultaneously. means gene must tested differential expression\nindividually. general, gene tested using statistical model,\ndifferential expression analysis package perform something equivalent\nfollowing:gene model statistics associated must\nprocess interpret analysis complete.","code":"\nad_metadata## # A tibble: 20 x 8\n##    ID    age_at_death condition    tau  abeta   iba1   gfap braak_stage\n##    <chr>        <dbl> <fct>      <dbl>  <dbl>  <dbl>  <dbl> <fct>      \n##  1 A1              81 AD         98543 166078  77299  75277 4          \n##  2 A2              80 AD         32711  56919  49571 117441 1          \n##  3 A3              91 AD        134427 295407 106724  67719 6          \n##  4 A4              90 AD         81515 221801 171212  56346 4          \n##  5 A5              80 AD         42442  97918   8321 109406 2          \n##  6 A6              81 AD         54802  49358  82374  42797 2          \n##  7 A7              77 AD        127166   8417  62675  85377 6          \n##  8 A8              74 AD        103015  28478 131168 133892 5          \n##  9 A9              82 AD        114270 188208 194818  61935 5          \n## 10 A10             83 AD         90785  46279 121555 131853 4          \n## 11 C1              79 Control      991    237   1062  64752 0          \n## 12 C2              76 Control     2804   4423   3441  23384 0          \n## 13 C3              73 Control     3120   7073   1402 135049 0          \n## 14 C4              73 Control     7167    798   8365  18961 0          \n## 15 C5              69 Control     6591   2590   4885  65094 0          \n## 16 C6              77 Control    38697 103664  33430  23136 2          \n## 17 C7              73 Control    58381  72071  16864  17295 3          \n## 18 C8              75 Control     5102  10533   5321  89479 0          \n## 19 C9              76 Control    59906  47967  19103 124651 3          \n## 20 C10             81 Control    46553  49670  90763  52684 2\nmodel.matrix(~ condition, data=ad_metadata)##    (Intercept) conditionAD\n## 1            1           1\n## 2            1           1\n## 3            1           1\n## 4            1           1\n## 5            1           1\n## 6            1           1\n## 7            1           1\n## 8            1           1\n## 9            1           1\n## 10           1           1\n## 11           1           0\n## 12           1           0\n## 13           1           0\n## 14           1           0\n## 15           1           0\n## 16           1           0\n## 17           1           0\n## 18           1           0\n## 19           1           0\n## 20           1           0\n## attr(,\"assign\")\n## [1] 0 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"# portions in [] are optional\n[<outcome variable>] ~ <predictor variable 1> [+ <predictor variable 2>]...\n\n# examples\n\n# model Gene 3 expression as a function of disease status\n`Gene 3` ~ condition\n\n# model the amount of tau pathology as a function of abeta pathology,\n# adjusting for age at death\ntau ~ age_at_death + abeta\n\n# create a model without an outcome variable that can be used to create the\n# model matrix to test for differences with disease status adjusting for age at death\n~ age_at_death + condition\nmodel.matrix(~ age_at_death + condition, data=ad_metadata)##    (Intercept) age_at_death conditionAD\n## 1            1           81           1\n## 2            1           80           1\n## 3            1           91           1\n## 4            1           90           1\n## 5            1           80           1\n## 6            1           81           1\n## 7            1           77           1\n## 8            1           74           1\n## 9            1           82           1\n## 10           1           83           1\n## 11           1           79           0\n## 12           1           76           0\n## 13           1           73           0\n## 14           1           73           0\n## 15           1           69           0\n## 16           1           77           0\n## 17           1           73           0\n## 18           1           75           0\n## 19           1           76           0\n## 20           1           81           0\n## attr(,\"assign\")\n## [1] 0 1 2\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$condition\n## [1] \"contr.treatment\"Gene 1 ~ age_at_death + condition\nGene 2 ~ age_at_death + condition\nGene 3 ~ age_at_death + condition\n...\nGene N ~ age_at_death + condition"},{"path":"biology-bioinformatics.html","id":"microarray-gene-expression-data","chapter":"8 Biology & Bioinformatics","heading":"8.7.3 Microarray Gene Expression Data","text":"high level, four steps involved analyzing gene expression\nmicroarray data:Summarization probes probesets. gene represented \nmultiple probes different sequences. Summarization statistical\nprocedure computes single value probeset corresponding\nprobes.Normalization. includes removing background signal individual\narrays well adjusting probeset intensities comparable\nacross multiple sample arrays.Quality control. Compare normalized samples within sample set \nidentify, mitigate, eliminate potential outlier samples.Analysis. Using quality controlled expression data, Implement\nstatistical analysis answer research questions.full details microarray analysis beyond scope book.\nHowever following sections cover basic entry points \nperforming steps R Bioconductor.CEL data files set microarrays can loaded R analysis\nusing affy Bioconductor\npackage.\npackage provides functions necessary loading data \nperforming key preprocessing operations. Typically, two samples \nprocessed way, resulting set CEL files processed\ntogether. CEL files typically stored directory,\nmay loaded using affy::ReadAffy function:affy_batch variable AffyBatch container, stores information\nprobe definitions based type microarray, probe-level intensity\nsample, information experiment contained within \nCEL files.affy package provides functions perform probe summarization \nnormalization. popular method accomplish time writing\ncalled Robust Multi-array Average RMA (Irizarry et al. 2003), performs\nsummarization normalization multiple arrays simultaneously. , \nRMA algorithm used normalize example dataset provided \naffydata\nBioconductor package.Certain Bioconductor packages provide example datasets use companion\nanalysis packages. example, affydata package provides Dilution\ndataset, generated using two concentrations cDNA human liver\ntissue central nervous system cell line. load data package R,\nfirst run library(<data package>) data(<dataset name>)., first apply RMA Dilution microarray dataset using \naffy::rma() function. plot log 2 intensities probes \narray, first using raw intensities afysis seeking \nidentify associated gene expression one variables \ninterest. example,","code":"\n# read all CEL files in a single directory\naffy_batch <- affy::ReadAffy(celfile.path=\"directory/of/CELfiles\")\n\n# or individual files in different directories\ncel_filenames <- c(\n  list.files( path=\"CELfile_dir1\", full.names=TRUE, pattern=\".CEL$\" ),\n  list.files( path=\"CELfile_dir2\", full.names=TRUE, pattern=\".CEL$\" )\n)\naffy_batch <- affy::ReadAffy(filenames=cel_filenames)\nlibrary(affy)\nlibrary(affydata)##      Package    LibPath                                     Item      \n## [1,] \"affydata\" \"C:/Users/Adam/Documents/R/win-library/4.1\" \"Dilution\"\n##      Title                        \n## [1,] \"AffyBatch instance Dilution\"\ndata(Dilution)\n\n# normalize the Dilution microarray expression values\n# note Dilution is an AffyBatch object\neset_rma <- affy::rma(Dilution,verbose=FALSE)\n\n# plot distribution of non-normalized probes\n# note rma normalization takes the log2 of the expression values,\n# so we must do so on the raw data to compare\nraw_intensity <- as_tibble(exprs(Dilution)) %>%\n  mutate(probeset_id=rownames(exprs(Dilution))) %>%\n  pivot_longer(-probeset_id, names_to=\"Sample\", values_to = \"Intensity\") %>%\n  mutate(\n    `log2 Intensity`=log2(Intensity),\n    Category=\"Before Normalization\"\n  )\n\n# plot distribution of normalized probes\nrma_intensity <- as_tibble(exprs(eset_rma)) %>%\n  mutate(probesetid=featureNames(eset_rma)) %>%\n  pivot_longer(-probesetid, names_to=\"Sample\", values_to = \"log2 Intensity\") %>%\n  mutate(Category=\"RMA Normalized\")\n\ndplyr::bind_rows(raw_intensity, rma_intensity) %>%\n  ggplot(aes(x=Sample, y=`log2 Intensity`)) +\n  geom_boxplot() +\n  facet_wrap(vars(Category))"},{"path":"biology-bioinformatics.html","id":"differential-expression-microarrays-limma","chapter":"8 Biology & Bioinformatics","heading":"8.7.4 Differential Expression: Microarrays (limma)","text":"limma, \nshort linear models microarrays, one top \ndownloaded Bioconductor packages.\nlimma utilized analyzing microarray gene expression data, focus \nanalyses using linear models integrate data experiment.\nlimma developed microarray analysis prior development \nsequencing based gene expression methods (.e. RNASeq) since added\nfunctionality analyze types gene expression data.limma excels analyzing types data can support arbitrarily\ncomplex experimental designs maintaining strong statistical power. \nexperiment large number conditions predictors can still analyze\neven small sample sizes. method accomplishes using \nempirical Bayes approach\nborrows information across genes dataset better control\nerror individual gene models. See Ritchie et al. (2015) details \nlimma works.limma requres expression matrix like stored ExpressionSet \nSummarizedExperiment container design matrix:now conducted differential expression analysis limma. can\nextract results question interest - genes \nassociated AD - using limma::topTable() function:table, none probesets experiment significant FDR\n< 0.05.HomepageBioConductorPublication","code":"\n# intensities is an example gene expression matrix that corresponds to our\n# AD sample metadata of 10 AD and 10 Control individuals\nad_se  <- SummarizedExperiment(\n  assays=list(intensities=intensities),\n  colData=ad_metadata,\n  rowData=rownames(intensities)\n)\n\n# define our design matrix for AD vs control, adjusting for age at death\nad_vs_control_model <- model.matrix(~ age_at_death + condition, data=ad_metadata)\n\n# now run limma\n# first fit all genes with the model\nfit <- limma::lmFit(\n  assays(se)$intensities,\n  ad_vs_control_model\n)\n\n# now better control fit errors with the empirical Bayes method\nfit <- limma::eBayes(fit)\n# the coefficient name conditionAD is the column name in the design matrix\n# adjust=\"BH\" means perform Benjamini-Hochberg multiple testing adjustment\n# on the nominal p-values from each gene test\n# topTable only returns the top 10 results sorted by ascending p-value by default\ntopTable(fit, coef=\"conditionAD\", adjust=\"BH\")##                   logFC  AveExpr         t      P.Value adj.P.Val         B\n## 234942_s_at   1.1231942 8.397425  4.890779 9.613196e-05  0.770605 -2.987573\n## 202423_at    -0.7499088 8.646687 -4.785281 1.221646e-04  0.770605 -3.025525\n## 1557538_at   -0.8057310 4.348859 -4.697520 1.492072e-04  0.770605 -3.057737\n## 1558290_a_at -0.8580750 7.278878 -4.658725 1.630238e-04  0.770605 -3.072162\n## 234596_at     0.4925167 3.038448  4.564888 2.020432e-04  0.770605 -3.107523\n## 225986_x_at  -0.6030385 6.958092 -4.502976 2.328354e-04  0.770605 -3.131216\n## 203124_s_at  -1.2857166 8.565022 -4.488589 2.406451e-04  0.770605 -3.136763\n## 209346_s_at   0.6578475 6.010219  4.421869 2.804637e-04  0.770605 -3.162688\n## 224825_at     1.0266849 8.147104  4.421597 2.806391e-04  0.770605 -3.162795\n## 205647_at    -0.8569475 4.399029 -4.399700 2.951151e-04  0.770605 -3.171376"},{"path":"biology-bioinformatics.html","id":"rnaseq","chapter":"8 Biology & Bioinformatics","heading":"8.7.5 RNASeq","text":"RNA sequencing (RNASeq) HTS technology measures abundance RNA\nmolecules sample. Briefly, RNA molecules extracted sample using\nbiochemical protocol isolates RNA molecules (.e. DNA,\nproteins, etc) sample. Ribosomal RNA removed sample (see Note\nbox ), remaining RNA molecules size selected retain \nshort molecules (~15-300 nucleotides length, depending protocol). \nsize selected molecules reverse transcribed cDNA converted \ndouble stranded molecules. Sequencing libraries prepared cDNA\nusing proprietary biochemical protocols make suitable sequencing \nsequencing instrument. sequencing, FASTQ files containing millions \nreads one samples produced. Except initial RNA\nextraction, steps typically performed specialized\ninstrumentation facilities called sequencing cores provide data \nFASTQ format investigators.80%-90% RNA mass cell ribosomal RNA (rRNA), RNA makes \nribosomes responsible maintaining protein translation processes \ncell. total RNA sequenced, large majority reads \ncorrespond rRNAs therefore little use. reason,\nrRNA molecules removed RNA material goes sequencing\nusing either poly-tail enrichment ribo-depletion strategy. two\nmethods maintain different populations RNA therefore sequencing datasets\ngenerated different strategies require different interpretations. \ndetail topic beyond scope book, (Zhao et al. 2018)\nprovide good introduction comparison approaches.reads produced RNASeq experiment represent RNA fragments \ntranscripts. number reads correspond specific transcripts \nassumed proportional copy number transcript original\nsample. alignment process described Preliminary HTS Data Analysis\nsection produces alignments reads genome transcriptome, \nnumber reads align gene transcript counted using\nreference annotation defines locations every known gene \ngenome. resulting counts therefore estimates copy number \nmolecules gene transcript original sample. Conceptually, \nreads map gene, higher abundance transcripts \ngene, therefore infer higher expression gene.Gene expression different genes can vary orders magnitude within \nsingle cell, highly expressed genes may billions transcripts \nothers comparatively . Therefore, likelihood obtaining read \ngiven transcript proportional relative abundance transcript\ncompared transcripts. Relatively rare transcripts low\nprobability sampled sequencing procedure, extremely lowly\nexpressed genes may sampled . library size (.e. total\nnumber reads) sequencing dataset sample determines range \ngene expression dataset likely detect, datasets reads\nunlikely sample lowly expressed transcripts even though truly\nexpressed. reason absence evidence evidence absence \nRNASeq data. read observed given gene, say \ngene expressed; can say relative expression \ngene detection threshold dataset generated, given \nnumber sequenced reads.single sample, output read counting procedure vector \nread counts gene annotation. Genes reads mapping \ncount zero, others read count one ;\nmentioned Count Data section, read counts typically non-zero\nintegers. procedure typically repeated separately set samples\nmake experiment, counts sample concatenated\ntogether counts matrix. count matrix primary result \ninterest passed downstream analysis, commonly differential\nexpression analysis.","code":""},{"path":"biology-bioinformatics.html","id":"rnaseq-gene-expression-data","chapter":"8 Biology & Bioinformatics","heading":"8.7.6 RNASeq Gene Expression Data","text":"read counts gene transcript genome estimates \nrelative abundance molecules original sample. number genes\ncounted depends organism studied maturity gene\nannotation used. Complex multicellular organisms order \nthousands tens thousands genes; example, humans mice \n20k-30k genes respective genomes. RNASeq experiment thus may yield\ntens thousands measurements sample.shall consider example RNASeq dataset generated (O’Meara et al. 2015)\ngenerated study mouse cardiac cells can regenerate one week\nage, lose ability . dataset eight\nRNASeq samples, two four time points across developmental window\nadulthood. load expression data tibble:annotation counted reads 55,416 annnotated Ensembl gene mouse\ngenome, described Gene Identifier Systems section.typically three steps analyzing RNASeq count matrix:Filter genes unlikely differentially expressed.Normalize filtered counts make samples comparable one another.Differential expression analysis filtered, normalized counts.steps described detail .","code":"\ncounts <- read_tsv(\"verse_counts.tsv\")\ncounts## # A tibble: 55,416 x 9\n##    gene                  P0_1  P0_2  P4_1  P4_2  P7_1  P7_2  Ad_1  Ad_2\n##    <chr>                <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 ENSMUSG00000102693.2     0     0     0     0     0     0     0     0\n##  2 ENSMUSG00000064842.3     0     0     0     0     0     0     0     0\n##  3 ENSMUSG00000051951.6    19    24    17    17    17    12     7     5\n##  4 ENSMUSG00000102851.2     0     0     0     0     1     0     0     0\n##  5 ENSMUSG00000103377.2     1     0     0     1     0     1     1     0\n##  6 ENSMUSG00000104017.2     0     3     0     0     0     1     0     0\n##  7 ENSMUSG00000103025.2     0     0     0     0     0     0     0     0\n##  8 ENSMUSG00000089699.2     0     0     0     0     0     0     0     0\n##  9 ENSMUSG00000103201.2     0     0     0     0     0     0     0     1\n## 10 ENSMUSG00000103147.2     0     0     0     0     0     0     0     0\n## # ... with 55,406 more rows"},{"path":"biology-bioinformatics.html","id":"filtering-counts","chapter":"8 Biology & Bioinformatics","heading":"8.7.7 Filtering Counts","text":"Filtering genes likely differentially expressed \nimportant step differential expression analysis. primary reason \nreduce number genes tested differential expression, thereby\nreducing multiple testing burden results. many approaches\nrationales picking filters, choice generally subjective \nhighly dependent upon specific dataset. section discuss \nrationale considerations different filtering strategies.Genes detected sample easiest filter, often\neliminate many genes consideration. filter genes \nzero counts samples:32,613 genes remain filtering genes zeros, eliminating\nnearly 23,000 genes, 40% genes annotation. Filtering\ngenes zeros samples done somewhat liberal, \nmay many genes reads one sample. Genes single\nsample non-zero count also differentially expressed, can\neasily decide eliminate well:now reduced number genes 28,979 including genes \nleast two non-zero counts. may now wonder many genes include\nrequired least \\(n\\) samples non-zero counts. Consider \nfollowing bar chart number genes detected least \\(n\\) samples:largest number genes detected none samples, followed \ngenes detected samples, many fewer . Based plot,\nobvious choice many nonzero samples sufficient use \nfilter.may gain additional insight problem considering distribution\ncounts genes present least \\(n\\) samples , rather just\nnumber nonzero counts. plot similar , \nplots distribution mean counts across samples within number \nnonzero samples:plot, see mean count genes even single sample\nzero substantially lower genes without zeros. might\ntherefore decide filter genes zeros, rationale\ngenes appreciable abundance still included. However, \ngenes high average count samples zero \neliminated chose use genes expressed 8 samples.\n, clear choice filtering threshold, subjective\ndecision necessary.One important property RNASeq expression data genes higher mean\ncount also higher variance. mean-variance dependence means count\ndata heteroskedastic, term used describe data inconstant\nvariance across different values. can visualize dependence \ncalculating mean variance normalized count genes \nplotting :Unlike microarray data, probesets can filtered based variance,\nusing low variance filter count data result filtering \ngenes low mean, lead undesirable bias toward highly\nexpressed genes.Depending experiment, may meaningful gene detected\nsamples. cases, genes 1 read single\nsample might kept, even though genes result confident\nstatistical inference subjected differential expression. scientific\nquestion asked experiment important consideration deciding\nthresholds.goal RNASeq experiment differential expression analysis, \ngood rule thumb choosing filtering threshold choose genes \nsufficiently many non-zero counts statistical method \nenough variance perform reliable inference. example, reasonable\nfiltering threshold might include genes non-zero counts \nleast 50% samples. threshold depends upon number samples\nconsidered. experimental design two sample groups, \nreasonable strategy might require least 50% samples within least\none group within groups included. Taking experimental design\naccount filtering can ensure interesting genes, e.g. \nexpressed one condition, captured.Regardless filtering strategy used, biological meaning \nfiltering threshold. number reads detected gene \ndependent primarily upon library size, larger libraries \nlikely sample lowly abundant genes. Therefore, filter genes \nintent “removing lowly expressed genes”; instead can filter \ngenes detection threshold afforded library size \ndataset.","code":"\nnonzero_genes <- rowSums(counts[-1])!=0\nfiltered_counts <- counts[nonzero_genes,]\nfiltered_counts## # A tibble: 32,613 x 9\n##    gene                   P0_1  P0_2  P4_1  P4_2  P7_1  P7_2  Ad_1  Ad_2\n##    <chr>                 <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 ENSMUSG00000051951.6     19    24    17    17    17    12     7     5\n##  2 ENSMUSG00000102851.2      0     0     0     0     1     0     0     0\n##  3 ENSMUSG00000103377.2      1     0     0     1     0     1     1     0\n##  4 ENSMUSG00000104017.2      0     3     0     0     0     1     0     0\n##  5 ENSMUSG00000103201.2      0     0     0     0     0     0     0     1\n##  6 ENSMUSG00000103161.2      0     0     1     0     0     0     0     0\n##  7 ENSMUSG00000102331.2      5     8     2     6     6    11     4     3\n##  8 ENSMUSG00000102343.2      0     0     0     1     0     0     0     0\n##  9 ENSMUSG00000025900.14     4     3     6     7     7     3    36    35\n## 10 ENSMUSG00000102948.2      1     0     0     0     0     0     0     1\n## # ... with 32,603 more rows\nnonzero_genes <- rowSums(counts[-1])>=2\nfiltered_counts <- counts[nonzero_genes,]\nfiltered_counts## # A tibble: 28,979 x 9\n##    gene                   P0_1  P0_2  P4_1  P4_2  P7_1  P7_2  Ad_1  Ad_2\n##    <chr>                 <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 ENSMUSG00000051951.6     19    24    17    17    17    12     7     5\n##  2 ENSMUSG00000103377.2      1     0     0     1     0     1     1     0\n##  3 ENSMUSG00000104017.2      0     3     0     0     0     1     0     0\n##  4 ENSMUSG00000102331.2      5     8     2     6     6    11     4     3\n##  5 ENSMUSG00000025900.14     4     3     6     7     7     3    36    35\n##  6 ENSMUSG00000102948.2      1     0     0     0     0     0     0     1\n##  7 ENSMUSG00000025902.14   195   186   204   229   157   173   196   129\n##  8 ENSMUSG00000104238.2      0     4     0     2     4     1     3     3\n##  9 ENSMUSG00000102269.2      7    16     6     6     4     6     8     3\n## 10 ENSMUSG00000118917.1      0     1     0     0     1     1     0     0\n## # ... with 28,969 more rows\nlibrary(patchwork)\n\nnonzero_counts <- mutate(counts,`Number of samples`=rowSums(counts[-1]!=0)) %>%\n  group_by(`Number of samples`) %>%\n  summarize(`Number of genes`=n() ) %>%\n  mutate(`Cumulative number of genes`=sum(`Number of genes`)-lag(cumsum(`Number of genes`),1,default=0))\n\nsum_g <- ggplot(nonzero_counts) +\n  geom_bar(aes(x=`Number of samples`,y=`Cumulative number of genes`,fill=\"In at least n\"),stat=\"identity\") +\n  geom_bar(aes(x=`Number of samples`,y=`Number of genes`,fill=\"In exactly n\"),stat=\"identity\") +\n  labs(title=\"Number of genes detected in n samples\") +\n  scale_fill_discrete(name=\"Nonzero in:\")\n\nsum_g\n# calculate the mean count for non-zero counts in genes with exactly n non-zero counts\nmutate(counts,`Number of nonzero samples`=factor(rowSums(counts[-1]!=0))) %>%\n  pivot_longer(-c(gene,`Number of nonzero samples`),names_to=\"Sample\",values_to=\"Count\") %>%\n  group_by(`Number of nonzero samples`) %>%\n  group_by(gene, .add=TRUE) %>% summarize(`Nonzero Count Mean`=mean(Count[Count!=0]),.groups=\"keep\") %>%\n  ungroup() %>%\n  ggplot(aes(x=`Number of nonzero samples`,y=`Nonzero Count Mean`,fill=`Number of nonzero samples`)) +\n  geom_violin(scale=\"width\") +\n  scale_y_log10() +\n  labs(title=\"Nonzero count mean for genes with exactly n nonzero counts\")"},{"path":"biology-bioinformatics.html","id":"count-distributions","chapter":"8 Biology & Bioinformatics","heading":"8.7.8 Count Distributions","text":"range gene expression cell varies orders magnitude. \nfollowing histogram plots distribution read count values one \nadult samples:add 1 count, called pseudocount, avoid taking log zero, therefore\ngenes \\(x=0\\) genes reads. plot, largest\ncount nearly \\(10^6\\) smallest 1, spanning nearly six orders\nmagnitude. relatively highly abundant genes, however,\ngenes betwen range 100-10,000 reads. see \ndistribution similar samples, displayed ridgeline\nplot:general shape distributions similar, large mode counts\nzero, representing genes zero counts sample, wider\nmode 100 10,000 consistently expressed genes fall.\nHowever, note peak count larger modes vary also recall\nthough difference appears small, log scale x axis means\ndifferences counts may substantial. discuss implications\nnext section.","code":"\ndplyr::select(filtered_counts, Ad_1) %>%\n  mutate(`log10(counts+1)`=log10(Ad_1+1)) %>%\n  ggplot(aes(x=`log10(counts+1)`)) +\n  geom_histogram(bins=50) +\n  labs(title='log10(counts) distribution for Adult mouse')\nlibrary(ggridges)\n\npivot_longer(filtered_counts,-c(gene),names_to=\"Sample\",values_to=\"Count\") %>%\n  mutate(`log10(Count+1)`=log10(Count+1)) %>%\n  ggplot(aes(y=Sample,x=`log10(Count+1)`,fill=Sample)) +\n  geom_density_ridges(bandwidth=0.132) +\n  labs(title=\"log10(Count+1) Distributions\")"},{"path":"biology-bioinformatics.html","id":"count-normalization","chapter":"8 Biology & Bioinformatics","heading":"8.7.8.1 Count Normalization","text":"number reads generated sample RNASeq dataset varies \nsample sample due randomness biochemical process generates\ndata. relationship number reads maps given\ngene relative abundance molecule sample dependent upon\nlibrary size sample. direct number counts, raw\ncounts, therefore directly comparable samples. simple example\nsuffice make clear:Imagine two RNASeq datasets generated RNA sample. One \nsamples sequenced 1000 reads, 2000 reads. Now imagine\none gene makes half RNA original sample. sample\n1000 reads, find gene 500 reads. sample 2000 reads,\ngene 1000 reads. cases fraction reads map\ngene 0.5, absolute number reads differs greatly. \nraw counts gene directly comparable samples \ndifferent numbers reads. Since general every sample unique\nnumber reads, raw counts never directly comparable. way \nmitigate problem statistical procedure called count\nnormalization.Count normalization process number raw counts \nsample scaled factor make multiple samples comparable. Many\nstrategies proposed , simplest family \nmethods divide factor library size. common method \nfamily compute counts per million reads CPM normalization, \nscales gene count number millions reads library:\\[\ncpm_{s,} =  \\frac{c_{s,}}{l_s} * 10^6\n\\], \\(c_{s,}\\) raw count gene \\(\\) sample \\(s\\), \\(l_s\\) \nnumber reads dataset sample \\(s\\). sample scaled \nlibrary size, principle proportion read counts assigned \ngene reads sample comparable across samples.\nadditional knowledge distribution genes system,\nlibrary size normalization methods general make fewest\nassumptions. ridgeline plot counts distribution CPM\nnormalization:distributions look quite different CPM normalization, modes\nnow look consistent raw counts.One drawback library size normalization sensitive extreme values;\nindividual genes many reads one sample samples. \noutliers can lead pathological effects comparing gene count values\nacross samples. However, measuring gene expression single\nbiological system like mouse human, often case, methods \ndeveloped better use understanding gene expression\ndistribution perform robust biologically meaningful normalization.\nrobust approach possible make reasonable assumption ,\nset biological conditions experiment, genes \ndifferentially expressed. assumption made DESeq2\nnormalization method.DESeq2 normalization method statistical procedure uses median\ngeometric mean computed across samples determine scale factor \nsample. procedure somewhat complicated encourage reader\nexamine detail DESeq2 publication (Love, Huber, Anders 2014). method \nproven consistent reliable compared strategies\n(Dillies et al. 2013) default normalization populare DESeq2\ndifferential expression method, described later section. \nDESeq2 normalization method may implemented using DESeq2 Bioconductor\npackage:Using DESeq2 package, first construct DESeq object using raw counts\nmatrix, sample information tibble sample name column, \ntrivial design formula, explained depth \nDESeq2/edgeR section. estimateSizeFactors() function performs \nnormalization routine counts, counts(dds,normalized=TRUE) call\nextracts normalized counts matrix. plotted ridgeline plot \n, see gene expression distribution modes better behaved\ncompared raw counts, shape distribution preserved unlike\nCPM normalized counts:DESeq2 normalization procedure two important considerations keep \nmind:procedure borrows information across samples. geometric mean\ncounts across samples computed part normalization procedure.\nmeans size factors computed depend samples \ndataset. normalized counts one sample likely change \nnormalized different set samples.procedure use genes zero counts. geometric mean\ncalculation across samples means sample zero count results\nentire geometric mean zero. reason genes \nnonzero counts every sample used calculate normalization factors.\none samples large number zeros, example due small\nlibrary size, dramatically influence normalization factors \nentire experiment.CPM normalization procedure borrow information across samples,\ntherefore subject considerations.two many possible normalization methods, DESeq2 \npopular time writing. detailed explanation many normalization\nmethods, see (Dillies et al. 2013).","code":"\nlibrary(DESeq2)\n# DESeq2 requires a counts matrix, column data (sample information), and a formula\n# the counts matrix *must be raw counts*\ncount_mat <- as.matrix(filtered_counts[-1])\n\nrow.names(count_mat) <- filtered_counts$gene\n\ndds <- DESeqDataSetFromMatrix(\n  countData=count_mat,\n  colData=tibble(sample_name=colnames(filtered_counts[-1])),\n  design=~1 # no formula is needed for normalization, ~1 produces a trivial design matrix\n)\n\n# compute normalization factors\ndds <- estimateSizeFactors(dds)\n\n# extract the normalized counts\ndeseq_norm_counts <- as_tibble(counts(dds,normalized=TRUE)) %>%\n  mutate(gene=filtered_counts$gene) %>%\n  relocate(gene)\npivot_longer(deseq_norm_counts,-c(gene),names_to=\"Sample\",values_to=\"Count\") %>%\n  mutate(`log10(Count+1)`=log10(Count+1)) %>%\n  ggplot(aes(y=Sample,x=`log10(Count+1)`,fill=Sample)) +\n  geom_density_ridges(bandwidth=0.132) +\n  labs(title=\"log10(Count+1) Distributions\")"},{"path":"biology-bioinformatics.html","id":"count-transformation","chapter":"8 Biology & Bioinformatics","heading":"8.7.8.2 Count Transformation","text":"mentioned Count Data section, one way deal non-normality\ncount data perform data transformation makes counts data\nfollow normal distribution. Transformation normality allows common \npowerful statistical methods like linear regression used. basic\ncount transformation take logarithm counts. However, can\nproblematic genes low counts, causing values spread \napart genes high counts, increasing likelihood false positive\nresults. address issue, DESeq2 package provides rlog()\nfunction performs regularized logarithmic transformation adjusts \nlog transform based mean count gene.rlog transformation also desirable effect making count data\nhomoskedastic, meaning variance gene dependent mean. \nstatistical methods assume dataset property. relationship \nmean count vs variance discussed detail next section.can clearlysee effect rlog counts distribution plotting \nrlog’ed counts using ridgeline plot :generally accepted count transformations significant drawbacks\ncompared statistical methods model counts errors explicitly, like\ngeneralized linear models (GLM) Poisson Negative Binomial\nregressions (St-Pierre, Shikon, Schneider 2018). Whenever possible, GLM based methods\nused instead transformations. However, transformations may\nsometimes necessary depending required analysis, rlog function\nDESeq2 may useful cases.DESeq2 package vignette Count data transformations","code":"\n# the dds is the DESeq2 object from earlier\nrld <- rlog(dds)\n\n# extract rlog values as a tibble\nrlog_counts <- as_tibble(assay(rld))\nrlog_counts$gene <- filtered_counts$gene\npivot_longer(rlog_counts,-c(gene),names_to=\"Sample\",values_to=\"rlog count\") %>%\n  ggplot(aes(y=Sample,x=`rlog count`,fill=Sample)) +\n  geom_density_ridges(bandwidth=0.132) +\n  labs(title=\"rlog Distributions\")"},{"path":"biology-bioinformatics.html","id":"differential-expression-rnaseq","chapter":"8 Biology & Bioinformatics","heading":"8.7.9 Differential Expression: RNASeq","text":"read counts gene transcript form primary data used \nidentify genes whose expression correlates condition interest. \ncounts matrix created concatenating read counts genes across set \nsamples rows genes transcripts columns samples \ncommon form expression data used purpose. mentioned \nCount Data section, counts normally distributed therefore\nrequire statistical approach can accommodate counts distribution.strong relationship mean variance characteristic \nRNASeq expression data, motivates use generalized linear model\ncalled negative binomial regression. Negative binomial regression models count\ndata using negative binomial distribution,\nallows relationship mean variance counts distribution \nvary. statistical details negative binomial distribution negative\nbinomial regression beyond scope book, discuss\naspects necessary implement differential expression analysis.","code":""},{"path":"biology-bioinformatics.html","id":"deseq2edger","chapter":"8 Biology & Bioinformatics","heading":"8.7.9.1 DESeq2/EdgeR","text":"DESeq2 \nedgeR bioconductor packages implement negative binomial\nregression perform differential expression RNASeq data. perform\nraw counts normalization part function, though differ \nnormalization method (DESeq2 method described , edgeR uses\ntrimmed mean M-values (TMM), see (Robinson Oshlack 2010)). interface using\npackages also similar, focus DESeq2 section.mentioned briefly , DESeq2 requires three pieces information \nperform differential expression:raw counts matrix genes rows samples columnsA data frame metadata associated columnsA design formula differential expression modelDESeq2 requires raw counts due normalization procedure, borrows\ninformation across samples computing size factors, described \nCount Normalization section.three pieces information, construct DESeq object:design formula ~ timepoint similar supplied \nmodel.matrix() function described Differential Expression Analysis\nsection, DESeq2 implicitly creates model matrix combining \ncolumn data formula. differential expression analysis, wish\nidentify genes different timepoints P0, P4, P7,\nAd. three pieces information prepared, can create DESeq\nobject perform differential expression analysis:DESeq2 performed differential expression gene estimated parameters\nthree different comparisons - P0 vs Ad, P4 vs Ad, P7 vs Ad. , Ad \nreference group may interpret statistics relative \nadult animals. choice reference group change magnitude \nresults, may change sign estimated coefficients, \nlog2 fold changes. extract differential expression statistics \nP0 vs Ad comparison, can use results() function:results differential expression comparing P0 Ad; \ncontrasts (P4 vs Ad P7 vs Ad) may extracted specifying \nappropriate result names name argument results() function. columns DESeq2 results defined follows:baseMean - mean normalized count samples genelog2FoldChange - estimated coefficient (.e. log2FoldChange) comparison interestlfcSE - standard error log2FoldChange estimatestat - Wald statistic associated log2FoldChange estimatepvalue - nominal p-value Wald test (.e. signifiance association)padj - multiple testing adjusted p-value (.e. false discovery rate)P0 vs Ad comparison, can check number significant results easily:7,557 significantly differentially expressed genes associated \ncomparison, includes - -regulated genes (examine \nsigns log2FoldChange column). common diagnostic plot differenial\nexpression results -called volcano plot plots \nlog2FoldChange x-axis -log10 adjusted p-value y-axis:plot shows significant genes FDR < 0.05 increased\ndecreased comparison.DESeq2 many capabilities beyond basic differential expression\nfunctionality described comprehensive documentation vignette.mentioned, edgeR implements similar model DESeq2 also \nexcellent\ndocumentation.","code":"\nlibrary(DESeq2)\n\n# filter counts to retain genes with at least 6 out of 8 samples with nonzero counts\nfiltered_counts <- counts[rowSums(counts[-1]!=0)>=6,]\n\n# DESeq2 requires a counts matrix, column data (sample information), and a formula\n# the counts matrix *must be raw counts*\ncount_mat <- as.matrix(filtered_counts[-1])\nrow.names(count_mat) <- filtered_counts$gene\n\n# create a sample matrix from sample names\nsample_info <- tibble(\n  sample_name=colnames(filtered_counts[-1])\n) %>%\n  separate(sample_name,c(\"timepoint\",\"replicate\"),sep=\"_\",remove=FALSE) %>%\n  mutate(\n    timepoint=factor(timepoint,levels=c(\"Ad\",\"P0\",\"P4\",\"P7\"))\n  )\n\ndesign <- formula(~ timepoint)\n\nsample_info## # A tibble: 8 x 3\n##   sample_name timepoint replicate\n##   <chr>       <fct>     <chr>    \n## 1 P0_1        P0        1        \n## 2 P0_2        P0        2        \n## 3 P4_1        P4        1        \n## 4 P4_2        P4        2        \n## 5 P7_1        P7        1        \n## 6 P7_2        P7        2        \n## 7 Ad_1        Ad        1        \n## 8 Ad_2        Ad        2\ndds <- DESeqDataSetFromMatrix(\n  countData=count_mat,\n  colData=sample_info,\n  design=design\n)\ndds <- DESeq(dds)\nresultsNames(dds)## [1] \"Intercept\"          \"timepoint_P0_vs_Ad\" \"timepoint_P4_vs_Ad\"\n## [4] \"timepoint_P7_vs_Ad\"\nres <- results(dds, name=\"timepoint_P0_vs_Ad\")\np0_vs_Ad_de <- as_tibble(res) %>%\n  mutate(gene=rownames(res)) %>%\n  relocate(gene) %>%\n  arrange(pvalue)\np0_vs_Ad_de## # A tibble: 21,213 x 7\n##    gene                  baseMean log2FoldChange lfcSE  stat    pvalue      padj\n##    <chr>                    <dbl>          <dbl> <dbl> <dbl>     <dbl>     <dbl>\n##  1 ENSMUSG00000000031.17   19624.           6.75 0.190  35.6 5.70e-278 1.07e-273\n##  2 ENSMUSG00000026418.17    9671.           9.84 0.285  34.5 2.29e-261 2.15e-257\n##  3 ENSMUSG00000051855.16    3462.           5.35 0.157  34.1 2.84e-255 1.77e-251\n##  4 ENSMUSG00000027750.17    9482.           4.05 0.125  32.4 2.26e-230 1.06e-226\n##  5 ENSMUSG00000042828.13    3906.          -5.32 0.167 -31.8 1.67e-221 6.25e-218\n##  6 ENSMUSG00000046598.16    1154.          -6.08 0.197 -30.8 4.77e-208 1.49e-204\n##  7 ENSMUSG00000019817.19    1952.           5.15 0.171  30.1 1.43e-198 3.83e-195\n##  8 ENSMUSG00000021622.4    17146.          -4.57 0.157 -29.0 4.41e-185 1.03e-181\n##  9 ENSMUSG00000002500.16    2121.          -6.20 0.217 -28.5 6.69e-179 1.39e-175\n## 10 ENSMUSG00000024598.10    1885.           5.90 0.209  28.2 5.56e-175 1.04e-171\n## # ... with 21,203 more rows\n# number of genes significant at FDR < 0.05\nfilter(p0_vs_Ad_de,padj<0.05)## # A tibble: 7,557 x 7\n##    gene                  baseMean log2FoldChange lfcSE  stat    pvalue      padj\n##    <chr>                    <dbl>          <dbl> <dbl> <dbl>     <dbl>     <dbl>\n##  1 ENSMUSG00000000031.17   19624.           6.75 0.190  35.6 5.70e-278 1.07e-273\n##  2 ENSMUSG00000026418.17    9671.           9.84 0.285  34.5 2.29e-261 2.15e-257\n##  3 ENSMUSG00000051855.16    3462.           5.35 0.157  34.1 2.84e-255 1.77e-251\n##  4 ENSMUSG00000027750.17    9482.           4.05 0.125  32.4 2.26e-230 1.06e-226\n##  5 ENSMUSG00000042828.13    3906.          -5.32 0.167 -31.8 1.67e-221 6.25e-218\n##  6 ENSMUSG00000046598.16    1154.          -6.08 0.197 -30.8 4.77e-208 1.49e-204\n##  7 ENSMUSG00000019817.19    1952.           5.15 0.171  30.1 1.43e-198 3.83e-195\n##  8 ENSMUSG00000021622.4    17146.          -4.57 0.157 -29.0 4.41e-185 1.03e-181\n##  9 ENSMUSG00000002500.16    2121.          -6.20 0.217 -28.5 6.69e-179 1.39e-175\n## 10 ENSMUSG00000024598.10    1885.           5.90 0.209  28.2 5.56e-175 1.04e-171\n## # ... with 7,547 more rows\nmutate(\n  p0_vs_Ad_de,\n  `-log10(adjusted p)`=-log10(padj),\n  `FDR<0.05`=padj<0.05\n  ) %>%\n  ggplot(aes(x=log2FoldChange,y=`-log10(adjusted p)`,color=`FDR<0.05`)) +\n  geom_point()"},{"path":"biology-bioinformatics.html","id":"limmavoom","chapter":"8 Biology & Bioinformatics","heading":"8.7.9.2 limma/voom","text":"limma package initially developed microarray analysis, \nauthors added support RNASeq soon HTS technique became available.\nlimma accommodates count data first performing Count Transformation using\n\nvoom\ntransformation procedure. voom transforms counts first performing counts\nper million normalization, taking logarithm CPM values, finally\nestimates mean-variance relationship across genes use empirical\nBayes statistical framework. counts data thus transformed, limma\nperforms statistical inference using usual linear model framework produce\ndifferential expression results arbitrarily complex statistical models.brief example limma User\nGuide\nchapter 15, covers loading\nprocessing data RNA-seq experiment. go depth \nworking limma assignment 6.Without going much detail, design variable inform limma\nexperimental conditions, limma draws construct \nlinear models correctly. design relatively simple, just four samples\nbelonging two different conditions (swirls refer swirl \nzebra fish, can just see phenotypic difference)DGEList() function edgeR, limma borrows loading \nfiltering functions. experiment filters expression level, uses\nsquare bracket notation ([]) reduce number rows.Finally, expression data transformed CPM, counts per million, \nlinear model applied data lmFit(). topTable() used view \ndifferentially expressed data.","code":"\ndesign <- data.frame(swirl = c(\"swirl.1\", \"swirl.2\", \"swirl.3\", \"swirl.4\"),\n                     condition = c(1, -1, 1, -1))\ndge <- DGEList(counts=counts)\nkeep <- filterByExpr(dge, design)\ndge <- dge[keep,,keep.lib.sizes=FALSE]\n# limma trend\nlogCPM <- cpm(dge, log=TRUE, prior.count=3)\nfit <- lmFit(logCPM, design)\nfit <- eBayes(fit, trend=TRUE)\ntopTable(fit, coef=ncol(design))"},{"path":"biology-bioinformatics.html","id":"gene-set-enrichment-analysis","chapter":"8 Biology & Bioinformatics","heading":"8.8 Gene Set Enrichment Analysis","text":"constant evolution high-throughput sequencing (HTS) technologies,\nsize dimensionality data generated ever increasing. \nquestion interest shifted generate data \nmake meaningful biological interpretations genome-wide level. simplest\ncommon output HTS experiments list “interesting” genes. \nspecific case differential gene expression analysis, possible \nquite common obtain hundreds even thousands differentially expressed\ngenes single experiment may directly indirectly related \nphenotype interest.can helpful fruitful research genes individually, \nform personal inspection limited one’s domain knowledge size\nresults. biological level, complicated fact \nprocesses often coordinated actions many genes/gene products\nworking concert. Cellular signaling pathways, phenotypic differences \nresponses various stimuli typically associated changes \nexpression pattern many genes share common biological functions \nregulation.Gene set enrichment analysis umbrella term methods designed analyze\nexpression data capture changes higher-level biological processes \npathways organizing genes biologically relevant groups gene sets. \ndiscuss background two common gene set enrichment analyses\n(-representation analysis rank-based Gene Set Enrichment Analysis),\nadvantages disadvantages, walk-example can\nimplemented R. , briefly touch upon definition\ngene set describe construct obtain gene sets.","code":""},{"path":"biology-bioinformatics.html","id":"gene-sets","chapter":"8 Biology & Bioinformatics","heading":"8.8.1 Gene Sets","text":"Gene sets curated lists genes grouped together logical\nassociation pre-existing domain knowledge. primary use facilitate\nbiological interpretation expression data capturing high-level\ninteractions biologically relevant groups genes. sets \nhighly flexible may constructed based priori knowledge \nclassification. example, one define gene set includes genes\nmembers biochemical pathway gene set consists \ngenes induced upon treatment particular pharmacological\nagent. perfectly valid construct new gene sets, many\nexisting curated collections gene sets maintained \ncontributed communities scientists throughout world. , \nhighlight major collections commonly used cited \npublished work:KEGG PathwaysThe Kyoto Encyclopedia Genes Genomes (KEGG) repository \nbiological pathway information authors describe means \n“computerize functional interpretations part pathway reconstruction\nprocess based hierarchically structured knowledge genomic,\nchemical, network spaces”. KEGG Pathways database consists maps\ndisplaying functional regulatory relationships genes within various\nmetabolic cell signaling pathways. KEGG offers web service allows\nextraction genes information belonging specific\npathways.GO AnnotationsThe Gene Ontology provides network representation biological systems \nmolecular-level organismal level defining graph based\nrepresentation connected terms using controlled vocabulary. high\nlevel, GO annotations consist three broad ontologies termed molecular\nfunction, cellular component, biological process. molecular function \nspecific activity gene product performs molecular level (.e. \nprotein kinase annotated protein kinase activity). \nbiological process defines higher-level programs pathways ’s\naccomplished activities gene product (.e. take previous\nexample protein kinase, ’s biological process might assigned \nsignal transduction). cellular component refers localization \ngene product within cell (.e. cellular component protein kinase\nmight cytosol). gene product can annotated zero terms\nontology, annotations based multiple levels \nevidence published work. GO annotations may accessed downloaded\ndirectly website, exist number web services \nuse GO annotations background (DAVID,\nenrichR, etc.)Molecular Signatures DatabaseThe Molecular Signatures Database (MSigDB) collection gene sets curated,\nmaintained provided Broad Institute UC San Diego. Although\nintended designed specific use Gene Set Enrichment Analysis\nMethodology, freely available require proper attribution \nuses. MSigDB consists 9 major collections gene sets: H (hallmark\ngene sets), C1 (positional gene sets), C2 (curated gene sets), c3 (regulatory\ntarget gene sets), c4 (computational gene sets), c5 (ontology gene sets), c6\n(oncogenic signature gene sets), c7 (immunologic signature gene sets), c8 (cell\ntype signature gene sets). available formats ready use \nGSEA methodology formats easily imported various\nsettings custom use. gene sets well GSEA methodology \navailable directly website. number R-specific packages\ndeveloped working directly gene sets \nGSEABase \nfgsea, \ndiscuss later.","code":""},{"path":"biology-bioinformatics.html","id":"working-with-gene-sets-in-r","chapter":"8 Biology & Bioinformatics","heading":"8.8.2 Working with gene sets in R","text":"walk two quick examples read example collection\ngene sets. utilize Hallmarks gene set collection, downloaded\ndirectly MSigDB website, consists 50 gene sets representing\nwell-defined biological processes generated aggregating together many\npre-existing gene sets. MSigDB provides gene set collections \nGMT format. files tab-delimited row format represents\none gene set first column name gene set, \nsecond column short description. remaining columns represent gene\nunequal lengths columns per row allowed. , GMT format \ndesigned work specifically GSEA methodology developed provided\nBroad Institute. However, also show two ways manually parse\ngene collections exploration use R.first way parse gene sets use various tidyverse\nfunctions familiar already construct tibble.\nEssentially, read file, rename first two columns convenience, \nuse combination pivot_longer() group_by() quickly access genes\npathway. , results operations used \ndisplay summary number genes pathways contained \nhallmark pathways gene collection.can see 50 total pathways hallmarks gene collection\ncontain varying numbers genes. discarded description column \nsave results using pivot_longer() instead piping\ngroup_by summarise(), tibble “long”\nformat row representing single pathway single gene.However, exist various packages developed \nspecifically handle gene set data R previously mentioned\nGSEABase. collection functions class-based objects \nfacilitate working gene sets. foundation GSEABase package \nGeneSet GeneSetCollection classes store gene sets metadata \ncollection GeneSet objects, respectively. use GSEABase package \nread collection gene sets previously downloaded.simply access hallmarks_gmt variable, can see \nGeneSetCollection object containing 0 total gene\nsets encompass 4383 unique identifiers HGNC symbols. Although \npackage supports range functions, focus basics. \nthorough description classes methods, please read extended\ndocumentation available\n.geneIds method return list pathway named vector associated gene ids:names method return gene set names contained within specific collection:can access specific gene set contained within collection referring\nname using following notation:Simply accessing object provide high-level summary information\ncontained within. access specific value GeneSet object, \ncall one slots (core concept object-oriented programming). \nparticular case, extract gene names contained assigned \nGeneSet calling geneIds slot shown return vector \ngene names. can see first five also length returned\nvector using base R length() function:GSEABase includes number built-functions reading gene sets \nvarious sources performing common operations set intersections, set\ndifferences, ID conversions. demonstrate usage \nnext section covering -representation analysis.","code":"\nread_delim('h.all.v7.5.1.symbols.gmt', delim='\\t', col_names=FALSE) %>%\n  dplyr::rename(Pathway = X1, Desc=X2) %>%\n  dplyr::select(-Desc) %>%\n  pivot_longer(!Pathway, values_to='genes', values_drop_na=TRUE, names_to = NULL) %>%\n  group_by(Pathway) %>%\n  summarise(n=n())## # A tibble: 50 x 2\n##    Pathway                              n\n##    <chr>                            <int>\n##  1 HALLMARK_ADIPOGENESIS              200\n##  2 HALLMARK_ALLOGRAFT_REJECTION       200\n##  3 HALLMARK_ANDROGEN_RESPONSE         100\n##  4 HALLMARK_ANGIOGENESIS               36\n##  5 HALLMARK_APICAL_JUNCTION           200\n##  6 HALLMARK_APICAL_SURFACE             44\n##  7 HALLMARK_APOPTOSIS                 161\n##  8 HALLMARK_BILE_ACID_METABOLISM      112\n##  9 HALLMARK_CHOLESTEROL_HOMEOSTASIS    74\n## 10 HALLMARK_COAGULATION               138\n## # ... with 40 more rows\nlibrary('GSEABase')\nhallmarks_gmt <- getGmt(con='h.all.v7.5.1.symbols.gmt')\nhallmarks_gmt## GeneSetCollection\n##   names: HALLMARK_TNFA_SIGNALING_VIA_NFKB, HALLMARK_HYPOXIA, ..., HALLMARK_PANCREAS_BETA_CELLS (50 total)\n##   unique identifiers: JUNB, CXCL2, ..., SRP14 (4383 total)\n##   types in collection:\n##     geneIdType: NullIdentifier (1 total)\n##     collectionType: NullCollection (1 total)\nhead(geneIds(hallmarks_gmt), 2)## $HALLMARK_TNFA_SIGNALING_VIA_NFKB\n##   [1] \"JUNB\"     \"CXCL2\"    \"ATF3\"     \"NFKBIA\"   \"TNFAIP3\"  \"PTGS2\"   \n##   [7] \"CXCL1\"    \"IER3\"     \"CD83\"     \"CCL20\"    \"CXCL3\"    \"MAFF\"    \n##  [13] \"NFKB2\"    \"TNFAIP2\"  \"HBEGF\"    \"KLF6\"     \"BIRC3\"    \"PLAUR\"   \n##  [19] \"ZFP36\"    \"ICAM1\"    \"JUN\"      \"EGR3\"     \"IL1B\"     \"BCL2A1\"  \n##  [25] \"PPP1R15A\" \"ZC3H12A\"  \"SOD2\"     \"NR4A2\"    \"IL1A\"     \"RELB\"    \n##  [31] \"TRAF1\"    \"BTG2\"     \"DUSP1\"    \"MAP3K8\"   \"ETS2\"     \"F3\"      \n##  [37] \"SDC4\"     \"EGR1\"     \"IL6\"      \"TNF\"      \"KDM6B\"    \"NFKB1\"   \n##  [43] \"LIF\"      \"PTX3\"     \"FOSL1\"    \"NR4A1\"    \"JAG1\"     \"CCL4\"    \n##  [49] \"GCH1\"     \"CCL2\"     \"RCAN1\"    \"DUSP2\"    \"EHD1\"     \"IER2\"    \n##  [55] \"REL\"      \"CFLAR\"    \"RIPK2\"    \"NFKBIE\"   \"NR4A3\"    \"PHLDA1\"  \n##  [61] \"IER5\"     \"TNFSF9\"   \"GEM\"      \"GADD45A\"  \"CXCL10\"   \"PLK2\"    \n##  [67] \"BHLHE40\"  \"EGR2\"     \"SOCS3\"    \"SLC2A6\"   \"PTGER4\"   \"DUSP5\"   \n##  [73] \"SERPINB2\" \"NFIL3\"    \"SERPINE1\" \"TRIB1\"    \"TIPARP\"   \"RELA\"    \n##  [79] \"BIRC2\"    \"CXCL6\"    \"LITAF\"    \"TNFAIP6\"  \"CD44\"     \"INHBA\"   \n##  [85] \"PLAU\"     \"MYC\"      \"TNFRSF9\"  \"SGK1\"     \"TNIP1\"    \"NAMPT\"   \n##  [91] \"FOSL2\"    \"PNRC1\"    \"ID2\"      \"CD69\"     \"IL7R\"     \"EFNA1\"   \n##  [97] \"PHLDA2\"   \"PFKFB3\"   \"CCL5\"     \"YRDC\"     \"IFNGR2\"   \"SQSTM1\"  \n## [103] \"BTG3\"     \"GADD45B\"  \"KYNU\"     \"G0S2\"     \"BTG1\"     \"MCL1\"    \n## [109] \"VEGFA\"    \"MAP2K3\"   \"CDKN1A\"   \"CCN1\"     \"TANK\"     \"IFIT2\"   \n## [115] \"IL18\"     \"TUBB2A\"   \"IRF1\"     \"FOS\"      \"OLR1\"     \"RHOB\"    \n## [121] \"AREG\"     \"NINJ1\"    \"ZBTB10\"   \"PLPP3\"    \"KLF4\"     \"CXCL11\"  \n## [127] \"SAT1\"     \"CSF1\"     \"GPR183\"   \"PMEPA1\"   \"PTPRE\"    \"TLR2\"    \n## [133] \"ACKR3\"    \"KLF10\"    \"MARCKS\"   \"LAMB3\"    \"CEBPB\"    \"TRIP10\"  \n## [139] \"F2RL1\"    \"KLF9\"     \"LDLR\"     \"TGIF1\"    \"RNF19B\"   \"DRAM1\"   \n## [145] \"B4GALT1\"  \"DNAJB4\"   \"CSF2\"     \"PDE4B\"    \"SNN\"      \"PLEK\"    \n## [151] \"STAT5A\"   \"DENND5A\"  \"CCND1\"    \"DDX58\"    \"SPHK1\"    \"CD80\"    \n## [157] \"TNFAIP8\"  \"CCNL1\"    \"FUT4\"     \"CCRL2\"    \"SPSB1\"    \"TSC22D1\" \n## [163] \"B4GALT5\"  \"SIK1\"     \"CLCF1\"    \"NFE2L2\"   \"FOSB\"     \"PER1\"    \n## [169] \"NFAT5\"    \"ATP2B1\"   \"IL12B\"    \"IL6ST\"    \"SLC16A6\"  \"ABCA1\"   \n## [175] \"HES1\"     \"BCL6\"     \"IRS2\"     \"SLC2A3\"   \"CEBPD\"    \"IL23A\"   \n## [181] \"SMAD3\"    \"TAP1\"     \"MSC\"      \"IFIH1\"    \"IL15RA\"   \"TNIP2\"   \n## [187] \"BCL3\"     \"PANX1\"    \"FJX1\"     \"EDN1\"     \"EIF1\"     \"BMP2\"    \n## [193] \"DUSP4\"    \"PDLIM5\"   \"ICOSLG\"   \"GFPT2\"    \"KLF2\"     \"TNC\"     \n## [199] \"SERPINB8\" \"MXD1\"    \n## \n## $HALLMARK_HYPOXIA\n##   [1] \"PGK1\"     \"PDK1\"     \"GBE1\"     \"PFKL\"     \"ALDOA\"    \"ENO2\"    \n##   [7] \"PGM1\"     \"NDRG1\"    \"HK2\"      \"ALDOC\"    \"GPI\"      \"MXI1\"    \n##  [13] \"SLC2A1\"   \"P4HA1\"    \"ADM\"      \"P4HA2\"    \"ENO1\"     \"PFKP\"    \n##  [19] \"AK4\"      \"FAM162A\"  \"PFKFB3\"   \"VEGFA\"    \"BNIP3L\"   \"TPI1\"    \n##  [25] \"ERO1A\"    \"KDM3A\"    \"CCNG2\"    \"LDHA\"     \"GYS1\"     \"GAPDH\"   \n##  [31] \"BHLHE40\"  \"ANGPTL4\"  \"JUN\"      \"SERPINE1\" \"LOX\"      \"GCK\"     \n##  [37] \"PPFIA4\"   \"MAFF\"     \"DDIT4\"    \"SLC2A3\"   \"IGFBP3\"   \"NFIL3\"   \n##  [43] \"FOS\"      \"RBPJ\"     \"HK1\"      \"CITED2\"   \"ISG20\"    \"GALK1\"   \n##  [49] \"WSB1\"     \"PYGM\"     \"STC1\"     \"ZNF292\"   \"BTG1\"     \"PLIN2\"   \n##  [55] \"CSRP2\"    \"VLDLR\"    \"JMJD6\"    \"EXT1\"     \"F3\"       \"PDK3\"    \n##  [61] \"ANKZF1\"   \"UGP2\"     \"ALDOB\"    \"STC2\"     \"ERRFI1\"   \"ENO3\"    \n##  [67] \"PNRC1\"    \"HMOX1\"    \"PGF\"      \"GAPDHS\"   \"CHST2\"    \"TMEM45A\" \n##  [73] \"BCAN\"     \"ATF3\"     \"CAV1\"     \"AMPD3\"    \"GPC3\"     \"NDST1\"   \n##  [79] \"IRS2\"     \"SAP30\"    \"GAA\"      \"SDC4\"     \"STBD1\"    \"IER3\"    \n##  [85] \"PKLR\"     \"IGFBP1\"   \"PLAUR\"    \"CAVIN3\"   \"CCN5\"     \"LARGE1\"  \n##  [91] \"NOCT\"     \"S100A4\"   \"RRAGD\"    \"ZFP36\"    \"EGFR\"     \"EDN2\"    \n##  [97] \"IDS\"      \"CDKN1A\"   \"RORA\"     \"DUSP1\"    \"MIF\"      \"PPP1R3C\" \n## [103] \"DPYSL4\"   \"KDELR3\"   \"DTNA\"     \"ADORA2B\"  \"HS3ST1\"   \"CAVIN1\"  \n## [109] \"NR3C1\"    \"KLF6\"     \"GPC4\"     \"CCN1\"     \"TNFAIP3\"  \"CA12\"    \n## [115] \"HEXA\"     \"BGN\"      \"PPP1R15A\" \"PGM2\"     \"PIM1\"     \"PRDX5\"   \n## [121] \"NAGK\"     \"CDKN1B\"   \"BRS3\"     \"TKTL1\"    \"MT1E\"     \"ATP7A\"   \n## [127] \"MT2A\"     \"SDC3\"     \"TIPARP\"   \"PKP1\"     \"ANXA2\"    \"PGAM2\"   \n## [133] \"DDIT3\"    \"PRKCA\"    \"SLC37A4\"  \"CXCR4\"    \"EFNA3\"    \"CP\"      \n## [139] \"KLF7\"     \"CCN2\"     \"CHST3\"    \"TPD52\"    \"LXN\"      \"B4GALNT2\"\n## [145] \"PPARGC1A\" \"BCL2\"     \"GCNT2\"    \"HAS1\"     \"KLHL24\"   \"SCARB1\"  \n## [151] \"SLC25A1\"  \"SDC2\"     \"CASP6\"    \"VHL\"      \"FOXO3\"    \"PDGFB\"   \n## [157] \"B3GALT6\"  \"SLC2A5\"   \"SRPX\"     \"EFNA1\"    \"GLRX\"     \"ACKR3\"   \n## [163] \"PAM\"      \"TGFBI\"    \"DCN\"      \"SIAH2\"    \"PLAC8\"    \"FBP1\"    \n## [169] \"TPST2\"    \"PHKG1\"    \"MYH9\"     \"CDKN1C\"   \"GRHPR\"    \"PCK1\"    \n## [175] \"INHA\"     \"HSPA5\"    \"NDST2\"    \"NEDD4L\"   \"TPBG\"     \"XPNPEP1\" \n## [181] \"IL6\"      \"SLC6A6\"   \"MAP3K1\"   \"LDHC\"     \"AKAP12\"   \"TES\"     \n## [187] \"KIF5A\"    \"LALBA\"    \"COL5A1\"   \"GPC1\"     \"HDLBP\"    \"ILVBL\"   \n## [193] \"NCAN\"     \"TGM2\"     \"ETS1\"     \"HOXB9\"    \"SELENBP1\" \"FOSL2\"   \n## [199] \"SULT2B1\"  \"TGFB3\"\nhead(names(hallmarks_gmt),2)## [1] \"HALLMARK_TNFA_SIGNALING_VIA_NFKB\" \"HALLMARK_HYPOXIA\"\nhallmarks_gmt[['HALLMARK_ANGIOGENESIS']]## setName: HALLMARK_ANGIOGENESIS \n## geneIds: VCAN, POSTN, ..., CXCL6 (total: 36)\n## geneIdType: Null\n## collectionType: Null \n## details: use 'details(object)'\nhead(hallmarks_gmt[['HALLMARK_ANGIOGENESIS']]@geneIds)## [1] \"VCAN\"   \"POSTN\"  \"FSTL1\"  \"LRPAP1\" \"STC1\"   \"LPL\"\nlength(hallmarks_gmt[['HALLMARK_ANGIOGENESIS']]@geneIds)## [1] 36"},{"path":"biology-bioinformatics.html","id":"over-representation-analysis","chapter":"8 Biology & Bioinformatics","heading":"8.8.3 Over-representation Analysis","text":"One common ways utilize gene sets evaluate gene expression\nresults perform -representation Analysis (ORA). Let us assume \nobtained list differentially expressed genes experiment. \ncurious know within list differentially expressed genes, \nsee -representation enrichment genes belonging gene sets \ninterest? general terms, goal ORA determine likely \nnon-random association gene differentially\nexpressed membership chosen (ideally relevant) gene set. \nR, can simple ORA utilizing Fisher’s exact test contingency\ntable.purely hypothetical example, let us assume performed\ndifferential gene expression analysis two different cell lines. \nobtain list 10,000 total genes (background) discovered \nexperiment find chosen statistical threshold, 1,000 \ndifferentially expressed. keep things simple, perform single ORA\ntest Hallmarks Angiogenesis gene set using sample list 1000\ndifferentially expressed genes selected data generated Marisa et al.\n2013. Hallmarks Angiogenesis\ngene set consists 36 genes find 13 also present \nlist differentially expressed genes.Please note removing list genes original meaning\ncontext found publication simply using demonstrate \nbasic steps occurring ORA. numbers lists \narbitrarily chosen experimental setup purely hypothetical.Another issue note typically “background” list represent\nentire pool genes differentially expressed genes \nselected. expression experiments, typical choose \ndetected genes (regardless significance) background. number \ngenes organism’s genome potentially also appropriate\n“background”Also, important keep mind reality, ORA nearly always\nperformed larger scale variety different gene sets. \nallows unbiased discovery potentially novel unexpected enrichment\nbiological areas interest. also necessitates need \nmultiple-testing correction, discussed multiple hypothesis\ntesting.begin, want prepare contingency table describes \nvarious overlaps sets interest. 2x2 contingency table,\nfour values :Genes present list differentially expressed genes present gene setGenes present list differentially expressed genes present gene setGenes present list differentially expressed genes present gene setGenes present list differentially expressed genes present gene setTo demonstrate look like, manually constructed \ncontingency table labels totals added . look margins\ntable recall previously given values , can reconstruct\nlogic used generate values cells.purposes example, reading differentially expressed\ngenes external file, vector generated number \nways depending upon results stored. Following good coding\npractices, write small function takes list DE genes \nGeneSet object programmatically generate contingency table:many ways construct contingency table. just one way \nchosen make calculations values contained within table\ntransparent easy understand.perform Fisher’s Exact test using built-R function fisher.test()\nview summarized output simply calling variable stored\ntest results:Specific values results can accessed $notation (.e.\nfisher_results$p.value). full list returned values may found R\ndocumentation fisher.test()Back hypothetical example focusing p-value returned r fisher_results$p.value, can interpret probability randomly\nobtaining results extreme observed assuming null\nhypothesis association differential expression gene\nset membership true. Based results p-value \npre-determined statistical threshold, make conclusion \nenrichment -representation differentially expressed genes\nexperiment Hallmark Angiogenesis gene set. Relating back\nexperiment, might hypothesize differences two\ncell lines might driving gene expression changes result alterations\ngenes involved angiogenesis. might motivate potential \nvitro experiments cell lines, including migration proliferation\nassays, reveal enrichment angiogenesis genes reflected\nphenotypic functional level.ORA quick useful way generate hypotheses investigate\nspecific mechanisms action regulation. example, identifying \ngene set enriched -represented, one test \nspecific genes set examining directionality change asking\ndependent pathways/networks also perturbed.One major limitation ORA relies choice arbitrary\nstatistical thresholds define “interesting” “differentially expressed”\ngenes. reiterate , p-value thresholds hold inherent biological\nmeaning subjectively determined. Changing p-value threshold may\nresult dramatic differences outcome ORA. Additionally, expression\ndatasets may measure tens thousands genes single experiment, \nfiltering p-value threshold may discard potentially useful information.ORA (though often modified slightly different statistical methodologies) \nimplemented number different R packages \ntopGO \nvarious web services including DAVID \nenrichR","code":"\n#load and read our list of DE genes contained within a newline delimited txt file\nde_genes <- scan('example_de_list.txt', what=character(), sep='\\n')\n\n#define a function that takes a list of DE genes, and a specific GeneSet from a GeneSetCollection to generate a contingency table\n#using set operations in GSEABase\nmake_contingency <- function(de_list, GeneSetobj) {\n\n\n  #make our de list into a simple GeneSet object using GSEABase\n  de_geneset <- GeneSet(de_list, setName='1000 DE genes')\n\n  #If we had the full results, we could determine this value without manually setting it\n  background_len <- 10000\n\n  #Calculate the values inside the contingency table using set operations\n  de_in <- length((de_geneset & GeneSetobj)@geneIds)\n  de_notin <- length(setdiff(de_geneset, GeneSetobj)@geneIds)\n  set_notde <- length(setdiff(GeneSetobj, de_geneset)@geneIds)\n  notin_notde <- background_len - (de_in + de_notin + set_notde)\n\n\n  #return a matrix of the contingency values\n  return(matrix(c(de_in, de_notin, set_notde, notin_notde), nrow=2))\n\n\n}\n\ncontingency_table <- make_contingency(de_genes, hallmarks_gmt[['HALLMARK_ANGIOGENESIS']])\ncontingency_table##      [,1] [,2]\n## [1,]   13   23\n## [2,]  987 8977\nfisher_results <- fisher.test(contingency_table, alternative='greater')\nfisher_results## \n##  Fisher's Exact Test for Count Data\n## \n## data:  contingency_table\n## p-value = 2.382e-05\n## alternative hypothesis: true odds ratio is greater than 1\n## 95 percent confidence interval:\n##  2.685749      Inf\n## sample estimates:\n## odds ratio \n##   5.139308"},{"path":"biology-bioinformatics.html","id":"rank-based-analysis","chapter":"8 Biology & Bioinformatics","heading":"8.8.4 Rank-based Analysis","text":"refer specific method developed Broad Institute UC San\nDiego Gene Set Enrichment Analysis (note capitalization). specific\nmethodology confused gene set enrichment analysis, \nuse umbrella term covering many statistical methodologies used \nanalyze gene sets.Gene Set Enrichment Analysis (https://www.gsea-msigdb.org/gsea/index.jsp) \nmethod developed facilitate biological analysis genome-wide\nexpression experiments. rank-based method utilizes \ninformation expression dataset instead relying pre-determined\nstatistical thresholds. simplest case, GSEA organizes expression data\ntwo classes, ranks discovered genes chosen metric correlating\nexpression classes. , pre-defined set genes, GSEA\ntests whether members particular gene set occur frequently \ntop bottom ranked list randomly distributed throughout.\nranking can done number different measures, common ones\ninclude signal--noise ratio, signed log p-value log fold change estimates.\npre-defined gene sets flexible may manually constructed \ndrawn many curated databases gene sets.Behind scenes, GSEA functions descending ranked list \nincreasing cumulative score gene encountered ’s contained within\nchosen gene set decreasing score encounters gene \nwithin gene set. incrementing weighted put emphasis \ngenes found extremes ranked list final score\ntaken enrichment score (ES) maximum deviation zero.\nscore, p-value determined permuting phenotype labels \ngenerate null distribution compare . account multiple\nhypothesis correction, ES normalized size gene set \ncreate normalized enrichment score (NES) p-values subjected \nBenjamini-Hochberg correction generate FDR values NES.make explanation transparent, let us assume performed\nRNAseq experiment determine genes changed knock \ngene interest model cell line. obtained results, \nconsists list genes discovered experiment associated\nstatistical measures p-value log fold change estimates. perform \nbasic GSEA, sort list genes, without respect significance,\ndescending fold change generate ranked list genes \nupregulated (positive fold change) “top” list genes \ndownregulated (negative fold change) “bottom” list. \nuse ranked list number combination gene sets perform\nGSEA using one available means.GSEA available java application graphical user interface well\ncommand-line utility use HPC cluster environments. also exist\nseveral packages R also implement core GSEA algorithm along \nvarious changes refinements underlying statistical methodology. \nfollowing section, refer usage one package, fgsea.\ncircumstances, GSEA can run default parameters please see\nofficial documentation list customizable parameters situations\nadjust .GSEA return list results gene sets tested consisting \nassociated enrichment scores, various statistical measures given\nranked input GSEA results, typical set permissive FDR threshold\nconsider gene sets significant. inspection individual\ngene sets may performed analyzing leading edge subset gene\nset. group genes occur contribute maximum\ndeviation zero score given gene set analysis. genes\ntypically interpreted relevant subset genes likely\ncontribute enrichment signal, typically used \ninvestigations comparing leading edge subsets gene sets applying\ndomain knowledge predict regulatory effects functions respect\nannotated biological pathway gene set.GSEA highly flexible method incorporates information \ndata generated HTS experiments. choice ranking methods gene sets\ncan tailored specific experiment question hand, \nsimple yet fast method can detect subtle changes gene networks \nORA.interpretation GSEA result needs made care. Let’s assume\nperformed GSEA using fold Change ranking metric find\n“KEGG Glycolysis” pathway found enriched positive\nnormalized enrichment score statistically significant. Alone, \nnecessarily imply glycolysis active conditions \ninterest. simply allows conclusion enrichment \ngenes belonging gene set among genes positive log fold change\nexperiment. might imply experimental conditions \naffecting underlying transcriptional regulation results genes\n-regulated. However, also explained feedback mechanisms,\ndirect indirect, related biological pathways gene networks. \nallow directly conclude glycolysis pathway \nactive functional level. make assertion, need perform\nvitro vivo functional assays directly measure \nactivity outputs glycolysis given experimental conditions.important remember unless specified, gene sets often contain\nmix genes different regulatory activities including activators\ninhibitors pathway. addition, many biological pathways, \nfactors influence regulation may apparent \ngene expression level. specific example metabolic pathways, \ncommon enzymatic products subsequent reactions feedback \nregulate activity flux pathway connected pathways. \nregulation easily discerned gene expression level alone. \nsay results GSEA must interpreted care \nintegrated analyses knowledge gain holistic understanding\nmeaning.","code":""},{"path":"biology-bioinformatics.html","id":"fgsea","chapter":"8 Biology & Bioinformatics","heading":"8.8.5 fgsea","text":"perform GSEA analysis R, using fgsea\n(http://bioconductor.org/packages/release/bioc/html/fgsea.html) package. fgsea\nuses core algorithm behind rank-based GSEA additional statistical\nmethodologies meant provide better estimations small p-values. \nexample, generated simulated completely artificial data \nranked list. chosen 1000 genes represent genes discovered\nexperiment, annotated random values fold change \nsorted list descending order. top list corresponds genes\npositive fold change bottom negative fold change.\nhypothetical experiment, let us assume genes positive\nfold change upregulated condition interest vs. reference \ngenes negative fold change downregulated condition \ninterest vs. reference.genes served ranked list input taken experiment\nlooking differences cancer subtypes. Although fold change values\nassociated generated randomly, likely strong residual\nsignal / bias towards various cancer pathways see results\nbelowfgsea takes input, named vector gene-level statistics (ranked list)\nnamed list gene sets. gene names ranked list must exactly\nmatch appear gene sets (see note additional\nconsiderations keep mind). default, fgsea() function run \npre-ranked GSEA meaning must generate ranked list manually\nbeforehand. Importantly, check ranked list input \nassume sorted numerically descending order ranking metric.:::{.box .warning}\nGene sets downloaded MSigDB sets human genes \nrepresented either NCBI Entrez IDs HGNC symbols. convenience, \nworking gene sets containing HGNC symbols. important remember\ncheck identifier system species gene set provided .\nalso becomes relevant performing rank-based GSEA data generated \nnon-human species. situations, need convert gene IDs\nspecies assuming ’re using pre-defined gene set, match \nidentifiers ranked list format found gene set.issue typically becomes relevant trying use MSigDB gene sets \ncompare ranked list genes generated HTS experiments Mus\nmusculus. MGI symbols typically begin uppercase letter followed\nlowercase letters numbers. examples exact\nconcordance true orthologous mouse human genes matching MGI \nHGNC symbols differing case (.e. Gzmb GZMB), \nsimply convert MGI symbols uppercase. work subset \ngenes orthologs symbol misidentify many genes\nshare base name. One way properly perform \nkind ‘orthology’ mapping use biomaRt, discussed\npreviously. :::example, already made simulated vectors containing gene\nnames associated fold change values. generate appropriate\nformat using setNames function two vectors can use \nhead tail quickly check sorting. non-simulated data, \ngenerate vectors number ways depending upon format\nresults data.top 5 genes ranked list:bottom 5 genes list:gene sets, use Hallmarks gene set provided MSigDB.\ntwo ways can load gene sets appropriate format\nexpected. first use one built-functions fgsea,\ngmtPathways, read directly GMT file. displayed\ncontents randomly selected gene set :previously loaded gene sets using GSEABase, also\nsimply following:Now run fgsea, use default parameters besides manually setting\nthresholds ignore gene sets analysis based minimum maximum\nvalues size:noted official documentation original GSEA, small gene\nsets large gene sets may lead issues artificially high scores\npoor normalization, respectively. values can adjusted needed, \nused range suggested official documentation.convert fgsea results tibble, perform basic\nexploration results. can see results sorted ascending\npadj pathways lowest adjusted p-values largely associated\ncancer pathways including Epithelial Mesenchymal Transition, Apical\nJunctions.begin explore results, filter FDR threshold \nsubset significant gene sets direction NES (positive indicating\nenrichment “top” list negative indicating enrichment \n“bottom”).provide basic visualization results, can display normalized\nenrichment scores pathways bar chart fill bars \nwhether meet padj threshold.Rank-based GSEA often used hypothesis-generating experiment can\nquickly capture potentially interesting global patterns regulation among many\nbiological pathways processes considering information\ngenerated HTS experiment. typical set permissive FDR \nconsidering “significant” gene sets. results rank-based GSEA need \ninspected often confirmed complementary follow-\nexperiments. Thus, choose use relaxed FDR threshold < .25.\n(#fig:top level results plot)GSEA results suggest enrichment cancer-related pathways\namongst upregulated genes. GSEA performed using list genes\ngenerated RNAseq ranked descending fold change. GSEA run\nusing default parameters minSize = 15 maxSize = 500. Gene sets \nBenjamini-Hochberg adjusted p-value < .25 considered significant.\nGiven artificial nature data choice small gene set\ncollection, results able display \nintelligibly one plot. real experiments, may several hundred\nsignificant gene sets interest. cases like , may need restrict\nnumber gene sets taking top ten ranked NES (positive\nnegative) plotting figure like one . \nsuggestion many ways choose ‘interesting’ gene sets plot\ntogether depending upon experimental context future questions \ninterest.wished investigate single gene set list results, \ncreate enrichment plot displaying Enrichment Score ’s\ncalculated ranked list. convenience, wrapped \npre-built function, plotEnrichment fgsea, user-defined function \nallows us specify specific pathway. plotEnrichment function expects\nminimum list genes contained within pathway plot input\nranked list gene-level statistics. displayed enrichment plot \none significantly enriched gene sets :\n(#fig:enrichment plot)Hallmarks EPITHELIAL_MESENCHYMAL_TRANSITION Pathway\nsignificantly enriched amongst upregulated genes. 56 genes \nleading edge subset including GAS1, POSTN, LOX. Normalized Enrichment\nScore calculated gene set 2.59 adjusted p-value \n7.28e-10\nmentioned prior section, often useful investigation\ninspect leading edge subset genes contained within ‘interesting’ gene\nsets. fgsea stores genes form list associated \npathway returned table. convert save values named\nlist convenience. can accomplished making use deframe()\nfunction. saved genes contained within leading edge\nsubset significant gene sets, displayed belonging \nHALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION gene set determined \nexperiment:named list contains leading edge genes gene sets meeting \ncertain padj threshold. import GSEABase GeneSetCollection\nobject analyses R write file pass \ncollaborators.examples just ways can utilize explore GSEA\nresults returned fgsea. also methods one found \nbuilt-function, fgsea::plotGseaTable, provide ways \nvisualize results groups different gene sets. can also perform\nnumber different filtering sorting options table results\ndepending upon interests.","code":"\nlibrary('fgsea')\nrnk_list <- setNames(rnks, de_genes)\nhead(rnk_list)##  RBMS1P1    RBMS1     GAS1    SFRP2   CCDC80      MGP \n## 9.965527 9.907958 9.888917 9.828172 9.826594 9.802787\ntail(rnk_list)##      KLF7   ANKDD1A    RIMKLB     RGS19    PLPPR4     TENM4 \n## -9.865186 -9.922858 -9.930453 -9.931198 -9.959923 -9.984908\nhallmark_pathways_fgsea <- fgsea::gmtPathways('h.all.v7.5.1.symbols.gmt')\nhallmark_pathways_fgsea$HALLMARK_TGF_BETA_SIGNALING##  [1] \"TGFBR1\"   \"SMAD7\"    \"TGFB1\"    \"SMURF2\"   \"SMURF1\"   \"BMPR2\"   \n##  [7] \"SKIL\"     \"SKI\"      \"ACVR1\"    \"PMEPA1\"   \"NCOR2\"    \"SERPINE1\"\n## [13] \"JUNB\"     \"SMAD1\"    \"SMAD6\"    \"PPP1R15A\" \"TGIF1\"    \"FURIN\"   \n## [19] \"SMAD3\"    \"FKBP1A\"   \"MAP3K7\"   \"BMPR1A\"   \"CTNNB1\"   \"HIPK2\"   \n## [25] \"KLF10\"    \"BMP2\"     \"ENG\"      \"APC\"      \"PPM1A\"    \"XIAP\"    \n## [31] \"CDH1\"     \"ID1\"      \"LEFTY2\"   \"CDKN1C\"   \"TRIM33\"   \"RAB31\"   \n## [37] \"TJP1\"     \"SLC20A1\"  \"CDK9\"     \"ID3\"      \"NOG\"      \"ARID4B\"  \n## [43] \"IFNGR2\"   \"ID2\"      \"PPP1CA\"   \"SPTBN1\"   \"WWTR1\"    \"BCAR3\"   \n## [49] \"THBS1\"    \"FNTA\"     \"HDAC1\"    \"UBE2D3\"   \"LTBP2\"    \"RHOA\"\nhallmark_pathways_GSEABase <- geneIds(hallmarks_gmt)\nhallmark_pathways_GSEABase$HALLMARK_TGF_BETA_SIGNALING##  [1] \"TGFBR1\"   \"SMAD7\"    \"TGFB1\"    \"SMURF2\"   \"SMURF1\"   \"BMPR2\"   \n##  [7] \"SKIL\"     \"SKI\"      \"ACVR1\"    \"PMEPA1\"   \"NCOR2\"    \"SERPINE1\"\n## [13] \"JUNB\"     \"SMAD1\"    \"SMAD6\"    \"PPP1R15A\" \"TGIF1\"    \"FURIN\"   \n## [19] \"SMAD3\"    \"FKBP1A\"   \"MAP3K7\"   \"BMPR1A\"   \"CTNNB1\"   \"HIPK2\"   \n## [25] \"KLF10\"    \"BMP2\"     \"ENG\"      \"APC\"      \"PPM1A\"    \"XIAP\"    \n## [31] \"CDH1\"     \"ID1\"      \"LEFTY2\"   \"CDKN1C\"   \"TRIM33\"   \"RAB31\"   \n## [37] \"TJP1\"     \"SLC20A1\"  \"CDK9\"     \"ID3\"      \"NOG\"      \"ARID4B\"  \n## [43] \"IFNGR2\"   \"ID2\"      \"PPP1CA\"   \"SPTBN1\"   \"WWTR1\"    \"BCAR3\"   \n## [49] \"THBS1\"    \"FNTA\"     \"HDAC1\"    \"UBE2D3\"   \"LTBP2\"    \"RHOA\"\nfgsea_results <- fgsea(hallmark_pathways_GSEABase, rnk_list, minSize = 15, maxSize=500)\nfgsea_results <- fgsea_results %>% as_tibble()\nfgsea_results %>% arrange(padj)## # A tibble: 15 x 8\n##    pathway                 pval     padj log2err     ES    NES  size leadingEdge\n##    <chr>                  <dbl>    <dbl>   <dbl>  <dbl>  <dbl> <int> <list>     \n##  1 HALLMARK_EPITHELIA~ 4.86e-11 7.28e-10  0.851   0.509  2.60    107 <chr [56]> \n##  2 HALLMARK_APICAL_JU~ 1.50e- 3 1.12e- 2  0.455   0.479  1.87     32 <chr [16]> \n##  3 HALLMARK_MYOGENESIS 3.61e- 3 1.81e- 2  0.432   0.428  1.76     40 <chr [24]> \n##  4 HALLMARK_COAGULATI~ 5.16e- 2 1.55e- 1  0.277   0.383  1.46     30 <chr [13]> \n##  5 HALLMARK_COMPLEMENT 5.15e- 2 1.55e- 1  0.277   0.380  1.49     33 <chr [11]> \n##  6 HALLMARK_KRAS_SIGN~ 1.08e- 1 2.37e- 1  0.188   0.329  1.30     35 <chr [13]> \n##  7 HALLMARK_UV_RESPON~ 1.11e- 1 2.37e- 1  0.186   0.325  1.31     37 <chr [17]> \n##  8 HALLMARK_APOPTOSIS  1.28e- 1 2.39e- 1  0.171   0.378  1.31     21 <chr [10]> \n##  9 HALLMARK_HYPOXIA    2.24e- 1 3.74e- 1  0.124   0.298  1.16     31 <chr [12]> \n## 10 HALLMARK_ADIPOGENE~ 2.73e- 1 4.09e- 1  0.111   0.329  1.13     20 <chr [11]> \n## 11 HALLMARK_IL2_STAT5~ 5.71e- 1 7.14e- 1  0.0747 -0.253 -0.925    24 <chr [6]>  \n## 12 HALLMARK_INTERFERO~ 5.63e- 1 7.14e- 1  0.0749 -0.254 -0.924    23 <chr [7]>  \n## 13 HALLMARK_ALLOGRAFT~ 6.92e- 1 7.99e- 1  0.0646 -0.236 -0.857    23 <chr [4]>  \n## 14 HALLMARK_INFLAMMAT~ 9.10e- 1 9.72e- 1  0.0479  0.184  0.683    26 <chr [7]>  \n## 15 HALLMARK_TNFA_SIGN~ 9.72e- 1 9.72e- 1  0.0501 -0.171 -0.591    20 <chr [4]>\ntop_positive_nes <- fgsea_results %>%\n  filter(padj < .25 & NES > 0) %>%\n  slice_max(NES, n=5)\n\ntop_positive_nes## # A tibble: 5 x 8\n##   pathway                    pval     padj log2err    ES   NES  size leadingEdge\n##   <chr>                     <dbl>    <dbl>   <dbl> <dbl> <dbl> <int> <list>     \n## 1 HALLMARK_EPITHELIAL_M~ 4.86e-11 7.28e-10   0.851 0.509  2.60   107 <chr [56]> \n## 2 HALLMARK_APICAL_JUNCT~ 1.50e- 3 1.12e- 2   0.455 0.479  1.87    32 <chr [16]> \n## 3 HALLMARK_MYOGENESIS    3.61e- 3 1.81e- 2   0.432 0.428  1.76    40 <chr [24]> \n## 4 HALLMARK_COMPLEMENT    5.15e- 2 1.55e- 1   0.277 0.380  1.49    33 <chr [11]> \n## 5 HALLMARK_COAGULATION   5.16e- 2 1.55e- 1   0.277 0.383  1.46    30 <chr [13]>\nfgsea_results %>%\n  mutate(pathway = forcats::fct_reorder(pathway, NES)) %>%\n  ggplot() +\n  geom_bar(aes(x=pathway, y=NES, fill = padj < .25), stat='identity') +\n  scale_fill_manual(values = c('TRUE' = 'red', 'FALSE' = 'blue')) + \n  theme_minimal() +\n  ggtitle('fgsea results for Hallmark MSigDB gene sets') +\n  ylab('Normalized Enrichment Score (NES)') +\n  xlab('') +\n  coord_flip()\nmake_gsea_plot <- function(pathway_name, rnks) {\nplotEnrichment(hallmark_pathways_fgsea[[pathway_name]], rnks) +\n    labs(title=pathway_name)\n}\nmake_gsea_plot('HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION', rnk_list)\ngather_leadingedge <- function(results, threshold) {\n\n  genes <- results %>% \n  filter(padj < threshold) %>%\n  dplyr::select(pathway, leadingEdge) %>%\n  deframe()\n  return(genes)\n}\n\nleading_edge_genes <- gather_leadingedge(fgsea_results, .25)\nleading_edge_genes$HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION##  [1] \"GAS1\"   \"MGP\"    \"SPOCK1\" \"EFEMP2\" \"FERMT2\" \"FBN1\"   \"TAGLN\"  \"FSTL1\" \n##  [9] \"TIMP3\"  \"VIM\"    \"VCAN\"   \"THBS2\"  \"ACTA2\"  \"DCN\"    \"BGN\"    \"NNMT\"  \n## [17] \"CALD1\"  \"NTM\"    \"FBLN1\"  \"SPARC\"  \"COL5A2\" \"MYL9\"   \"SLIT2\"  \"DPYSL3\"\n## [25] \"VEGFC\"  \"COL3A1\" \"LGALS1\" \"THY1\"   \"PCOLCE\" \"ADAM12\" \"SFRP4\"  \"WIPF1\" \n## [33] \"COL6A3\" \"CTHRC1\" \"TIMP1\"  \"TPM2\"   \"FN1\"    \"COL5A1\" \"PMP22\"  \"MMP2\"  \n## [41] \"HTRA1\"  \"COL1A2\" \"POSTN\"  \"PDGFRB\" \"EMP3\"   \"MXRA5\"  \"FLNA\"   \"PRRX1\" \n## [49] \"FAP\"    \"LOX\"    \"BASP1\"  \"GREM1\"  \"CCN2\"   \"CDH11\"  \"LUM\"    \"LAMC1\""},{"path":"biology-bioinformatics.html","id":"biological-networks","chapter":"8 Biology & Bioinformatics","heading":"8.9 Biological Networks","text":"Biological systems made many components work together spatial,\ntemporal, chemical ways maintain biological fidelity. components\nmay molecular components like metabolites, transcripts, proteins, entire\norganisms case microbial ecologies, even abstract concepts like\ngenes metabolic reactions. studying individual components \nnecessary part biological research, understanding components\ninteract influence can help gain greater understanding \nbiological systems whole. biology, mathematics, fields, \ncan conceptualize set components relationships \nnetwork.general terms, network defined set components (entities)\nrelationships . networks physical, like computer\nnetworks different hardware devices connected together wires.\nHowever, networks abstract, like social networks friendships\npeople, ecological networks like forests soil different\norganisms influence predation, synergistic interactions, etc.Biological networks molecular biology similarly abstract, \ninteraction entities can represent many different types \nrelationships. primary entities molecular networks genes,\nmacromolecules (DNA, RNA transcripts, proteins, etc), metabolites \nsmall molecules, cellular organelles components, entire cells\n. Relationships entities may include spatial colocality,\ndirect physical association, shared molecular functions, genetic \nevolutionary relationships, chemical interactions like biochemical \nstructural modifications, modulation anabolic catabolic rates (e.g.\nrate transcription, translation, etc), many . reality, entities\nmay types relationships entities, \npotentially occuring simultaneously moment time.substantial challenge incorporate possible types interactions\nentities mind time, typically express \nsubsets holistic network. subnetworks reflect different aspects \nsystem, e.g. genes control transcription genes, \nproteins physically complex one another. Fortunately, concept \nnetwork generic, may generally use tools techniques \nexpress analyze networks network type.section first describes different types biological networks\nfound molecular biology, covers basics \nmathematical computational expressions, properties, algorithms used \nanalyze networks.","code":""},{"path":"biology-bioinformatics.html","id":"biological-pathways","chapter":"8 Biology & Bioinformatics","heading":"8.9.1 Biological Pathways","text":"Biological pathways, just pathways, groups genes work together \ncreate product cause change cell. coordinated\nactivites cell performed pathways induced repressed\nbased conditions within without cell. Hundreds biological pathways\ncharacterized organized publicly available databases.","code":""},{"path":"biology-bioinformatics.html","id":"protein-protein-interaction-network-.","chapter":"8 Biology & Bioinformatics","heading":"8.9.2 Protein-Protein Interaction Network .","text":"protein-protein interaction (PPI) networks? information \nrepresent (direct association, functional association, etc)? PPI\ninformation come (datasets, databases, etc)? ways can use\nPPI information interpreting biological data (like differential\nexpression? sure)?","code":""},{"path":"biology-bioinformatics.html","id":"gene-regulatory-networks-.","chapter":"8 Biology & Bioinformatics","heading":"8.9.3 Gene Regulatory Networks .","text":"gene regulatory networks? important? identify\n(data driven correlation, wetlab experiments, others?)? \nuseful?","code":""},{"path":"biology-bioinformatics.html","id":"wgcna-.","chapter":"8 Biology & Bioinformatics","heading":"8.9.4 WGCNA .","text":"Weighted correlation network analysis (WGCNA) identifies genes (biology features) highly correlated , group clusters / modules.\nTypical inputs WGCNA expression matrix genes, proteins biology features. , WGCNA construct undirected weighted co-expression network. nodes genes (features), edges connecting pairwise correlation value expression level. Modules simply clusters highly connected genes. modules detected, stream analyses may include summarizing module “eigengenes”, inferring functionality gene genes module, compare different modules.\ninformation, may refer article manual WGCNA R package.","code":""},{"path":"biology-bioinformatics.html","id":"how-to-run-wgcna-in-r","chapter":"8 Biology & Bioinformatics","heading":"8.9.4.1 How to run WGCNA in R","text":"First, install package WGCNA.note: receive error message saying dependencies available, ’s probably used try install using standard method install.package(\"WGCNA\"). ’s case, try following re-install WGCNA:Let’s load toy gene expression matrix. 15 samples 7 genes.optional choice WGCNA allow multithreadsNow choose “soft-thresholding power” construct network. single answer power choose.\nrefer WGCNA faq, details power choosing.Generally, want choose power give us “Scale Free Topology Model Fit, signed R^2” around 0.8. toy dataset small can’t show properly.\nillustration, pick power 4 example.Construct network.\nlot parameters. cases, don’t need modify . , make sure choose proper power, networkType (“signed” “unsigned”), minModuleSize analysis.\n- networkType Using “signed” option, direction correlation considered, e.g. two genes won’t clustered together correlation value perfect -1. hand, use “unsigned” option consider absolute value correlation.\n- minModuleSize minimum umber features, e.g. genes, module. example 7 genes, set argument 2. real analysis, typical choice may 20 30 matrix several hundreds thousands genes.Now can see module membership assignment:CRAB, Lobst, Octop Coral assigned module 1. Whale, BabyShark Orca assigned module 2.can use function labels2colors() conveniently convert module membership color labels.Plot dendrogram module colors:real analysis lot genes, see much modules colors.\ndendrogram, able roughly see size module looking area different colors.point, WGCNA analysis finished.\ncan summarize module information data frame:","code":"BiocManager::install(\"WGCNA\")\nlibrary(WGCNA)BiocManager::install(\"WGCNA\",force = T)##            CRAB     Whale     Lobst      Octop    Coral BabyShark     Orca\n## samp1  6.867731  8.796212  7.820130  6.6081601 15.37307 14.047536 32.96333\n## samp2 10.918217  6.775947 16.545788  7.5685123 28.59193  8.923224 19.13865\n## samp3  5.821857  8.682239  5.089099  2.8030264 17.99007 14.643098 36.93207\n## samp4 17.976404  5.741274 27.596721  8.8660342 34.62918  9.473205 18.68929\n## samp5 11.647539 10.866047 14.852970  2.2034360 14.15414 15.949892 42.21472\n## samp6  5.897658 11.960800 15.915636 -0.1724085 21.78596 16.336686 44.93125\nallowWGCNAThreads(4)## Allowing multi-threading with up to 4 threads.\n# A set of soft-thresholding powers to choose from:\npowers <- seq(1, 10)\n\n# Call the network topology analysis function\nsft <- pickSoftThreshold(\n  data = dat,\n  powerVector = powers,\n  verbose = 5\n)## pickSoftThreshold: will use block size 7.\n##  pickSoftThreshold: calculating connectivity for given powers...\n##    ..working on genes 1 through 7 of 7\n##    Power SFT.R.sq   slope truncated.R.sq mean.k. median.k. max.k.\n## 1      1 0.026500 -12.800        0.38900   1.920     1.820  2.350\n## 2      2 0.007510   2.420       -0.18500   1.240     1.180  1.730\n## 3      3 0.001090   0.582        0.00340   0.931     0.908  1.430\n## 4      4 0.001300  -0.460       -0.14200   0.733     0.708  1.220\n## 5      5 0.003130   0.526       -0.03820   0.592     0.556  1.060\n## 6      6 0.018100  -1.150        0.04810   0.486     0.440  0.922\n## 7      7 0.087400  -2.130       -0.15900   0.404     0.349  0.807\n## 8      8 0.063500  -1.570       -0.11100   0.338     0.279  0.708\n## 9      9 0.000378  -0.206        0.00319   0.285     0.224  0.621\n## 10    10 0.003650  -0.600       -0.00478   0.241     0.181  0.545\npar(mfrow = c(1, 2))\ncex1 <- 0.9\nplot(sft$fitIndices[, 1],\n  -sign(sft$fitIndices[, 3]) * sft$fitIndices[, 2],\n  xlab = \"Soft Threshold (power)\",\n  ylab = \"Scale Free Topology Model Fit, signed R^2\",\n  main = paste(\"Scale independence\")\n)\ntext(sft$fitIndices[, 1],\n  -sign(sft$fitIndices[, 3]) * sft$fitIndices[, 2],\n  labels = powers, cex = cex1, col = \"red\"\n)\nabline(h = 0.90, col = \"red\")\nplot(sft$fitIndices[, 1],\n  sft$fitIndices[, 5],\n  xlab = \"Soft Threshold (power)\",\n  ylab = \"Mean Connectivity\",\n  type = \"n\",\n  main = paste(\"Mean connectivity\")\n)\ntext(sft$fitIndices[, 1],\n  sft$fitIndices[, 5],\n  labels = powers,\n  cex = cex1, col = \"red\"\n)\npicked_power <- 4\n\n# fix a namespace conflict issue, force R to use the cor() function in WGCNA:\ntemp_cor <- cor\ncor <- WGCNA::cor\nnetwk <- blockwiseModules(\n  datExpr = dat,\n  power = picked_power,\n  networkType = \"signed\",\n  deepSplit = 2,\n  pamRespectsDendro = F,\n  minModuleSize = 2,\n  maxBlockSize = 4000,\n  reassignThreshold = 0,\n  mergeCutHeight = 0.25,\n  saveTOMs = T, # Archive the run results in TOM file (saves time)\n  saveTOMFileBase = \"ER\",\n  numericLabels = T,\n  verbose = 3\n)##  Calculating module eigengenes block-wise from all genes\n##    Flagging genes and samples with too many missing values...\n##     ..step 1\n##  ..Working on block 1 .\n##     TOM calculation: adjacency..\n##     ..will not use multithreading.\n##      Fraction of slow calculations: 0.000000\n##     ..connectivity..\n##     ..matrix multiplication (system BLAS)..\n##     ..normalization..\n##     ..done.\n##    ..saving TOM for block 1 into file ER-block.1.RData\n##  ....clustering..\n##  ....detecting modules..\n##  ....calculating module eigengenes..\n##  ....checking kME in modules..\n##  ..merging modules that are too close..\n##      mergeCloseModules: Merging modules whose distance is less than 0.25\n##        Calculating new MEs...\nnetwk$colors##      CRAB     Whale     Lobst     Octop     Coral BabyShark      Orca \n##         1         2         1         1         1         2         2\nmergedColors <- labels2colors(netwk$colors)\nplotDendroAndColors(\n  netwk$dendrograms[[1]],\n  mergedColors[netwk$blockGenes[[1]]],\n  \"Module colors\",\n  dendroLabels = FALSE,\n  hang = 0.03,\n  addGuide = TRUE,\n  guideHang = 0.05\n)\nmodule_df <- data.frame(\n  gene_id = names(netwk$colors),\n  modules = labels2colors(netwk$colors)\n)\nmodule_df##     gene_id   modules\n## 1      CRAB turquoise\n## 2     Whale      blue\n## 3     Lobst turquoise\n## 4     Octop turquoise\n## 5     Coral turquoise\n## 6 BabyShark      blue\n## 7      Orca      blue"},{"path":"engineering.html","id":"engineering","chapter":"9 EngineeRing","heading":"9 EngineeRing","text":"","code":""},{"path":"engineering.html","id":"unit-testing","chapter":"9 EngineeRing","heading":"9.1 Unit Testing","text":"Writing code mean often harder might\nseem, especially R. Also, code grows size complexity, \nuse good programming practice like writing functions,\nchanging one part code may unexpected effects parts \ndidn’t change. Unless using programming language support\nproof-based correctness\nguarantees, \nmay impossible determine code always correct. might\nimagine, -called “total correctness” difficult attain, often\nrequires time implement practical (unless ’re programming\nsomething correctness important, e.g. self-driving car).\nHowever, collection approaches can give us reasonable\nassurances code mean . approaches \ncalled software testing frameworks explicitly test code \ncorrectness.many different testing frameworks, employ general\nprinciple test codes correctness passing inputs \nknow output . example, consider following function\nsums two numbers:can test function using known set inputs explicitly comparing\nresult expected output:test instance case input x=1,y=2 expected output \n3. comparing result input expected output, \nshowed least specific inputs function behaves intended.\ntesting terminology used case test passed. result\nanything 3, test failed.example test, informal test; yet\nintegrated framework since manually inspect result \npassing failing. testing framework, developer code\nalso write tests code runs tests frequently code\nevolves make sure continues behave expect time.R package testthat provides testing\nframework “tries make testing fun possible, get \nvisceral satisfaction writing tests.” ’s true writing tests \ncode may feel tedious fun, tradeoff tested code\nlikely correct, saving potentially embarrassing (\nworse) errors!Writing tests using testthat easy, using example test written\n(remember install package using install.packages(\"testthat\")\nfirst).Test passed! satisfying! test_that function accepts two arguments:concise, human readable description testone tests enclosed {} written using expect_X\nfunctions testthat\npackageIn example , explicitly testing result add(1,2) \nequal 3 add(5,6) equal 11; specifically, called\nexpect_equal, accepts two arguments uses test equality. \nwritten two explicit test cases (.e. 1+2 == 3 5+6 == 11) \ntest heading.mistake test expected output wrong,\ntestthat inform us failure, details \nhappened compared asked expect:case, test case incorrect, helpful\ninformation correctly specified input expected output \ntest failed! means something wrong, now aware \ncan fix .general testing strategy usually involves writing R script \ncontains tests like example analysis code; tests\ntest script call functions written scripts\ncheck correctness exactly like . , whenever make\nsubstantial changes analysis code, can simply run test script\ncheck whether everything went ok. course, add functions \nanalysis script need add new tests code. put \ntest script file called test_functions.R run \nanalysis code like following:ultimate testing strategy called test driven\ndevelopment \nwrite tests developing analysis code, even functions \ndon’t exist yet. Imagine decide need new function multiplies two\nnumbers together haven’t written yet. testtthat can handle \nsituation call function isn’t defined yet:case, test failed mul() defined yet, \nalready done hard part writing test! Now write\nmul() function keep working tests pass. Writing tests\nfirst analysis code later great way thoughtful \ncode structured, along usual benefit testing means \ncode likely correct.Testing - R packagestestthat Reference","code":"add <- function(x,y) {\n  return(x+y)\n}result <- add(1,2)\nresult == 3\n[1] TRUElibrary(testthat)\ntest_that(\"add() correctly adds things\", {\n    expect_equal(add(1,2),3)\n    expect_equal(add(5,6),11)\n  }\n)\nTest passedtest_that(\"add() correctly adds things\", {\n    expect_equal(add(1,2),3)\n    expect_equal(add(5,6),10)\n  }\n)\n-- Failure (Line 3): add() correctly adds things -------------------------------\nadd(5, 6) not equal to 10.\n1/1 mismatches\n[1] 11 - 10 == 1\n\nError: Test failedadd <- function(x,y) {\nreturn(x+y)\n}\ntestthat::test_file(\"test_functions.R\")\n\n== Testing test_functions.R =======================================================\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ] Done!test_that(\"mul() correctly multiplies things\",{\nexpect_equal(mul(1,2),2)\n})\n-- Error (Line 1): new function ------------------------------------------------\nError in `mul(1, 2)`: could not find function \"mul\"\nBacktrace:\n 1. testthat::expect_equal(mul(1, 2), 2)\n 2. testthat::quasi_label(enquo(object), label, arg = \"object\")\n 3. rlang::eval_bare(expr, quo_get_env(quo))\n\nError: Test failed"},{"path":"engineering.html","id":"toolification","chapter":"9 EngineeRing","heading":"9.2 Toolification","text":"RStudio highly effective tool developing R code, analyses \nconduct suitable running interactive environment. However, \ncertain circumstances running R scripts interactive manner \noptimal even suitable:script must run many different inputsThe data analyzed stored system interactive\nRStudio session availableThe script takes long time run (e.g. days weeks)computational demands analysis exceed resources available \ncomputer RStudio run (e.g. personal laptop limited\nmemory)cases, necessary run R scripts outside RStudio, often \ncommand line interface (CLI) like found linux clusters cloud based\nvirtual machines. scripts can run R interpreter \nCLI produce behavior, steps help\nconvert R script tool can run way. process \ntransforming R script generally usable tool run \nCLI termed toolification text. following sections describe \nstrategies toolifying running R scripts developed RStudio \nCLI.","code":""},{"path":"engineering.html","id":"the-r-interpreter","chapter":"9 EngineeRing","heading":"9.2.1 The R Interpreter","text":"bears mentioning RStudio R related independent programs.\nSpecifically, RStudio runs R program behind interface using \ncalled R interpreter.\nrun R CLI, given interactive interface commands\nwritten R language can evaluated. RStudio, interpreter \nrunning behind scenes can accessed Console tab bottom right:R Interpreter RStudio linux terminalThe interpreter useful, since meaningful analyses\nrequire many lines code run sequentially unit. interpreter can \nhelpful test individual lines code examine help documentation \nR functions.simplest way run R script command line first run R\ninterpreter use source() function,\naccepts filename R script first argument executes \nlines code script.However, method requires interactivity, namely running R\ninterpreter first, sufficient run non-interactive fashion.\nmay need run script without interactivity, example \npart computational pipeline.","code":"$ cat simple_script.R\nprint('hello moon')\nprint(1+2)\nprint(str(c(1,2,3,4)))\n$ R\n> source(\"simple_script.R\")\n[1] \"hello moon\"\n[1] 3\n num [1:4] 1 2 3 4"},{"path":"engineering.html","id":"rscript","chapter":"9 EngineeRing","heading":"9.2.2 Rscript","text":"R program also includes Rscript command\ncan run command line:CLI command accepts R script input executes commands \nfile passed source(), example:simplest way toolify R script; simply run command\nline Rscript. Toolifying simple R scripts need accept\ninputs execute different data generally require changes.Rscript convenience function runs following R command:reason run Rscript directly, can use \narguments R command attain result.","code":"$ Rscript\nUsage: /path/to/Rscript [--options] [-e expr [-e expr2 ...] | file] [args]\n\n--options accepted are\n  --help              Print usage and exit\n  --version           Print version and exit\n  --verbose           Print information on progress\n  --default-packages=list\n                      Where 'list' is a comma-separated set\n                        of package names, or 'NULL'\nor options to R, in addition to --no-echo --no-restore, such as\n  --save              Do save workspace at the end of the session\n  --no-environ        Don't read the site and user environment files\n  --no-site-file      Don't read the site-wide Rprofile\n  --no-init-file      Don't read the user R profile\n  --restore           Do restore previously saved objects at startup\n  --vanilla           Combine --no-save, --no-restore, --no-site-file\n                        --no-init-file and --no-environ\n\n'file' may contain spaces but not shell metacharacters\nExpressions (one or more '-e <expr>') may be used *instead* of 'file'\nSee also  ?Rscript  from within R$ cat simple_script.R\nprint('hello moon')\nprint(1+2)\nprint(str(c(1,2,3,4)))\n$ Rscript simple_script.R\n[1] \"hello moon\"\n[1] 3\n num [1:4] 1 2 3 4\n$$ Rscript simple_script.R # is equivalent to:\n$ R --no-echo --no-restore --file=simple_script.R"},{"path":"engineering.html","id":"commandargs","chapter":"9 EngineeRing","heading":"9.2.3 commandArgs()","text":"However, sometimes may wish control behavior script directly \ncommand line, rather editing script directly every different\nexecution. pass information script run, can pass\narguments Rscript command:Although passed argument abc, output script didn’t change\nscript wasn’t written receive . order script gain\naccess command line arguments, must call commandArgs()\nfunction:Now execute script, arguments passed available \nargs variable:last case arguments passed, R telling us args\nvariable character vector length zero.default, commandArgs() function return full command \nrun, including Rscript command additional arguments:trailingOnly=TRUE argument returns arguments provided end\ncommand, Rscript portion:Note can provide individual commands instead script Rscript \n-e argument.commandArgs() function needed toolify R script.\nConsider simple script named inspect_csv.R loads CSV file \nsummarizes tibble:can now run script Rscript give filename CSV file:Note input handling arguments, usage script \nhelpful error message printed script quits \nexactly one argument provided.example , filename passed script argument. \nfilename encoded character string case, commandArgs()\nalways produces vector strings. need pass arguments control\nnumerical values, need parse arguments first. Consider \nfollowing R implementation linux command head, prints \ntop \\(n\\) lines file screen:tested number arguments passed correct usage, \nassigned arguments variables. n argument cast character\nstring integer process, enabling use \ndplyr::slice_head() function. can print first three lines file\nfollows:Reading command line arguments variables script can become tedious \nscript large number arguments. Fortunately, argparser\npackage can help handle many \nrepetitive operations, including specifying arguments, providing default values,\nautomatically casting appropriate types like numbers, printing usage\ninformation:library, can rewrite head.R script concise:Note didn’t explicitly parse top argument integer \ntype=\"numeric\" handled us. can print top three lines \nfile like using new parser arguments:can also inspect usage passing -h flag:tools required toolify R script.Note scripts developed RStudio can run command line, \nscripts written CLI use strategies easily run\ninside RStudio! However, followed good practices implemented \nscript set functions, can easily write CLI wapper script \ncalls functions, thereby enabling continue developing code \nRStudio maintain CLI tool functionality.","code":"$ Rscript simple_script.R abc\n[1] \"hello moon\"\n[1] 3\n num [1:4] 1 2 3 4\nargs <- commandArgs(trailingOnly=TRUE)$ cat echo_args.R\nprint(commandArgs(trailingOnly=TRUE))\n$ Rscript echo_args.R abc\n[1] \"abc\"\n$ Rscript echo_args.R abc 123\n[1] \"abc\" \"123\"\n$ Rscript echo_args.R # no args\ncharacter(0)$ Rscript -e \"commandArgs()\" abc 123\n[1] \"/usr/bin/R\"\n[2] \"--no-echo\"\n[3] \"--no-restore\"\n[4] \"-e\"\n[5] \"commandArgs()\"\n[6] \"--args\"\n[7] \"abc\"\n[8] \"123\"$ Rscript -e \"commandArgs(trailingOnly=TRUE)\" abc 123\n[1] \"abc\" \"123\"\nargs <- commandArgs(trailingOnly=TRUE)\nif(length(args) != 1) {\n  cat(\"Usage: simple_script.R <csv file>\\n\") # cat() writes characters to the screen\n  cat(\"Provide exactly one argument that is a CSV filename\\n\")\n  quit(save=\"no\", status=1)\n}\nfn <- args[1]\nlibrary(tidyverse)\nread_csv(fn)$ cat data.csv\ngene,sampleA,sampleB,sampleC\ng1,1,35,20\ng2,32,56,99\ng3,392,583,444\ng4,39853,16288,66928\n$ Rscript inspect_csv.R data.csv\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n✔ tibble  3.1.6     ✔ dplyr   1.0.7\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): gene\ndbl (3): sampleA, sampleB, sampleC\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message   .\n# A tibble: 4 × 4\n  gene  sampleA sampleB sampleC\n  <chr>   <dbl>   <dbl>   <dbl>\n1 g1          1      35      20\n2 g2         32      56      99\n3 g3        392     583     444\n4 g4      39853   16288   66928\nargs <- commandArgs(trailingOnly=TRUE)\nif(length(args) != 2) {\n      cat(\"Usage: head.R <filename> <N>\\n\")\n  cat(\"Provide exactly two arguments: a CSV filename and an integer N\\n\")\n    quit(save=\"no\", status=1)\n}\n\n# read in arguments\nfn <- args[1]\nn <- as.integer(args[2])\n\n# the csv output will include a header row, so reduce n by 1\nn <- n-1\n\n# suppressMessages() prevents messages like library loading text from being printed to the screen\nsuppressMessages(\n    {\n        library(tidyverse, quietly=TRUE)\n        read_csv(fn) %>%\n            slice_head(n=n) %>%\n            write_csv(stdout())\n    }\n)$ Rscript head.R data.csv 3\ngene,sampleA,sampleB,sampleC\ng1,1,35,20\ng2,32,56,99\nlibrary(argparser, quietly=TRUE)\nparser <- arg_parser(\"R implementation of GNU coreutils head command\")\nparser <- add_argument(parser, \"filename\", help=\"file to print lines from\")\nparser <- add_argument(parser, \"-n\", help=\"number of lines to print\", type=\"numeric\", default=10)\nparser <- parse_args(parser, c(\"-n\",3,\"data.csv\"))\nprint(paste(\"printing from file:\",parser$filename))## [1] \"printing from file: data.csv\"\nprint(paste(\"printing top n:\",parser$n))## [1] \"printing top n: 3\"\nlibrary(argparser, quietly=TRUE)\n\n# instantiate parser\nparser <- arg_parser(\"R implementation of GNU coreutils head command\")\n\n# add arguments\nparser <- add_argument(\n  parser,\n  arg=\"filename\",\n  help=\"file to print lines from\"\n)\nparser <- add_argument(\n  parser,\n  arg=\"--top\",\n  help=\"number of lines to print\",\n  type=\"numeric\",\n  default=10,\n  short='-n'\n)\n\nargs <- parse_args(parser)\n\nfn <- args$filename\n\n# the csv output will include a header row, so reduce n by 1\nn <- args$top-1\n\nsuppressMessages(\n    {\n        library(tidyverse, quietly=TRUE)\n        read_csv(fn) %>%\n            slice_head(n=n) %>%\n            write_csv(stdout())\n    }\n)$ Rscript head.R -n 3 data.csv\ngene,sampleA,sampleB,sampleC\ng1,1,35,20\ng2,32,56,99$ Rscript head.R -h\nusage: head.R [--] [--help] [--opts OPTS] [--top TOP] filename\n\nR implementation of GNU coreutils head command\n\npositional arguments:\n  filename    file to print lines from\n\nflags:\n  -h, --help  show this help message and exit\n\noptional arguments:\n  -x, --opts  RDS file containing argument values\n  -n, --top   number of lines to print [default: 10]"},{"path":"engineering.html","id":"parallel-processing","chapter":"9 EngineeRing","heading":"9.3 Parallel Processing","text":"modern computers multiple processing CPU cores, e.g. 4-core \n8-core processor. principally means computers can execute\nmultiple processes simultaneously thereby dividing wall clock time required\nrun computations one core machine factor equal number \ncores machine . can meaningful performance improvement,\ne.g. 8 core machine reduce running time computation \nweek less day. data centers possess compute nodes 28 even\n64 cores, meaning computation normally run two months \ncomplete single day cores used! Processes running \ntime different cores said running parallel, \ncomputation parallelized runs serially.","code":""},{"path":"engineering.html","id":"brief-introduction-to-parallelization","chapter":"9 EngineeRing","heading":"9.3.1 Brief Introduction to Parallelization","text":"computations can parallelized, can made parallel \nusing sophisticated mathematical models algorithms. However, certain classes\nproblems pleasingly\nparallel, meaning \nstructure makes easy trivial divide parts can run \nparallel. general, “parallelizeable” computation depends \nsubparts computation relate one another.Consider problem mapping high throughput sequencing\nreads reference genome. task \nidentify location(s) genome read matches, locations\none read influence locations another read; independent\n. Since independent, principle can done \nparallel, meaning , priniple, every one millions reads\naligned genome time. mean wall\nclock time required align reads multi-million read dataset \ntake long takes align single read. makes short read alignment\npleasingly parallel problem. Unfortunately, practice technical\nconstraints preventing increase performance, modern alignment\nprograms like bwa \nSTAR exploit inherently parallel\nstructure improve performance using multiple cores time.general, splitting computation pieces can run parallel \nalways lead performance improvements. several ways computations\nconstrained based aspects algorithm program require \ntime run. Briefly, algorithm can :compute-bound - computations take largest amount timememory-bound - amount main memory (.e. RAM) machine determines quickly computation can completeinput/output (IO)-bound - writing hard disk takes timenetwork-bound - transfering data network, usually processes running parallel, takes timeThese concepts covered field high-performance computing \nbeyond scope class. However, purposes section, \nproblems concerned parallelizing computations \ncompute-bound computations.Parallelism can attained multiprocessor distributed\ncomputing level. distributed computing, different subtasks \nparallelizable computation executed different physical computers. \nsubtasks possibly parallelized using multiple cores \nsystem. cluster computing cloud environments, enormous performance\nimprovements can attained utilizing levels parallelization. \nexample, submit 100 jobs via qsub compute cluster run\ntime, job uses 16 cores local machine, amounts\n1,600x speedup. parallelism reduce computation taking 4\nyears run single day!","code":""},{"path":"engineering.html","id":"apply-and-friends-are-pleasingly-parallel","chapter":"9 EngineeRing","heading":"9.3.2 apply and Friends Are Pleasingly Parallel","text":"R, loops efficiently expressed using apply\nfunction variants lapply vapply. Recall\nfunctions accept function collection inputs, usually \nlist. apply function iterates input collection executes \nfunction . general, inputs assumed independent,\nfunction outside dependencies. precisely \npattern needed pleasantly parallel tasks. general, iteration using \napply function can parallelized R.","code":""},{"path":"engineering.html","id":"the-parallel-package","chapter":"9 EngineeRing","heading":"9.3.3 The parallel package","text":"R can leverage multiple core architectures execute processes parallel \n[parallel package], installed base R package. main\nfunction package enables parallelism \nmclapply(),\nmulticore apply. mclapply accepts arguments \nlapply additional starting mc.. argument mclapply\nenables parallelism mc.cores, specifies number cores \nuse executing tasks parallel.mclapply available Windows, Mac linux.Consider following script mclapply.R runs function prints \ninput current time waits (.e. sleeps) three seconds:Note number cores used passed command line argument. \nrun script one core, can see functions run serially:instead run script three cores, can see printed times\nfunctions executed groups 3:Finally, use 10 cores (machine using least 10\ncores), functions execute simultaneously:introduction parallelization R likely sufficient \nsimple parallel tasks, many details won’t cover . \n-depth explanation parallelism R works refer links \nread box .Parallel Computation section R Programming Data Science, Roger PengQuick Intro Parallel Computing R, National Center Ecological Analysis & Synthesismclapply documentationParallel processing R Windows, Water Programming Blog, Reed Lab, Cornell","code":"\nlibrary(parallel)\n\nargs <- commandArgs(trailingOnly=TRUE)\ncores <- as.integer(args[1])\n\nret <- mclapply(\n    1:10,\n    function(x) {\n         print(paste(Sys.time(),'process',x))\n         Sys.sleep(3)\n    },\n    mc.cores=cores\n)$ Rscript mclapply.R 1\n[1] \"2022-03-23 20:33:09 process 1\"\n[1] \"2022-03-23 20:33:12 process 2\"\n[1] \"2022-03-23 20:33:15 process 3\"\n[1] \"2022-03-23 20:33:18 process 4\"\n[1] \"2022-03-23 20:33:21 process 5\"\n[1] \"2022-03-23 20:33:24 process 6\"\n[1] \"2022-03-23 20:33:27 process 7\"\n[1] \"2022-03-23 20:33:30 process 8\"\n[1] \"2022-03-23 20:33:33 process 9\"\n[1] \"2022-03-23 20:33:36 process 10\"\n$ Rscript mclapply.R 3\n[1] \"2022-03-23 20:29:56 process 1\"\n[1] \"2022-03-23 20:29:56 process 2\"\n[1] \"2022-03-23 20:29:56 process 3\"\n[1] \"2022-03-23 20:29:59 process 4\"\n[1] \"2022-03-23 20:29:59 process 5\"\n[1] \"2022-03-23 20:29:59 process 6\"\n[1] \"2022-03-23 20:30:02 process 7\"\n[1] \"2022-03-23 20:30:02 process 8\"\n[1] \"2022-03-23 20:30:02 process 9\"\n[1] \"2022-03-23 20:30:05 process 10\"- Rscript mclapply.R 10\n[1] \"2022-03-23 20:34:59 process 1\"\n[1] \"2022-03-23 20:34:59 process 2\"\n[1] \"2022-03-23 20:34:59 process 3\"\n[1] \"2022-03-23 20:34:59 process 4\"\n[1] \"2022-03-23 20:34:59 process 5\"\n[1] \"2022-03-23 20:34:59 process 6\"\n[1] \"2022-03-23 20:34:59 process 7\"\n[1] \"2022-03-23 20:34:59 process 8\"\n[1] \"2022-03-23 20:34:59 process 9\"\n[1] \"2022-03-23 20:34:59 process 10\""},{"path":"engineering.html","id":"object-oriented-programming-in-r","chapter":"9 EngineeRing","heading":"9.4 Object Oriented Programming in R","text":"Object-orietned programming\n(OOP) programming paradigm encapsulates related code data \nconceptual “objects”. objects generally used organize code make\neasier write passing encapsulated data code together \nunit. several different styles OOP, programming language \nimplement one style, none .Although R primarily functional programming\nlanguage support OOP several different\nways. Hadley Wickham written excellent book chapter OOP \nR thus replicated .OOP R Hadley Wickam","code":""},{"path":"engineering.html","id":"building-r-packages","chapter":"9 EngineeRing","heading":"9.5 Building R Packages","text":"Packages fundamental unit reproducible code R. time,\npractitioners simply use packages provided others.\nHowever, sometimes, developed methodology others might find\nuseful, may advantageous write R packages share \ncommunity. Writing R package involves creating directory \nspecific structure adding code, documentation, package metadata \nway allows distributed.Hadley Wickham Jenny Bryan written excellent book write\nR packages. ever need write R package,\nplace need go.R packages Hadley Wickam Jenny Bryan","code":""},{"path":"rshiny.html","id":"rshiny","chapter":"10 RShiny","heading":"10 RShiny","text":"","code":""},{"path":"rshiny.html","id":"rshiny-introduction","chapter":"10 RShiny","heading":"10.1 RShiny Introduction","text":"R Shiny R package enables seamless creation interactive web applications using exclusively R. quite feat since R much web programming language like JavaScript. allows data scientists like us, much comfortable creating scripts analyses, leverage skills make attractive, functional web apps share collaborators beyond.basic shiny app inside RStudioShiny much easier use thanks built-shorthands common web elements. paragraph tab, used normal text, can types p(\"Text\") instead HTML’s clunkier <p>Text<\/p>. applies even complicated objects like buttons, sliders, outputting plots tables.chapter bring basics constructing app, nature reactive programming. still R, still mostly feel like R, Shiny veneer utility turns average data scientist web developer fully equipped impress lab peer.","code":""},{"path":"rshiny.html","id":"how-does-a-web-page-work","chapter":"10 RShiny","heading":"10.1.1 How does a web page work?","text":"get nuts bolts Shiny’s usefulness, first outline need library. typical, static (meaning every time visit ) webpage three components:\n1. HTML - hypertext markup language, bones web page\n2. CSS - cascading style sheets, design aesthetic portions\n3. JavaScript - programming language made web, used making web pages interactive adaptive.Normally, elements create separately different files, placed onto web server, delivered user load web page. ’re probably one three one sense another, can see work opening browser’s console.HTML lives inside Google.comDealing HTML tags (things look like : <p>Woa<\/p>) far much work, can use Shiny put together bones website, can easily customize CSS JavaScript elements well, give applications fully configurable R-like feel users hungry pretty data science bioinformatics.","code":""},{"path":"rshiny.html","id":"getting-started-and-development","chapter":"10 RShiny","heading":"10.1.1.1 Getting started and development","text":"Getting start Shiny easy whether ’re SCC computer. Simple select new project, choose new directory, select Shiny option seen .New Shiny AppFurther reading:\n- Parts chapter adapted Hadley Wickham’s Book Shiny\n- Official R Shiny Cheat Sheet\n- widgets unique outputs: HTML Widgets","code":""},{},{"path":"rshiny.html","id":"application-structure","chapter":"10 RShiny","heading":"10.2 Application Structure","text":"R Shiny applications split two sections: UI server. web development, usually known front end back end. front end portion application deal users. web page, design, login screen, code controls user can see . back end everything user doesn’t see, structure behind web page supports functionality: database, processing data. One Shiny application controls aspects paradigm. UI creates simple web page allows customizable inputs control displays, server manages inputs transforms outputs user can see.many cases front end runs user’s machine (web browser) back end lives another computer, usually server. development Shiny app machine, wish actually publish web page others use application run server communicate user elsewhere.","code":""},{"path":"rshiny.html","id":"a-simple-example","chapter":"10 RShiny","heading":"A simple example","text":"Example appThis simplest Shiny app structure, displays text pass UI (front end) absolutely nothing else (relatable). first line loads R Shiny package, second creates simple user interface text output, line three controls back end (, nothing), final line launches app using objects created lines two three.order build actual content app, ’ll first look UI, user sees.","code":"library(shiny)\nui <- fluidPage(\"Hello, worm!\")\nserver <- function(input, output, session) { }\nshinyApp(ui, server)"},{"path":"rshiny.html","id":"ui-design","chapter":"10 RShiny","heading":"10.2.1 UI Design","text":"Much like rest content course (’m sure ’re sick hearing ), user interfaces user design taught course. innumerable ways people interact software, designing software make least angry possible now focus.R Shiny makes designing user sees fairly easy us. Gone days opening text editor tweak HTML tags, can just use built functions like span() div() organize page.example, copying extra simple app, paste whole text block get new line text:great appearance-wiseBut ’s rather clunky. want include elements ? image, link? can better just big block text next plots. Instead can use p() tags, short paragraph, split things reasonably add convenient customization:Finally, good poetryWe still need include commas end, since tags essentially arguments passed fluidPage() function. case server side object, normal function split line breaks.Much nicer! Shelley tribute page really coming together. Let’s talk specific interactive elements can use actually work user!","code":"\nui <- fluidPage(\n  paste(\"Nothing beside remains. Round the decay\",\n  \"Of that colossal wreck, boundless and bare\",\n  \"The lone and level sands stretch far away.\")\n)\nui <- fluidPage(\n  p(\"Nothing beside remains. Round the decay\"),\n  p(\"Of that colossal wreck, boundless and bare\"),\n  img(src=\"https://coloringhome.com/coloring/RiA/yEj/RiAyEjzKT.gif\",\n      style=\"width:300px;height:200px;\"),\n  p(\"The lone and level sands stretch far away.\"),\n  markdown(\">Percy Bysshe Shelley\")\n)"},{},{"path":"rshiny.html","id":"inputs","chapter":"10 RShiny","heading":"10.2.1.1 Inputs","text":"Built-inputs one major draws Shiny. Instead dealing fickle JavaScript DOM (.k.. get skip half BF768), get type one line R code taken care us!Let’s break function quickly:\n1. inputId =  - input ID string text (unique, numbers special characters) can use inside server function identify input. want know many pyramids user selected, can call input$pyramids. later.\n2. max =, min = - Controls upper lower bounds slider.\n3. label =  - Text display user slider.\n4. value =  - value slider start ?\n5. **step = * - amount slider can move . Useful (less) precise input required.simplest inputsThe vital part, something input functions share, inputId. text used manage bit input data server function, connection user app manages .generates multiple inputs can get results using input$birthday, input$photo, input$button.couple neat inputsThe results file upload nested. ask CSV, needed load read.csv(), need use input$csv$datapath point R uploaded object.better idea common useful input options, see Shiny docs cheat sheet. additional, cooler, fancier input options try shinyWidgets::shinyWidgetsGallery().","code":"\nui <- fluidPage(\n  sliderInput(inputId = \"pyramids\", min = 0, max = 10,\n              label = \"How many pyramids are there?\", value = 0, step = 1)\n)\nui <- fluidPage(\n  dateInput(\"birthday\", \"When is your birthday?\"),\n  fileInput(\"photo\", paste0(\"Please upload an embarassing photo of you at the \",\n                            \"Christmas party\")),\n  radioButtons(\"button\", \"Which is your favorite?\",\n               choices = c(\"Fall\", \"Winter\", \"Spring\", \"Pyramids\"))\n)"},{"path":"rshiny.html","id":"outputs","chapter":"10 RShiny","heading":"10.2.1.2 Outputs","text":"Now beautiful UI page takes user input can figure page put . part also relatively easy, every kind output corresponding output function. want insert base plot ggplot, use plotOutput(). table data frame tibble go tableOutput().One advanced option use uiOutput() place input options HTML back front end. can used create nice effects, inserting results one option selected spawning follow question option. ’s small example:first example server function…anything…’re gonna ignore exactly ’s anything can comment uiOutput(). Based -else statement line 7, follow question changes. can used show/hide responses users. user hits check box indicating ’re veteran, can insert questions veteran status info.works way better RStudioWe figure whole server thing detail made page look pretty possible.","code":"\nlibrary(shiny)\nui <- fluidPage(radioButtons(\"button\", \"Peanut butter or jelly?\",\n                             c(\"Peanut butter\", \"Jelly\")),\n                uiOutput(\"choice\"))\nserver <- function(input, output, session) {\n  output$choice <- renderUI({\n    if (input$button == \"Peanut butter\") {\n      radioButtons(\"pbChoice\", \"PB is great. Bread or english muffin?\",\n                   c(\"Bread\", \"English Muffin\"))\n    } else {\n      radioButtons(\"jellychoice\", \"What kind of jelly?\",\n        c(\"Strawberry\", \"Raspberry\", \"Toxic sludge\"))\n    }\n  })\n}\nshinyApp(ui, server)"},{"path":"rshiny.html","id":"sidebar-main","chapter":"10 RShiny","heading":"10.2.1.3 Sidebar-Main","text":"Now functions populate page inane content possible, ’re gonna start running issues organization. Even mastery HTML elements, prove difficult fit everything want one blank page:Maybe ’ll stay different hotel.Instead, can use popular Sidebar layout help organize web page. works placing two functions inside UI argument: sidebarPanel() mainPanel(). offers us nice compartmentalization design without demanding many arguments details us put together. useful quick apps don’t need make look good. just need nest two panels inside sidebarLayout() like :Much better.can add elements, like titlePanel(), bring app together. Notice title panel located outside sidebarLayout() call, since don’t want inside sidebar main panel elements. Nesting functions nesting layout.","code":"\nui <- fluidPage(\n  HTML(paste(rep(\"<p>All work and no play makes Jack a dull boy<\/p>\", 1000),\n        collapse = \"\"))\n)\nui <- fluidPage(\n  titlePanel(\"I never get tired of these inane examples.\"),\n  sidebarLayout(\n    sidebarPanel(\n      HTML(paste(rep(\"<p>This is the sidebar<\/p>\", 10), collapse = \"\"))),\n    mainPanel(\n      h1(\"and this is the main panel!\"))\n    ))"},{"path":"rshiny.html","id":"fluidpage","chapter":"10 RShiny","heading":"10.2.1.4 FluidPage","text":"alternative sidebar-main panel layout, little constricting don’t want super big sidebar taking everything. FluidPage (using parent function whole time! Woa!) can allow grid-like customization application.lot ways control height width fluid rows, ’ll look quick example see comically bad example .fun lol.main gist fluidRow() creates new row inside whatever function , can use column function inside row split page. , can use input output functions inside , along whatever normal HTML wish. overall, standard width page 12 unnamed shiny units, keep ensure widths don’t add 12.Splitting page fluidRows.","code":"\nui <- fluidPage(style = 'background-color: #007BA7',\n                fluidRow(style = 'background-color: #F3A712',\n                         p(\"I'm in the first row!\"),\n                         column(style = 'background-color: #A8C686',\n                                width = 4,\n                                p(\"Left columm!\")),\n                         column(style = 'background-color: #DB162F',\n                                width = 8,\n                                p(\"Right column!\"))),\n                fluidRow(style = 'background-color: #E6BCCD',\n                         p(\"I'm in the second row!\"),\n                         column(style = 'background-color: #BEE3DB',\n                                width=6,\n                                p(\"Left, lower\")),\n                         column(style = 'background-color: #9D44B5',\n                                width=6,\n                                p(\"Right, lower!\"))))"},{},{"path":"rshiny.html","id":"server-functionality","chapter":"10 RShiny","heading":"10.2.2 Server Functionality","text":"place front end, can start manipulate server side control run application. interface nothing without back end controls; back end can start write R manipulate data inform user.server function two components worry : input, delivers us information user, output, can pass data objects rendered webpage.isn’t ggplot default theme fine.","code":"\nlibrary(shiny)\nui <- fluidPage(\n  radioButtons(\"radio\", label = \"Number of points\", choices = c(10, 20)),\n  plotOutput(\"plot\")\n)\nserver <- function(input, output, session) {\n  output$plot <- renderPlot({\n    plot(rnorm(input$radio), 1:input$radio)\n  })\n}\nshinyApp(ui, server)"},{"path":"rshiny.html","id":"input","chapter":"10 RShiny","heading":"10.2.2.1 Input","text":"can pick apart teeny example application see inputs translate server function. create radio buttons, assign input id “radio”. application started first choice, 10, selected default. value immediately entered input variable server. user selects different option, new value updated inside input variable.can reference input variable inside server change plot case. select button 20 instead 10, random number points put plot() changes 10 20. variable static, can use multiple times server function.","code":""},{"path":"rshiny.html","id":"output","chapter":"10 RShiny","heading":"10.2.2.2 Output","text":"Finally, server function delivers output back web interface. example, created plotOutput() radio buttons display plot. Inside server function, use output$plot match output ID created plotOutput(). rendered plot inserted part document. process matching IDs mirrors input process.number base outputs types: tables, plots, UI (additional elements), . also additional libraries elements like maps, networks, even 3D objects. DataTables especially useful sharing data users.spent like week trying get work old boss canned project ¯\\(ツ)/¯cool example using package ’ve never tried .think toy like doctor’s ","code":"\nlibrary(shiny)\nlibrary(DT)\nui <- fluidPage(dataTableOutput(\"dt\"))\nserver <- function(input, output, session) {\n  output$dt <- DT::renderDataTable(\n    DT::datatable(iris, extensions = 'Buttons', class = \"display\",\n                  options = list(paging = TRUE, searching = TRUE,\n                                 fixedColumns = TRUE, autoWidth = TRUE,\n                                 ordering = TRUE, dom = 'Bfrtip',\n                                 buttons = c('copy', 'csv'))))\n  }\nshinyApp(ui, server)\nlibrary(shiny)\nlibrary(threejs)\nui <- fluidPage(\n  sliderInput(\"maxz\", \"Height of graph\", 5, 25, 10),\n  scatterplotThreeOutput(\"scatterplot\")\n)\nserver <- function(input, output, session) {\n  output$scatterplot <- renderScatterplotThree({\n    z <- seq(-10, input$maxz, 0.01)\n    x <- cos(z)\n    y <- sin(z)\n    scatterplot3js(x,y,z, color=rainbow(length(z)))\n  })\n}\nshinyApp(ui, server)"},{},{"path":"rshiny.html","id":"traceback","chapter":"10 RShiny","heading":"10.2.2.2.1 Traceback","text":"One essential feature Shiny determining errors crop . may familiarity kind troubleshooting base R, Shiny abstracting R entire application, errors can turn edlritch horrors without compare.One main tool automatic, traceback. shiny app fails specific way, error printed lists application arrived issue. function used listed, reverse order, called. way, can track exactly problem lies.swear useful looksThe application fails three functions beginning.Now know error limited f() function line 13, can one two things:\n1. can use print() statement try deduce failing.\n2. can enable debugger take step step approach code running.Debugging…well…write course based . However, printing often useful quick dirty technique parsing code troubles quickly. sounds like argument problem, input right now input$n, let see looks like print(input$n) print(typeof(input$n)).quick .numeric() fix app right allow plot car data? 🚗 🏎️","code":"\nlibrary(shiny)\n\nf <- function(x) g(x)\ng <- function(x) h(x)\nh <- function(x) x * 2\n\nui <- fluidPage(\n  selectInput(\"n\", \"N\", 1:10),\n  plotOutput(\"plot\")\n)\nserver <- function(input, output, session) {\n  output$plot <- renderPlot({\n    n <- f(input$n)\n    plot(head(cars, n))\n  }, res = 96)\n}\nshinyApp(ui, server)"},{},{"path":"rshiny.html","id":"reactivity","chapter":"10 RShiny","heading":"10.3 Reactivity","text":"one final hurdle understanding Shiny intermediate level. entirety course, probably programming career code written run sequentially. start script, load library, load data, execute functions, return result. ’s never chance code run order designed go one direction. style programming used called imperative programming.Unfortunately, Shiny runs order, can run multiple times. user can start app, trigger one function run, click button change data entirely. applications need flexible enough handle situation, write way used user left looking outdated results.Enter declarative programming, reactivity.Reactivity idea code can react changes made inputs. perfect R Shiny since users constantly getting way changing inputs left right (’s okay want ). Imperative programming telling software go make sandwich. follow prescribed instructions make sandwich every time. ask reactive program, Shiny, make sure sandwich whenever open fridge, maybe change sandwich whenever move sandwich slider.R Shiny lot different levels reactivity, ’ll using one cool idea reactive expression.Say application reads CSV rainfall data spits graph along statistics. load data calculate mean raininess last week . Next, load data run ggplot plot histogram number toads rained certain zip code.learned Shiny far, can processes separately, output$x <- section. mean need write uploading multiple times, also need parse data . may trivial rainfall data spans two rows, application needs parse lot data sudden user stuck looking blank results angry emails just pour inbox (without toads).solve , Shiny way, use reactive expression. somewhat like function runs every time input changes. best part results cached, meaning aren’t re-run unless absolutely must . means can load data plot process without loading anything .Reactivity can used control flow data actions application. can stop plot updating button pressed, ensure user doesn’t see errors waiting every input selected progressing.don’t think teach whole class reactivity Shiny maybe little one two lectures.","code":"\nserver <- function(input, output, session) {\n  dataset <- reactive({\n    read.csv(input$data$datapath, header = TRUE)\n  })\n\n  output$plot <- renderPlot({\n    ggplot(dataset(), aes(x = Toads, y = cm_rain_week))\n  })\n\n  output$stats <- renderTable({\n    csv <- dataset()\n    data.frame(mean = mean(csv$Toads))\n  })\n}"},{"path":"rshiny.html","id":"publishing","chapter":"10 RShiny","heading":"10.4 Publishing","text":"Finally, app ready, need put somewhere. host , ’s hard, want share World Wide Web (www short)?One option using shinyapps.io host completed app cloud. get like hours month free hosting, ’s fun little button RStudio let’s us automatically.Follow sign steps log shinyapps.io use rsconnect setup connection app.","code":""},{},{"path":"communicating-with-r.html","id":"communicating-with-r","chapter":"11 Communicating with R","heading":"11 Communicating with R","text":"SlidesNo matter job , point need communicate \nfindings analysis others. usually entail combination \ndifferent forms communication, including text, tables, visualizations\nlike plots. Depending audience, communicating may also include \ncode describes precisely (ideally,\nreproducibly) analysis. \nchapter describes tools techniques can use create static\nreports help communicate ways.","code":""},{"path":"communicating-with-r.html","id":"rmarkdown-knitr","chapter":"11 Communicating with R","heading":"11.1 RMarkdown & knitr","text":"markup language special\nkind programming language used annotate decorate plain text \ninformation intended formatting structure. syntax markup\nlanguage intended easy read write humans also machine\nreadable, may processed formatting programs different\nformats, e.g. markup text might converted HTML PDF.markdown one markup\nlanguage. markup simple, provides basic formatting syntax default.\nfollowing contains examples markdown syntax:markdown might formatted follows:can emphasize text, really emphasize . Lists pretty easy \nread well:item 1item 2item 3If need enumerated list can :item 1item 2item 3You can easily include links web sites like Google \nimages:image master \nuniverseRefer markdown\ndocumentation complete listing supported markdown syntax.markup languages might recognize:ReStructured Text - markup language used python documentationLaTeX - markup language designed help format mathematical expressionHTML - web markup language (HyperText Markup Language)wikitext - markup language used WikipediaAs name suggests, RMarkdown \nextension markdown works R. important extension \nability include code blocks R markdown formatted text can\nexecuted. enables writing executable reports update results\nwhenever document rerun interspersed explanatory text \ndescriptive elements. reason, RMarkdown files sometimes called\nRMarkdown notebooks, can used record narrative text\nresults, similar traditional lab\nnotebook. RMarkdown files typically\nend .Rmd.RStudio full RMarkdown integration make writing RMarkdown notebooks \neasy. screenshot RMarkdown document loaded RStudio:RMarkdown notebook exampleThe grey lines starting ```{r} define special code\nblock contains R code executed output placed \nblock. run, RStudio show output notebook within \ninterface:RMarkdown notebook example runningNotice now plot rendered document right. code\nwithin block run, plot generated, inserted report. \ncase, notebook created HTML document RStudio knows \ndisplay, also opened standard web browser.blocks standard R syntax, instead understood processed\nknitr R package. name suggests, \nprocess generating report RMarkdown document called “knitting”;\nask RStudio knit RMarkdown report.RMarkdown documents can knitted many different formats, including HTML,\nPDF, Microsoft Word, even slide presentation\nformats. exporting HTML,\nreport may include interactive\nelements including plots,\ncollapsible sections, code blocks, maps.RMarkdown documents can also written accept\nparameters. means can\nwrite single RMarkdown document can run different inputs.\nReports standardized analytical pipelines, often implemented \nhigh throughput sequencing cores, can thus generated trivially new\ndatasets generated without writing additional code.RMarkdown - Getting StartedRMarkdown package documentation guideLiterate programming","code":"You can *emphasize* text, or **really emphasize it**. Lists are pretty easy to\nread as well:\n\n* item 1\n* item 2\n* item 3\n\nIf you need an enumerated list you can do that too:\n\n1. item 1\n2. item 2\n3. item 3\n\nYou can easily include links to web sites like [Google](http://google.com) and\nimages:\n![an image of a master of the\nuniverse](https://upload.wikimedia.org/wikipedia/commons/b/bc/Juvenile_Ragdoll.jpg){width=50%}"},{"path":"communicating-with-r.html","id":"bookdown","chapter":"11 Communicating with R","heading":"11.2 bookdown","text":"Another R package processed RMarkdown documents \nbookdown. name suggests, package \ndesigned write books using RMarkdown. book reading written\nRMarkdown generated using bookdown! can look source\ncode GitHub.","code":""},{"path":"contribution-guide.html","id":"contribution-guide","chapter":"12 Contribution Guide","heading":"12 Contribution Guide","text":"page contains conventions tips writing material book.","code":""},{"path":"contribution-guide.html","id":"custom-blocks","chapter":"12 Contribution Guide","heading":"12.1 Custom blocks","text":"number custom blocks can used highlighting salient\ninformation throughout text. code inside corresponding block \ncan used insert different standout boxes:","code":"::: {.box .tldr}\nThis is a tl;dr\n:::::: {.box .note}\nThis is a note\n:::::: {.box .hint}\nThis is a hint\n:::::: {.box .important}\nThis is an important\n:::::: {.box .warning}\nThis is a warning\n:::"},{"path":"starting-an-assignment.html","id":"starting-an-assignment","chapter":"Starting an Assignment","heading":"Starting an Assignment","text":":^) good luckWhen starting project small number steps get things set \njust right order smooth seamless experience. \nabsolutely possible complete projects machine \ninstallation R RStudio, easier faster work \nBU’s SCC. experience straightforward basically identical \n.","code":""},{"path":"starting-an-assignment.html","id":"starting-your-own-git-repository-and-cloning-it","chapter":"Starting an Assignment","heading":"Starting your own git repository and cloning it","text":"development assignments used git github.com\nmanage updates allow easy sharing. can now pass \nrepositories (repos) along using GitHub classroom can follow \nsteps within.","code":""},{"path":"starting-an-assignment.html","id":"assignment-setup-tutorial","chapter":"Starting an Assignment","heading":"Assignment setup tutorial","text":"Every assignment made available via GitHub\nClassroom link.instructor give assignment link assignment. Access\nlink web browser. may need log GitHub. bring\npage like one:\n\nStep 1\ninstructor give assignment link assignment. Access\nlink web browser. may need log GitHub. bring\npage like one:Step 1Click “Accept assignment”, see confirmation page like\none:\n\nStep 2\nClick “Accept assignment”, see confirmation page like\none:Step 2Navigate GitHub BF591-R Organization\npage. see link clone repo well pinned\ntemplate repos.\n\nMake sure find repo ends GitHub\nusername! repos tag “Private” next name. \nclone repositories labeled “Public template”!\n\nassignment repo look something like following:\n\nStep 3\nNavigate GitHub BF591-R Organization\npage. see link clone repo well pinned\ntemplate repos.Make sure find repo ends GitHub\nusername! repos tag “Private” next name. \nclone repositories labeled “Public template”!assignment repo look something like following:Step 3When click link repo, see something like :\n\nStep 4\nclick link repo, see something like :Step 4Inside repo, click big g r e e n\nbutton copy HTTPS link repository.\n\nStep 5\n\nUse link create project RStudio, described next.Inside repo, click big g r e e n\nbutton copy HTTPS link repository.Step 5Use link create project RStudio, described next.Login https://scc-ondemand.bu.edu/. Start \ninteractive R session (Interactive Apps > RStudio Server), use default\nsettings increase hours like 12 something.\n\nAlternatively, may run RStudio personal computer, beware \ninstructions tutorial may exactly see. \nneed install git machine. guide\nLogin https://scc-ondemand.bu.edu/. Start \ninteractive R session (Interactive Apps > RStudio Server), use default\nsettings increase hours like 12 something.Alternatively, may run RStudio personal computer, beware \ninstructions tutorial may exactly see. \nneed install git machine. guideOnce session loaded login , select “New Project”\nbutton “Edit” button.\nsession loaded login , select “New Project”\nbutton “Edit” button.prompt open, select Version Control > git, paste repo’s\nurl first prompt. Feel free rename project. can change\nfolder setup SCC, find ~/Documents \n~/Documents/BF591 good home organizing various projects.\nprompt open, select Version Control > git, paste repo’s\nurl first prompt. Feel free rename project. can change\nfolder setup SCC, find ~/Documents \n~/Documents/BF591 good home organizing various projects.Follow \ninstructions\ncreate GitHub Personal Access Token.\nRStudio prompts username, enter GitHub username.\nprompted password, enter Personal Access\nToken just created.Follow \ninstructions\ncreate GitHub Personal Access Token.RStudio prompts username, enter GitHub username.prompted password, enter Personal Access\nToken just created.R download repository contents, add .RProj file \ndirectory. RProject file useful switching R assignments\nRStudio. context menu top right RStudio interface:\nR download repository contents, add .RProj file \ndirectory. RProject file useful switching R assignments\nRStudio. context menu top right RStudio interface:","code":""},{"path":"starting-an-assignment.html","id":"committing-and-pushing-with-r-and-without","chapter":"Starting an Assignment","heading":"Committing and Pushing with R (and without)","text":"RProject set , now directory SCC cloned \nremote repository GitHub. moment files , since\nmade progress assignment (depressing, know). burst \nproductive energy, start make changes main.R write \nbeautiful, buggy functions. ’re done little bit, next? \nleave SCC probably pretty safe, want\nshare code easily? cataclysm strikes Western Massachusetts? \nchanges gone life lost meaning.prevent cataclysm, must add, commit, push code\nback GitHub.","code":""},{"path":"starting-an-assignment.html","id":"using-rstudio","chapter":"Starting an Assignment","heading":"Using RStudio","text":"Save files want commit, select Git tab top right\nviewer (default).\nSave files want commit, select Git tab top right\nviewer (default).’ll notice files changed listed , along varying\nsymbols. question mark ? means file untracked, M means \ntracked file modified. D means tracked file deleted.\nselect file want add back GitHub, can tick checkbox \nswitch left column. called staging file.\n’ll notice files changed listed , along varying\nsymbols. question mark ? means file untracked, M means \ntracked file modified. D means tracked file deleted.\nselect file want add back GitHub, can tick checkbox \nswitch left column. called staging file.’ve staged files want add, can click Commit\nbutton. brings another window, list files \nstaging actually show differences ’re making (called \ndiff, based GNU utility name). important part \nCommit box. commit series file updates made together,\ntypically solve task. paramount include commit messages,\ncan track ’re work! Just sentence \nusually enough describe changes commit .\n’ve staged files want add, can click Commit\nbutton. brings another window, list files \nstaging actually show differences ’re making (called \ndiff, based GNU utility name). important part \nCommit box. commit series file updates made together,\ntypically solve task. paramount include commit messages,\ncan track ’re work! Just sentence \nusually enough describe changes commit .’ve pressed commit button, ’s left push\ncommit. sends commit GitHub can safely live forever,\nduplicated across world safe cataclysms.’ve pressed commit button, ’s left push\ncommit. sends commit GitHub can safely live forever,\nduplicated across world safe cataclysms.vein, pull button useful GitHub \nupdated recently current SCC directory. can happen ’re\nediting files another computer GitHub.com\ndirectly.","code":""},{"path":"starting-an-assignment.html","id":"using-the-command-line","chapter":"Starting an Assignment","heading":"Using the command line","text":"kind lot clicking funny little menus, doctor says \nhigh risk repetitive stress injury, can’t just use keyboard\ninstead? Yes, course. ’ll use terminal, also built-\nRStudio really just emulates standard bash terminal use\ninteract SCC normally.Click terminal tab bottom layout (default).\nClick terminal tab bottom layout (default).Marvel wonderful monospace font rest hands upon keyboard.\nsee directory , can type git status see files\nchanged. Wow! Looks like lot stuff deal . ’s\nnice git command offers lot helpful tips getting way.\nMarvel wonderful monospace font rest hands upon keyboard.\nsee directory , can type git status see files\nchanged. Wow! Looks like lot stuff deal . ’s\nnice git command offers lot helpful tips getting way.want add files, really don’t care many \ngoing use git add *, basically adds everything can found \ncurrent directory. wanted one two things, specify git    add file1.txt file2.txt leave files alone.want add files, really don’t care many \ngoing use git add *, basically adds everything can found \ncurrent directory. wanted one two things, specify git    add file1.txt file2.txt leave files alone.added (removed git rm file1.txt) files want commit,\nsimply use git commit -m \"Added lot content tutorials! create\ncommit add important message. Messages mandatory \nslack descriptive regret come back later \nfigure God’s name day.added (removed git rm file1.txt) files want commit,\nsimply use git commit -m \"Added lot content tutorials! create\ncommit add important message. Messages mandatory \nslack descriptive regret come back later \nfigure God’s name day.Finally, commit done files added, simply say git push send\nwonderful code back GitHub, can sit back marvel \nhandiwork.Finally, commit done files added, simply say git push send\nwonderful code back GitHub, can sit back marvel \nhandiwork.Put simply, three commands git add *, git commit -m \"message\", git push save changes code keep repository date.\nEasier clicking around RStudio ways super valid!","code":""},{"path":"assignment-overview.html","id":"assignment-overview","chapter":"Assignment Overview","heading":"Assignment Overview","text":"assignment similar format follows similar workflow. \nfollowing figure describes files repos interact.assignment repo following files within:reference_report.html - completed “report” assignment.\nendeavoring replicate completing assignment,\nroom creative differences elements like plotting.reference_report.html - completed “report” assignment.\nendeavoring replicate completing assignment,\nroom creative differences elements like plotting.main.R - main R script assignment, \nlot programming. script contains function definitions\ndescriptions, functions empty. Complete functions \ndescribed script ensure pass tests.main.R - main R script assignment, \nlot programming. script contains function definitions\ndescriptions, functions empty. Complete functions \ndescribed script ensure pass tests.README.md - Every repository README. Usually contains\nuseful information installation, usage, licensing. case, \nhelpful links back assignment instructions.README.md - Every repository README. Usually contains\nuseful information installation, usage, licensing. case, \nhelpful links back assignment instructions.report.Rmd - Another empty file, markdown script source() \nmain.R script, loading functions environment. \nempty code blocks throughout document, use functions developed \nmain.R replicate figures match captions seen \nfinalized_report.pdf.report.Rmd - Another empty file, markdown script source() \nmain.R script, loading functions environment. \nempty code blocks throughout document, use functions developed \nmain.R replicate figures match captions seen \nfinalized_report.pdf.test_main.R - completed testing script produced us help\nquickly determine code running. file sources\nmain.R file, meaning runs completion stores functions\nvariables active environment. runs tests ensure \nfunctions working predictable way. Tests incredibly valuable\ntools, potentially one strongest programmer’s toolkit.test_main.R - completed testing script produced us help\nquickly determine code running. file sources\nmain.R file, meaning runs completion stores functions\nvariables active environment. runs tests ensure \nfunctions working predictable way. Tests incredibly valuable\ntools, potentially one strongest programmer’s toolkit.assignments beyond assignment 1 complementary guide hosted \nwebsite page. go background functions tests work\nuse ’re feeling stuck like context.","code":"├── reference_report.html\n├── main.R\n├── README.md\n├── report.Rmd\n└── test_main.R"},{"path":"assignment-0.html","id":"assignment-0","chapter":"Assignment 0","heading":"Assignment 0","text":"","code":""},{"path":"assignment-0.html","id":"problem-statement","chapter":"Assignment 0","heading":"Problem Statement","text":"“assignment” walk thruogh getting RStudio environment\nconnected GitHub can later assignments. coding \nassignment.","code":""},{"path":"assignment-0.html","id":"learning-objectives-and-skill-list","chapter":"Assignment 0","heading":"Learning Objectives and Skill List","text":"Run RStudio, cluster personal computerCreate GitHub account requiredConnect RStudio GitHubgit clone Assignment 0 repo\nRStudio using provided GitHub Classroom link","code":""},{"path":"assignment-0.html","id":"instructions","chapter":"Assignment 0","heading":"Instructions","text":"Follow instructions Starting Assignment page GitHub\nClassroom link given class communication.never used git , follow instructions make first\ncommit push GitHub! successfully cloned GitHub\nClassroom repo RStudio:Make edit README.md file, instance write name \npoem like, whateverStage (.e. add) changeCommit brief commit message, e.g. “added name readme”Push GitHubConfirm changes examining repo GitHub web browser","code":""},{"path":"assignment-0.html","id":"hints","chapter":"Assignment 0","heading":"Hints","text":"add needed","code":""},{"path":"assignment-1.html","id":"assignment-1","chapter":"Assignment 1","heading":"Assignment 1","text":"","code":""},{"path":"assignment-1.html","id":"problem-statement-1","chapter":"Assignment 1","heading":"Problem Statement","text":"assignment focus basic functions R emphasis \ntidyverse implementations. tidyverse collection packages, pioneered\nHadley Wickham RStudio, looks standardize procedures,\nfunctionality, syntax R.gain familiarity R, working microarray dataset \ncontains gene probe expression data various samples collected cancer\npatients. bioinformatics, common multiple datasets \ndifferent modes data (.e. microarray expression data kept separate\nclinical data detailing samples). get opportunity work\ndatasets, required cross reference two.","code":""},{"path":"assignment-1.html","id":"learning-objectives-and-skill-list-1","chapter":"Assignment 1","heading":"Learning Objectives and Skill List","text":"Install various packages needed analysisLoad DataGain familiarity common tidyverse operations groupby(),\nmutate, summarize.Create small plot display resultsUtilize R Markdown create attractive format sharing data.","code":""},{"path":"assignment-1.html","id":"instructions-1","chapter":"Assignment 1","heading":"Instructions","text":"main focus assignment installing packages, manipulating data,\nsummarizing important statistics across samples features (genes). \nempty project repo can found :https://github.com/bu-bioinfo/bf591-assignment-1The project laid :step assignment explained R markdown file, report.Rmd.\nfind list tasks explicity implement functions \nempty main.R script. main.R script contains skeletons function\n’ll need implement, explaining function , parameters\nexpects receive, type output expected returned. \nreference report, reference_report.html also provided. Assuming successfully\nimplement functions main.R, generated report look\nidentical information displayed reference_report.html. way,\ncan use reference_report.html guide determine correctly\nimplementing functions.suggested workflow developing checking code \nassignment:main.R contains function definitions, including signature descriptions, \nnumber functions, bodies functions currently blankreport.Rmd code chunks call functions defined main.R - \nneed write anything Rmd file (may)task read function descriptions text Rmarkdown\ndocument fill function bodies produce desired behavior \nmain.RYou can test work executing individual code chunks report.Rmd \ncomparing output example compiled report repoIn workflow, go back forth developing code main.R\nrunning code chunks report.RmdWhen developed function bodies functions executed\ncode chunks report successfully, able knit \nentire report","code":"main.R\nreport.Rmd\nreference_report.html"},{"path":"assignment-1.html","id":"hints-1","chapter":"Assignment 1","heading":"Hints","text":"developing period_to_underscore() function, might find \nstringr::str_replace_all()\nfunction helpful. pattern argument functions interpreted \nregular expression “regex” short. regular\nexpression sequence characters (.e. string) written language\ndescribes patterns text, similar “Find Replace” operations \nword processing software, powerful flexible kinds \npatterns can detect. characters special meaning regular\nexpressions, one . character. order identify literal\nperiod character like trying , must instruct regular\nexpression either escaping character \\\\. place \nrange [.]. Either two methods work replace literal .\n_. See section Regular expressions information.getting type conversion errors loading expression\nCSV file, check make sure aren’t supplying column names. \nCSV file column headers already, supplying cause first\nline read data. Since first row character values \ncase, values columns coerced characters \nwell, instead reading numbers.","code":""},{"path":"assignment-2.html","id":"assignment-2","chapter":"Assignment 2","heading":"Assignment 2","text":"","code":""},{"path":"assignment-2.html","id":"problem-statement-2","chapter":"Assignment 2","heading":"Problem Statement","text":"Arranging structure data inputs vital using R, creating\ncorrect environment analysis installing packages. also\nimportant share data results easily using R markdown.","code":""},{"path":"assignment-2.html","id":"learning-objectives","chapter":"Assignment 2","heading":"Learning Objectives","text":"Install various packages needed analysisLoad data, filter data, retrieve HGNC ids dataCreate small plot display resultsUtilize R Markdown create attractive format sharing data.","code":""},{"path":"assignment-2.html","id":"skill-list","chapter":"Assignment 2","heading":"Skill List","text":"utilize R markdown create report data analysisInstalling loading packages RUtilizing Bioconductor equate affy ids HGNC ids (gene names)","code":""},{"path":"assignment-2.html","id":"instructions-2","chapter":"Assignment 2","heading":"Instructions","text":"main focus assignment installing packages, manipulating data,\nplotting manipulated data. borrowing CSV \nexpression data BF528’s Project 1, available\n.\nempty project repo can found :https://github.com/bu-bioinfo/bf591-assignment-2The project laid :skeleton functions need complete main.R. Tests \npre-written test code help ensure running correctly, \ntest_main.R. Finally, also introducing concept R\nMarkdown, assignment report.Rmd. document goes\ngreater detail, :Complete functions main.R use\ntestthat:test_file('test_main.R') ensure work correctly.Read R Markdown file complete section called “Assignment”. \n, can source('main.R') bring functions wrote \nstep one.Finally, annotate functions wrote Knit R Markdown report,\ncomplete additional comments code execution.page go detail functions associated tests\nwork.","code":"main.R\ntest_main.R\nreport.Rmd"},{"path":"assignment-2.html","id":"function-details","chapter":"Assignment 2","heading":"Function Details","text":"","code":""},{"path":"assignment-2.html","id":"bioconductor","chapter":"Assignment 2","heading":"1. Bioconductor","text":"many useful R packages can loaded \nCRAN using install.packages() syntax, lot\nspecifically bioinformatics packages exclusively released \nBioconductor. assignment \nneed package called\nbiomaRt.\nR programmers fancy clever, Rs show lot.Naturally, want load packages beginning script \ncode write beneath can access runs. However, user \nalready package installed don’t want waste time\ninstalling . function require() can help us avoid unnecessary\ninstallation time help us develop faster. Bioconductor link \nexample method.section untested.","code":""},{"path":"assignment-2.html","id":"load_expression","chapter":"Assignment 2","heading":"2. load_expression()","text":"Perhaps integral part using many data wrangling abilities R\nactually entering data R environment. many ways\nR, ultimately want data tibble, means\ncurrent form CSV make R angry attempt load .\ntibbles don’t support row names well, first column\ndata doesn’t name. Try use load_expression() data load\ndata filename parameter return tibble information. \ncalled firs column “probeids”.","code":""},{"path":"assignment-2.html","id":"tests","chapter":"Assignment 2","heading":"Tests","text":"tests function :test uses load_expression() write store returned tibble result_tib.test using file data input , may always case.test compares dimensions result, expects 54,675 rows\n36 columns. dimensions input CSV. also checks \nconfirm tibble object (tibbles better dataframes).","code":"\ntest_that(\"loading csv correctly\", {\n  result_tib <- load_expression(\"/project/bf528/project_1/data/example_intensity_data.csv\")\n  expect_equal(dim(result_tib), c(54675, 36))\n  expect_true(is_tibble(result_tib))\n})"},{"path":"assignment-2.html","id":"filter_15","chapter":"Assignment 2","heading":"3. filter_15()","text":"order filter numerous rows data, introduce \nfunction filters probe IDs tibble data stored . want\ncapture probes suitably high level expression, \nsetting log2(15) cutoff expression level. keep row \n15% values row exceed log2(15) (3.9). Since may want\nexamine probe IDs find, function simple returns values \nprobe IDs (column 1) instead returning entire tibble.function presents important concept R: using built-ins speed \ncode. Built-ins functions packages optimized process\ndata certain way. Since ’re looking row table, \nsimply use loop iterate one row time. slow, though, \nfunction might take 5-10 seconds run (long time program like\n!). Instead, use function like apply() lapply() filter\nevery row . solution takes mere moments.","code":""},{"path":"assignment-2.html","id":"tests-1","chapter":"Assignment 2","heading":"Tests","text":"order test function, create small sample tibble expression\ndata containing seven samples four IDs. Two rows \n15% values exceeding log2(15), two . test\nensures filter_15() selects correct rows. Creating small sample\ntable like can useful testing code since don’t\nneed look large amount data see ’s working correctly .","code":"\nlibrary(tibble)\ntest_that(\"the correct rows are filtered out in filter_15()\", {\n  test_tib <- tibble(probeids=c('1_s_at', '2_s_at', '3_s_at', '4_s_at'),\n                     GSM1=c(1.0, 3.95, 4.05, 0.5),\n                     GSM2=rep(1.6, 4),\n                     GSM3=rep(2.5, 4),\n                     GSM4=rep(3.99, 4),\n                     GSM5=rep(3.0, 4),\n                     GSM6=rep(1.0, 4),\n                     GSM7=rep(0.5, 4))\n  expect_equal(pull(filter_15(test_tib)), c(\"2_s_at\", \"3_s_at\"))\n})"},{"path":"assignment-2.html","id":"affy_to_hgnc","chapter":"Assignment 2","heading":"4. affy_to_hgnc()","text":"important, sometimes painful, part using R. great\nbuilt-package connecting Ensembl (database genomic information\nmany species) called biomaRt. use biomaRt connect \naffymetrix probe IDs recognizable HGNC gene IDs. problem \nbiomaRt depends external API (application program interface) retrieve\ndata, connection sometimes (oftentimes) doesn’t work. may\nnuanced approaches unstable resource like like automatically\nretrying failed connections, best advice time try\nrunning function times doesn’t work first. errors \nclear comes failed connection, know get \nstage likely isn’t code’s fault.build biomaRt query, read documentation section 3\n.\nbiomart use ENSEMBL_MART_ENSEMBL, data set\nhsapiens_gene_ensembl, want find attributes\nc(\"affy_hg_u133_plus_2\", \"hgnc_symbol\"). data filter using\nfilter_15() returns list affy_hg_u133_plus_2 probe IDs, gene\nnames ’re interested stored hgnc_symbol.function return tibble, biomaRt’s getBM() accept\nreturn data.frame. can use dplyr::pull() turn tibble \nsimple character vector, dplyr::as_tibble() go data frame \ntibble.","code":""},{"path":"assignment-2.html","id":"tests-2","chapter":"Assignment 2","heading":"Tests","text":"fun try get biomaRt function connect correctly, \neven fun test . Since failure connect doesn’t indicate \nactual failure code must use try() block order capture \nconnection error.Note try() function part programming called\nerror-handling extends many languages. often expect\nerrors running programs (right now) don’t want shut\nentire operation ’s error can expect. Using try,\nexcept, finally (latter two appearing ) can account \n*issues outside control adapt code change outcome.Using try except replacement writing code doesn’t\ngenerate errors. can avoid error first place, far\nbetter using error-handling.case try() use affy_to_hgnc() connect Ensembl store\nresulting error response. check: “Error” \nthrow warning() testing output. doesn’t stop testing\nhappening, ensures know something isn’t quite right. \nresponse contain error, simply test returned \ncorrect gene symbols random affy probe ID choice.","code":"\ntest_that(\"affy ids can be converted to HGNC names properly using affy_to_hgnc()\", {\n  # biomaRt super buggy so we can try to account for not connecting well\n  response <- try(affy_to_hgnc(tibble('1553551_s_at')), TRUE)\n  if (grepl(\"Error\", response[1])) {\n    expect_warning(warning(\"Could not connect to ENSEMBL.\"))\n  }\n  else {\n    expect_equal(response$hgnc_symbol, c(\"MT-ND1\", \"MT-TI\", \"MT-TM\", \"MT-ND2\"))\n  }\n})"},{"path":"assignment-2.html","id":"reduce_data","chapter":"Assignment 2","heading":"5. reduce_data()","text":"one final step manipulating data plot . \noriginal data, probe IDs associated HGNC symbols, list \ngood gene names bad gene names. reduce_data takes four inputs \nreturns tibble reduced expression data genes \ninterest column describing set genes belongs (“good” \n“bad”). Changing shape data incredibly useful ggplot, \ntidyverse package use plotting. flexibility \nusing ggplot plot data, data \nlong format \ntypically ideal., multiple ways reorganize data way. used \nbase function match() connect probe IDs HGNC IDs \nname_ids. used tibble::add_column() insert new data \ncorrect location. Finally, created two tibbles good bad genes using\n() %% modifier. evaluates true conditions across \nrange data, can pass list genes want select correct\nones. instance:, many ways reshape data (maybe elegant\n!) need data correctly shaped \nreturned.","code":"\nlibrary(tibble)\ntib <- tibble(gene = c(\"gene1\", \"gene2\", \"gene3\", \"gene4\"),\n              affy = c(\"a_s_1\", \"a_s_2\", \"a_s_3\", \"a_s_4\"))\nwhich(tib$gene %in% c(\"gene2\", \"gene3\")) # returns the index of the TRUE rows## [1] 2 3\ntib$affy[which(tib$gene %in% c(\"gene2\", \"gene3\"))] # use [] to get data back## [1] \"a_s_2\" \"a_s_3\""},{"path":"assignment-2.html","id":"tests-3","chapter":"Assignment 2","heading":"Tests","text":"order test function changes tibble, need use tibble. \ncreate test table three rows four columns, enough\nget gist function. simply pass four parameters \nreduce_data() expect create tibble like result. Ensure \noutput column names tests fail. crucial\nsuccess assignment, maintaining correct column names across\nmultiple data transformations important skill.","code":"\ntest_that(\"reduce_data() is correctly changing the size and shape of the tibble\", {\n  t_tibble <- tibble(probeids = c(\"1_s_at\", \"2_s_at\", \"3_s_at\"),\n                     GSM1 = c(9.5, 7.6, 5.5),\n                     GSM2 = c(9.7, 7.2, 2.9),\n                     GSM3 = c(6.9, 4.3, 6.8))\n  names <- tibble(affy_hg_u133_plus_2 = c(\"1_s_at\", \"2_s_at\", \"3_s_at\"),\n                  hgnc_symbol = c(\"A-REAL-GENE\", \"SONIC\", \"UTWPU\"))\n  good <- c(\"A-REAL-GENE\")\n  bad <- c(\"SONIC\")\n  reduce_test <- reduce_data(t_tibble, names, good, bad)\n  result <- tibble(probeids = c(\"1_s_at\", \"2_s_at\"),\n                   hgnc_symbol = c(\"A-REAL-GENE\", \"SONIC\"),\n                   gene_set = c(\"good\", \"bad\"),\n                   GSM1 = c(9.5, 7.6),\n                   GSM2 = c(9.7, 7.2),\n                   GSM3 = c(6.9, 4.3))\n  expect_equal(reduce_test, result)\n})"},{"path":"assignment-3.html","id":"assignment-3","chapter":"Assignment 3","heading":"Assignment 3","text":"","code":""},{"path":"assignment-3.html","id":"problem-statement-3","chapter":"Assignment 3","heading":"Problem Statement","text":"High dimensional data typically time resource-intensive analyze,\ninterpret visualize. Gene expression data (including microarray data) often\nconsists measurements tens thousands known genes every sample.\nLearning methods inspect, reduce display high dimensional data \nnecessary many machine learning bioinformatics problems.","code":""},{"path":"assignment-3.html","id":"learning-objectives-1","chapter":"Assignment 3","heading":"Learning Objectives","text":"Understand basic principles Principal Component Analysis (PCA) hierarchical clustering / heatmapsPerform PCA R evaluate basic outputs PCAGenerate basic clustered heatmaps","code":""},{"path":"assignment-3.html","id":"skill-list-1","chapter":"Assignment 3","heading":"Skill List","text":"R: scale(), transpose t(), prcomp(), heatmap(), ggplot2","code":""},{"path":"assignment-3.html","id":"background-on-microarrays","chapter":"Assignment 3","heading":"Background on Microarrays","text":"microarray consists thousands specifically\ndesigned single stranded DNA sequences affixed bound solid surface\n(glass, nylon, etc.). Extracted RNA DNA samples interest labeled\nfluorescent dyes, hybridized bound probes flowing across\nsurface. Non-hybridized molecules washed away b laser excite\nattached dye produce light detected scanner converted \ndigital image. Finally, image processing transform spot (bound probe)\nnumerical measurement can used infer expression levels \ngenes.","code":""},{"path":"assignment-3.html","id":"background-on-principal-component-analysis","chapter":"Assignment 3","heading":"Background on Principal Component Analysis","text":"Principal component analysis exploratory\ndata analysis technique commonly used reduce dimensionality data \ntrying simultaneously minimize information loss. understand inner\nworkings PCA require -depth review linear algebra beyond\nscope specific assignment. provide several links \nreferences wish . core, PCA creates new\nuncorrelated linear combinations original variables successively\nmaximize variance. Thus, first principle component (PC) represents \ndirection data explains maximal amount variance. second PC\nrepresents direction captures second variance \nforth. can infer, small number PCs capture majority variance\ncontained within dataset, can used lower dimensional\nrepresentation data.","code":""},{"path":"assignment-3.html","id":"marisa-et-al.-gene-expression-classification-of-colon-cancer-into-molecular-subtypes-characterization-validation-and-prognostic-value.-plos-medicine-may-2013.-pmid-23700391","chapter":"Assignment 3","heading":"Marisa et al. Gene Expression Classification of Colon Cancer into Molecular Subtypes: Characterization, Validation, and Prognostic Value. PLoS Medicine, May 2013. PMID: 23700391","text":"example intensity data taken listed publication. , \nauthors proposed use gene expression profiling (microarray\ntechnology) generate robust reproducible classifier identify\nsubtypes colorectal cancer samples. identified six subtypes \ndemonstrated significantly associated distinct molecular pathways \nclinical pathologies. provided sample expression matrix \nnormalized subjected batch correction. use \nexample_intensity_data.csv data matrix going forward.","code":""},{"path":"assignment-3.html","id":"scaling-data-using-r-scale","chapter":"Assignment 3","heading":"Scaling data using R scale()","text":"Oftentimes, necessary analyze plot data contains variables \ndiffer multiple orders magnitude. bioinformatics, extremely\ncommon situation different genes may expressed wildly different levels.\ninstance, typical HTS experiments see later \ncourse, may detect genes counts ranging single digits \ncounts tens thousands. Heatmaps gene expression data \ncommonly transformed standard scaling order improve visualization\nretaining underlying pattern expression samples.One way standardize data scale() function R. Scale\nessentially takes vector values determines mean standard\ndeviation entire vector. Scale() subtract mean \nelement vector divide standard deviation. Z-scores thus\ncorrespond number standard deviations data point \nmean observations (.e. Z-score 0 represents \nvalue equal mean Z-score 1 represents value one standard\ndeviation greater mean value). effect representing \ndata points “scale” preserving pattern profile\ndifferences inherent values across different observations.transpose function t()\nfunction covered textbook course. covered earlier, \nconverts \\(m \\times n\\) matrix \\(n \\times m\\). built-R function\nscale() operates column-wise basis. Transpose matrix \nscaling occurs within genes rather samples. may also use tidyverse\npivot functions perform transpose operation \nprefer.Using ’ve learned course prior assignments, read \nexample_intensity_data.csv proper format. Return scaled matrix.#1: want center scale matrix.Deliverables\n1. example_intensity_data.csv read dataframe.","code":""},{"path":"assignment-3.html","id":"proportion-of-variance-explained","chapter":"Assignment 3","heading":"Proportion of variance explained","text":"mentioned earlier, principal components represent successively maximal\namount variance dataset. often helpful visualize \npercentage total variance explained principal component. Write \nfunction determine proportion variance explained principal\ncomponent. Use results generated function create bar chart\ndisplaying variance explained successive PCs. plot, make \nscatter line plot showing cumulative proportion variance explained \nprincipal component.#1: summary() function R provides top-level information model fitting statistical objects.\n#2: may access standard deviation PCA results object $sdevDeliverables\n1. vector variance explained PC.\n2. tibble labels PC, variance explained \ncumulative variance explained.\n3. barchart proportion individual variance explained \nprincipal component overlayed scatter plot cumulative sum \nvariance explained successive principal component. Label relevant\naxes provide descriptive caption figure.","code":""},{"path":"assignment-3.html","id":"plotting-and-visualization-of-pca","chapter":"Assignment 3","heading":"Plotting and visualization of PCA","text":"typical HTS experiments, PCA common tool used analyze similarity\nsamples determine experimental variable interest\n(genotype, knockout, etc.) represents large source variance. One \ncommon visualization plot first two principal components (\nmay always represent majority variance dataset) \n. Typically, one examine pattern clustering plot see\nsamples grouped together meaningful pattern according important\nexperimental variables. value score sample terms \nprincipal components may found pca results object accessed $x.plotting individual PCs one another, remember different\nprincipal components explain different amounts variance often\ncase scale “importance” across axes \ndifferent.Deliverables\n1. scatterplot PC1 vs PC2 pca_results$x values. Read \nmetadata CSV use annotate sample corresponding\nassignment SixSubtypesClassification made original publication\n(samples belong either c3 c4). Label points plot color \ncorresponding Six Subtypes Classification.","code":""},{"path":"assignment-3.html","id":"hierarchical-clustering-and-heatmaps","chapter":"Assignment 3","heading":"Hierarchical Clustering and Heatmaps","text":"Now ’ve discussed basics PCA, move discuss use\nhierarchical clustering heatmaps. Heatmaps common plot used \nquickly visualize expression pattern many relevant observations\nacross samples. HTS data, essentially depict expression genes\n(rows) sample (columns) colors denote magnitude \nexpression enable quick identification patterns changes \nsamples/experimental groups. Prior constructing heatmap, common \nuse form clustering algorithm learn potential groups patterns \nexpression data.One method known hierarchical clustering (specifically focusing \nagglomerative clustering), method attempts cluster data \nhierarchies single observations begin separate points \nsuccessively merged larger clusters points grouped.\nTypically, agglomerative clustering begin dissimilarity matrix \ndefines “close” observations calculating “distance” metric\nevery pair observations. multiple “distance” metrics can\nchosen, one commonly used simple euclidean distance. \nlinkage function use distance metrics define clusters \nformed based distance values. example, complete linkage define\nclusters maximum value pairwise distances two different\nclusters.using base R function heatmap() default\nuses euclidean distance hclust()`` function produce clustered heatmap. Theheatmap()` function also scale data row-wise aid \nvisualization.begin, provided representative output differentially\nexpressed probes (differential_expression_results.csv) C3 C4\nsubtypes. Filter table return probes adjusted p-value <\n.01. Use ’ve learned extract normalized intensity values \nsubset DE probes. Use heatmap() function R create heatmap\nnormalized intensity values DE genes.#1: important remember commonly used color combinations biology\n(red-green) quite difficult discern color blindness. \nsimplify design decision, going use RColorBrewer, \ngenerates appropriate color palettes number built-color blind\nfriendly palettes. may use command\ndisplay.brewer.(colorblindFriendly=TRUE) view curated palettes.Deliverables\n1. list significant probes differential_expression_results.csv.\n2. matrix example_intensity_data.csv filtered contain probes\ncontained within list significant probes.\n3. heatmap normalized intensity values differentially expressed probes (adjusted p-value < .01).","code":""},{"path":"assignment-3.html","id":"references","chapter":"Assignment 3","heading":"References","text":"\nhttps://www.genome.gov/-genomics/fact-sheets/DNA-Microarray-TechnologyGovindarajan, R., Duraiyan, J., Kaliyappan, K. & Palanisamy, M. Microarray applications. Journal Pharmacy & Bioallied Sciences 4, S310 (2012).Trevino, V., Falciani, F. & Barrera-Saldaña, H. . DNA microarrays: powerful genomic tool biomedical clinical research. Molecular Medicine 13, 527–541 (2007).Shlens, J. Tutorial Principal Component Analysis.(https://arxiv.org/abs/1404.1100)Lever, J., Krzywinski, M. & Altman, N. Points Significance: Principal component analysis. Nature Methods 14, 641–642 (2017).Hastie, T., Hastie, T., Tibshirani, R., & Friedman, J. H. (2001). elements statistical learning: Data mining, inference, prediction. New York: Springer.https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf","code":""},{"path":"assignment-4.html","id":"assignment-4","chapter":"Assignment 4","heading":"Assignment 4","text":"","code":""},{"path":"assignment-4.html","id":"problem-statement-4","chapter":"Assignment 4","heading":"Problem Statement","text":"dealing gene counts mRNA-seq dataset, important \nnormalize data performing analyses can make accurate\ncomparisons gene expression samples. number different\nmethods can use accomplish method use depend \nkinds samples analyses want perform. first part\nassignment guide two commonly used methods: Counts Per\nMillion DESeq2 Normalization.normalize data can exploratory analysis construct \nappropriate graphs. bioinformaticians, need able present \ndata tidy reports generated plots- sometimes blocks text. R\nMarkdown good tool accomplish , allowing display tables\nplots alongside bulk writing. ’ve exposure R\nMarkdown previous assignments, now start writing code \ngenerate visualizations report. important remember\nneed keep code separate depending function: function\ndeclarations implementation- code work behind--scenes \nperform data manipulation process inputs without user interaction \n“back end”- stay main.R file calls construct \ndisplay tibbles visuals created back end put \nreport.Rmd file- despite user directly manipulating editing \noutputs generate, still interacting display viewing \ntherefore lines code call functions create outputs\nbelong “front end.”","code":""},{"path":"assignment-4.html","id":"learning-objectives-2","chapter":"Assignment 4","heading":"Learning Objectives","text":"Data normalization methods (CPM, DESeq2)Plotting data TidyverseDisplaying results compiling reports R MarkdownApplication Tidyverse functions","code":""},{"path":"assignment-4.html","id":"skill-list-2","chapter":"Assignment 4","heading":"Skill List","text":"DESeq normalization, referencing linked Bioconductor vignette neededTibble manipulationCreating different types graphs ggplot2Running PCA","code":""},{"path":"assignment-4.html","id":"instructions-3","chapter":"Assignment 4","heading":"Instructions","text":"empty project repo can found :https://github.com/BF591-R/bf591-assignment-4You givenLike previous assignments, bulk work done skeleton\nfile main.R, unlike previous assignments provided report.Rmd also\nskeleton file. documents provide information needed complete\nassignment. Please ensure inputs outputs match \nspecifications listed- least can properly handle inputs \ninstructed return expected output since functions \ntested.help complete assignment, provided sample report \nset test functions similar ones use grade. \nsample report provided appropriately named sample_report.html \ntests use can found test_main.R. Please note \nsample report display code, okay - preferred, really-\nreport display code blocks like done previous\nassignments.","code":"main.R\ntest_main.R\nreport.Rmd\nverse_counts.tsv\nexample_report.html\n"},{"path":"assignment-4.html","id":"tasks","chapter":"Assignment 4","heading":"Tasks","text":"Implement functions described main.R.[optional highly recommended] Test functions using `test_main.RFill report.Rmd instructed. need functions implemented main.RKnit report.Rmd create report.html","code":""},{"path":"assignment-4.html","id":"deliverables","chapter":"Assignment 4","heading":"Deliverables","text":"main.R functions completedreport.Rmd filled instructedreport.html knitted report.Rmd","code":""},{"path":"assignment-4.html","id":"function-details-1","chapter":"Assignment 4","heading":"Function Details","text":"","code":""},{"path":"assignment-4.html","id":"read_data","chapter":"Assignment 4","heading":"1. read_data()","text":"Load tsv located specified locationInput (1): string path fileOutput: (g x m) tibbleDetails: function tested handling various input strings \nreturning proper output: dimensions, column names, column ‘gene’ \nlocated first column, output type tibble. test_main include\nadditional testing output column types lack row names \nreference, help catch errors early .","code":""},{"path":"assignment-4.html","id":"filter_zero_var_genes","chapter":"Assignment 4","heading":"2. filter_zero_var_genes()","text":"Filter genes zero varianceInput (1): (g x m) tibbleOutput: (n x m) tibbleDetails: function tested handling input (g x m) tibble \nreturning proper output: column names, column ‘gene’ located \nfirst column, names ‘genes’ returned, output type tibble. test_main\ninclude additional testing row consistency- ensuring sample\ndata still correlates gene names- reference help catch\nerrors early ","code":""},{"path":"assignment-4.html","id":"timepoint_from_sample","chapter":"Assignment 4","heading":"3. timepoint_from_sample()","text":"Extract time point information sample nameInput (1): string (length 5) sample name format v[-Z][-z,1-9]_[1-9]\n(words: v[α][β]_[γ], α capital modern English letter, β\nlower case modern English letter Arabic number 0-9, γ \nArabic number 0-9)Output: string (length 2) substring [-Z][-z,1-9] sample name:Details: function tested handling various strings length 5\nform v[-Z][-z,1-9]_[1-9] outputting proper string, preserving\nletter case letter case provided.","code":""},{"path":"assignment-4.html","id":"sample_replicate","chapter":"Assignment 4","heading":"4. sample_replicate()","text":"Grab sample replicate number sample nameInput (1): Input (1): string (length 5) sample name format\nv[-Z][-z,1-9]_[1-9] (words: v[α][β]_[γ], α capital\nmodern English letter, β lower case modern English letter Arabic\nnumber 0-9, γ Arabic number 0-9)Output: string (length 1 substring [1-9] sample name:\nv[-Z][-z,1-9]_[1-9] (words: [γ] v[α][β]_[γ])Details: function tested handling various strings length 5\nform v[-Z][-z,1-9]_[1-9] outputting proper character string.","code":""},{"path":"assignment-4.html","id":"meta_info_from_labels","chapter":"Assignment 4","heading":"5. meta_info_from_labels()","text":"Generate sample-level metadata sample names stores data \ntibble. include columns named “sample”, “timepoint”, “replicate” \nstore sample names, sample time points, sample replicate, respectively.Input (1): Character vector length _S_ sample names column names “sample”, “timepoint”, “replicate”Output: (_S_ x 3) tibbleDetails: function tested handling character vector \nlength _S_, element vector string length 5 \nform v[-Z][-z,1-9]_[1-9], properly outputting (_S_ x 3)\ntibble columns named “sample”, “timepoint”, “replicate” rows \ncorrespond input samples. test_main include additional\ntesting order elements column ‘sample’’s correspondence \norder elements input vector reference. column types \noutput tibble tested.","code":""},{"path":"assignment-4.html","id":"get_library_size","chapter":"Assignment 4","heading":"6. get_library_size()","text":"Calculate total read counts sample counts dataset.Input (1): (n x m) tibble raw read countsOutput: tibble named vector read totals sample. Vectors must \nlength _S_, tibble can (1 x _S_) sample names columns names \n(_S_ x 2) sample name first column library size \nsecond columnDetails: function tested return tibble named vector\nsample names correspond appropriate library size.","code":""},{"path":"assignment-4.html","id":"normalize_by_cpm","chapter":"Assignment 4","heading":"7. normalize_by_cpm()","text":"Normalize raw counts data counts per million using (counts) /\n(sample_library_size) * 10^6Input (1): (n x m) tibble raw read countsOutput: (n x m) tibble read count normalized counts per millionDetails: function tested handle (n x m) tibble. output\ntested dimensions, column names, location string numeric\ncolumn(s), performance desired equation numeric columns, gene\nnames still correspond rows.","code":""},{"path":"assignment-4.html","id":"deseq_normalize","chapter":"Assignment 4","heading":"8. deseq_normalize()","text":"Normalize raw counts data using DESeq2Input (1): (n x m) tibble raw read countsOutput: (n x m) tibble DESeq2 normalized counts dataDetails:function tested handle (n x m) tibble. output \ntested dimensions, column names, location string numeric\ncolumn(s), performance desired equation numeric columns, gene\nnames still correspond rows.","code":""},{"path":"assignment-4.html","id":"plot_pca","chapter":"Assignment 4","heading":"9. plot_pca()","text":"Input (3): (n x _S_) tibble data, (_S_ x 3) tibble sample-level\nmeta information, stringOutput: ggplot scatter plot showing sample, PC1 x-axis PC2 y-axisDetails: output tested appropriate test run PCs\nused plot. may visually inspected part grade","code":""},{"path":"assignment-4.html","id":"plot_sample_distributions","chapter":"Assignment 4","heading":"10. plot_sample_distributions()","text":"Input (3): (n x _S_) tibble data, boolean determine whether scale\n‘y’ axis log10 values, stringOutput: ggplot boxplot shows gene count distributionsDetails: function tested (n x _S_) tibble. tested\nfunctionality inputs, handling data, expected graph elements, \ngraph type. may also visually inspected part grade","code":""},{"path":"assignment-4.html","id":"plot_variance_vs_mean","chapter":"Assignment 4","heading":"11. plot_variance_vs_mean()","text":"Input (3): (n x _S_) tibble data, boolean determine whether scale\n‘y’ axis log10 values, stringOutput: ggplot scatter plot x-axis rank gene ordered \nmean count samples, y-axis observed variance \ngiven gene. dot transparency increased. scatter plot\nalso accompanied line representing average mean variance\nvaluesDetails: function tested (n x _S_) tibble. tested\nfunctionality inputs, handling data, expected graph elements, \ngraph type. may also visually inspected part grade","code":""},{"path":"assignment-4.html","id":"hints-2","chapter":"Assignment 4","heading":"Hints","text":"Make sure don’t rownames tibbles function(s)Make sure don’t rownames tibbles function(s)return. get warning R console .return. get warning R console .Sometimes Tydyverse best tool job, sometimes isn’tSometimes Tydyverse best tool job, sometimes isn’tSome tasks easier accomplish dataframes tibbles. everyone\nuse methods, welcome use dataframes within\nfunction long inputs outputs ones specified (returning\ndataframe instead tibble may points).tasks easier accomplish dataframes tibbles. everyone\nuse methods, welcome use dataframes within\nfunction long inputs outputs ones specified (returning\ndataframe instead tibble may points).Bioconductor vignette DESeq2 linked part 3 report.Rmd.\ncan also found\n)Bioconductor vignette DESeq2 linked part 3 report.Rmd.\ncan also found\n)previous version assignment, output \nfilter_zero_var_genes() transformed (n x m) (n x _s_) \ncalled counts_matrix. One TAs thought use ‘matrix’ \nconfusing (since want use datasets type tibble) \nre-worded, piece trivia might helpful interpreting \nBioconductor vignette- see sort ‘matrix’ referenced \nreport.Rmd got overlooked editing.previous version assignment, output \nfilter_zero_var_genes() transformed (n x m) (n x _s_) \ncalled counts_matrix. One TAs thought use ‘matrix’ \nconfusing (since want use datasets type tibble) \nre-worded, piece trivia might helpful interpreting \nBioconductor vignette- see sort ‘matrix’ referenced \nreport.Rmd got overlooked editing.may occasions need reshape data wideThere may occasions need reshape data wideformat long formatformat long formatSymbols used main.R~ g: initial number Genes, m: initial number \ncolumns expected import verse_counts.tsv, n: number genes\nexpected filter part 1b, _S_: number SamplesSymbols used main.R~ g: initial number Genes, m: initial number \ncolumns expected import verse_counts.tsv, n: number genes\nexpected filter part 1b, _S_: number Samples","code":""},{"path":"assignment-5.html","id":"assignment-5","chapter":"Assignment 5","heading":"Assignment 5","text":"","code":""},{"path":"assignment-5.html","id":"problem-statement-5","chapter":"Assignment 5","heading":"Problem Statement","text":"One main questions asked high throughput sequencing whether \nvariable condition (e.g. treatment/disease status) induces measurable,\ndistinguishable change. context mRNAseq, interested whether\ncan determine mRNA levels (actually measure) genes (\ninfer) significantly - downregulated conditions can\nrelate changes back biological phenotype either directly \nindirectly.","code":""},{"path":"assignment-5.html","id":"learning-objectives-3","chapter":"Assignment 5","heading":"Learning Objectives","text":"Understand methodology rationale DESeq2 normalizes data,\nmodels counts, performs differential expressionPerform differential expression analysis using DESeq2 learn one method \nanalyze time course dataUnderstand basic inspection evaluation differential\nexpression (DE) results","code":""},{"path":"assignment-5.html","id":"skill-list-3","chapter":"Assignment 5","heading":"Skill List","text":"DESeq2Basic diagnostic plots DE resultsHigh-quality figures plots using R ggplot","code":""},{"path":"assignment-5.html","id":"deseq2-background","chapter":"Assignment 5","heading":"DESeq2 Background","text":"DESeq2 well validated highly cited method differential expression\nanalysis HTS data. original paper describing methodology \ndetail can found :Love, M.., Huber, W. & Anders, S. Moderated estimation fold change \ndispersion RNA-seq data DESeq2. Genome Biol 15, 550 (2014).\nhttps://doi.org/10.1186/s13059-014-0550-8We link back original DESeq2 vignette head applicable\nsection reference. strongly encourage read associated\ndocumentation succinctly summarizes important concepts methodology\nunderlying step process. also provides example code snippets \nanswers many common questions regarding usage DESeq2 can already \nfound documentation.Briefly, core, DESeq2 models read counts tests differential\nexpression negative binomial generalized linear models. DESeq2\nnormalizes count data scaling size factors calculated \nmedian--ratios (developed original publication DESeq). \nsignificance testing, DESeq2 performs Wald test model coefficients \ngenerate p-values adjusted multiple testing correction \nBenjamini-Hochberg method.","code":""},{"path":"assignment-5.html","id":"generating-a-counts-matrix","chapter":"Assignment 5","heading":"Generating a counts matrix","text":"DESeq2 takes input counts matrix genes correspond rows samples\ncolumns. one common use cases differential expression\nanalysis, mRNAseq experiments, interested counts associated \ngenes; however, DESeq2 broadly applicable many kinds high-throughout\nsequencing (HTS) count data.Returning analysis mRNAseq specifically, three main\napproaches map reads:Alignment reference genome splice-aware aligner counting \nreads falling exonic regions using reference annotation fileTranscript-based assembly approaches (TopHat, Cufflinks) transcript abundance\nquantification (Salmon, Kallisto)Reference free assembly followed mapping\ncountingDESeq2 expects values input matrix counts (integers) \npurposes assignment, provided count matrix \nsamples generated first method. Reads aligned mouse genome\n(GENCODE GRCm39) STAR quantified gene counts Verse using \nmatching GTF file.","code":""},{"path":"assignment-5.html","id":"prefiltering-counts-matrix","chapter":"Assignment 5","heading":"Prefiltering Counts matrix","text":"DESeq2 Vignette - Pre-filteringUnless experiment quite large, filtering counts matrix prior \nDESeq2 strictly required. experiment incorporates hundreds \nsamples complicated model design many interaction terms, may wish\nperform combination manual filtering optimization DESeq2\nsettings. Otherwise, datasets, default settings DESeq2 run \nreasonably short amount time relatively modest hardware even without\npre-filtering.many different strategies filter count datasets \nchoose employ informed objective, data. \ncommon filters include removing genes mean count across samples \nthreshold removing genes based many samples zero\ncount. general, extensive pre-filtering required DESeq2\nincorporates downstream methods increase statistical power \nindependent filtering (described methods). assignment, \nfiltering dataset prior running DE analysis.","code":""},{"path":"assignment-5.html","id":"median-of-ratios-normalization","chapter":"Assignment 5","heading":"Median-of-ratios normalization","text":"Original DESeq PublicationAs mentioned earlier, DESeq2 expects non-transformed, non-normalized matrix \ninteger “counts” input. DESeq2 performs normalization based \nunderlying assumption small amount genes truly\ndifferentially expressed. encapsulated median--ratios method\naccounts library size well RNA composition. Although DESeq2\nperforms normalization background, can help conceptually \nunderstand process .First, pseudo-reference sample created set equal geometric mean\nacross samples given gene. , ratios calculated comparing \nsample pseudo-reference gene-wise basis. Next, sample-wise\nbasis, median value ratios taken sample’s normalization\nfactor given sample, counts divided factor. \nlearned extract normalized counts size factors last\nassignment.","code":""},{"path":"assignment-5.html","id":"deseq2-preparation","chapter":"Assignment 5","heading":"DESeq2 preparation","text":"DESeq2 Vignette - PreparationThe DESeqDataSet object holds read counts associated\nstatistical measures generated DESeq2 algorithm. Although \nseveral ways generate object, focus DESeqDataSet method\ndirectly takes SummarizedExperiment object.","code":""},{"path":"assignment-5.html","id":"omeara-et-al.-transcriptional-reversion-of-cardiac-myocyte-fate-during-mammalian-cardiac-regeneration.-circ-res.-feb-2015.-pmid-25477501l","chapter":"Assignment 5","heading":"O’Meara et al. Transcriptional Reversion of Cardiac Myocyte Fate During Mammalian Cardiac Regeneration. Circ Res. Feb 2015. PMID: 25477501l","text":"counts matrix provided generated data made available listed\npublication. taken subset samples, focusing specifically\nmRNAseq experiment vivo heart development 0,\n4, 7 days birth well adult mice.start, going consider simplest use case differential\nexpression subset data contain samples postnatal day 0\nadult hearts. addition subsetting counts matrix, also need \nconstruct sample data information dataframe lists samplenames\n(columns counts matrix) well associated information (\ncase, timepoint).","code":""},{"path":"assignment-5.html","id":"reading-and-subsetting-the-data-from-verse_counts.tsv-and-sample_metadata.csv","chapter":"Assignment 5","heading":"1. Reading and subsetting the data from verse_counts.tsv and sample_metadata.csv","text":"use demonstration one simplest use cases DESeq2,\ncomparing two groups samples based single experimental variable. Subset\nsamples include timepoints vP0 vAd, \ncorrespond total 4 samples (vP0_1, vP0_2, vAd_1, vAd_2). Store \ncounts matrix sample dataframe SummarizedExperiment object.may ignore columns metadata CSV used \ngeneration data . Focus samplenames timepoints\ncolumns.may ignore columns metadata CSV used \ngeneration data . Focus samplenames timepoints\ncolumns.columns (sample names) counts matrix sample dataframe need\norderThe columns (sample names) counts matrix sample dataframe need\norderYou may need convert counts dataframe matrixYou may need convert counts dataframe matrixThe counts matrix verse_counts.tsv sample metadata \nsample_metadata.csvThe counts matrix verse_counts.tsv sample metadata \nsample_metadata.csv","code":""},{"path":"assignment-5.html","id":"running-deseq2","chapter":"Assignment 5","heading":"2. Running DESeq2","text":"","code":""},{"path":"assignment-5.html","id":"understanding-factor-levels","chapter":"Assignment 5","heading":"Understanding Factor Levels","text":"DESeq2 Vignette - Factor LevelsThe factor level DESeq2 determines level represents “control” group\nwant compare . default, reference level factors \nchosen alphabetically. can either manually change set reference\nfactor level specify comparison interest using DESeq2 Contrast() \nperforming first steps DE analysis.","code":""},{"path":"assignment-5.html","id":"performing-differential-expression-analysis","chapter":"Assignment 5","heading":"Performing Differential Expression Analysis","text":"may find vignette example usage DESeq2. Please read \nfollowing section instruct perform differential expression\nanalysis DESeq2: Running\nDESeq2We running DESeq2 model design ~timepoint vP0 set \nreference level. test differentially expressed genes adult\nmouse heart ventricles vs. postnatal day 0. Return results \ndataframe dds object running DESeq2 list. Assign results\ndataframe dds object variables can use later Rmd.","code":""},{"path":"assignment-5.html","id":"annotating-results-to-construct-a-labeled-volcano-plot","chapter":"Assignment 5","heading":"3. Annotating results to construct a labeled volcano plot","text":"Later analysis, construct volcano plot. Typically, volcano\nplots label genes significance well direction \nchange. Convert results dataframe tibble add column labeled\nvolc_plot_status can easily input ggplot2 color points \ncriteria.labels encompass three groups: 1. Positive log fold change \nsignificant given padj threshold, 2. Negative log fold change \nsignificant given padj threshold, 3. Remaining non-significant genesEnsure column named exactly volc_plot_status test \nwork without modification.values labels `’, ‘’, ‘NS’ respectively.","code":""},{"path":"assignment-5.html","id":"diagnostic-plot-of-the-raw-p-values-for-all-genes","chapter":"Assignment 5","heading":"4. Diagnostic plot of the raw p-values for all genes","text":"Separate differential expression analysis, always important\ndiagnostic plot unadjusted distribution p-values obtained \nexperiment. Built definition p-value idea \nnull distribution given assumptions made true, p-values follow\nuniform distribution. general, typically observe peak \nvalues close 0 roughly uniform distribution approach 1. \nvalues closer 0 mix situations alternative hypothesis\ntrue well potential false positives. Significant deviations \ngeneral pattern likely require close examination data \nsignificance test employing. Make histogram raw p-values \ngenes discovered experiment.","code":""},{"path":"assignment-5.html","id":"plotting-the-logfoldchanges-for-differentially-expressed-genes","chapter":"Assignment 5","heading":"5. Plotting the LogFoldChanges for differentially expressed genes","text":"often helpful visualize log2FoldChanges differentially\nexpressed genes gain sense distribution - downregulated\ngenes gain global view genes changing. Subset results \ninclude genes significant padj threshold < .10. Plot histogram \nlog2FoldChange values genes.","code":""},{"path":"assignment-5.html","id":"the-choice-of-fdr-cutoff-depends-on-cost","chapter":"Assignment 5","heading":"The choice of FDR cutoff depends on cost","text":"choice FDR cutoff p-value cutoff always subjective based\nobjectives experiment cost false discoveries. \nexample, initial sequencing experiment exploratory meant \ngenerate hypotheses followed vitro vivo, may \nappropriate set permissive cutoff (padj < .20). P-value thresholds set\nexperiment experiment basis adjusted simply \nincrease decrease number DE genes retroactively.","code":""},{"path":"assignment-5.html","id":"plotting-the-normalized-counts-of-differentially-expressed-genes","chapter":"Assignment 5","heading":"6. Plotting the normalized counts of differentially expressed genes","text":"can helpful visualize normalized counts differentially\nexpressed genes quick diagnostic analysis working intended.\nAlthough likely need every RNAseq analysis, can\nprovide confidence results also help remember meaning \ndirectionality fold change values. Make scatter / jitter plot \nDESeq2 normalized counts top ten significantly differentially expressed\ngenes ranked ascending padjIf plotting genes plot, may find \nhelpful take logarithm counts different genes often counts\ndiffer several orders magnitude.plotting genes plot, may find \nhelpful take logarithm counts different genes often counts\ndiffer several orders magnitude.may find helpful add slight horizontal “jitter” points \naid visualization cases counts replicate similar.may find helpful add slight horizontal “jitter” points \naid visualization cases counts replicate similar.already used DESeq2 extract normalized counts. also\ndirections available vignette.already used DESeq2 extract normalized counts. also\ndirections available vignette.","code":""},{"path":"assignment-5.html","id":"volcano-plot-to-visualize-differential-expression-results","chapter":"Assignment 5","heading":"7. Volcano Plot to visualize differential expression results","text":"volcano plot common data plot intended quickly display points\ninterest plotting statistical significance magnitude change.\nspecific case mRNAseq data, entails plotting -log(p-values /\nadjusted p-values) estimated log fold changes reported \ndifferential expression methods.Statistically significant genes low p-values appear higher \ny-axis plot separated magnitude \ndirectionality change (.e. Upregulated genes larger fold change\nestimates along right origin x-axis, \ndownregulated genes farther left origin x-axis).\nTypically, genes interest additional biological relevance \nexperiment directly annotated using domain knowledge.Make volcano plot (log2FoldChange vs. -log10(padj)) genes\ndiscovered experiment. Use column generated part 3 color \ngenes appropriate label.","code":""},{"path":"assignment-5.html","id":"running-fgsea-vignette","chapter":"Assignment 5","heading":"8. Running fgsea vignette","text":"performing GSEA results generated running DESeq2 using\nfgsea package. use log2FoldChange ranking metric test\npathways C2 Canonical Pathways gene set collection\nprovided MSigDB already downloaded (c2.cp.v7.5.1.symbols.gmt). Read\nsection textbook fgsea documentation appropriate\nformats ranked gene list gene sets.may read gene sets proper format using GSEABase fgseaYou may read gene sets proper format using GSEABase fgseaYou need convert mouse ensembl gene IDs human HGNC symbols \nmatch genes provided C2 Canonical Pathways gene sets. \nencountering issues, may need remove version number annotation.need convert mouse ensembl gene IDs human HGNC symbols \nmatch genes provided C2 Canonical Pathways gene sets. \nencountering issues, may need remove version number annotation.discovered, biomaRt can difficult work . \nencountering frequent issues, may consider mapping gene IDs saving\nresults locally avoid re-run biomaRt query multiple times.discovered, biomaRt can difficult work . \nencountering frequent issues, may consider mapping gene IDs saving\nresults locally avoid re-run biomaRt query multiple times.inevitably genes duplicate names /genes \nduplicate values log2FoldChange. Choose method remove values.inevitably genes duplicate names /genes \nduplicate values log2FoldChange. Choose method remove values.","code":""},{"path":"assignment-5.html","id":"plotting-the-top-ten-positive-nes-and-top-ten-negative-nes-pathways","chapter":"Assignment 5","heading":"9. Plotting the top ten positive NES and top ten negative NES pathways","text":"Ensure pathway labels visibleEnsure pathway labels visibleYou probably get warnings duplicate values ranks, best\naddress don’t get hung trying find .probably get warnings duplicate values ranks, best\naddress don’t get hung trying find .","code":""},{"path":"assignment-5.html","id":"references-1","chapter":"Assignment 5","heading":"References","text":" Dobin , Davis CA, Schlesinger F, Drenkow J, Zaleski C, Jha S, Batut P,\nChaisson M, Gingeras TR. STAR: ultrafast universal RNA-seq aligner.\nBioinformatics. 2013 Jan 1;29(1):15-21. doi: 10.1093/bioinformatics/bts635. Epub\n2012 Oct 25. PMID: 23104886; PMCID: PMC3530905.Zhu, Q., Fisher, S.., Shallcross, J., Kim, J. (Preprint). VERSE: versatile\nefficient RNA-Seq read counting tool. bioRxiv 053306. doi:http://dx.doi.org/10.1101/053306https://www.gencodegenes.org/mouse/Anders, S. & Huber, W. Differential expression analysis sequence count data.\nGenome Biology 11, 1–12 (2010).Love, M. ., Huber, W. & Anders, S. Moderated estimation fold change \ndispersion RNA-seq data DESeq2. Genome Biology 15, 550 (2014). ","code":""},{"path":"assignment-6.html","id":"assignment-6","chapter":"Assignment 6","heading":"Assignment 6","text":"Volcanoes don’t really look like , ?","code":""},{"path":"assignment-6.html","id":"problem-statement-6","chapter":"Assignment 6","heading":"Problem Statement","text":"R utilized bioinformatics communities years point, many different approaches one task. last assignment just used DESeq2, performing differential expression analysis two additional packages (edgeR Limma) comparing results find.","code":""},{"path":"assignment-6.html","id":"learning-objectives-4","chapter":"Assignment 6","heading":"Learning Objectives","text":"basic operations ggplot “Grammar Graphics”.Using differential gene expression analysis packages generate data plotting.Creating simple plots using default colors themes.Combining multiple plots one image using facet_wrap()","code":""},{"path":"assignment-6.html","id":"skill-list-4","chapter":"Assignment 6","heading":"Skill List","text":"tempered heart mind understands plot may ever look exactly want .intermediate understanding R’s popular plotting package, ggplot.understanding differential expression analysis plotting.sense superiority whenever see publication use base R plotting package figures.","code":""},{"path":"assignment-6.html","id":"instructions-4","chapter":"Assignment 6","heading":"Instructions","text":"Complete main.R way satisfies tests test_main.R. Follow instructions function descriptions main.R read details tests code satisfy. use functions complete figures report.Rmd.can use testthat:test_file('test_main.R') R console run tests test_main.R. modify tests. difficult write tests cover every solution problem, tests written ensure outputs function aligned analysis perform. feel test working incorrectly, let TA know.","code":""},{"path":"assignment-6.html","id":"function-details-2","chapter":"Assignment 6","heading":"Function Details","text":"","code":""},{"path":"assignment-6.html","id":"load_n_trim","chapter":"Assignment 6","heading":"1. load_n_trim()","text":"data load R order manipulate. ’ll loading counts file, matrix genes (rows) samples (columns). cell number gene found sample. good opportunity use data frame tibble, package restrictions sticking data frames assignment. ’re using couple different packages process data, input/output exclusively data frames, consistent save trouble conversions.want return data frame essentially identical input file (always good idea use command line see ’re working ). Ensure gene names stored row.names separate column, data frames distinction two. function also reduce columns just interest (P0 Adult).Row names can changed using row.names() function. new row names must character vector exactly length.","code":"df <- data.frame(a = c(0, 0, 0), b = c(1, 3, 5))\nrow.names(df) <- c(\"Row1\", \"Row2\", \"Row3\")\nprint(df)\n     a b\nRow1 0 1\nRow2 0 3\nRow3 0 5"},{"path":"assignment-6.html","id":"tests-4","chapter":"Assignment 6","heading":"Tests","text":"tests assignment relatively straightforward, mostly ensuring data loaded ends right shape format. first test ensures columns filtered, second ensures size correct, final ensures data frame tibble. tibbles. Tibble-less?","code":"\ntest_that(\"test data is loading correctly and reduced\", {\n  test_df <- load_n_trim(csv)\n  expect_equal(names(test_df), c(\"vP0_1\", \"vP0_2\", \"vAd_1\", \"vAd_2\"))\n  expect_equal(dim(test_df), c(55416, 4))\n  expect_equal(class(test_df), \"data.frame\")\n})"},{"path":"assignment-6.html","id":"run_deseq","chapter":"Assignment 6","heading":"2. run_deseq()","text":"first three functions technically identical things (different ways). One popular Bioconductor packages, DESeq2 number options available differential expression analysis. load counts data, select variables interest, use DESeq() process . Links included function description DESeq2 documentation, find answers questions , especially since documents used write assignment. function return results analysis.","code":""},{"path":"assignment-6.html","id":"tests-5","chapter":"Assignment 6","heading":"Tests","text":"Less concerning writing tests larger data sets complicated functions like …tricky. problem can’t written like script, can’t use results load_n_trim() test function, one fails independently ? case, create RData object correctly loaded data works even load_n_trim() absolutely broken. Also, case, using different sets test (days 4 7 instead 0 adult) can’t just load test data say wrote code! disingenuous , disingenuous graduate.test loads sample data (data frame counts) creates coldata object according DESeq2 specifications. wrap run_deseq() expect_warning() DESeq throws warning performs expected behavior: converting strings factors (bad programming! warnings , package functioning expected). similar series tests: ensure dimensions correct, ensure results object DESeq, ensure column names need present.","code":"\ntest_that(\"deseq2 function is returning correct results\", {\n  load(\"mock_counts_df.RData\") # loads the counts_df object into env\n  coldata <- data.frame(condition = rep(c(\"day4\", \"day7\"), each=2),\n                        type=\"paired-end\")\n  row.names(coldata) <- c(\"vP4_1\", \"vP4_2\", \"vP7_1\", \"vP7_2\")\n  expect_warning(deseq <- run_deseq(counts_df,\n                                    coldata,\n                                    10, \"condition_day7_vs_day4\"))\n  expect_equal(dim(deseq), c(19127, 6))\n  expect_equal(class(deseq)[1], \"DESeqResults\")\n  expect_equal(c(\"pvalue\", \"padj\") %in% names(deseq), c(TRUE, TRUE))\n})"},{"path":"assignment-6.html","id":"run_edger","chapter":"Assignment 6","heading":"3. run_edger()","text":"mentioned previous function, EdgeR implementation running differential expression analysis based counts file. , function follow roughly portion documentation mentioned function description. EdgeR number additional plotting functions (plotMDS()) may like include. optional pretty fun see data working expected (similar days clustered close together ). return entire results object, filtering necessary yet.","code":""},{"path":"assignment-6.html","id":"tests-6","chapter":"Assignment 6","heading":"Tests","text":"Similar DESeq2 tests, aren’t necessarily looking carbon copy results. process entirely deterministic, better allow wiggle room merely look columns ’re interested present data right shape. Note EdgeR doesn’t perform BH correction (p-adjusted column non-existant). ’s either option ’s default just feature! problem, p-values can (see markdown).","code":"\ntest_that(\"edger function is returning correct results\", {\n  load(\"mock_counts_df.RData\")\n  group <- factor(rep(c(1,2), each=2))\n  edger_res <- run_edger(counts_df, group)\n  expect_equal(names(edger_res), c(\"logFC\", \"logCPM\", \"PValue\"))\n  expect_equal(dim(edger_res), c(15026, 3))\n})"},{"path":"assignment-6.html","id":"run_limma-and-run_voom","chapter":"Assignment 6","heading":"4. run_limma() and run_voom()","text":"final set three, ’re testing two related functions Limma package. can create analysis uses Limma, case can apply voom workflow refine data little . info work cited documentation. little technique can useful optional workflows like including boolean flag parameters function. running function add_numbers(1, -4, abs=TRUE), parameter abs might change functions operation takes absolute value numbers adding together. create behavior like :useful technique re-using functions, meaning don’t need write two functions similar things can change behavior boolean parameter.","code":"\nadd_numbers <- function(x, y, abs=FALSE) {\n  if (abs) { # code enters here if abs is TRUE\n    return(abs(x) + abs(y)) # return means function exits, doesn't go to \"else {}\"\n  } else { # code enters here if abs was false\n    return(x + y)\n  }\n}"},{"path":"assignment-6.html","id":"tests-7","chapter":"Assignment 6","heading":"Tests","text":"ultimately functions similarly first two series tests: load test data, apply function , make sure results right size, shape, type. includes voom procedure, using limma results yield failures.Note Limma uses EdgeR input data, first couple steps look pretty familiar.create design object use, defining experimental conditions. using day 4 day 7 trials heart data.","code":"\ntest_that(\"test limma + voom work + work together\", {\n  load(\"data/mock_counts_df.RData\")\n  group <- factor(rep(c(1,2), each=2))\n  # design\n  design <- data.frame(day4=1, day4vsday7=c(0, 0, 1, 1))\n  row.names(design) <- c(\"vP4_1\", \"vP4_2\", \"vP7_1\", \"vP7_2\")\n  # limma + voom\n  expect_warning(voom_res <- run_limma(counts_df, design, group)) # voom yelling, ignore\n  expect_equal(dim(voom_res), c(15026, 6))\n  expect_equal(names(voom_res), c(\"logFC\", \"AveExpr\", \"t\", \"P.Value\",\n                                  \"adj.P.Val\", \"B\"))\n})"},{"path":"assignment-6.html","id":"combine_pval","chapter":"Assignment 6","heading":"5. combine_pval()","text":"final three functions concern modifying differential expression results creating plots combined results. trivial make three plots using code function, R generate one consistent plot changes data method extremely useful.use combine_pval() coerce data long format, applicable facet wrapping. facet_wrap() ggplot2 function uses property data create separate plots one image. case use package property (DESeq2, edgeR, Limma) differentiate data. couple ways reorganize data way, suggest using tidyr::gather(). key package value p-value.","code":""},{"path":"assignment-6.html","id":"tests-8","chapter":"Assignment 6","heading":"Tests","text":"tests organized one larger testing function use , randomly generated input data (bunch data frames uniform distributions ranges approximate actual data). like test parts testing function, use octothorpe (#) comment tests haven’t reached yet.first nine lines establish test data frames. random, let us test functions working correctly.test data frames mimic normal results exactly. rows interested name, different numbers columns. reason, suggest function uses $-notation referencing columns, instead position. deseq$pvalue work better deseq[,1] test data.test 2 columns 3,000 rows, fake p-values within expected range. Finally, use table() function test 1,000 one package data frame.","code":"test_that(\"ggplot and data formatting works\", {\n  deseq <- data.frame(pvalue = runif(1000, 1e-100, 1e-5),\n                      log2FoldChange = runif(1000, -9, 9),\n                      padj = runif(1000, 1e-300, 1e-15))\n  edger <- data.frame(PValue = runif(1000, 1e-100, 1e-5),\n                      logFC = runif(1000, -9, 9),\n                      padj = runif(1000, 1e-100, 1e-14))\n  limma <- data.frame(P.Value = runif(1000, 1e-100, 1e-5),\n                      logFC = runif(1000, -9, 9),\n                      adj.P.Val = runif(1000, 1e-6, 1e-4))\n  # combine_pval\n  res <- combine_pval(deseq, edger, limma)\n  expect_equal(dim(res), c(3000, 2))\n  expect_true(mean(res$pval) < 5e-5 && mean(res$pval) > 5e-7)\n  expect_equal(table(res$package)[[1]], 1000)\n  ..."},{"path":"assignment-6.html","id":"create_facets","chapter":"Assignment 6","heading":"6. create_facets()","text":"Now examined p-values, can look adjusted p-values. ’ll constructing volcano plots, compare log2 fold-change adjusted p-value, create_facets() function somewhat complicated. Rather use gather() , may easier create three tibbles data frames variables interest combine rbind().","code":""},{"path":"assignment-6.html","id":"tests-9","chapter":"Assignment 6","heading":"Tests","text":"Another simple set tests, time want ensure 3,000 rows 3 columns. also want ensure aren’t 1,000 rows package type.","code":"\n  ...\n  # create_facets\n  facets <- create_facets(deseq, edger, limma)\n  expect_equal(dim(facets), c(3000, 3))\n  expect_equal(table(facets$package)[[2]], 1000)\n  ..."},{"path":"assignment-6.html","id":"theme_plot","chapter":"Assignment 6","heading":"7. theme_plot()","text":"Finally, data right shape can plotting. lot learn ggplot, mention resources function description, don’t afraid experiment! Try different colors, themes, styles. easiest part getting data plot, hard part making look attractive.","code":""},{"path":"assignment-6.html","id":"tests-10","chapter":"Assignment 6","heading":"Tests","text":"create little fake data, pipe theme_plot() function, save ggplot object. tests simply look inside ggplot object confirm data right shape, ’re plotting using geom_point(), facets present.tests passing, take look report see everything can work together!","code":"  ...\n  # theme_plot\n  tibble(logFC = runif(3000, -9, 9),\n         padj = runif(3000, 1e-300, 1e-5),\n         package = rep(c(\"DESeq2\", \"edgeR\", \"Limma\"), each=1000)) %>%\n  theme_plot() -> volc_plot\n  expect_equal(dim(volc_plot$data), c(3000, 3))\n  expect_equal(class(volc_plot$layers[[1]]$geom)[1], \"GeomPoint\")\n  expect_equal(class(volc_plot$facet$params), \"list\")\n})"},{"path":"assignment-7.html","id":"assignment-7","chapter":"Assignment 7","heading":"Assignment 7","text":"Now working companion package: R Dully","code":""},{"path":"assignment-7.html","id":"problem-statement-7","chapter":"Assignment 7","heading":"Problem Statement","text":"Communicating data results one nefariously troubling aspects data science. bioinformatics often colleagues less experienced comes viewing manipulating data, clinicians biologists think computers living objects evil (). must utilize better ways communicate work .","code":""},{"path":"assignment-7.html","id":"learning-objectives-5","chapter":"Assignment 7","heading":"Learning Objectives","text":"Develop two sides R Shiny application: server UI.Understand reactivity comes application.Take user input use change state web application.","code":""},{"path":"assignment-7.html","id":"skill-list-5","chapter":"Assignment 7","heading":"Skill List","text":"R ShinyTroubleshooting R ShinyTesting R Shiny? Like, nobody pretty good edge .","code":""},{"path":"assignment-7.html","id":"instructions-5","chapter":"Assignment 7","heading":"Instructions","text":"Complete app.R (main.R), including server ui functions. looking exact parity comes style application, just replicate user experience inputs create two methods output: table volcano plot.sample application hosted : https://taylorfalk.shinyapps.io/bf591-assignment-7/View document along sample application try determine parts UI output mentioned document associated functioning application.Test available assignment, course . can run using testthat::test_file(\"test_app.R\"). can also use source(\"test_app.R\") (’s less informative). Shiny apps add another dimensionality complexity, relatively limited can test . always, feel test isn’t working correctly please let TA know.Note: design, instructions rather sparse. likely entirely new territory , gotten far lot information available R Shiny. Please read documentation look help online help understand application works.","code":""},{"path":"assignment-7.html","id":"function-details-3","chapter":"Assignment 7","heading":"“Function” Details","text":"Without writing entire treatise R Shiny (see book), Shiny application broken two components: UI function Server function. web development parlance, represent “front end” “back end”, respectively. front end code controls user sees (goop web browser), back end connects front end server code, processing power, data live.can use two files , set app.R contain entire Shiny application. ’s general format like :loading Shiny library, create front end parameters fluidPage() function. Fluid page simple wrapper designing Shiny page, inside can use functions like titlePanel(), sidebarLayout(), tabPanel() organize displayed.Next, create server function much similar writing base R. input, output, server automatically handled Shiny, use objects pass data front end (UI) back end (server). user can select color graph, can store color input$color1, read input inside server, pass resulting graph output place onto page.Finally, shinyApp() called run everything. heavy lifting can worry themes pretty colors app (library(bslib), themeing graded). goes without saying, much like working fonts headers essay, wait app mostly complete start changing aesthetics.","code":"\nlibrary(shiny)\nui <- fluidPage(\n  # front end\n)\nserver <- function(input, output, session) {\n   # back end\n}\nshinyApp(ui = ui, server = server) # run app"},{"path":"assignment-7.html","id":"ui---fluidpage...","chapter":"Assignment 7","heading":"1. ui <- fluidPage(...)","text":"R Shiny highly customization package, describe inputs outputs ’re expecting, leave theme, placement, arrangement, design elements . R Shiny’s default layout usable sidebayLayout(), want control feel free use fluidRow().sidebar, six elements user input. fileInput(), followed two radioButtons(), followed two colourInputs(), finally sliderInput(). important part inputs inputId argument, later used inside server function’s input. make inputId=fileupload, can access upload using input$fileupload later .","code":""},{"path":"assignment-7.html","id":"file-input","chapter":"Assignment 7","heading":"File Input","text":"file input function take CSV file, helpfully included repository data/ folder. vital, nice method can allow CSV file types using accept=. useful users easily confused try enter data.","code":""},{"path":"assignment-7.html","id":"radio-buttons","chapter":"Assignment 7","heading":"Radio Buttons","text":"Since ultimately plotting graph based data uploaded, want select variables plotted. set radio buttons using names column header, can match later ggplot selecting data plot. Make sure two radio button functions different inputIds.","code":""},{"path":"assignment-7.html","id":"colour-inputs","chapter":"Assignment 7","heading":"Colour Inputs","text":"bonus library useful working color. Installing colourpicker using colourInput() inside side panel layout lets us choose two colors place graph. Feel free choose default colors, make sure separate input IDs.","code":""},{"path":"assignment-7.html","id":"slider-input","chapter":"Assignment 7","heading":"Slider Input","text":"last piece input slider input. allows user select magnitude adjusted p-value like color graph. \\(x\\) \\(1 \\times 10^{x}\\).","code":""},{"path":"assignment-7.html","id":"the-button","chapter":"Assignment 7","heading":"The Button","text":"Finally, button hit everything ready. can use submitButton() actionButton(). work slightly differently button just lets us control plot created. can use icon = icon() choose different icon button.","code":""},{"path":"assignment-7.html","id":"tabs-and-outputs","chapter":"Assignment 7","heading":"12.1.1 2. Tabs and Outputs","text":"inputs established page, want somewhere put everything ’s done run! accomplished *Output() functions. two outputs consider, plot table, can use plotOutput() tableOutput() respectively, using names inside output server function. can placed simply mainPanel().one additional step like use tabs instead, lets us create space outputs. Instead using output functions directly, can first use tabsetPanel() place two tabPanel() functions inside. finally place outputs. tab setup look something like :might look little messy can keep eye hierarchy start make sense.","code":"ui <- fluidPage(\n  ...\n  mainPanel(\n    tabsetPanel(\n      tabPanel(\"Tab1 Name\"\n        tableOutput(...)\n      ),\n      tabPanel(\"Tab2 Name\"\n        plotOutput(...)\n      )\n    )\n  )\n)"},{"path":"assignment-7.html","id":"server---function...","chapter":"Assignment 7","heading":"2. server <- function(...","text":"second main component, lot “work”, server function. written like normal function R, three specific parameters arguments. arguments can used inside server communicate elements wrote . input object can used take user selections, color chosen. output object use assign results output functions included .","code":""},{"path":"assignment-7.html","id":"load_data","chapter":"Assignment 7","heading":"load_data()","text":"Much like normal R script, can save trouble splitting code blocks functions. case, can load user select data different parts app, like displaying plot table.load_data() function written reactive expression, covered detail book chapter. user asked upload data want process data twice, slow application.reaction function update inputs inside change, case file name file uploader. Ultimately, just want function return data frame loading whatever upload box.Note need specify datapath fileInput() function’s input ID. input ID “taylorsfile” actual input input$taylorsfile$datapath.","code":""},{"path":"assignment-7.html","id":"volcano_plot","chapter":"Assignment 7","heading":"volcano_plot()","text":"feel like meeting old friend: make volcano plot. function takes data frame, column names choose axes, negative integer use cutoff, two colors make points cutoff. Simple exciting. Please normal ggplot things like labeling axes changing theme.little hint, sometimes tidy elements don’t play nice string names (like function asks ). can use !!sym(x_name) access columns. also tidy friendly ways , uhh like way!","code":""},{"path":"assignment-7.html","id":"draw_table","chapter":"Assignment 7","heading":"draw_table()","text":", function even simpler. Take data frame slider magnitude, cut rows aren’t lower slider threshold. See second tab example app slider value interacts populates table. also want format p-value columns display digits, otherwise show “0” table (informative). can use tidyverse functions formatC() accomplish .","code":""},{"path":"assignment-7.html","id":"output...","chapter":"Assignment 7","heading":"output$...","text":"Finally cram results onto web page. Use functions renderPlot() renderTable() return volcano plot filtered data table using two functions. outputs checked testing script, bare minimum looking . Please reference test file test_app.R see tested. table’s p-values magnitude, just 0.outputs show correctly load CSV file, congrats! ’ve built app correctly. didn’t work, sorry. R Shiny can difficult troubleshoot afraid look strange errors see ask help.","code":""},{"path":"final-project.html","id":"final-project","chapter":"Final Project","heading":"Final Project","text":"final project develop R Shiny application features\nmultiple bioinformatics processes implemented R. application must\ncontain three mandatory components followed fourth component \nchoice, described . may organize application however wish,\nmust integrated one singular application; four separate\napplications acceptable. However, may helpful develop \ncomponent app, combine complete. suggest \nlayout like following:Example final project app layout nested tabsNote components subcomponents input controls. might\nconsider organizing components follows:Nested tab input controlsPart project identify data necessary loaded \napplication. provide sets starting information, need \nuse R code ’ve written assignments one seven process \nraw data appropriate format loaded Shiny application.\nDifferent processing steps analyses required dataset, \nmust investigate dataset closely understand available \nmust done .must submit individual app. group project.\nHowever, encouraged share ideas, troubleshoot brainstorm together\nsharing code forbidden. final submission include GitHub\nrepository containing app data well five minute pre-recorded\nscreen grab video presentation offering demonstration application. \nview presentation grade project based well fits \nfollowing criteria contents quality coding.","code":""},{"path":"final-project.html","id":"general-guidelines","chapter":"Final Project","heading":"12.2 General Guidelines","text":"Simple file input validation (check ’s CSV TSV well formatted,\nerror message otherwise)Write comments code elaborating functions, Shiny commands, code\nimmediately obvious someone isn’t familiar codeYour App user friendly. Buttons need labels . Text\ndescriptions put UI describe section input \n.(Optional) Data sharing across different tabs. E.g. analysis done \ndifferential expression tab, proceed GSEA tab, \nautomatically load results differential expression tab.","code":""},{"path":"final-project.html","id":"video-presentation-guidelines","chapter":"Final Project","heading":"12.3 Video Presentation Guidelines","text":"screen recorded presentation detailing ins outs Shiny\napplication (Zoom can used record)Narrate steps presentation goPresentations 5 minutes include:\nbrief introduction, tour, demonstration tab application\nDiscuss challenges solutions used programming\nbrief explanation input data generated. \nexample: “data comparing control lung cancer samples Paper et\nal. filtered counts matrix contain genes half\nsamples within condition 0 counts. performed DE \ncounts matrix comparing vs. B using DESeq2 default parameters \nconsidered genes significant FDR < .15”\nbrief introduction, tour, demonstration tab applicationDiscuss challenges solutions used programmingA brief explanation input data generated. \nexample: “data comparing control lung cancer samples Paper et\nal. filtered counts matrix contain genes half\nsamples within condition 0 counts. performed DE \ncounts matrix comparing vs. B using DESeq2 default parameters \nconsidered genes significant FDR < .15”Quick walkthrough Shiny functionalities checklistWe recommend writing script first use presentation. average\nnumber spoken words per minute English ~140, might use \nguide length.","code":""},{"path":"final-project.html","id":"input-data-sets","chapter":"Final Project","heading":"12.4 Input Data Sets","text":"identified several different datasets may use \nprojects. gene expression dataset generated RNASeq data, \nsample information counts matrix available. need identify\ndata files containing information references provided \nperform necessary pre-processing (e.g. counts normalization,\ndifferential expression, etc.) make data suitable app. \ndataset slightly different way obtaining data, \nfirst step. app components described later page indicate \ninput using language provided .hints:need investigate data available provided\nsources find sample info. datasets, might easier \nsimply create sample info matrix based filenames, others need\ndetailed (e.g. covariate) information need find use.need closely examine available counts matrices understand\nprocessed.datasets precomputed differential expression statistics. \nmay use analyses, sure understand comparison \nusing describing dataset demonstrating app.may choose following datasets building demonstrating \napp:1. Soybean cotyledon gene expression across developmentThis experiment assayed gene expression cotyledons, embryonic\nleaves, soybean plants different ages planting.bigPint R package2. Post-mortem Huntington’s Disease prefrontal cortex compared neurologically healthy controlsThis dataset profiled gene expression RNASeq post-mortem human\ndorsolateral prefrontal cortex patients died Huntington’s\nDisease age- sex-matched neurologically healthy controls.mRNA-Seq Expression profiling human post-mortem BA9 brain tissue Huntington’s Disease neurologically normal individuals3. Transcriptional Reversion Cardiac Myocyte Fate Mammalian Cardiac RegenerationThis dataset profiled gene expression RNASeq murine cardiac tissue\nacross different stages development identify genes associated \nloss cell’s capacity regenerate.Transcriptional Reversion Cardiac Myocyte Fate Mammalian Cardiac Regeneration4. Molecular mechanisms underlying plasticity thermally varying environmentThis dataset profiled gene expression RNASeq drosophila melanogaster\ndifferent stages development exposed constant fluctuating\ntemperatures.Molecular mechanisms underlying plasticity thermally varying environmentSalachan, Paul Vinu, Jesper Givskov Sørensen. 2022. “Molecular Mechanisms Underlying Plasticity Thermally Varying Environment.” Molecular Ecology, April. https://doi.org/10.1111/mec.16463.Note: article open; need find article \nlibrary academic institution5. ideaYou may identify gene expression dataset interest. dataset\nmust :publication associated itA publicly available raw normalized gene counts matrix least 6\nsamples 5,000 genesAvailable sample information allows compute differential\nexpression analysisBe sure explain background dataset demonstration video.","code":""},{"path":"final-project.html","id":"required-components","chapter":"Final Project","heading":"12.5 Required Components","text":"","code":""},{"path":"final-project.html","id":"sample-information-exploration","chapter":"Final Project","heading":"12.5.1 Sample Information Exploration","text":"distinct values distributions sample information important \nunderstand conducting analysis corresponding sample data. \ncomponent allows user load examine sample information matrix.Inputs:Sample information matrix CSV formatShiny Functionalities:Tab summary table includes summary type values column, e.g.:\nNumber rows: X\nNumber columns: Y\nColumn Name\nType\nMean (sd) Distinct Values\nAge\ndouble\n61 (+/- 15)\nCondition\nfactor\nAD, Control\nTau\ndouble\n1401 (+/- 310)\netc\n\n\nTab summary table includes summary type values column, e.g.:\nNumber rows: X\nNumber columns: YTab data table displaying sample information, sortable columnsTab data table displaying sample information, sortable columnsTab histograms, density plots, violin plots continuous variables.\nwant make fancy, allow user choose column plot\nanother column group !\nTab histograms, density plots, violin plots continuous variables.want make fancy, allow user choose column plot\nanother column group !","code":""},{"path":"final-project.html","id":"counts-matrix-exploration","chapter":"Final Project","heading":"12.5.2 Counts Matrix Exploration","text":"Exploring visualizing counts matrices can aid selecting gene count\nfiltering strategies understanding counts data structure. component\nallows user choose different gene filtering thresholds assess \neffects using diagnostic plots counts matrix.Inputs:Normalized counts matrix, method , CSV formatInput controls filter genes based statistical properties:\nSlider include genes least X percentile variance\nSlider include genes least X samples non-zero\nSlider include genes least X percentile varianceSlider include genes least X samples non-zeroShiny Functionalities:Tab text table summarizing effect filtering, including:\nnumber samples\ntotal number genes\nnumber % genes passing current filter\nnumber % genes passing current filter\nnumber samplestotal number genesnumber % genes passing current filternumber % genes passing current filterTab diagnostic scatter plots, genes passing filters marked \ndarker color, genes filtered lighter:\nmedian count vs variance (consider log scale plot)\nmedian count vs number zeros\nmedian count vs variance (consider log scale plot)median count vs number zerosTab clustered heatmap counts remaining filtering\nconsider enabling log-transforming counts visualization\nsure include color bar legend\nconsider enabling log-transforming counts visualizationbe sure include color bar legendTab scatter plot principal component\nanalysis projections. may either:\nallow user select principal components plot scatter\nplot (e.g. PC1 vs PC2)\nallow user plot top \\(N\\) principal components beeswarm plot\nsure include % variance explained component plot\nlabels\nallow user select principal components plot scatter\nplot (e.g. PC1 vs PC2)allow user plot top \\(N\\) principal components beeswarm plotbe sure include % variance explained component plot\nlabels","code":""},{"path":"final-project.html","id":"differential-expression","chapter":"Final Project","heading":"12.5.3 Differential Expression","text":"Differential expression identifies genes, , implicated \nspecific biological comparison. component allows user load \nexplore differential expression dataset.Inputs:Results differential expression analysis CSV format.\nresults already made available, may use \nOtherwise perform differential expression analysis using DESeq2, limma, \nedgeR provided counts file\nresults already made available, may use thoseOtherwise perform differential expression analysis using DESeq2, limma, \nedgeR provided counts fileShiny Functionalities:Tab sortable table displaying differential expression results\nOptional: enable gene name search functionality filter rows table\nOptional: enable gene name search functionality filter rows tableTab content similar described Assignment 7","code":""},{"path":"final-project.html","id":"choose-your-own-adventure","chapter":"Final Project","heading":"12.6 Choose-your-own Adventure","text":"Implement one following components part app.","code":""},{"path":"final-project.html","id":"gene-set-enrichment-analysis-1","chapter":"Final Project","heading":"12.6.1 Gene Set Enrichment Analysis","text":"Use differential gene expression results compute gene set enrichment\nanalysis fgsea. need identify\nappropriate gene set database matches organism studied \ndataset.Input:table fgsea results differential expression data.\nchoose appropriate ranking metric (log fold change, -log(pvalue), etc)\ndifferential expression results\nrun fgsea appropriate parameters gene set database choice\nsave results CSV/TSV\nchoose appropriate ranking metric (log fold change, -log(pvalue), etc)\ndifferential expression resultsrun fgsea appropriate parameters gene set database choicesave results CSV/TSVfile upload buttonShiny Functionalities:Tab 1\nSidebar\nSlider adjust number top pathways plot adjusted p-value\n\nMain Panel\nBarplot fgsea NES top pathways selected slider\nOptional: Click barplot pathway display table entry\n\nSidebar\nSlider adjust number top pathways plot adjusted p-value\nSlider adjust number top pathways plot adjusted p-valueMain Panel\nBarplot fgsea NES top pathways selected slider\nOptional: Click barplot pathway display table entry\nBarplot fgsea NES top pathways selected sliderOptional: Click barplot pathway display table entryTab 2\nSidebar\nSlider filter table adjusted p-value (Reactive)\nRadio buttons select , positive negative NES pathways\nDownload button export current filtered displayed table results\n\nMain panel\nSortable data table displaying results\n\nSidebar\nSlider filter table adjusted p-value (Reactive)\nRadio buttons select , positive negative NES pathways\nDownload button export current filtered displayed table results\nSlider filter table adjusted p-value (Reactive)Radio buttons select , positive negative NES pathwaysDownload button export current filtered displayed table resultsMain panel\nSortable data table displaying results\nSortable data table displaying resultsTab 3\nSidebar\nSlider filter table adjusted p-value (Reactive)\n\nMain panel\nScatter plot NES x-axis -log10 adjusted p-value y-axis, \ngene sets threshold grey color\n\nSidebar\nSlider filter table adjusted p-value (Reactive)\nSlider filter table adjusted p-value (Reactive)Main panel\nScatter plot NES x-axis -log10 adjusted p-value y-axis, \ngene sets threshold grey color\nScatter plot NES x-axis -log10 adjusted p-value y-axis, \ngene sets threshold grey color","code":""},{"path":"final-project.html","id":"correlation-network-analysis","chapter":"Final Project","heading":"12.6.2 Correlation Network Analysis","text":"Create app computes pairwise gene expression correlation specific\nset input genes. genes genes particular pathway,\nassociated disease, perhaps top \\(N\\) differentially expressed genes.\nSubset gene expression matrix include genes specified \ninput text area compute pairwise correlations. app report \ninput genes found matrix. Also provide slider input\nallows user set minimum threshold edge two genes \nincluded (e.g. draw edges genes correlation > 0.9). \nslider reactive, tables plots update appropriately. Use \n[igraph] package described Network Analysis section construct\ngraph compute network statistics node.Input:Normalized counts matrix, method , CSV formatA multi-line text box (hint: textAreaInput)\naccepts one gene name per lineA slider setting minimum correlation drawing edgeShiny Functionalities:Tab clustered heatmap normalized counts corresponding genes\nentered text areaTab visualization correlation network\nVertices colored default color igraph\nVertices labeled gene names\n(Optional) user can select two vertices, click button, show shortest path\n. required question :\nTwo drop menus (selectizeInput) select two vertices\nShow shortest path text output\n\nVertices colored default color igraphVertices labeled gene names(Optional) user can select two vertices, click button, show shortest path\n. required question :\nTwo drop menus (selectizeInput) select two vertices\nShow shortest path text output\nTwo drop menus (selectizeInput) select two verticesShow shortest path text outputTab table following metrics calculated gene input:\nDegree\nCloseness centrality\nBetweenness centrality\nDegreeCloseness centralityBetweenness centrality","code":""},{"path":"final-project.html","id":"rshiny-test-suite","chapter":"Final Project","heading":"12.6.3 RShiny Test Suite","text":"Instead creating fourth tab part project, instead\ncreate test application file use “testthat” “shinytest”\npackages.testthat can used create headless (visual elements) shiny application\ncan set inputs use testthat functions like expect_equal() \ncompare outputs. shinytest involves manually recording state \napplication comparing subsequent runs state.Create test file uses either packages test various parts \napplication. tab must least 5 distinct tests measure\ninputs outputs application (least 15 total).Tests relevant test parts application relevant \napp’s running. instance, test confirming counts file filtered\ncorrectly acceptable. Tests can measure output functions inside\napplication output$ app .Spend portion presentation video running test show \nworking (doesn’t) discuss briefly approach writing \nrunning tests. test file reviewed project submission.","code":""},{"path":"final-project.html","id":"visualization-of-individual-gene-expressions","chapter":"Final Project","heading":"12.6.4 Visualization of Individual Gene Expression(s)","text":"Visualizing individual gene counts sometimes useful examining \nverifying patterns identified differential expression analysis. \nmany different ways visualizing counts single gene. app allows\ncounts arbitrary gene selected visualized broken \ndesired sample information variable.Input:Normalized counts matrix, method , CSV formatSample information matrix CSV formatInput control allows user choose one categorical fields\nfound sample information matrix fileInput control allows user choose one genes found \ncounts matrix (hint: try implementing search box)Input control allowing user select one bar plot,\nboxplot, violin plot, beeswarm plotA button makes thing goShiny Functionalities:Content displaying plot selected type normalized gene counts\nselected gene split categorical variable chosen","code":""},{"path":"final-project.html","id":"custom-option","chapter":"Final Project","heading":"12.6.5 Custom Option","text":"explored number different bioinformatics topics class \nBF528, many . like create fourth\ncomponent, draft small proposal like Shiny app \ndata use send Adam TAs.General requirements:\n* 3 5 input elements (e.g. upload handler, radio selection input, text\ninput, drop menu, slidebar input, color selection…)\n* 2 3 output elements (e.g. plots, table, text…)\n* Basic preprocessing data input Shiny app required. \ndon’t need comprehensive analysis, “download\nsomething use directly”. may refer sections see\namount pre-processing required.\n* may also adopt one pre-defined optional tab (Gene Set\nEnrichment Analysis, Network Analysis, RShiny Test Suite) slight\nmodifications (e.g. different types plot)","code":""},{"path":"class-outlines.html","id":"class-outlines","chapter":"A Class Outlines","heading":"A Class Outlines","text":"specific topics covered week.","code":""},{"path":"class-outlines.html","id":"week-1","chapter":"A Class Outlines","heading":"A.1 Week 1","text":"Course Intro & Details (slides)Assignment 0Intro: Data Biology (slides)Prelim: R Language (slides)Prelim: RStudio SCC (slides)Prelim: R Script (slides)Prelim: Scripting Workflow (slides)Comm: RMarkdown & knitr (slides)Prelim: Git + github (slides)R Prog: R Syntax Basics (slides)R Prog: Functions (slides)R Prog: Data structures (slides)R Prog: Factors (slides)R Prog: Iteration (slides)R Prog: Troubleshooting Debugging (slides)","code":""},{"path":"class-outlines.html","id":"week-2","chapter":"A Class Outlines","heading":"A.2 Week 2","text":"Week 2 SlidesAssignment 1Data Wrangle: Tidyverse (slides)Data Wrangle: Tidyverse Basics (slides)Data Wrangle: Importing Data (slides)Bioinfo: CSV Files (slides)Data Wrangle: tibble (slides)Data Wrangle: pipes (slides)Data Wrangle: Arranging Data (slides)Data Wrangle: Regular expressions (slidesData Wrangle: Rearranging Data (slides)Bioinfo: R BiologyBioinfo: Types Biological DataBioinfo: BioconductorBioinfo: Gene IdentifiersBioinfo: Mapping Identifier SystemsBioinfo: Mapping HomologsData Wrangle: Relational DataData Viz: Grammar GraphicsData Viz: Plotting One DimensionData Viz: Visualizing DistributionsR Prog: Unit Testing","code":""},{"path":"class-outlines.html","id":"week-3","chapter":"A Class Outlines","heading":"A.3 Week 3","text":"Assignment 1 ReviewAssignment 2Data Sci: Data ModelingData Sci: Worked Modeling ExampleData Sci: Data SummarizationData Sci: Linear ModelsData Sci: Flavors Linear ModelsBioinfo: Gene ExpressionBioinfo: Gene Expression Data BioconductorBioinfo: MicroarraysBioinfo: Microarray Gene Expression DataBioinfo: Differential Expression: Microarrays (limma)","code":""},{"path":"class-outlines.html","id":"week-4","chapter":"A Class Outlines","heading":"A.4 Week 4","text":"Assignment 3Data Sci: Exploratory Data AnalysisData Sci: Principal Component AnalysisData Sci: Cluster AnalysisData Sci: Hierarchical ClusteringData Viz: HeatmapsData Viz: Specifying Heatmap ColorsData Viz: DendrogramsAssignment 2 Review","code":""},{"path":"class-outlines.html","id":"week-5","chapter":"A Class Outlines","heading":"A.5 Week 5","text":"Assignment 4Bioinfo: High Throughput SequencingBioinfo: Count DataBioinfo: RNASeqBioinfo: RNASeq Gene Expression DataBioinfo: Filtering CountsBioinfo: Count DistributionsBioinfo: Count NormalizationBioinfo: Count TransformationBioinfo: Differential Expression: RNASeqAssignment 3 ReviewBioinfo: DESeq2/EdgeRBioinfo: limma/voomBioinfo: Gene Set Enrichment AnalysisBioinfo: Gene SetsBioinfo: -representation AnalysisBioinfo: Rank-based AnalysisBioinfo: fgsea","code":""},{"path":"class-outlines.html","id":"week-6","chapter":"A Class Outlines","heading":"A.6 Week 6","text":"Assignment 5Data Sci: Statistical DistributionsData Sci: Random VariablesData Sci: Statistical Distribution BasicsData Sci: Distributions RData Sci: Discrete DistributionsData Sci: Continuous DistributionsData Sci: Empirical DistributionsAssignment 4 ReviewData Sci: Statistical TestsData Sci: p-valuesData Sci: [Multiple Hypothesis Testing]Data Sci: Statistical power","code":""},{"path":"class-outlines.html","id":"week-7","chapter":"A Class Outlines","heading":"A.7 Week 7","text":"Assignment 6Data Viz: Responsible PlottingData Viz: Human Visual PerceptionData Viz: Visual EncodingsData Viz: Elementary Perceptual TasksData Viz: ggplot MechanicsData Viz: Plotting Two DimensionsData Viz: Scatter PlotsData Viz: Bubble PlotsData Viz: Connected Scatter PlotsData Viz: Line PlotsData Viz: Parallel Coordinate PlotsData Viz: Chord Diagrams Circos PlotsAssignment 5 ReviewData Viz: Use Heatmaps ResponsiblyData Viz: Multiple PlotsData Viz: Facet wrappingData Viz: Publication Ready PlotsR Prog: Coding Style ConventionsR Prog: styler package","code":""},{"path":"class-outlines.html","id":"week-8","chapter":"A Class Outlines","heading":"A.8 Week 8","text":"Assignment 7RShiny: RshinyRShiny: IntroductionRShiny: Application StructureAssignment 6 ReviewRShiny: ReactivityRShiny: Publishing","code":""},{"path":"class-outlines.html","id":"week-9","chapter":"A Class Outlines","heading":"A.9 Week 9","text":"EngineeRing: ToolificationEngineeRing: R InterpreterEngineeRing: RscriptEngineeRing: commandArgs()EngineeRing: Parallel ProcessingEngineeRing: Brief Introduction ParallelizationEngineeRing: apply Friends Pleasingly ParallelEngineeRing: parallel packageAssignment 7 CheckinEngineeRing: Object Oriented Programming REngineeRing: Building R Packages","code":""},{"path":"class-outlines.html","id":"week-10","chapter":"A Class Outlines","heading":"A.10 Week 10","text":"Bioinfo: Biological PathwaysBioinfo: [Gene Regulatory Networks]Bioinfo: [Protein-Protein Networks]Bioinfo: [WGCNA]Assignment 7 ReviewData Sci: Network AnalysisData Viz: Network visualization","code":""},{"path":"class-outlines.html","id":"week-11","chapter":"A Class Outlines","heading":"A.11 Week 11","text":"Project work, class","code":""},{"path":"class-outlines.html","id":"week-12","chapter":"A Class Outlines","heading":"A.12 Week 12","text":"Project work, class","code":""},{"path":"class-outlines.html","id":"week-13","chapter":"A Class Outlines","heading":"A.13 Week 13","text":"Project work, class","code":""},{"path":"class-outlines.html","id":"week-14","chapter":"A Class Outlines","heading":"A.14 Week 14","text":"Wrap & feedback","code":""}]
